
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Concept &#8212; Omniverse</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=bb35926c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MYW5YKC2WF"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"defeq": "\\overset{\\text{def}}{=}", "defa": "\\overset{\\text{(a)}}{=}", "defb": "\\overset{\\text{(b)}}{=}", "defc": "\\overset{\\text{(c)}}{=}", "defd": "\\overset{\\text{(d)}}{=}", "st": "\\mid", "mod": "\\mid", "S": "\\Omega", "s": "\\omega", "e": "\\exp", "P": "\\mathbb{P}", "R": "\\mathbb{R}", "expectation": "\\mathbb{E}", "v": "\\mathbf{v}", "a": "\\mathbf{a}", "b": "\\mathbf{b}", "c": "\\mathbf{c}", "u": "\\mathbf{u}", "w": "\\mathbf{w}", "x": "\\mathbf{x}", "y": "\\mathbf{y}", "z": "\\mathbf{z}", "0": "\\mathbf{0}", "1": "\\mathbf{1}", "A": "\\mathbf{A}", "B": "\\mathbf{B}", "C": "\\mathbf{C}", "E": "\\mathcal{F}", "eventA": "\\mathcal{A}", "lset": "\\left\\{", "rset": "\\right\\}", "lsq": "\\left[", "rsq": "\\right]", "lpar": "\\left(", "rpar": "\\right)", "lcurl": "\\left\\{", "rcurl": "\\right\\}", "pmf": "p_X", "pdf": "f_X", "pdftwo": "f_{X,Y}", "pdfjoint": "f_{\\mathbf{X}}", "pmfjointxy": "p_{X, Y}", "pdfjointxy": "f_{X, Y}", "cdf": "F_X", "pspace": "(\\Omega, \\mathcal{F}, \\mathbb{P})", "var": "\\operatorname{Var}", "std": "\\operatorname{Std}", "bern": "\\operatorname{Bernoulli}", "binomial": "\\operatorname{Binomial}", "geometric": "\\operatorname{Geometric}", "poisson": "\\operatorname{Poisson}", "uniform": "\\operatorname{Uniform}", "normal": "\\operatorname{Normal}", "gaussian": "\\operatorname{Gaussian}", "gaussiansymbol": "\\mathcal{N}", "exponential": "\\operatorname{Exponential}", "iid": "\\textbf{i.i.d.}", "and": "\\text{and}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'influential/gaussian_mixture_models/02_concept';</script>
    <link rel="canonical" href="https://www.gaohongnan.com/influential/gaussian_mixture_models/02_concept.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Gaussian Mixture Models Implementation" href="03_implementation.html" />
    <link rel="prev" title="Mixture Models" href="01_intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Omniverse - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Omniverse - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    ðŸŒŒ Omniverse: A Journey Through Knowledge
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notations/machine_learning.html">Machine Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Influential Ideas and Papers</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../generative_pretrained_transformer/01_intro.html">Generative Pre-trained Transformers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../generative_pretrained_transformer/02_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../generative_pretrained_transformer/03_concept.html">The Concept of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../generative_pretrained_transformer/04_implementation.html">The Implementation of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../generative_pretrained_transformer/05_adder.html">Training a Mini-GPT to Learn Two-Digit Addition</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../low_rank_adaptation/01_intro.html">Low-Rank Adaptation Of Large Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../low_rank_adaptation/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../low_rank_adaptation/03_implementation.html">Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../empirical_risk_minimization/01_intro.html">Empirical Risk Minimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../empirical_risk_minimization/02_concept.html">Concept: Empirical Risk Minimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../empirical_risk_minimization/03_bayes_optimal_classifier.html">Bayes Optimal Classifier</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../learning_theory/01_intro.html">Is The Learning Problem Solvable?</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../learning_theory/02_concept.html">Concept: Learning Theory</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../kmeans_clustering/01_intro.html">K-Means</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/02_concept.html">Concept: K-Means Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/03_implementation.html">Implementation: K-Means (Lloyd)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/04_image_segmentation.html">Application: Image Compression and Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/05_conceptual_questions.html">Conceptual Questions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../naive_bayes/01_intro.html">Naive Bayes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/03_implementation.html">Naives Bayes Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/04_example_penguins.html">Naive Bayes Application: Penguins</a></li>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/05_application_mnist.html">Naive Bayes Application (MNIST)</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="01_intro.html">Mixture Models</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_implementation.html">Gaussian Mixture Models Implementation</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Playbook</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_calculate_flops_in_transformer_based_models.html">How to Calculate the Number of FLOPs in Transformer Based Models?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_inspect_function_and_class_signatures.html">How to Inspect Function and Class Signatures in Python?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/why_cosine_annealing_warmup_stabilize_training.html">Why Does Cosine Annealing With Warmup Stabilize Training?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html">Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling.</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_finetune_decoder_with_last_token_pooling.html">How To Fine-Tune Decoder-Only Models For Sequence Classification Using Last Token Pooling?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_finetune_decoder_with_cross_attention.html">How To Fine-Tune Decoder-Only Models For Sequence Classification With Cross-Attention?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_teacher_student_knowledge_distillation.html">How To Do Teacher-Student Knowledge Distillation?</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Probability Theory</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/intro.html">Chapter 1. Mathematical Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/01_combinatorics.html">Permutations and Combinations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/02_calculus.html">Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/03_contours.html">Contour Maps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/02_probability/intro.html">Chapter 2. Probability</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0202_probability_space.html">Probability Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0203_probability_axioms.html">Probability Axioms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0204_conditional_probability.html">Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0205_independence.html">Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0206_bayes_theorem.html">Bayeâ€™s Theorem and the Law of Total Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/summary.html">Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/intro.html">Chapter 3. Discrete Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0301_random_variables.html">Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0302_discrete_random_variables.html">Discrete Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0303_probability_mass_function.html">Probability Mass Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0304_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0305_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0306_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/intro.html">Discrete Uniform Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/intro.html">Bernoulli Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/iid.html">Independent and Identically Distributed (IID)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/intro.html">Binomial Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_implementation.html">Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_application.html">Real World Examples</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/geometric/intro.html">Geometric Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/geometric/0310_geometric_distribution_concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/intro.html">Poisson Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_implementation.html">Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/summary.html">Important</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/intro.html">Chapter 4. Continuous Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/from_discrete_to_continuous.html">From Discrete to Continuous</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0401_continuous_random_variables.html">Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0402_probability_density_function.html">Probability Density Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0403_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0404_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0405_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0406_mean_median_mode.html">Mean, Median and Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0407_continuous_uniform_distribution.html">Continuous Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0408_exponential_distribution.html">Exponential Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0409_gaussian_distribution.html">Gaussian Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0410_skewness_and_kurtosis.html">Skewness and Kurtosis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0411_convolve_and_sum_of_random_variables.html">Convolution and Sum of Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0412_functions_of_random_variables.html">Functions of Random Variables</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/intro.html">Chapter 5. Joint Distributions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/05_joint_distributions/from_single_variable_to_joint_distributions.html">From Single Variable to Joint Distributions</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/intro.html">Joint PMF and PDF</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/intro.html">Joint Expectation and Correlation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/intro.html">Conditional PMF and PDF</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/intro.html">Conditional Expectation and Variance</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0505_sum_of_random_variables/intro.html">Sum of Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0505_sum_of_random_variables/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0506_random_vectors/intro.html">Random Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0506_random_vectors/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/intro.html">Multivariate Gaussian Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/application_transformation.html">Application: Plots and Transformations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/psd.html">Covariance Matrix is Positive Semi-Definite</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/eigendecomposition.html">Eigendecomposition and Covariance Matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/geometry_of_multivariate_gaussian.html">The Geometry of Multivariate Gaussians</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/intro.html">Chapter 6. Sample Statistics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/intro.html">Moment Generating and Characteristic Functions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function.html">Moment Generating Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function_application_sum_of_rv.html">Application: Moment Generating Function and the Sum of Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/characteristic_function.html">Characteristic Function</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/intro.html">Probability Inequalities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/concept.html">Probability Inequalities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/application.html">Application: Learning Theory</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/intro.html">Law of Large Numbers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/convergence.html">Convergence of Sample Average</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/application.html">Application: Learning Theory</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/08_estimation_theory/intro.html">Chapter 8. Estimation Theory</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/08_estimation_theory/maximum_likelihood_estimation/intro.html">Maximum Likelihood Estimation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html">Concept</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Operations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../operations/distributed/intro.html">Distributed Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/01_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/02_basics.html">Basics Of Distributed Data Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/03_how_to_setup_slurm_in_aws.html">How to Setup SLURM and ParallelCluster in AWS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/04_ablation.html">Ablations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../operations/profiling/intro.html">Profiling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/01_synchronize.html">Synchronize CUDA To Time CUDA Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/02_timeit.html">Profiling Code With Timeit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/03_time_profiler.html">PyTorchâ€™s Event And Profiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/04_small_gpt_profile.html">Profile GPT Small Time And Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/05_memory_leak.html">CUDA Memory Allocations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../operations/machine_learning_lifecycle/00_intro.html">The Lifecycle of an AIOps System</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/01_problem_formulation.html">Stage 1. Problem Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/02_project_scoping.html">Stage 2. Project Scoping And Framing The Problem</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/03_dataops_pipeline.html">Stage 3. Data Pipeline (Data Engineering and DataOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/031_data_source_and_format.html">Stage 3.1. Data Source and Formats</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/032_data_model_and_storage.html">Stage 3.2. Data Model and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/033_etl.html">Stage 3.3. Extract, Transform, Load (ETL)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/04_mlops_data_pipeline.html">Stage 4. Data Extraction (MLOps), Data Analysis (Data Science), Data Preparation (Data Science)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/05_ml_training_pipeline.html">Stage 5. Model Development and Training (MLOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/051_model_selection.html">Stage 5.1. Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/052_metric_selection.html">Stage 5.2. Metric Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/053_experiment_tracking.html">Stage 5.3. Experiment Tracking And Versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/054_model_testing.html">Stage 5.4. Model Testing</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/06_model_evaluation.html">Stage 6. Model Evaluation (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/07_model_validation_registry_and_pushing_model_to_production.html">Stage 7. Model Validation, Registry and Pushing Model to Production (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/08_model_deployment_and_serving.html">Stage 8. Model Serving (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/09_model_monitoring.html">Stage 9. Model Monitoring (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/010_continuous_integration_deployment_learning_and_training.html">Stage 10. Continuous Integration, Deployment, Learning and Training (DevOps, DataOps, MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/011_infrastructure_and_tooling_for_mlops.html">Stage 11. Infrastructure and Tooling for MLOps</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/config_management/concept.html">Configuration Management</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/config_management/01-pydra.html">Pydantic And Hydra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/config_management/02-state.html">State And Metadata Management</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/design_patterns/intro.html">Design Patterns</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/dependency-inversion-principle.html">Dependency Inversion Principle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/strategy.html">Strategy Pattern</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/registry.html">Registry Design Pattern</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/python/intro.html">Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/decorator.html">Decorator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/pydantic.html">Pydantic Is All You Need - Jason Liu</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/intro.html">Concurrency, Parallelism and Asynchronous Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/generator_yield.html">A Rudimentary Introduction to Generator and Yield in Python</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../computer_science/type_theory/intro.html">Type Theory, A Very Rudimentary Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/01-subtypes.html">Subtypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/02-type-safety.html">Type Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/03-subsumption.html">Subsumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/04-generics.html">Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/05-typevar-bound-constraints.html">Bound and Constraint in Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/06-invariance-covariance-contravariance.html">Invariance, Covariance and Contravariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/07-pep-3124-overloading.html">Function Overloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/08-pep-661-sentinel-values.html">Sentinel Types</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Structures and Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/complexity_analysis/intro.html">Complexity Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/complexity_analysis/master_theorem.html">Master Theorem </a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/stack/intro.html">Stack</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/stack/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/intro.html">Linear Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/intro.html">Binary Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/problems/875-koko-eating-bananas.html">Koko Eating Bananas</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/01_preliminaries/intro.html">Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/01-fields.html">Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/02-systems-of-linear-equations.html">Systems of Linear Equations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/02_vectors/intro.html">Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/01-vector-definition.html">Vector and Its Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/02-vector-operation.html">Vector and Its Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/03-vector-norm.html">Vector Norm and Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/04-vector-products.html">A First Look at Vector Products</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../citations.html">IEEE (Style) Citations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../api/reproducibility.html">API Reference</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/gao-hongnan/omniverse/blob/main/omniverse/influential/gaussian_mixture_models/02_concept.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse/issues/new?title=Issue%20on%20page%20%2Finfluential/gaussian_mixture_models/02_concept.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/influential/gaussian_mixture_models/02_concept.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../../_sources/influential/gaussian_mixture_models/02_concept.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Concept</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">Intuition</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-bi-modal-distribution">Simple Bi-Modal Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-story">Generative Story</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-story">Inference Story</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-with-2d-data">Inference with 2D Data</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-is-a-latent-variable">Prior is a Latent Variable</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-formulation">Problem Formulation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-primer">A Primer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-mixture-model">Gaussian Mixture Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-perpectives">The Perpectives</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mixture-model-perspective">The Mixture Model Perspective</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-latent-variable-perspective">The Latent Variable Perspective</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">The Mixture Model Perspective</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gaussian-mixture-model">The Gaussian Mixture Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-responsibilities">The Responsibilities</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">The Latent Variable Perspective</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-generative-process">The Generative Process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-1-the-distribution-of-the-data-point-boldsymbol-x-n-given-the-latent-variable-boldsymbol-z-n">Assumption 1: The Distribution of the Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> given the Latent Variable <span class="math notranslate nohighlight">\(\boldsymbol{z}^{(n)}\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-latent-clusters">The Latent Clusters</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-data-points-boldsymbol-x-n-is-the-likelihood">The Data Points <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> is the Likelihood</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-likelihood-of-one-single-data-point-boldsymbol-x-n">The Likelihood of One Single Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-likelihood-of-the-entire-dataset-boldsymbol-x">The Likelihood of the Entire Dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-2-the-latent-variable-boldsymbol-z">Assumption 2: The Latent Variable <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-prior-distribution-of-boldsymbol-z">The Prior Distribution of <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-categorical-distribution">The Categorical Distribution</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-distribution-of-the-entire-dataset-mathcal-s">Prior Distribution of the Entire Dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-3-the-joint-distribution-of-boldsymbol-x-n-and-boldsymbol-z-n">Assumption 3: The Joint Distribution of <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{z}^{(n)}\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-the-joint-distribution-the-product-of-the-likelihood-and-prior">Why is the Joint Distribution the Product of the Likelihood and Prior?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weighted-likelihood">Weighted Likelihood</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weighted-likelihood-of-one-single-data-point-boldsymbol-x-n">Weighted Likelihood of One Single Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weighted-likelihood-of-the-entire-dataset-boldsymbol-x">Weighted Likelihood of the Entire Dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-distribution-fully-determines-the-model">Joint Distribution Fully Determines the Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gaussian-mixture-model-and-the-marginal-distribution">The Gaussian Mixture Model and the Marginal Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">The Gaussian Mixture Model</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-marginal-distribution">The Marginal Distribution</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#marginalizing-out-the-latent-variable">Marginalizing Out the Latent Variable</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-of-one-single-data-point-boldsymbol-x-n">Marginal of One Single Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-of-the-entire-dataset-boldsymbol-x">Marginal of the Entire Dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-posterior-distribution">The Posterior Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-of-one-single-data-point-boldsymbol-x-n">Posterior of One Single Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-of-the-entire-dataset-boldsymbol-x">Posterior of the Entire Dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-estimation-mixture-model-perspective">Parameter Estimation (Mixture Model Perspective)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-vectorized-parameters">The Vectorized Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mixture-weights-boldsymbol-pi">The Mixture Weights <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-means-boldsymbol-mu">The Means <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-covariances-boldsymbol-sigma">The Covariances <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-and-log-likelihood-of-marginal-distribution">Likelihood and Log-Likelihood of Marginal Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#no-closed-form-solution">No Closed-Form Solution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-estimation-the-necessary-conditions">Parameter Estimation (The Necessary Conditions)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chain-rule-matrix-calculus">The Chain Rule (Matrix Calculus)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#running-example">Running Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-mean-parameters-boldsymbol-mu-k">Estimating the Mean Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#some-intuition">Some Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#update-mean-of-running-example">Update Mean of Running Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-mean-parameters-boldsymbol-mu-k-in-python">Estimating the Mean Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span> in Python</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-covariance-parameters-boldsymbol-sigma-k">Estimating the Covariance Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#update-covariance-matrix-of-running-example">Update Covariance Matrix of Running Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Some Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-covariance-matrix-boldsymbol-sigma-k-in-python">Estimating the Covariance Matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span> in Python</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-mixing-coefficients-prior-weights-pi-k">Estimating the Mixing Coefficients (Prior/Weights) <span class="math notranslate nohighlight">\(\pi_{k}\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Some Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#update-weight-prior-of-running-example">Update Weight/Prior of Running Example</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-gmm-has-no-closed-form-solution">Why GMM has no Closed-Form Solution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-expectation-maximization-em-algorithm">The Expectation-Maximization (EM) Algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-maximization-em-gaussion-mixture-model-perspective">Expectation-Maximization (EM) (Gaussion Mixture Model Perspective)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-maximization-em-latent-variable-perspective">Expectation-Maximization (EM) (Latent Variable Perspective)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-expectation-step-posterior-inference-and-responsibilities">The Expectation Step (Posterior Inference and Responsibilities)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-maximization-step-parameter-estimation">The Maximization Step (Parameter Estimation)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gmm-and-its-relation-to-k-means">GMM and its Relation to K-Means</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-small-example">A Small Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-initialization">Random Initialization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-and-hard-assignments">K-Means and Hard Assignments</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#e-step">E-Step</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#m-step">M-Step</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gmm-and-soft-assignments">GMM and Soft Assignments</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">E-Step</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">M-Step</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#murphy-s-plots">Murphyâ€™s Plots</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gmm-demo">GMM Demo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gmm-2d-sklearn">GMM 2D (sklearn)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="concept">
<h1>Concept<a class="headerlink" href="#concept" title="Link to this heading">#</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#intuition" id="id23">Intuition</a></p>
<ul>
<li><p><a class="reference internal" href="#simple-bi-modal-distribution" id="id24">Simple Bi-Modal Distribution</a></p></li>
<li><p><a class="reference internal" href="#generative-story" id="id25">Generative Story</a></p></li>
<li><p><a class="reference internal" href="#inference-story" id="id26">Inference Story</a></p>
<ul>
<li><p><a class="reference internal" href="#inference-with-2d-data" id="id27">Inference with 2D Data</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#prior-is-a-latent-variable" id="id28">Prior is a Latent Variable</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#problem-formulation" id="id29">Problem Formulation</a></p>
<ul>
<li><p><a class="reference internal" href="#a-primer" id="id30">A Primer</a></p></li>
<li><p><a class="reference internal" href="#gaussian-mixture-model" id="id31">Gaussian Mixture Model</a></p></li>
<li><p><a class="reference internal" href="#the-perpectives" id="id32">The Perpectives</a></p>
<ul>
<li><p><a class="reference internal" href="#the-mixture-model-perspective" id="id33">The Mixture Model Perspective</a></p></li>
<li><p><a class="reference internal" href="#the-latent-variable-perspective" id="id34">The Latent Variable Perspective</a></p></li>
<li><p><a class="reference internal" href="#summary" id="id35">Summary</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#id3" id="id36">The Mixture Model Perspective</a></p>
<ul>
<li><p><a class="reference internal" href="#the-gaussian-mixture-model" id="id37">The Gaussian Mixture Model</a></p></li>
<li><p><a class="reference internal" href="#the-responsibilities" id="id38">The Responsibilities</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#id5" id="id39">The Latent Variable Perspective</a></p>
<ul>
<li><p><a class="reference internal" href="#the-generative-process" id="id40">The Generative Process</a></p></li>
<li><p><a class="reference internal" href="#assumption-1-the-distribution-of-the-data-point-boldsymbol-x-n-given-the-latent-variable-boldsymbol-z-n" id="id41">Assumption 1: The Distribution of the Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> given the Latent Variable <span class="math notranslate nohighlight">\(\boldsymbol{z}^{(n)}\)</span></a></p>
<ul>
<li><p><a class="reference internal" href="#the-latent-clusters" id="id42">The Latent Clusters</a></p></li>
<li><p><a class="reference internal" href="#the-data-points-boldsymbol-x-n-is-the-likelihood" id="id43">The Data Points <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> is the Likelihood</a></p></li>
<li><p><a class="reference internal" href="#the-likelihood-of-one-single-data-point-boldsymbol-x-n" id="id44">The Likelihood of One Single Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span></a></p></li>
<li><p><a class="reference internal" href="#the-likelihood-of-the-entire-dataset-boldsymbol-x" id="id45">The Likelihood of the Entire Dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#assumption-2-the-latent-variable-boldsymbol-z" id="id46">Assumption 2: The Latent Variable <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span></a></p>
<ul>
<li><p><a class="reference internal" href="#the-prior-distribution-of-boldsymbol-z" id="id47">The Prior Distribution of <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span></a></p></li>
<li><p><a class="reference internal" href="#the-categorical-distribution" id="id48">The Categorical Distribution</a></p></li>
<li><p><a class="reference internal" href="#prior-distribution-of-the-entire-dataset-mathcal-s" id="id49">Prior Distribution of the Entire Dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#assumption-3-the-joint-distribution-of-boldsymbol-x-n-and-boldsymbol-z-n" id="id50">Assumption 3: The Joint Distribution of <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{z}^{(n)}\)</span></a></p>
<ul>
<li><p><a class="reference internal" href="#why-is-the-joint-distribution-the-product-of-the-likelihood-and-prior" id="id51">Why is the Joint Distribution the Product of the Likelihood and Prior?</a></p></li>
<li><p><a class="reference internal" href="#weighted-likelihood" id="id52">Weighted Likelihood</a></p></li>
<li><p><a class="reference internal" href="#weighted-likelihood-of-one-single-data-point-boldsymbol-x-n" id="id53">Weighted Likelihood of One Single Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span></a></p></li>
<li><p><a class="reference internal" href="#weighted-likelihood-of-the-entire-dataset-boldsymbol-x" id="id54">Weighted Likelihood of the Entire Dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#joint-distribution-fully-determines-the-model" id="id55">Joint Distribution Fully Determines the Model</a></p></li>
<li><p><a class="reference internal" href="#the-gaussian-mixture-model-and-the-marginal-distribution" id="id56">The Gaussian Mixture Model and the Marginal Distribution</a></p>
<ul>
<li><p><a class="reference internal" href="#id9" id="id57">The Gaussian Mixture Model</a></p></li>
<li><p><a class="reference internal" href="#the-marginal-distribution" id="id58">The Marginal Distribution</a></p></li>
<li><p><a class="reference internal" href="#marginalizing-out-the-latent-variable" id="id59">Marginalizing Out the Latent Variable</a></p></li>
<li><p><a class="reference internal" href="#marginal-of-one-single-data-point-boldsymbol-x-n" id="id60">Marginal of One Single Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span></a></p></li>
<li><p><a class="reference internal" href="#marginal-of-the-entire-dataset-boldsymbol-x" id="id61">Marginal of the Entire Dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#the-posterior-distribution" id="id62">The Posterior Distribution</a></p>
<ul>
<li><p><a class="reference internal" href="#posterior-of-one-single-data-point-boldsymbol-x-n" id="id63">Posterior of One Single Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span></a></p></li>
<li><p><a class="reference internal" href="#posterior-of-the-entire-dataset-boldsymbol-x" id="id64">Posterior of the Entire Dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span></a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#parameter-estimation-mixture-model-perspective" id="id65">Parameter Estimation (Mixture Model Perspective)</a></p>
<ul>
<li><p><a class="reference internal" href="#the-vectorized-parameters" id="id66">The Vectorized Parameters</a></p>
<ul>
<li><p><a class="reference internal" href="#the-mixture-weights-boldsymbol-pi" id="id67">The Mixture Weights <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span></a></p></li>
<li><p><a class="reference internal" href="#the-means-boldsymbol-mu" id="id68">The Means <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span></a></p></li>
<li><p><a class="reference internal" href="#the-covariances-boldsymbol-sigma" id="id69">The Covariances <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#likelihood-and-log-likelihood-of-marginal-distribution" id="id70">Likelihood and Log-Likelihood of Marginal Distribution</a></p></li>
<li><p><a class="reference internal" href="#no-closed-form-solution" id="id71">No Closed-Form Solution</a></p></li>
<li><p><a class="reference internal" href="#parameter-estimation-the-necessary-conditions" id="id72">Parameter Estimation (The Necessary Conditions)</a></p></li>
<li><p><a class="reference internal" href="#the-chain-rule-matrix-calculus" id="id73">The Chain Rule (Matrix Calculus)</a></p></li>
<li><p><a class="reference internal" href="#running-example" id="id74">Running Example</a></p></li>
<li><p><a class="reference internal" href="#estimating-the-mean-parameters-boldsymbol-mu-k" id="id75">Estimating the Mean Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span></a></p>
<ul>
<li><p><a class="reference internal" href="#some-intuition" id="id76">Some Intuition</a></p></li>
<li><p><a class="reference internal" href="#update-mean-of-running-example" id="id77">Update Mean of Running Example</a></p></li>
<li><p><a class="reference internal" href="#estimating-the-mean-parameters-boldsymbol-mu-k-in-python" id="id78">Estimating the Mean Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span> in Python</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#estimating-the-covariance-parameters-boldsymbol-sigma-k" id="id79">Estimating the Covariance Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span></a></p>
<ul>
<li><p><a class="reference internal" href="#update-covariance-matrix-of-running-example" id="id80">Update Covariance Matrix of Running Example</a></p></li>
<li><p><a class="reference internal" href="#id17" id="id81">Some Intuition</a></p></li>
<li><p><a class="reference internal" href="#estimating-the-covariance-matrix-boldsymbol-sigma-k-in-python" id="id82">Estimating the Covariance Matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span> in Python</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#estimating-the-mixing-coefficients-prior-weights-pi-k" id="id83">Estimating the Mixing Coefficients (Prior/Weights) <span class="math notranslate nohighlight">\(\pi_{k}\)</span></a></p>
<ul>
<li><p><a class="reference internal" href="#id20" id="id84">Some Intuition</a></p></li>
<li><p><a class="reference internal" href="#update-weight-prior-of-running-example" id="id85">Update Weight/Prior of Running Example</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#why-gmm-has-no-closed-form-solution" id="id86">Why GMM has no Closed-Form Solution</a></p></li>
<li><p><a class="reference internal" href="#the-expectation-maximization-em-algorithm" id="id87">The Expectation-Maximization (EM) Algorithm</a></p>
<ul>
<li><p><a class="reference internal" href="#expectation-maximization-em-gaussion-mixture-model-perspective" id="id88">Expectation-Maximization (EM) (Gaussion Mixture Model Perspective)</a></p></li>
<li><p><a class="reference internal" href="#expectation-maximization-em-latent-variable-perspective" id="id89">Expectation-Maximization (EM) (Latent Variable Perspective)</a></p></li>
<li><p><a class="reference internal" href="#the-expectation-step-posterior-inference-and-responsibilities" id="id90">The Expectation Step (Posterior Inference and Responsibilities)</a></p></li>
<li><p><a class="reference internal" href="#the-maximization-step-parameter-estimation" id="id91">The Maximization Step (Parameter Estimation)</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#gmm-and-its-relation-to-k-means" id="id92">GMM and its Relation to K-Means</a></p></li>
<li><p><a class="reference internal" href="#a-small-example" id="id93">A Small Example</a></p>
<ul>
<li><p><a class="reference internal" href="#random-initialization" id="id94">Random Initialization</a></p></li>
<li><p><a class="reference internal" href="#k-means-and-hard-assignments" id="id95">K-Means and Hard Assignments</a></p>
<ul>
<li><p><a class="reference internal" href="#e-step" id="id96">E-Step</a></p></li>
<li><p><a class="reference internal" href="#m-step" id="id97">M-Step</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#gmm-and-soft-assignments" id="id98">GMM and Soft Assignments</a></p>
<ul>
<li><p><a class="reference internal" href="#id21" id="id99">E-Step</a></p></li>
<li><p><a class="reference internal" href="#id22" id="id100">M-Step</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#murphy-s-plots" id="id101">Murphyâ€™s Plots</a></p>
<ul>
<li><p><a class="reference internal" href="#gmm-demo" id="id102">GMM Demo</a></p></li>
<li><p><a class="reference internal" href="#gmm-2d-sklearn" id="id103">GMM 2D (sklearn)</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#references-and-further-readings" id="id104">References and Further Readings</a></p></li>
</ul>
</nav>
<p>Gaussian Mixture Models are a statistical technique used for approximating the probability distribution of data, known as <a class="reference external" href="https://en.wikipedia.org/wiki/Density_estimation#:~:text=In%20statistics%2C%20probability%20density%20estimation,unobservable%20underlying%20probability%20density%20function."><strong>density estimation</strong></a>.</p>
<p>From the introduction in <a class="reference internal" href="01_intro.html"><span class="std std-doc">the previous section</span></a>, one might ask the motivation of using a linear combination to approximate a probability distribution. The answer is that the linear combination of simple distributions is a flexible model that can approximate a wide variety of probability distributions.</p>
<p>Consider your dataset that exhibits a multimodal distribution (i.e. multiple modes), then a single Gaussian distribution will not be able to capture the distribution of the data.</p>
<section id="intuition">
<h2><a class="toc-backref" href="#id23" role="doc-backlink">Intuition</a><a class="headerlink" href="#intuition" title="Link to this heading">#</a></h2>
<section id="simple-bi-modal-distribution">
<h3><a class="toc-backref" href="#id24" role="doc-backlink">Simple Bi-Modal Distribution</a><a class="headerlink" href="#simple-bi-modal-distribution" title="Link to this heading">#</a></h3>
<p>The code below does the following:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">x_axis</span></code> is created as a NumPy array of evenly spaced values ranging from -15 to 15 with a step size of 0.001.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gaussian_1</span></code> and <code class="docutils literal notranslate"><span class="pre">gaussian_2</span></code> are defined as dictionaries representing two normal distributions with given means and standard deviations:</p>
<ul class="simple">
<li><p>Distribution 1 has a mean of -4 and a standard deviation of 2.</p></li>
<li><p>Distribution 2 has a mean of 4 and a standard deviation of 2.</p></li>
</ul>
</li>
<li><p>The probability density functions (PDFs) for <code class="docutils literal notranslate"><span class="pre">gaussian_1</span></code> and <code class="docutils literal notranslate"><span class="pre">gaussian_2</span></code> are calculated using the <code class="docutils literal notranslate"><span class="pre">norm.pdf</span></code> function from the SciPy library. The PDFs are computed for each value in <code class="docutils literal notranslate"><span class="pre">x_axis</span></code>, with the respective mean and standard deviation for each distribution.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">pdf_merged</span></code> variable is created by adding the PDFs of <code class="docutils literal notranslate"><span class="pre">gaussian_1</span></code> and <code class="docutils literal notranslate"><span class="pre">gaussian_2</span></code> element-wise, which represents the combined probability density function of both distributions.</p></li>
</ol>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="k">def</span> <span class="nf">get_gaussian_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">mu</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">sigma</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="linenos"> 2</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="linenos"> 3</span>    <span class="k">return</span> <span class="n">y</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span>
<span class="linenos"> 6</span><span class="k">def</span> <span class="nf">merge_gaussian_pdf</span><span class="p">(</span>
<span class="linenos"> 7</span>    <span class="n">prior</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">pdf_1</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">pdf_2</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
<span class="linenos"> 8</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="linenos"> 9</span>    <span class="n">pdfs</span> <span class="o">=</span> <span class="p">[</span><span class="n">pdf_1</span><span class="p">,</span> <span class="n">pdf_2</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">]</span>
<span class="linenos">10</span>
<span class="linenos">11</span>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">pdfs</span><span class="p">)</span>
<span class="linenos">12</span>
<span class="linenos">13</span>    <span class="c1"># w1 * f1 + w2 * f2 + ...</span>
<span class="linenos">14</span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">([</span><span class="n">prior</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">pdfs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">prior</span><span class="p">))],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">x_axis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>
<span class="linenos"> 2</span><span class="n">gaussian_1</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;std&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span>
<span class="linenos"> 3</span><span class="n">gaussian_2</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;mean&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;std&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="n">pdf_1</span> <span class="o">=</span> <span class="n">get_gaussian_pdf</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">gaussian_1</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">],</span> <span class="n">gaussian_1</span><span class="p">[</span><span class="s2">&quot;std&quot;</span><span class="p">])</span>
<span class="linenos"> 6</span><span class="n">pdf_2</span> <span class="o">=</span> <span class="n">get_gaussian_pdf</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">gaussian_2</span><span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">],</span> <span class="n">gaussian_2</span><span class="p">[</span><span class="s2">&quot;std&quot;</span><span class="p">])</span>
<span class="linenos"> 7</span>
<span class="linenos"> 8</span><span class="n">weights_1</span><span class="p">,</span> <span class="n">weights_2</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="n">pdf_merged</span> <span class="o">=</span> <span class="n">merge_gaussian_pdf</span><span class="p">(</span>
<span class="linenos">11</span>    <span class="p">[</span><span class="n">weights_1</span><span class="p">,</span> <span class="n">weights_2</span><span class="p">],</span> <span class="n">pdf_1</span><span class="p">,</span> <span class="n">pdf_2</span>
<span class="linenos">12</span><span class="p">)</span>  <span class="c1"># weights_1 * pdf_1 + weights_2 * pdf_2</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>This code below will create a figure with three subplots, where the first subplot (<code class="docutils literal notranslate"><span class="pre">ax1</span></code>) contains Distribution 1 and Distribution 2, the second subplot (<code class="docutils literal notranslate"><span class="pre">ax2</span></code>) contains the Merged Distribution, and the third subplot (<code class="docutils literal notranslate"><span class="pre">ax3</span></code>) contains Distribution 1 and Distribution 2 as dotted lines and the Merged Distribution as a solid line. All subplots share the same x-axis.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1"># Create a 3x1 grid of subplots with shared x-axis</span>
<span class="linenos"> 2</span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos"> 3</span>
<span class="linenos"> 4</span><span class="c1"># Plot Distribution 1 and Distribution 2 on the first subplot (ax1)</span>
<span class="linenos"> 5</span><span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">pdf_1</span><span class="p">,</span> <span class="s2">&quot;r:&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Gaussian 1&quot;</span><span class="p">)</span>
<span class="linenos"> 6</span><span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">pdf_2</span><span class="p">,</span> <span class="s2">&quot;b:&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Gaussian 2&quot;</span><span class="p">)</span>
<span class="linenos"> 7</span><span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="c1"># Plot Merged Distribution on the second subplot (ax2)</span>
<span class="linenos">10</span><span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">pdf_merged</span><span class="p">,</span> <span class="s2">&quot;g-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Gaussian Merged&quot;</span><span class="p">)</span>
<span class="linenos">11</span><span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="linenos">12</span>
<span class="linenos">13</span><span class="c1"># Plot pdf_1 and pdf_2 as dotted lines, and pdf_merged as solid line on the third subplot (ax3)</span>
<span class="linenos">14</span><span class="n">ax3</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">pdf_1</span><span class="p">,</span> <span class="s2">&quot;r:&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Gaussian 1 (Dotted)&quot;</span><span class="p">)</span>
<span class="linenos">15</span><span class="n">ax3</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">pdf_2</span><span class="p">,</span> <span class="s2">&quot;b:&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Gaussian 2 (Dotted)&quot;</span><span class="p">)</span>
<span class="linenos">16</span><span class="n">ax3</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">pdf_merged</span><span class="p">,</span> <span class="s2">&quot;g-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Gaussian Merged (Solid)&quot;</span><span class="p">)</span>
<span class="linenos">17</span><span class="n">ax3</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="linenos">18</span>
<span class="linenos">19</span><span class="c1"># Show the plots</span>
<span class="linenos">20</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/e6d0e1300529475fa1551dfa8022f0748f9aceb989750a852579cf2c7024211b.svg" src="../../_images/e6d0e1300529475fa1551dfa8022f0748f9aceb989750a852579cf2c7024211b.svg" />
</div>
</div>
<p>In this case, we say that</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}\left[\mathbf{X} ; \boldsymbol{\theta} = \left(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}\right)\right] = \textcolor{red}{0.5 \mathcal{N}\left(x \mid -4, 2\right)} + \textcolor{blue}{0.5 \mathcal{N}(x \mid 4, 2)}
\]</div>
<p>where this distribution is a mixture of two normal distributions with equal weights of 0.5.</p>
<p>The mixture components are:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{N}\left(x \mid -4, 2\right) \quad \mathcal{N}(x \mid 4, 2)
\]</div>
<p>parametrized by</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\mu} = \begin{bmatrix} \mu_1 &amp; \mu_2 \end{bmatrix} = \begin{bmatrix} -4 &amp; 4 \end{bmatrix} \quad \boldsymbol{\Sigma} = \begin{bmatrix} \Sigma_1 &amp; \Sigma_2 \end{bmatrix} = \begin{bmatrix} 2 &amp; 2 \end{bmatrix}
\]</div>
<p>The mixture weights are:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\pi} = \begin{bmatrix} \pi_1 &amp; \pi_2 \end{bmatrix} = \begin{bmatrix} 0.5 &amp; 0.5 \end{bmatrix}
\]</div>
</section>
<section id="generative-story">
<h3><a class="toc-backref" href="#id25" role="doc-backlink">Generative Story</a><a class="headerlink" href="#generative-story" title="Link to this heading">#</a></h3>
<p>Now the generative story of such a mixture model is as follows.</p>
<p>If we have a mixture model with <span class="math notranslate nohighlight">\(K\)</span> components, then we can sample from the mixture model by first sampling a component <span class="math notranslate nohighlight">\(k\)</span> from the categorical distribution with parameters <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>, and then sampling from the <span class="math notranslate nohighlight">\(k\)</span>-th component distribution with parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_k\)</span>.</p>
<p>More concretely, if we know the following parameters:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\pi} = \begin{bmatrix} \pi_1 &amp; \pi_2 \end{bmatrix} = \begin{bmatrix} 0.5 &amp; 0.5 \end{bmatrix}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\mu} = \begin{bmatrix} \mu_1 &amp; \mu_2 \end{bmatrix} = \begin{bmatrix} 2 &amp; -2 \end{bmatrix}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} = \begin{bmatrix} \Sigma_1 &amp; \Sigma_2 \end{bmatrix} = \begin{bmatrix} 3 &amp; 1 \end{bmatrix}\)</span></p></li>
</ul>
<p>then we can sample from the mixture model by</p>
<ul class="simple">
<li><p>first sampling a component <span class="math notranslate nohighlight">\(k\)</span> from the categorical distribution with parameters <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>, which means
either <span class="math notranslate nohighlight">\(k = 1\)</span> or <span class="math notranslate nohighlight">\(k = 2\)</span> with equal probability of 0.5.</p></li>
<li><p>then once we know which component we sampled from, we can sample from the component distribution, which in this case is a normal distribution with mean <span class="math notranslate nohighlight">\(\mu_k\)</span> and standard deviation <span class="math notranslate nohighlight">\(\Sigma_k\)</span>. For example, if we sampled <span class="math notranslate nohighlight">\(k = 1\)</span>, then we can sample from the first component distribution with mean <span class="math notranslate nohighlight">\(\mu_1 = 2\)</span> and standard deviation <span class="math notranslate nohighlight">\(\Sigma_1 = 3\)</span>.</p></li>
</ul>
<p>Note very carefully, this is the â€œgenerativeâ€ side, in machine learning we are interested in the â€œinferenceâ€ side, which is to infer the parameters of the mixture model from the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>!</p>
<p>Letâ€™s see in code how we can sample from a mixture model, and that if we sample enough
data points, the empirical distribution of the samples will converge to the true distribution of the mixture model.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="k">def</span> <span class="nf">generate_x</span><span class="p">(</span>
<span class="linenos"> 2</span>    <span class="n">prior</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
<span class="linenos"> 3</span>    <span class="n">mu</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
<span class="linenos"> 4</span>    <span class="n">sigma</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
<span class="linenos"> 5</span>    <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
<span class="linenos"> 6</span>    <span class="n">num_gaussians</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
<span class="linenos"> 7</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="linenos"> 8</span>    <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
<span class="linenos"> 9</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
<span class="linenos">10</span>        <span class="c1"># Select a Gaussian based on the prior</span>
<span class="linenos">11</span>        <span class="n">selected_gaussian</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">num_gaussians</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">prior</span><span class="p">)</span>
<span class="linenos">12</span>        <span class="c1"># Sample from the selected Gaussian</span>
<span class="linenos">13</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span>
<span class="linenos">14</span>            <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">[</span><span class="n">selected_gaussian</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">[</span><span class="n">selected_gaussian</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span>
<span class="linenos">15</span>        <span class="p">)</span>
<span class="linenos">16</span>        <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="linenos">17</span>    <span class="k">return</span> <span class="n">X</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">x_axis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>
<span class="linenos"> 2</span><span class="n">prior</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]</span>  <span class="c1"># weights</span>
<span class="linenos"> 3</span><span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="linenos"> 4</span><span class="n">sigma</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="linenos"> 5</span>
<span class="linenos"> 6</span><span class="n">gaussian_1</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="linenos"> 7</span><span class="n">gaussian_2</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="linenos"> 8</span><span class="n">gaussian_3</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="n">mixture_pdf</span> <span class="o">=</span> <span class="n">merge_gaussian_pdf</span><span class="p">(</span>
<span class="linenos">11</span>    <span class="n">prior</span><span class="p">,</span> <span class="n">gaussian_1</span><span class="p">,</span> <span class="n">gaussian_2</span><span class="p">,</span> <span class="n">gaussian_3</span>
<span class="linenos">12</span><span class="p">)</span>  <span class="c1"># prior[0] * gaussian_1 + prior[1] * gaussian_2 + prior[2] * gaussian_3</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">sample_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">10000</span><span class="p">]</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos"> 4</span><span class="n">axes</span> <span class="o">=</span> <span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="linenos"> 5</span>
<span class="linenos"> 6</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">mixture_pdf</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Mixture PDF&quot;</span><span class="p">)</span>
<span class="linenos"> 7</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Mixture PDF&quot;</span><span class="p">)</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sample_sizes</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="linenos">10</span>    <span class="n">samples</span> <span class="o">=</span> <span class="n">generate_x</span><span class="p">(</span><span class="n">prior</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">num_gaussians</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">prior</span><span class="p">))</span>
<span class="linenos">11</span>    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">12</span>    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">mixture_pdf</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Mixture PDF&quot;</span><span class="p">)</span>
<span class="linenos">13</span>    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated Samples (n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="linenos">14</span>
<span class="linenos">15</span><span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axes</span><span class="p">:</span>
<span class="linenos">16</span>    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;X-axis&quot;</span><span class="p">)</span>
<span class="linenos">17</span>    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Density&quot;</span><span class="p">)</span>
<span class="linenos">18</span>    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="linenos">19</span>
<span class="linenos">20</span><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="linenos">21</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/ed77e1037f57cd286f113a323f91210e51445750cda5aa45513056a1cb7bf2fd.svg" src="../../_images/ed77e1037f57cd286f113a323f91210e51445750cda5aa45513056a1cb7bf2fd.svg" />
</div>
</div>
<p>In this code, weâ€™re using a Gaussian Mixture Model (GMM) to generate and visualize samples from a mixture of two Gaussian distributions. The purpose of the visualization is to demonstrate how the GMM can approximate the true underlying distribution as the number of samples increases.</p>
<p>Hereâ€™s a step-by-step explanation of the code:</p>
<ol class="arabic simple">
<li><p>We define sample_sizes as a list containing the number of samples to generate for each subplot (100, 500, and 10000).</p></li>
<li><p>We create a 2x2 grid of subplots with shared x and y axes, with a figure size of 12x10.</p></li>
<li><p>We plot the mixture PDF on the first subplot (axes[0]). This represents the true underlying distribution that we are trying to approximate with our GMM.</p></li>
<li><p>We iterate over the sample_sizes list, and for each sample size, we use the generate_x function to generate samples from the GMM. The generate_x function takes the prior probabilities, the means and standard deviations of the Gaussians, the number of samples, and the number of Gaussians as input arguments.</p></li>
<li><p>For each sample size, we plot a histogram of the generated samples on the corresponding subplot. We normalize the histogram to match the density of the true underlying distribution. We also plot the mixture PDF on the same subplot to compare the generated samples with the true distribution.</p></li>
<li><p>We set the titles, x-axis labels, and y-axis labels for all subplots, and add a legend to each subplot.</p></li>
<li><p>We use plt.tight_layout() to adjust the spacing between subplots, and finally display the figure using plt.show().</p></li>
</ol>
<p>In the context of GMM, this code demonstrates how the GMM can be used to generate samples from a mixture of Gaussian distributions. The generated samples are visualized as histograms, which are compared to the true underlying distribution (the mixture PDF) to show how well the GMM approximates the true distribution. As the number of samples increases, the histograms of the generated samples become closer to the mixture PDF, indicating that the GMM is effectively approximating the true distribution.</p>
<p>Note carefully again that this is under the assumption that we already know the parameters of the mixture model, which is not the case in machine learning. In machine learning, we are interested in the â€œinferenceâ€ side, which is to infer the parameters of the mixture model from the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>!</p>
</section>
<section id="inference-story">
<h3><a class="toc-backref" href="#id26" role="doc-backlink">Inference Story</a><a class="headerlink" href="#inference-story" title="Link to this heading">#</a></h3>
<p>Now letâ€™s flip the table and see how we can infer the parameters of the mixture model from the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>.</p>
<p>The code above does the following:</p>
<ol class="arabic simple">
<li><p>Import necessary libraries: NumPy, Matplotlib, and GaussianMixture from scikit-learn.</p></li>
<li><p>Generate a synthetic dataset with three clusters:</p>
<ul class="simple">
<li><p>Set a random seed to ensure reproducibility.</p></li>
<li><p>Define the number of samples (500).</p></li>
<li><p>Create a dataset by concatenating samples from three normal distributions with different means (0, 5, and 10) and the same standard deviation (1). The dataset is reshaped into a 2D array.</p></li>
</ul>
</li>
<li><p>Fit a Gaussian Mixture Model (GMM) to the data:</p>
<ul class="simple">
<li><p>Instantiate a GaussianMixture object with three components and a fixed random state.</p></li>
<li><p>Fit the GMM to the dataset <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p></li>
</ul>
</li>
<li><p>Plot the data and the Gaussian Mixture Model:</p>
<ul class="simple">
<li><p>Create an array <code class="docutils literal notranslate"><span class="pre">x_plot</span></code> of 1000 linearly spaced values between -5 and 15.</p></li>
<li><p>Calculate the density of the GMM for each value in <code class="docutils literal notranslate"><span class="pre">x_plot</span></code> using the <code class="docutils literal notranslate"><span class="pre">score_samples</span></code> method.</p></li>
<li><p>Plot a histogram of the dataset with 30 bins, normalized by the total area.</p></li>
<li><p>Plot the GMM density estimation using a red line.</p></li>
<li><p>Add labels for the x-axis, y-axis, and a title for the plot.</p></li>
</ul>
</li>
<li><p>Display the plot using <code class="docutils literal notranslate"><span class="pre">plt.show()</span></code>.</p></li>
</ol>
<p>A reminder, we know the true distribution because we defined them ourselves. In reality,
we donâ€™t know the true distribution, and we want to infer the parameters of the mixture model from the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>.
The purpose of defining the true distribution is for pedagogical purposes, so that we can compare the true distribution with the estimated distribution from the GMM.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">x_axis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>
<span class="linenos"> 2</span><span class="n">prior</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]</span>  <span class="c1"># weights</span>
<span class="linenos"> 3</span><span class="n">mu</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="linenos"> 4</span><span class="n">sigma</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="linenos"> 5</span>
<span class="linenos"> 6</span><span class="n">gaussian_1</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="linenos"> 7</span><span class="n">gaussian_2</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="linenos"> 8</span><span class="n">gaussian_3</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">mu</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="n">mixture_pdf</span> <span class="o">=</span> <span class="n">merge_gaussian_pdf</span><span class="p">(</span>
<span class="linenos">11</span>    <span class="n">prior</span><span class="p">,</span> <span class="n">gaussian_1</span><span class="p">,</span> <span class="n">gaussian_2</span><span class="p">,</span> <span class="n">gaussian_3</span>
<span class="linenos">12</span><span class="p">)</span>  <span class="c1"># prior[0] * gaussian_1 + prior[1] * gaussian_2 + prior[2] * gaussian_3</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="n">num_samples</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="linenos">2</span><span class="n">samples</span> <span class="o">=</span> <span class="n">generate_x</span><span class="p">(</span>
<span class="linenos">3</span>    <span class="n">prior</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">num_gaussians</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">prior</span><span class="p">)</span>
<span class="linenos">4</span><span class="p">)</span>
<span class="linenos">5</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="linenos">6</span>
<span class="linenos">7</span><span class="c1"># Fit a Gaussian Mixture Model to the generated samples</span>
<span class="linenos">8</span><span class="n">gmm</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">prior</span><span class="p">),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1992</span><span class="p">)</span>
<span class="linenos">9</span><span class="n">gmm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "â–¸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "â–¾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GaussianMixture(n_components=3, random_state=1992)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;GaussianMixture<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.4/modules/generated/sklearn.mixture.GaussianMixture.html">?<span>Documentation for GaussianMixture</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>GaussianMixture(n_components=3, random_state=1992)</pre></div> </div></div></div></div></div></div>
</div>
<p>Remarkable! The parameters inferred from our <code class="docutils literal notranslate"><span class="pre">GMM</span></code> model has the following:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prior:&quot;</span><span class="p">,</span> <span class="n">gmm</span><span class="o">.</span><span class="n">weights_</span><span class="p">)</span>
<span class="linenos">2</span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean:&quot;</span><span class="p">,</span> <span class="n">gmm</span><span class="o">.</span><span class="n">means_</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
<span class="linenos">3</span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Std:&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">covariances_</span><span class="o">.</span><span class="n">ravel</span><span class="p">()))</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Prior: <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.39785786</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.20201207</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.40013007</span><span style="font-weight: bold">]</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Mean: <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.01573031</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">10.02728274</span>  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5.02481475</span><span style="font-weight: bold">]</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Std: <span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.01680719</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.97835049</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.99226471</span><span style="font-weight: bold">]</span>
</pre>
</div></div>
</div>
<p>When rounded to the nearest integer/decimal, the parameters inferred from our <code class="docutils literal notranslate"><span class="pre">GMM</span></code> model has the following:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\pi} = \begin{bmatrix} \pi_1 &amp; \pi_2 &amp; \pi_3 \end{bmatrix} = \begin{bmatrix} 0.398, 0.4, 0.202 \end{bmatrix}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\mu} = \begin{bmatrix} \mu_1 &amp; \mu_2 &amp; \mu_3 \end{bmatrix} = \begin{bmatrix} 0.0, 5.0, 10.0 \end{bmatrix}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\Sigma} = \begin{bmatrix} \Sigma_1 &amp; \Sigma_2 &amp; \Sigma_3 \end{bmatrix} = \begin{bmatrix} 1.02, 0.99, 0.98 \end{bmatrix}\)</span></p></li>
</ul>
<p>Almost spot on with the true parameters!!!</p>
<p>The plot shows promising results, which is not surprising since our
estimated parameters are very close to the true parameters.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1"># Plot the data and the Gaussian Mixture Model</span>
<span class="linenos"> 2</span><span class="n">x_plot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="linenos"> 3</span><span class="n">density</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
<span class="linenos"> 4</span>    <span class="n">gmm</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">x_plot</span><span class="p">)</span>
<span class="linenos"> 5</span><span class="p">)</span>  <span class="c1"># !!! gmm.score_samples(x_plot) returns the log-likelihood of the samples thus we need to take the exponential to get back raw probabilities.</span>
<span class="linenos"> 6</span>
<span class="linenos"> 7</span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Generated Samples&quot;</span><span class="p">)</span>
<span class="linenos"> 8</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_plot</span><span class="p">,</span> <span class="n">density</span><span class="p">,</span> <span class="s2">&quot;-r&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;GMM Approximation&quot;</span><span class="p">)</span>
<span class="linenos"> 9</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_axis</span><span class="p">,</span> <span class="n">mixture_pdf</span><span class="p">,</span> <span class="s2">&quot;-g&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True Mixture PDF&quot;</span><span class="p">)</span>
<span class="linenos">10</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="linenos">11</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;density&quot;</span><span class="p">)</span>
<span class="linenos">12</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Gaussian Mixture Model Approximation of Generated Samples&quot;</span><span class="p">)</span>
<span class="linenos">13</span><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="linenos">14</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/02c253c58d71c8e19cbcc9a19a32925748c2007121ca1eb4bd9ed54d04c648a8.svg" src="../../_images/02c253c58d71c8e19cbcc9a19a32925748c2007121ca1eb4bd9ed54d04c648a8.svg" />
</div>
</div>
<p>When you <code class="docutils literal notranslate"><span class="pre">predict</span></code> on the samples <code class="docutils literal notranslate"><span class="pre">X</span></code>, you get the cluster/component in which
each sample belongs to. In this case, the samples are generated from three
clusters, so the predicted labels are either 0, 1, or 2.</p>
<p>When you <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> on the samples <code class="docutils literal notranslate"><span class="pre">X</span></code>, you get the (log) probability
of each sample belonging to each cluster/component. In this case, the samples
are generated from three clusters, so the predicted probabilities are a 2D array
with shape <code class="docutils literal notranslate"><span class="pre">(n_samples,</span> <span class="pre">n_components)</span></code>.
Then the highest probability is the cluster/component in which the sample belongs to, which is the predicted label.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="n">gmm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">gmm</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([2, 2, 2, ..., 1, 2, 2]),
 array([[1.26320804e-03, 2.57233727e-09, 9.98736789e-01],
        [6.13952593e-06, 7.67030705e-07, 9.99993093e-01],
        [3.20714893e-07, 1.82204807e-05, 9.99981459e-01],
        ...,
        [1.19404179e-15, 9.92155660e-01, 7.84434023e-03],
        [3.76091554e-07, 1.53545161e-05, 9.99984269e-01],
        [1.08080724e-08, 6.99100159e-04, 9.99300889e-01]]))
</pre></div>
</div>
</div>
</div>
<section id="inference-with-2d-data">
<h4><a class="toc-backref" href="#id27" role="doc-backlink">Inference with 2D Data</a><a class="headerlink" href="#inference-with-2d-data" title="Link to this heading">#</a></h4>
<p>We can also visualize the inferred parameters of the mixture model in 2D.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="c1"># Generate a synthetic dataset with three clusters</span>
<span class="linenos">2</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="linenos">3</span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">500</span>
<span class="linenos">4</span><span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.4</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span>
<span class="linenos">5</span><span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.4</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span>
<span class="linenos">6</span><span class="n">X3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span>
<span class="linenos">7</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">X3</span><span class="p">])</span>
<span class="linenos">8</span><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">500</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span><span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1"># Fit a Gaussian Mixture Model to the data</span>
<span class="linenos"> 2</span><span class="n">gmm</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="linenos"> 3</span><span class="n">gmm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prior:&quot;</span><span class="p">)</span>
<span class="linenos"> 6</span><span class="n">pprint</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">weights_</span><span class="p">)</span>
<span class="linenos"> 7</span>
<span class="linenos"> 8</span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean:&quot;</span><span class="p">)</span>
<span class="linenos"> 9</span><span class="n">pprint</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">means_</span><span class="p">)</span>
<span class="linenos">10</span>
<span class="linenos">11</span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Covariance:&quot;</span><span class="p">)</span>
<span class="linenos">12</span><span class="n">pprint</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">covariances_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Prior:
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">array</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.40019011</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.19999749</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3998124</span> <span style="font-weight: bold">])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Mean:
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">array</span><span style="font-weight: bold">([[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4.93719589e+00</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4.98357806e+00</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">â”‚      </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.01268460e+01</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.01296468e+01</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">â”‚      </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6.10397816e-03</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3.69680874e-02</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Covariance:
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">array</span><span style="font-weight: bold">([[[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.02610394</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.02077184</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">â”‚   â”‚   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.02077184</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.00397773</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">â”‚      </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">â”‚      </span><span style="font-weight: bold">[[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.78282208</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.04590306</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">â”‚   â”‚   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.04590306</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.03591285</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">â”‚      </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">â”‚      </span><span style="font-weight: bold">[[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.91310488</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.02686654</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">â”‚   â”‚   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.02686654</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.92005989</span><span style="font-weight: bold">]]])</span>
</pre>
</div></div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">x</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="linenos"> 2</span><span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="linenos"> 3</span><span class="n">step</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="linenos"> 4</span><span class="n">X_plot</span><span class="p">,</span> <span class="n">Y_plot</span> <span class="o">=</span> <span class="n">make_meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
<span class="linenos"> 5</span><span class="nb">print</span><span class="p">(</span><span class="n">X_plot</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Y_plot</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="linenos"> 6</span><span class="c1"># Plot the data points and the Gaussian Mixture Model contours</span>
<span class="linenos"> 7</span><span class="c1"># x = np.linspace(-5, 15, 100)</span>
<span class="linenos"> 8</span><span class="c1"># y = np.linspace(-5, 15, 100)</span>
<span class="linenos"> 9</span><span class="c1"># X_plot, Y_plot = np.meshgrid(x, y)</span>
<span class="linenos">10</span><span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">X_plot</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="linenos">11</span><span class="n">pos</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_plot</span>
<span class="linenos">12</span><span class="n">pos</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y_plot</span>
<span class="linenos">13</span><span class="nb">print</span><span class="p">(</span><span class="n">pos</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1645</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1777</span><span style="font-weight: bold">)</span>
<span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1645</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1777</span><span style="font-weight: bold">)</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1645</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1777</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span><span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="linenos"> 2</span><span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="linenos"> 3</span>
<span class="linenos"> 4</span><span class="c1"># plot for each component a contour of the probability density function</span>
<span class="linenos"> 5</span><span class="k">for</span> <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">means_</span><span class="p">,</span> <span class="n">gmm</span><span class="o">.</span><span class="n">covariances_</span><span class="p">):</span>
<span class="linenos"> 6</span>    <span class="n">rv</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">)</span>
<span class="linenos"> 7</span>    <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X_plot</span><span class="p">,</span> <span class="n">Y_plot</span><span class="p">,</span> <span class="n">rv</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">pos</span><span class="p">),</span> <span class="n">levels</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="linenos">10</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="linenos">11</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Gaussian Mixture Model Approximation of a Multimodal Distribution in 2D&quot;</span><span class="p">)</span>
<span class="linenos">12</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/f191655364e467574ad5da894cf07aa042fd4f5aeba0a97b8c6ffbc9e6114236.svg" src="../../_images/f191655364e467574ad5da894cf07aa042fd4f5aeba0a97b8c6ffbc9e6114236.svg" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1"># Generate a synthetic dataset with overlapping clusters</span>
<span class="linenos"> 2</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="linenos"> 3</span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">500</span>
<span class="linenos"> 4</span><span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.4</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span>
<span class="linenos"> 5</span><span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.4</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span>
<span class="linenos"> 6</span><span class="n">X3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span>
<span class="linenos"> 7</span>    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]],</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="linenos"> 8</span><span class="p">)</span>
<span class="linenos"> 9</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">,</span> <span class="n">X3</span><span class="p">])</span>
<span class="linenos">10</span>
<span class="linenos">11</span><span class="c1"># Fit a Gaussian Mixture Model to the data</span>
<span class="linenos">12</span><span class="n">gmm</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="linenos">13</span><span class="n">gmm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="linenos">14</span>
<span class="linenos">15</span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prior:&quot;</span><span class="p">)</span>
<span class="linenos">16</span><span class="n">pprint</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">weights_</span><span class="p">)</span>
<span class="linenos">17</span>
<span class="linenos">18</span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean:&quot;</span><span class="p">)</span>
<span class="linenos">19</span><span class="n">pprint</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">means_</span><span class="p">)</span>
<span class="linenos">20</span>
<span class="linenos">21</span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Covariance:&quot;</span><span class="p">)</span>
<span class="linenos">22</span><span class="n">pprint</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">covariances_</span><span class="p">)</span>
<span class="linenos">23</span>
<span class="linenos">24</span>
<span class="linenos">25</span><span class="c1"># Plot the data points and the Gaussian Mixture Model contours</span>
<span class="linenos">26</span><span class="n">x</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="linenos">27</span><span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="linenos">28</span><span class="n">step</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="linenos">29</span><span class="n">X_plot</span><span class="p">,</span> <span class="n">Y_plot</span> <span class="o">=</span> <span class="n">make_meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
<span class="linenos">30</span><span class="nb">print</span><span class="p">(</span><span class="n">X_plot</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Y_plot</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="linenos">31</span><span class="c1"># x = np.linspace(-6, 6, 100)</span>
<span class="linenos">32</span><span class="c1"># y = np.linspace(-6, 10, 100)</span>
<span class="linenos">33</span><span class="c1"># X_plot, Y_plot = np.meshgrid(x, y)</span>
<span class="linenos">34</span>
<span class="linenos">35</span><span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">X_plot</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="linenos">36</span><span class="n">pos</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_plot</span>
<span class="linenos">37</span><span class="n">pos</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">Y_plot</span>
<span class="linenos">38</span><span class="nb">print</span><span class="p">(</span><span class="n">pos</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="linenos">39</span>
<span class="linenos">40</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="linenos">41</span><span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="linenos">42</span>
<span class="linenos">43</span><span class="k">for</span> <span class="n">mean</span><span class="p">,</span> <span class="n">cov</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">gmm</span><span class="o">.</span><span class="n">means_</span><span class="p">,</span> <span class="n">gmm</span><span class="o">.</span><span class="n">covariances_</span><span class="p">):</span>
<span class="linenos">44</span>    <span class="n">rv</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">)</span>
<span class="linenos">45</span>    <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">X_plot</span><span class="p">,</span> <span class="n">Y_plot</span><span class="p">,</span> <span class="n">rv</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">pos</span><span class="p">),</span> <span class="n">levels</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="linenos">46</span>
<span class="linenos">47</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="linenos">48</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="linenos">49</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Gaussian Mixture Model Approximation of Overlapping Distributions in 2D&quot;</span><span class="p">)</span>
<span class="linenos">50</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Prior:
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">array</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.39155155</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3990295</span> , <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.20941894</span><span style="font-weight: bold">])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Mean:
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">array</span><span style="font-weight: bold">([[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.08107823</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0093926</span> <span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">â”‚      </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3.07416739</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.85294415</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">â”‚      </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.17687862</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4.98805321</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">Covariance:
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">array</span><span style="font-weight: bold">([[[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.59221756</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.38233697</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">â”‚   â”‚   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.38233697</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.86035845</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">â”‚      </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">â”‚      </span><span style="font-weight: bold">[[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.99917348</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.37393606</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">â”‚   â”‚   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.37393606</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.85648915</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">â”‚      </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">â”‚      </span><span style="font-weight: bold">[[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.38890481</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.35019933</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">â”‚   â”‚   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.35019933</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.35042648</span><span style="font-weight: bold">]]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1348</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1176</span><span style="font-weight: bold">)</span>
<span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1348</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1176</span><span style="font-weight: bold">)</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1348</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1176</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span><span style="font-weight: bold">)</span>
</pre>
</div><img alt="../../_images/f0b46bd9aa9668de8bb7622619f8a61cf02bc89bbe803cebf7284ea7f77e5530.svg" src="../../_images/f0b46bd9aa9668de8bb7622619f8a61cf02bc89bbe803cebf7284ea7f77e5530.svg" />
</div>
</div>
</section>
</section>
<section id="prior-is-a-latent-variable">
<h3><a class="toc-backref" href="#id28" role="doc-backlink">Prior is a Latent Variable</a><a class="headerlink" href="#prior-is-a-latent-variable" title="Link to this heading">#</a></h3>
<p>The weights we have defined for each gaussian component are called the <strong>prior</strong> of the mixture model.
This just means if we draw a sample <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> from the mixture model, the probability of it belonging to the <span class="math notranslate nohighlight">\(k\)</span>-th component is <span class="math notranslate nohighlight">\(\pi_k\)</span>.</p>
<p>In our case above, we have defined the prior as <span class="math notranslate nohighlight">\(\boldsymbol{\pi} = \begin{bmatrix} 0.4, 0.4, 0.2 \end{bmatrix}\)</span>, which means if we draw a sample <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> from the mixture model, the probability of it belonging to the
1st component is 0.4, the probability of it belonging to the 2nd component is 0.4, and the probability of it belonging to the 3rd component is 0.2.</p>
<p>The prior is a latent variable, which means it is not observed in the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, but it is inferred from the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>. This may sound magical, but it actually is just the number of data points in each
component, divided by the total number of data points.</p>
<p>Recall our <code class="docutils literal notranslate"><span class="pre">samples</span></code> consist of 10000 data points with <span class="math notranslate nohighlight">\(3\)</span> components. We defined the <code class="docutils literal notranslate"><span class="pre">prior</span> <span class="pre">=</span> <span class="pre">[0.4,</span> <span class="pre">0.4,</span> <span class="pre">0.2]</span></code>, which means the number of data points in each component is <span class="math notranslate nohighlight">\(4000\)</span>, <span class="math notranslate nohighlight">\(4000\)</span>, and <span class="math notranslate nohighlight">\(2000\)</span> respectively.
This variable is unobserved, because we really do not know what it is when we were handed the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>.</p>
<p>Letâ€™s see an example.</p>
</section>
</section>
<section id="problem-formulation">
<h2><a class="toc-backref" href="#id29" role="doc-backlink">Problem Formulation</a><a class="headerlink" href="#problem-formulation" title="Link to this heading">#</a></h2>
<div class="proof remark admonition" id="prf:remark-notation-gmm">
<p class="admonition-title"><span class="caption-number">Remark 31 </span> (Notation Reminder)</p>
<section class="remark-content" id="proof-content">
<p>Firstly, notation wise:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{N}\left(\boldsymbol{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}\right)
\]</div>
<p>means the probability density function (PDF) of a multivariate normal distribution with mean <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> and covariance <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> evaluated at <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>. The <span class="math notranslate nohighlight">\(\mid\)</span> symbol means â€œgivenâ€ but
do not confuse with the conditional probability.</p>
</section>
</div><section id="a-primer">
<h3><a class="toc-backref" href="#id30" role="doc-backlink">A Primer</a><a class="headerlink" href="#a-primer" title="Link to this heading">#</a></h3>
<p><strong>Given</strong> a set <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> containing <span class="math notranslate nohighlight">\(N\)</span> data points:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{S} = \left\{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(N)}\right\} \subset \mathbb{R}^{D}
\]</div>
<p>where the vector <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> is the <span class="math notranslate nohighlight">\(n\)</span>-th sample with <span class="math notranslate nohighlight">\(D\)</span> number of features, given by:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^{(n)} \in \mathbb{R}^{D} = \begin{bmatrix} x_1^{(n)} &amp; x_2^{(n)} &amp; \cdots &amp; x_D^{(n)} \end{bmatrix}^{\mathrm{T}} \quad \text{where } n = 1, \ldots, N.
\]</div>
<p>We can further write <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> as a disjoint union of <span class="math notranslate nohighlight">\(K\)</span> sets, as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{S} &amp;:= \left\{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(N)}\right\} \subset \mathbb{R}^{D} = C_1 \sqcup C_2 \sqcup \cdots \sqcup C_K \\
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(C_k\)</span> is the set of data points that belong to cluster <span class="math notranslate nohighlight">\(k\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-cluster-def-gmm">
<span class="eqno">(90)<a class="headerlink" href="#equation-eq-cluster-def-gmm" title="Link to this equation">#</a></span>\[
C_k = \left\{\mathbf{x}^{(n)} \in \mathbb{R}^{D} \middle\vert z^{(n)} = k\right\} .
\]</div>
<p>Furthermore, we denote <span class="math notranslate nohighlight">\(z^{(n)}\)</span> as the <em><strong>true cluster assignment</strong></em> of the <span class="math notranslate nohighlight">\(n\)</span>-th data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span>.</p>
<p>However, in practice, we donâ€™t have access to the true cluster assignments <span class="math notranslate nohighlight">\(z^{(n)}\)</span>. Our goal is to estimate the cluster assignments <span class="math notranslate nohighlight">\(z^{(n)}\)</span> from the data points <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span>.</p>
<p>In K-Means, we discussed Lloydâ€™s algorithm, a hard clustering method that outputs estimated cluster assignments <span class="math notranslate nohighlight">\(\hat{y}^{(n)}\)</span> for each data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span>.
Note do not be confused by the notation, here <span class="math notranslate nohighlight">\(z\)</span> is the true cluster assignment, while <span class="math notranslate nohighlight">\(\hat{y}\)</span> is the estimated cluster assignment. In K-Means however,
I conveniently used <span class="math notranslate nohighlight">\(y\)</span> to denote the true cluster assignment, and <span class="math notranslate nohighlight">\(\hat{y}\)</span> to denote the estimated cluster assignment. We have
an additional layer of complexity here, as we will be using <span class="math notranslate nohighlight">\(z\)</span> to denote the true cluster assignment, and <span class="math notranslate nohighlight">\(y\)</span> to denote the a-posteriori probability of the data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> belonging to the <span class="math notranslate nohighlight">\(k\)</span>-th cluster <span class="math notranslate nohighlight">\(C_{k}\)</span>.</p>
<p>As we have seen from the examples earlier, it is desirable to quantify the degree by which a data point belongs to a cluster. Soft clustering methods use a continues range, such as the closed interval <span class="math notranslate nohighlight">\([0,1]\)</span>, of possible values for the degree of belonging. In contrast, hard clustering methods use only two possible values for the degree of belonging to a specific cluster, either â€œfull belongingâ€ or no â€œbelonging at allâ€. While hard clustering methods assign a given data point to precisely one cluster, soft clustering methods typically assign a data point to several different clusters with non-zero degree of belonging <span id="id1">[<a class="reference internal" href="../../bibliography.html#id40" title="Alexander Jung. Machine learning: The basics. Springer Nature Singapore, 2023.">Jung, 2023</a>]</span>.</p>
<p>Consequently, we can define for each data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)} \in \mathcal{S}\)</span>, an associated cluster assignment vector <span class="math notranslate nohighlight">\(\widehat{\mathbf{y}}^{(n)}\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[
\widehat{\mathbf{y}}^{(n)} = \begin{bmatrix} \hat{y}_1^{(n)} &amp; \hat{y}_2^{(n)} &amp; \cdots &amp; \hat{y}_K^{(n)} \end{bmatrix}^{\mathrm{T}} \quad \text{where } n = 1, \ldots, N.
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{y}_k^{(n)}\)</span> is the degree of belonging of the <span class="math notranslate nohighlight">\(n\)</span>-th data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> to the <span class="math notranslate nohighlight">\(k\)</span>-th cluster <span class="math notranslate nohighlight">\(C_k\)</span>. This is reminiscent of the your
usual classification problem, where we have a set of <span class="math notranslate nohighlight">\(K\)</span> classes, and we want to assign a data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> to one of the <span class="math notranslate nohighlight">\(K\)</span> classes.</p>
<p>In this case, we can think of <span class="math notranslate nohighlight">\(\widehat{\mathbf{y}}^{(n)}\)</span> as the <strong>posterior probability</strong> of the <span class="math notranslate nohighlight">\(n\)</span>-th data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> belonging to the <span class="math notranslate nohighlight">\(k\)</span>-th cluster <span class="math notranslate nohighlight">\(C_k\)</span>,
or with our current setup, the <strong>posterior probability</strong> of the <span class="math notranslate nohighlight">\(n\)</span>-th data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> given the cluster assignment <span class="math notranslate nohighlight">\(z^{(n)} = k\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\widehat{\mathbf{y}}^{(n)} &amp;= \mathbb{P}\left(z^{(n)} = k \mid \mathbf{x}^{(n)}\right) \\
\end{aligned}
\end{split}\]</div>
<div class="proof example admonition" id="prf:example-gmm-1">
<p class="admonition-title"><span class="caption-number">Example 14 </span> (Example)</p>
<section class="example-content" id="proof-content">
<p>Consider the following example:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\widehat{\mathbf{y}}^{(1)} &amp;= \begin{bmatrix} 0.1 &amp; 0.7 &amp; 0.2 \end{bmatrix}^{\mathrm{T}} \\
\end{aligned}
\end{split}\]</div>
<p>then it can be interpreted as the degree of belonging of the first data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(1)}\)</span> to each of the three clusters <span class="math notranslate nohighlight">\(C_1, C_2, C_3\)</span> respectively.
In this example, the first data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(1)}\)</span> has a <span class="math notranslate nohighlight">\(10\%\)</span> chance of belonging to cluster <span class="math notranslate nohighlight">\(C_1\)</span>, a <span class="math notranslate nohighlight">\(70\%\)</span> chance of belonging to cluster <span class="math notranslate nohighlight">\(C_2\)</span>, and a <span class="math notranslate nohighlight">\(20\%\)</span> chance of belonging to cluster <span class="math notranslate nohighlight">\(C_3\)</span>.
We can therefore say that the first data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(1)}\)</span> is more likely to belong to cluster <span class="math notranslate nohighlight">\(C_2\)</span> than to cluster <span class="math notranslate nohighlight">\(C_1\)</span> or cluster <span class="math notranslate nohighlight">\(C_3\)</span>.</p>
</section>
</div><p>However, to even find the posterior probability, we need to know what
the distribution of <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> is. In the next section, we will
discuss the distribution of the posterior probability and how we can
estimate it.</p>
</section>
<section id="gaussian-mixture-model">
<span id="gmm-problem-formulation-gaussian-mixture-model"></span><h3><a class="toc-backref" href="#id31" role="doc-backlink">Gaussian Mixture Model</a><a class="headerlink" href="#gaussian-mixture-model" title="Link to this heading">#</a></h3>
<p>A widely used soft clustering method is the <a class="reference external" href="https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model"><strong>Gaussian Mixture Model (GMM)</strong></a>, which uses a probabilistic model for the data points <span class="math notranslate nohighlight">\(\mathcal{S}=\left\{\mathbf{x}^{(n)}\right\}_{n=1}^N\)</span>.</p>
<p>A Gaussian mixture model is a density model where we combine a finite number of <span class="math notranslate nohighlight">\(K\)</span> Gaussian distributions <span class="math notranslate nohighlight">\(\mathcal{N}\left(\boldsymbol{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\right)\)</span> so that</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-def-1">
<span class="eqno">(91)<a class="headerlink" href="#equation-eq-gmm-def-1" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
&amp; p(\boldsymbol{x} ; \boldsymbol{\theta})=\sum_{k=1}^K \pi_k \mathcal{N}\left(\boldsymbol{x} \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k\right) &amp;&amp; (\star) \\
&amp; 0 \leqslant \pi_k \leqslant 1, \quad \sum_{k=1}^K \pi_k=1, &amp;&amp; (\star\star) \\
\end{aligned}
\end{split}\]</div>
<p>where we defined <span class="math notranslate nohighlight">\(\boldsymbol{\theta}:=\left\{\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k, \pi_k: k=1, \ldots, K\right\}\)</span> as the collection of all parameters of the model. This convex combination of Gaussian distribution gives us significantly more flexibility for modeling complex densities than a simple Gaussian distribution (which we recover from <span class="math notranslate nohighlight">\((\star \star)\)</span> for <span class="math notranslate nohighlight">\(K=1\)</span>) <span id="id2">[<a class="reference internal" href="../../bibliography.html#id9" title="Marc Peter Deisenroth, Cheng Soon Ong, and Aldo A. Faisal. Mathematics for Machine Learning. Cambridge University Press, 2021.">Deisenroth <em>et al.</em>, 2021</a>]</span>.</p>
<p>Overall, the main goal is to find the 3 parameters of the mixture model that best fit the data <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>.</p>
<p>To elaborate further the setup defined in <a class="reference internal" href="#equation-eq-gmm-def-1">(91)</a>, we have:</p>
<ul>
<li><p>The probability distribution above means that the (<em>joint</em>) probability of observing a data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> is the sum of the probability of observing <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> from each of the <span class="math notranslate nohighlight">\(K\)</span> clusters, parametrized by the mean and covariance vector/matrix, weighted by the probability of the cluster assignment <span class="math notranslate nohighlight">\(z^{(n)} = k\)</span>, parameterized by <span class="math notranslate nohighlight">\(\pi_k\)</span>. This is a mouthful, we will break it down further in the sections below.
Notation wise, we can write this as:</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{P}\left(\boldsymbol{X} ; \boldsymbol{\theta}\right) := \mathbb{P}_{\boldsymbol{\theta}}\left(\boldsymbol{X}\right) = \mathbb{P}_{\left\{\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}\right\}}\left(\boldsymbol{X}\right)
    \]</div>
</li>
<li><p>The first constraint <span class="math notranslate nohighlight">\((\star\star)\)</span> ensures that the probability of observing a data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> from any of the <span class="math notranslate nohighlight">\(K\)</span> clusters is <span class="math notranslate nohighlight">\(1\)</span>. This is a normalization constraint, and is necessary to ensure that the probability distribution is a valid probability distribution.</p></li>
<li><p>The shape and dimensions for the parameters are given:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> is a vector of mixing coefficients (prior weights):</p>
<div class="math notranslate nohighlight">
\[
      \boldsymbol{\pi} = \begin{bmatrix} \pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_k \end{bmatrix}^{\mathrm{T}} \in \mathbb{R}^K
      \]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span> is a vector of means for the <span class="math notranslate nohighlight">\(k\)</span>-th cluster:</p>
<div class="math notranslate nohighlight">
\[
      \boldsymbol{\mu}_k = \begin{bmatrix} \mu_{k1} &amp; \mu_{k2} &amp; \cdots &amp; \mu_{kD} \end{bmatrix}^{\mathrm{T}} \in \mathbb{R}^D
      \]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_k\)</span> is a covariance matrix for the <span class="math notranslate nohighlight">\(k\)</span>-th cluster:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
      \boldsymbol{\Sigma}_k = \begin{bmatrix} \Sigma_{k11} &amp; \Sigma_{k12} &amp; \cdots &amp; \Sigma_{k1D} \\ \Sigma_{k21} &amp; \Sigma_{k22} &amp; \cdots &amp; \Sigma_{k2D} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \Sigma_{kD1} &amp; \Sigma_{kD2} &amp; \cdots &amp; \Sigma_{kDD} \end{bmatrix} \in \mathbb{R}^{D \times D}
      \end{split}\]</div>
</li>
</ul>
</li>
</ul>
</section>
<section id="the-perpectives">
<h3><a class="toc-backref" href="#id32" role="doc-backlink">The Perpectives</a><a class="headerlink" href="#the-perpectives" title="Link to this heading">#</a></h3>
<p>There are two interpretations of the Gaussian mixture model: the latent variable perspective and the data likelihood perspective.</p>
<section id="the-mixture-model-perspective">
<h4><a class="toc-backref" href="#id33" role="doc-backlink">The Mixture Model Perspective</a><a class="headerlink" href="#the-mixture-model-perspective" title="Link to this heading">#</a></h4>
<p>Mixture model perspective: In this perspective, GMM is seen as a simple mixture of multiple Gaussian distributions. The goal is to model the probability density function (PDF) of the observed data as a weighted sum of the individual Gaussian PDFs. Each Gaussian component has its own mean and covariance matrix, and the model learns the weights, means, and covariances that best fit the data. This perspective focuses on the density estimation aspect of GMM and is less concerned with the underlying latent variables.</p>
</section>
<section id="the-latent-variable-perspective">
<h4><a class="toc-backref" href="#id34" role="doc-backlink">The Latent Variable Perspective</a><a class="headerlink" href="#the-latent-variable-perspective" title="Link to this heading">#</a></h4>
<p>Latent variable perspective: In this perspective, GMM is viewed as a generative probabilistic model that assumes there are some hidden (latent) variables responsible for generating the observed data points. Each hidden variable corresponds to one of the Gaussian components in the mixture. The data points are assumed to be generated by first sampling the latent variable (component) from a categorical distribution and then sampling the data point from the corresponding Gaussian distribution. This perspective is closely related to the Expectation-Maximization (EM) algorithm, which alternates between estimating the component assignments (latent variables) and updating the Gaussian parameters (mean, covariance) to maximize the likelihood of the observed data.</p>
</section>
<section id="summary">
<h4><a class="toc-backref" href="#id35" role="doc-backlink">Summary</a><a class="headerlink" href="#summary" title="Link to this heading">#</a></h4>
<p>Both perspectives ultimately lead to the same model, but they highlight different aspects of GMM and can be useful in different contexts. For example, the latent variable perspective is more suitable for clustering and classification tasks, while the mixture model perspective is more useful for density estimation and generating new samples from the modeled distribution.</p>
<p>In the next few sections, we will discuss the latent variable perspective, but note there
may be some mix of the two perspectives in the following sections. For example,
when we discuss about the posterior probability of the latent variables, we will
also mention that it is the â€œresponsibilitiesâ€ in the mixture model perspective.</p>
</section>
</section>
</section>
<section id="id3">
<h2><a class="toc-backref" href="#id36" role="doc-backlink">The Mixture Model Perspective</a><a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<p>In <a class="reference internal" href="#gmm-problem-formulation-gaussian-mixture-model"><span class="std std-ref">the previous section on</span></a>,
we have actually already defined the Mixture Model Perspective of the Gaussian
Mixture Model.</p>
<section id="the-gaussian-mixture-model">
<h3><a class="toc-backref" href="#id37" role="doc-backlink">The Gaussian Mixture Model</a><a class="headerlink" href="#the-gaussian-mixture-model" title="Link to this heading">#</a></h3>
<p>To recap, a Gaussian mixture model is a density model where we combine a finite number of <span class="math notranslate nohighlight">\(K\)</span> Gaussian distributions <span class="math notranslate nohighlight">\(\mathcal{N}\left(\boldsymbol{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\right)\)</span> so that</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-def-2">
<span class="eqno">(92)<a class="headerlink" href="#equation-eq-gmm-def-2" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
&amp; p(\boldsymbol{x} ; \boldsymbol{\theta})=\sum_{k=1}^K \pi_k \mathcal{N}\left(\boldsymbol{x} \mid \boldsymbol{\mu}_k, \mathbf{\Sigma}_k\right) &amp;&amp; (\star) \\
&amp; 0 \leqslant \pi_k \leqslant 1, \quad \sum_{k=1}^K \pi_k=1, &amp;&amp; (\star\star) \\
\end{aligned}
\end{split}\]</div>
<p>where we defined <span class="math notranslate nohighlight">\(\boldsymbol{\theta}:=\left\{\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k, \pi_k: k=1, \ldots, K\right\}\)</span> as the collection of all parameters of the model. This convex combination of Gaussian distribution gives us significantly more flexibility for modeling complex densities than a simple Gaussian distribution (which we recover from <span class="math notranslate nohighlight">\((\star \star)\)</span> for <span class="math notranslate nohighlight">\(K=1\)</span>) <span id="id4">[<a class="reference internal" href="../../bibliography.html#id9" title="Marc Peter Deisenroth, Cheng Soon Ong, and Aldo A. Faisal. Mathematics for Machine Learning. Cambridge University Press, 2021.">Deisenroth <em>et al.</em>, 2021</a>]</span>.</p>
<p>Overall, the main goal is to find the 3 parameters of the mixture model that best fit the data <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>.</p>
</section>
<section id="the-responsibilities">
<span id="gmm-responsibility"></span><h3><a class="toc-backref" href="#id38" role="doc-backlink">The Responsibilities</a><a class="headerlink" href="#the-responsibilities" title="Link to this heading">#</a></h3>
<p>Another quantity that will play an important role is the conditional probability of <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span> given <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>. We shall denote this quantity as the responsibility of the <span class="math notranslate nohighlight">\(k\)</span>-th component for generating the data point <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, and denote it as
<span class="math notranslate nohighlight">\(r^{(n)}_k\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-responsibility">
<span class="eqno">(93)<a class="headerlink" href="#equation-eq-gmm-responsibility" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
r^{(n)}_k \equiv p\left(z^{(n)}=k \mid \boldsymbol{x}\right) &amp; = \frac{p\left(\boldsymbol{x} \mid z^{(n)}=k\right) p\left(z^{(n)}=k\right)}{p\left(\boldsymbol{x}\right)} \\
&amp; =\frac{\pi_k \mathcal{N}\left(\boldsymbol{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\right)}{\sum_{k=1}^K \pi_k \mathcal{N}\left(\boldsymbol{x} \mid \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\right)}
\end{aligned}
\end{split}\]</div>
<p>Therefore, mixture components have a high responsibility for a data point when the data point could be a plausible sample from that mixture component.</p>
<p>Note that <span class="math notranslate nohighlight">\(\boldsymbol{r}^{(n)}\)</span> is a <span class="math notranslate nohighlight">\(K\)</span> dimensional vector:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{r}^{(n)} = \begin{bmatrix} r^{(n)}_1 \\ r^{(n)}_2 \\ \vdots \\ r^{(n)}_K \end{bmatrix} \in \mathbb{R}^K
\end{split}\]</div>
<p>is a (normalized) probability vector, i.e., <span class="math notranslate nohighlight">\(\sum_{k} r^{(n)}_{k}=1\)</span> with <span class="math notranslate nohighlight">\(r^{(n)}_{k} \geqslant 0\)</span>. This probability vector distributes probability mass among the <span class="math notranslate nohighlight">\(K\)</span> mixture components, and we can think of <span class="math notranslate nohighlight">\(\boldsymbol{r}^{(n)}\)</span> as a â€œsoft assignmentâ€ of <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> to the <span class="math notranslate nohighlight">\(K\)</span> mixture components. Therefore, the responsibility <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span> from <a class="reference internal" href="#equation-eq-gmm-responsibility">(93)</a> represents the probability that <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> has been generated by the <span class="math notranslate nohighlight">\(k\)</span> th mixture component.</p>
</section>
</section>
<section id="id5">
<h2><a class="toc-backref" href="#id39" role="doc-backlink">The Latent Variable Perspective</a><a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<p>Notice that if we approach the GMM from the latent variable perspective, we are more
interested in the probability of the latent variable
<span class="math notranslate nohighlight">\(\boldsymbol{z}^{(n)}\)</span> given the data <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span>, as we will see later.</p>
<p>Within this model, we assume the following.</p>
<section id="the-generative-process">
<h3><a class="toc-backref" href="#id40" role="doc-backlink">The Generative Process</a><a class="headerlink" href="#the-generative-process" title="Link to this heading">#</a></h3>
<p>Consider a mental model that there are <span class="math notranslate nohighlight">\(K\)</span> gaussian distributions, each representing a cluster. The data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> is generated by first sampling the latent variable <span class="math notranslate nohighlight">\(\boldsymbol{z}^{(n)}\)</span> from a categorical distribution and then sampling the data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> from the corresponding Gaussian distribution. This is the generative process of the GMM.</p>
<p>More concretely, the sampling process can be described below.</p>
<div class="proof algorithm admonition" id="alg:gmm-sampling">
<p class="admonition-title"><span class="caption-number">Algorithm 6 </span> (GMM Sampling Process)</p>
<section class="algorithm-content" id="proof-content">
<p>The construction of this latent-variable model (see the corresponding graphical model in Figure 11.9) lends itself to a very simple sampling procedure (generative process) to generate data:</p>
<ol class="arabic simple">
<li><p>Sample <span class="math notranslate nohighlight">\(z^{(n)} \sim p(\boldsymbol{z})=\boldsymbol{\pi}\)</span> where <span class="math notranslate nohighlight">\(z^{(n)}\)</span> is a discrete random variable with <span class="math notranslate nohighlight">\(K\)</span> possible values.</p></li>
<li><p>Sample <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)} \sim p\left(\boldsymbol{x} \mid z^{(n)}\right)\)</span>.</p></li>
</ol>
<p>In the first step, we select a mixture component <span class="math notranslate nohighlight">\(k\)</span> at random according to <span class="math notranslate nohighlight">\(p(\boldsymbol{z})=\boldsymbol{\pi}\)</span>; in the second step we draw a sample from the corresponding mixture component <span class="math notranslate nohighlight">\(k\)</span>. When we discard the samples of the latent variable so that we are left with the <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span>, we have valid samples from the GMM. This kind of sampling, where samples of random variables depend on samples from the variableâ€™s parents in the graphical model, is called <a class="reference external" href="https://en.wikipedia.org/wiki/Ancestral_sampling">ancestral sampling</a> <span id="id6">[<a class="reference internal" href="../../bibliography.html#id9" title="Marc Peter Deisenroth, Cheng Soon Ong, and Aldo A. Faisal. Mathematics for Machine Learning. Cambridge University Press, 2021.">Deisenroth <em>et al.</em>, 2021</a>]</span>.</p>
</section>
</div><p>This generative process prompts a few questions:</p>
<ul class="simple">
<li><p>How do we define the categorical distribution <span class="math notranslate nohighlight">\(p(\boldsymbol{z})\)</span> (i.e. the prior distribution of the latent variable <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span>)?</p></li>
<li><p>How do we define the Gaussian distribution <span class="math notranslate nohighlight">\(p\left(\boldsymbol{x} \mid z^{(n)}\right)\)</span> (i.e. the conditional distribution of the data <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> given the latent variable <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span>, also known as the likelihood)?</p></li>
<li><p>How do we define the joint distribution <span class="math notranslate nohighlight">\(p\left(\boldsymbol{x}, \boldsymbol{z}\right)\)</span> (i.e. the joint distribution of the data <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> and the latent variable <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span>)?</p></li>
<li><p>How do we define the marginal distribution <span class="math notranslate nohighlight">\(p\left(\boldsymbol{x}\right)\)</span> (i.e. the marginal distribution of the data <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>)?</p></li>
<li><p>How do we define the posterior distribution <span class="math notranslate nohighlight">\(p\left(\boldsymbol{z} \mid \boldsymbol{x}\right)\)</span> (i.e. the posterior distribution of the latent variable <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span> given the data <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>)?</p></li>
</ul>
</section>
<section id="assumption-1-the-distribution-of-the-data-point-boldsymbol-x-n-given-the-latent-variable-boldsymbol-z-n">
<h3><a class="toc-backref" href="#id41" role="doc-backlink">Assumption 1: The Distribution of the Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> given the Latent Variable <span class="math notranslate nohighlight">\(\boldsymbol{z}^{(n)}\)</span></a><a class="headerlink" href="#assumption-1-the-distribution-of-the-data-point-boldsymbol-x-n-given-the-latent-variable-boldsymbol-z-n" title="Link to this heading">#</a></h3>
<p>Consider a mental model that there are <span class="math notranslate nohighlight">\(K\)</span> gaussian distributions, each representing a cluster <span class="math notranslate nohighlight">\(C_k\)</span>. The data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> is generated by first sampling the latent variable <span class="math notranslate nohighlight">\(\boldsymbol{z}^{(n)}\)</span> from a categorical distribution and then sampling the data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> from the corresponding Gaussian distribution. This is the generative process of the GMM.</p>
<p>We first start by defining the <span class="math notranslate nohighlight">\(K\)</span> clusters, each represented by a different
(multivariate) gaussian distribution.</p>
<section id="the-latent-clusters">
<h4><a class="toc-backref" href="#id42" role="doc-backlink">The Latent Clusters</a><a class="headerlink" href="#the-latent-clusters" title="Link to this heading">#</a></h4>
<p>Each cluster <span class="math notranslate nohighlight">\(C_k\)</span> for <span class="math notranslate nohighlight">\(k=1, \ldots, K\)</span> is represented by a multivariate gaussian distribution:</p>
<div class="math notranslate nohighlight" id="equation-eq-cluster-def-gmm-2">
<span class="eqno">(94)<a class="headerlink" href="#equation-eq-cluster-def-gmm-2" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
C_1 &amp;:= \mathcal{N}(\boldsymbol{x} ; \boldsymbol{\mu}_1, \boldsymbol{\Sigma}_1) \\
C_2 &amp;:= \mathcal{N}(\boldsymbol{x} ; \boldsymbol{\mu}_2, \boldsymbol{\Sigma}_2) \\
&amp;\vdots \\
C_K &amp;:= \mathcal{N}(\boldsymbol{x} ; \boldsymbol{\mu}_K, \boldsymbol{\Sigma}_K) \\
\end{aligned}
\end{split}\]</div>
<p>and the geometry of the clusters are completely determined by their
mean vectors <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span> and covariance matrices <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_k\)</span>.</p>
<p>This notation can be confusing because <span class="math notranslate nohighlight">\(C_k\)</span> is not really a random variable, instead it is a probability distribution.
Consequently, the data points that <span class="math notranslate nohighlight">\(C_k\)</span>â€™s probability distribution generates are random variables, which we will define
next.</p>
</section>
<section id="the-data-points-boldsymbol-x-n-is-the-likelihood">
<h4><a class="toc-backref" href="#id43" role="doc-backlink">The Data Points <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> is the Likelihood</a><a class="headerlink" href="#the-data-points-boldsymbol-x-n-is-the-likelihood" title="Link to this heading">#</a></h4>
<p>Any data point <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> can be generated by sampling from one of the <span class="math notranslate nohighlight">\(K\)</span> clusters:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{x}^{(n)} \in C_k &amp;\sim \mathcal{N}(\boldsymbol{x} ; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \\
\end{aligned}
\end{split}\]</div>
<p>which means the following:</p>
<div class="math notranslate nohighlight" id="equation-eq-cluster-def-gmm-3">
<span class="eqno">(95)<a class="headerlink" href="#equation-eq-cluster-def-gmm-3" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\boldsymbol{x}^{(n)} \in C_k &amp;:= \mathcal{N}(\boldsymbol{x} ; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \\
&amp;= \frac{1}{\sqrt{(2\pi)^D \det{\boldsymbol{\Sigma}_k}}} \exp\left(-\frac{1}{2}(\boldsymbol{x} - \boldsymbol{\mu}_k)^{\mathrm{T}} \boldsymbol{\Sigma}_k^{-1} (\boldsymbol{x} - \boldsymbol{\mu}_k)\right) \quad \text{for } k = 1, \ldots, K.
\end{aligned}
\end{split}\]</div>
<p>where this distribution is parametrized by <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span> is the mean vector of the <span class="math notranslate nohighlight">\(k\)</span>-th cluster, and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_k\)</span> is the covariance matrix of the <span class="math notranslate nohighlight">\(k\)</span>-th cluster.</p>
<p>This formulation further allows us to interpret a specific data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> as a realization drawn from the probability distribution <a class="reference internal" href="#equation-eq-cluster-def-gmm-3">(95)</a> of a specific cluster <span class="math notranslate nohighlight">\(C_k\)</span>.</p>
<p>We can represent the distribution defined in <a class="reference internal" href="#equation-eq-cluster-def-gmm-3">(95)</a>
more concisely as:</p>
<div class="math notranslate nohighlight" id="equation-eq-cluster-def-gmm-4">
<span class="eqno">(96)<a class="headerlink" href="#equation-eq-cluster-def-gmm-4" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\boldsymbol{X}^{(n)} = \boldsymbol{x}^{(n)} \mid Z^{(n)} = k &amp;\sim \mathcal{N}(\boldsymbol{x} ; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
\begin{cases}
\boldsymbol{X}^{(n)} = \boldsymbol{x}^{(n)} \mid Z^{(n)} = 1 &amp;\sim \mathcal{N}(\boldsymbol{x} ; \boldsymbol{\mu}_1, \boldsymbol{\Sigma}_1) \\
\boldsymbol{X}^{(n)} = \boldsymbol{x}^{(n)} \mid Z^{(n)} = 2 &amp;\sim \mathcal{N}(\boldsymbol{x} ; \boldsymbol{\mu}_2, \boldsymbol{\Sigma}_2) \\
&amp;\vdots \\
\boldsymbol{X}^{(n)} = \boldsymbol{x}^{(n)} \mid Z^{(n)} = K &amp;\sim \mathcal{N}(\boldsymbol{x} ; \boldsymbol{\mu}_K, \boldsymbol{\Sigma}_K) \\
\end{cases}
\end{aligned}
\end{split}\]</div>
<p>Notice the similarity between the expression <a class="reference internal" href="#equation-eq-cluster-def-gmm-4">(96)</a> and the likelihood
expression in a Naive Bayes model? Yes, indeed this expression is none other than the
likelihood of observing the data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> given the latent variable <span class="math notranslate nohighlight">\(Z^{(n)} = k\)</span>.</p>
</section>
<section id="the-likelihood-of-one-single-data-point-boldsymbol-x-n">
<h4><a class="toc-backref" href="#id44" role="doc-backlink">The Likelihood of One Single Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span></a><a class="headerlink" href="#the-likelihood-of-one-single-data-point-boldsymbol-x-n" title="Link to this heading">#</a></h4>
<ul>
<li><p>Let <span class="math notranslate nohighlight">\(x^{(n)}\)</span> denote the <span class="math notranslate nohighlight">\(n\)</span>-th data point, with <span class="math notranslate nohighlight">\(n = 1, \dots, N\)</span>.</p></li>
<li><p>Let <span class="math notranslate nohighlight">\(z^{(n)}\)</span> denote the latent variable corresponding to the <span class="math notranslate nohighlight">\(n\)</span>-th data point, representing the Gaussian component it belongs to. <span class="math notranslate nohighlight">\(z^{(n)}\)</span> can take on values <span class="math notranslate nohighlight">\(1, \dots, K\)</span>, where <span class="math notranslate nohighlight">\(K\)</span> is the number of Gaussian components.</p></li>
<li><p>The likelihood of the <span class="math notranslate nohighlight">\(n\)</span>-th data point belonging to the <span class="math notranslate nohighlight">\(k\)</span>-th Gaussian component can be denoted as</p>
<div class="math notranslate nohighlight">
\[
  p(x^{(n)} | z^{(n)} = k ; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \quad \text{for } k = 1, \dots, K
  \]</div>
</li>
<li><p>Since each <span class="math notranslate nohighlight">\(p(x^{(n)} | z^{(n)} = k)\)</span> is parametrized by the mean and covariance vector/matrix, we can write the below without ambiguity:</p>
<div class="math notranslate nohighlight">
\[
    p(x^{(n)} | z^{(n)} = k) = \mathcal{N}(x^{(n)} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)
    \]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{N}\)</span> is the multivariate Gaussian distribution.</p>
<p>Consequently, we obtain all <span class="math notranslate nohighlight">\(K\)</span> components of the likelihood vector:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{aligned}
  \boldsymbol{L}^{(n)} &amp;= \begin{bmatrix} p(x^{(n)} | z^{(n)} = 1) \\ p(x^{(n)} | z^{(n)} = 2) \\ \vdots \\ p(x^{(n)} | z^{(n)} = K) \end{bmatrix}_{K \times 1} \\
  &amp;= \begin{bmatrix} \mathcal{N}(x^{(n)} | \boldsymbol{\mu}_1, \boldsymbol{\Sigma}_1) \\ \mathcal{N}(x^{(n)} | \boldsymbol{\mu}_2, \boldsymbol{\Sigma}_2) \\ \vdots \\ \mathcal{N}(x^{(n)} | \boldsymbol{\mu}_K, \boldsymbol{\Sigma}_K) \end{bmatrix}_{K \times 1} \\
  &amp;= \begin{bmatrix} L_1^{(n)} \\ L_2^{(n)} \\ \vdots \\ L_K^{(n)} \end{bmatrix}_{K \times 1}
  \end{aligned}
  \end{split}\]</div>
<p>and all elements sum to 1, fully representing the likelihood of the <span class="math notranslate nohighlight">\(n\)</span>-th data point belonging to each of the <span class="math notranslate nohighlight">\(K\)</span> Gaussian components.</p>
</li>
</ul>
</section>
<section id="the-likelihood-of-the-entire-dataset-boldsymbol-x">
<h4><a class="toc-backref" href="#id45" role="doc-backlink">The Likelihood of the Entire Dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span></a><a class="headerlink" href="#the-likelihood-of-the-entire-dataset-boldsymbol-x" title="Link to this heading">#</a></h4>
<p>We are only talking about the likelihood of a single data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> belonging to a specific cluster <span class="math notranslate nohighlight">\(C_k\)</span> (<span class="math notranslate nohighlight">\(z^{(n)} = k\)</span>). We will now discuss how to compute the likelihood of the entire dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> belonging to a specific cluster <span class="math notranslate nohighlight">\(C_k\)</span>.</p>
<p>Given <span class="math notranslate nohighlight">\(\mathcal{S} = \left\{ \boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)}, \ldots, \boldsymbol{x}^{(N)} \right\}\)</span>, the likelihood of the entire dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> belonging to a specific cluster <span class="math notranslate nohighlight">\(C_k\)</span> can be written as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{L} &amp;= \begin{bmatrix} L_1^{(1)} &amp; L_2^{(1)} &amp; \cdots &amp; L_K^{(1)} \\ L_1^{(2)} &amp; L_2^{(2)} &amp; \cdots &amp; L_K^{(2)} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ L_1^{(N)} &amp; L_2^{(N)} &amp; \cdots &amp; L_K^{(N)} \end{bmatrix}_{N \times K} \\
&amp;= \begin{bmatrix} p(\boldsymbol{x}^{(1)} | z^{(1)} = 1) &amp; p(\boldsymbol{x}^{(1)} | z^{(1)} = 2) &amp; \cdots &amp; p(\boldsymbol{x}^{(1)} | z^{(1)} = K) \\ p(\boldsymbol{x}^{(2)} | z^{(2)} = 1) &amp; p(\boldsymbol{x}^{(2)} | z^{(2)} = 2) &amp; \cdots &amp; p(\boldsymbol{x}^{(2)} | z^{(2)} = K) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ p(\boldsymbol{x}^{(N)} | z^{(N)} = 1) &amp; p(\boldsymbol{x}^{(N)} | z^{(N)} = 2) &amp; \cdots &amp; p(\boldsymbol{x}^{(N)} | z^{(N)} = K) \end{bmatrix}_{N \times K} \\
&amp;= \begin{bmatrix} \left(\boldsymbol{L}^{(1)}\right)^T \\ \left(\boldsymbol{L}^{(2)}\right)^T \\ \vdots \\ \left(\boldsymbol{L}^{(N)}\right)^T \end{bmatrix}_{N \times K} \\
\end{aligned}
\end{split}\]</div>
</section>
</section>
<section id="assumption-2-the-latent-variable-boldsymbol-z">
<h3><a class="toc-backref" href="#id46" role="doc-backlink">Assumption 2: The Latent Variable <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span></a><a class="headerlink" href="#assumption-2-the-latent-variable-boldsymbol-z" title="Link to this heading">#</a></h3>
<p>We have discussed about the likelihood of a single data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> belonging to a specific cluster <span class="math notranslate nohighlight">\(C_k\)</span> (<span class="math notranslate nohighlight">\(z^{(n)} = k\)</span>). The next logical question is to ask: what is the probability distribution of <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span>?</p>
<p>Similar to the feature vectors <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span>, the cluster assignment <span class="math notranslate nohighlight">\(z^{(n)}\)</span> can also be
interpreted as realization drawn from a <strong>latent</strong> discrete random variable <span class="math notranslate nohighlight">\(Z\)</span>.</p>
<section id="the-prior-distribution-of-boldsymbol-z">
<h4><a class="toc-backref" href="#id47" role="doc-backlink">The Prior Distribution of <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span></a><a class="headerlink" href="#the-prior-distribution-of-boldsymbol-z" title="Link to this heading">#</a></h4>
<p>In contrast to the feature vectors <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span>, we do not observe (know) the true cluster indices <span class="math notranslate nohighlight">\(z^{(n)}\)</span>. After all, the goal of soft clustering is to estimate the cluster indices <span class="math notranslate nohighlight">\(z^{(n)}\)</span>. We obtain a soft clustering
method by estimating the cluster indices <span class="math notranslate nohighlight">\(z^{(n)}\)</span> based solely on the data points in <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>. To compute these estimates we assume that the (true) cluster indices <span class="math notranslate nohighlight">\(z^{(n)}\)</span> are realizations of iid RVs with the common probability distribution (or probability mass function):</p>
<div class="math notranslate nohighlight">
\[
\pi_k := \mathbb{P}\left(Z^{(n)} = k ; \boldsymbol{\pi}\right) \quad \text{for } k = 1, \ldots, K.
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\pi} = \begin{bmatrix} \pi_1 &amp; \pi_2 &amp; \ldots &amp; \pi_K \end{bmatrix}^{\mathrm{T}}\)</span> is a <span class="math notranslate nohighlight">\(K\)</span>-dimensional vector of probabilities. It is also common to denote the prior distribution as a one-hot vector.</p>
<p>As mentioned in the previous step, one will soon realize that this is the <strong>prior</strong> in a Bayes model.
With this, we have answered the question of what the probability distribution of <span class="math notranslate nohighlight">\(Z\)</span> is.</p>
<p>The (prior) probabilities are either assumed known or estimated from data. The choice for the probabilities <span class="math notranslate nohighlight">\(\pi_k\)</span> could reflect some prior knowledge about different sizes of the clusters. For example, if cluster <span class="math notranslate nohighlight">\(C_1\)</span> is known to be larger than cluster <span class="math notranslate nohighlight">\(C_2\)</span>, we might choose the prior probabilities such that <span class="math notranslate nohighlight">\(\pi_1 &gt; \pi_2\)</span> <span id="id7">[<a class="reference internal" href="../../bibliography.html#id40" title="Alexander Jung. Machine learning: The basics. Springer Nature Singapore, 2023.">Jung, 2023</a>]</span>.</p>
</section>
<section id="the-categorical-distribution">
<h4><a class="toc-backref" href="#id48" role="doc-backlink">The Categorical Distribution</a><a class="headerlink" href="#the-categorical-distribution" title="Link to this heading">#</a></h4>
<p>Letâ€™s now discuss the probability distribution of the latent variable <span class="math notranslate nohighlight">\(Z\)</span>.</p>
<div class="proof definition admonition" id="categorical-distribution-gmm">
<p class="admonition-title"><span class="caption-number">Definition 53 </span> (Categorical Distribution)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(Z\)</span> be a discrete random variable with <span class="math notranslate nohighlight">\(K\)</span> number of states.
Then <span class="math notranslate nohighlight">\(Z\)</span> follows a categorical distribution with parameters <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> if</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(Z = k) = \pi_k \quad \text{for } k = 1, 2, \cdots, K
\]</div>
<p>Consequently, the PMF of the categorical distribution is defined more compactly as,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(Z = k) = \prod_{k=1}^K \pi_k^{I\{Z = k\}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(I\{Z = k\}\)</span> is the indicator function that is equal to 1 if <span class="math notranslate nohighlight">\(Z = k\)</span> and 0 otherwise.</p>
</section>
</div><p>More often, we use the <a class="reference external" href="https://en.wikipedia.org/wiki/One-hot">one-hot encoding</a> to represent the categorical distribution. The one-hot encoding is a vector of size <span class="math notranslate nohighlight">\(K\)</span> where all elements are 0 except for the <span class="math notranslate nohighlight">\(k\)</span>-th element which is 1. For example, if <span class="math notranslate nohighlight">\(K = 3\)</span>, the one-hot encoding of <span class="math notranslate nohighlight">\(k = 2\)</span> is <span class="math notranslate nohighlight">\(\mathbf{y} = \begin{bmatrix} 0 &amp; 1 &amp; 0 \end{bmatrix}^{\mathrm{T}}\)</span>.</p>
<div class="proof definition admonition" id="categorical-multinomial-distribution-gmm">
<p class="admonition-title"><span class="caption-number">Definition 54 </span> (Categorical (Multinomial) Distribution)</p>
<section class="definition-content" id="proof-content">
<p>This formulation is adopted by Bishopâ€™s<span id="id8">[<a class="reference internal" href="../../bibliography.html#id4" title="Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer, 1 edition, 2007. ISBN 0387310738. URL: http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738%3FSubscriptionId%3D13CT5CVB80YFWJEPWS02%26tag%3Dws%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0387310738.">Bishop, 2007</a>]</span>, the categorical distribution is defined as</p>
<div class="math notranslate nohighlight" id="equation-eq-categorical-distribution-bishop-gmm">
<span class="eqno">(97)<a class="headerlink" href="#equation-eq-categorical-distribution-bishop-gmm" title="Link to this equation">#</a></span>\[
\mathbb{P}(\mathbf{Z} = \mathbf{z}; \boldsymbol{\pi}) = \prod_{k=1}^K \pi_k^{z_k}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{z} = \begin{bmatrix} z_1 \\ z_2 \\ \vdots \\ z_K \end{bmatrix}
\end{split}\]</div>
<p>is an one-hot encoded vector of size <span class="math notranslate nohighlight">\(K\)</span>,</p>
<p>The <span class="math notranslate nohighlight">\(z_k\)</span> is the <span class="math notranslate nohighlight">\(k\)</span>-th element of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, and is equal to 1 if <span class="math notranslate nohighlight">\(Y = k\)</span> and 0 otherwise.
The <span class="math notranslate nohighlight">\(\pi_k\)</span> is the <span class="math notranslate nohighlight">\(k\)</span>-th element of <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>, and is the probability of <span class="math notranslate nohighlight">\(\mathbf{Z} = k\)</span>.</p>
<p>This notation alongside with the indicator notation in the previous definition allows us to manipulate
the <a class="reference external" href="https://en.wikipedia.org/wiki/Likelihood_function">likelihood function</a> in a more compact way.</p>
</section>
</div></section>
<section id="prior-distribution-of-the-entire-dataset-mathcal-s">
<h4><a class="toc-backref" href="#id49" role="doc-backlink">Prior Distribution of the Entire Dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span></a><a class="headerlink" href="#prior-distribution-of-the-entire-dataset-mathcal-s" title="Link to this heading">#</a></h4>
<p>The prior distribution <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> is shared by all the data points in <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>.</p>
<ul>
<li><p>Let <span class="math notranslate nohighlight">\(z^{(n)}\)</span> denote the latent variable corresponding to the <span class="math notranslate nohighlight">\(n\)</span>-th data point, representing the Gaussian component it belongs to. <span class="math notranslate nohighlight">\(z^{(n)}\)</span> can take on values <span class="math notranslate nohighlight">\(1, \dots, K\)</span>, where <span class="math notranslate nohighlight">\(K\)</span> is the number of Gaussian components.</p></li>
<li><p>The prior probability of the <span class="math notranslate nohighlight">\(k\)</span>-th Gaussian component can be denoted as <span class="math notranslate nohighlight">\(P(Z^{(n)} = k)\)</span>, for <span class="math notranslate nohighlight">\(k = 1, \dots, K\)</span>.</p></li>
<li><p>These probabilities can be represented as a vector</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \boldsymbol{\pi} = \begin{bmatrix} p(z^{(n)} = 1) \\ p(z^{(n)} = 2) \\ \vdots \\ p(z^{(n)} = K) \end{bmatrix}_{K \times 1} = \begin{bmatrix} \pi_1 \\ \pi_2 \\ \vdots \\ \pi_K \end{bmatrix}_{K \times 1}
    \end{split}\]</div>
</li>
<li><p>The sum of all prior probabilities should be equal to 1, as they represent probabilities: <span class="math notranslate nohighlight">\(\sum_{k=1}^K p(z^{(n)} = k) = 1\)</span>.</p></li>
</ul>
<p>In the context of GMM, the prior probabilities can be interpreted as the probability that a randomly chosen sample belongs to the <span class="math notranslate nohighlight">\(k\)</span>-th Gaussian component.</p>
<p>Note that this prior is a global one shared across all data points. In other words, the prior probability of a data point belonging to a Gaussian component is the same as the prior probability of any other data point belonging to the same Gaussian component.</p>
</section>
</section>
<section id="assumption-3-the-joint-distribution-of-boldsymbol-x-n-and-boldsymbol-z-n">
<h3><a class="toc-backref" href="#id50" role="doc-backlink">Assumption 3: The Joint Distribution of <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{z}^{(n)}\)</span></a><a class="headerlink" href="#assumption-3-the-joint-distribution-of-boldsymbol-x-n-and-boldsymbol-z-n" title="Link to this heading">#</a></h3>
<p>So far, what have we gotten? We have defined two distributions, one is the likelihood of
observation <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> given the cluster assignment <span class="math notranslate nohighlight">\(z^{(n)}\)</span> and the other is the prior
distribution of the cluster assignment <span class="math notranslate nohighlight">\(z^{(n)}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{x}^{(n)} \mid z^{(n)}=k &amp;\sim \mathcal{N}(\boldsymbol{x} ; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) &amp;&amp; \text{for } k = 1, \ldots, K \\
z^{(n)}=k &amp;\sim \text{Cat}(\boldsymbol{\pi}) &amp;&amp; \text{for } k = 1, \ldots, K.
\end{aligned}
\end{split}\]</div>
<p>Now, recall that when the likelihood and prior are
multiplied together, we obtain the <strong>joint distribution</strong> of the data and the cluster assignment,
as follows:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\left(z^{(n)}=k, \boldsymbol{x}^{(n)}\right) &amp;\sim \text{Cat}(\boldsymbol{\pi}) \times \mathcal{N}(\boldsymbol{x} ; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \quad \text{for } k = 1, \ldots, K.
\end{aligned}
\]</div>
<p>which is equivalent to the following:</p>
<div class="math notranslate nohighlight">
\[
\overbrace{\mathbb{P}\left(\boldsymbol{X}^{(n)} = \boldsymbol{x}^{(n)}, Z^{(n)} = k ; \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}\right)}^{\text{joint distribution}} = \overbrace{\mathbb{P}\left(Z^{(n)} = k ; \boldsymbol{\pi}\right)}^{\text{prior}=\text{Cat}(\boldsymbol{\pi})}\overbrace{\mathbb{P}\left(\boldsymbol{X}^{(n)} = \boldsymbol{x}^{(n)} \mid Z^{(n)} = k ; \boldsymbol{\mu}, \boldsymbol{\Sigma}\right)}^{\text{likelihood}=\mathcal{N}(\boldsymbol{x} ; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{X}^{(n)} = \boldsymbol{x}^{(n)} \mid Z^{(n)} = k \sim \mathcal{N}(\boldsymbol{x} ; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \)</span> is the probability distribution of the data points <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> given the cluster assignment <span class="math notranslate nohighlight">\(z^{(n)}=k\)</span>. This is also known as the mixture component.</p></li>
<li><p><span class="math notranslate nohighlight">\(Z = k \sim \text{Cat}(\boldsymbol{\pi})\)</span> is the probability distribution of the cluster assignment <span class="math notranslate nohighlight">\(z^{(n)}=k\)</span>, also known as the mixing coefficient <span class="math notranslate nohighlight">\(\pi_k\)</span>.</p></li>
</ul>
<section id="why-is-the-joint-distribution-the-product-of-the-likelihood-and-prior">
<h4><a class="toc-backref" href="#id51" role="doc-backlink">Why is the Joint Distribution the Product of the Likelihood and Prior?</a><a class="headerlink" href="#why-is-the-joint-distribution-the-product-of-the-likelihood-and-prior" title="Link to this heading">#</a></h4>
<p>One question that might come to mind is why the joint distribution is the product of the likelihood and prior. The answer is that the joint distribution is the product of the likelihood and prior because of the <a class="reference external" href="https://en.wikipedia.org/wiki/Chain_rule_(probability)">chain rule</a>.</p>
<p>Letâ€™s just state the base case to see why.</p>
<p>By Bayesâ€™ rule, we have:</p>
<div class="math notranslate nohighlight">
\[
P(A | B) = \frac{P(A \cap B)}{P(B)} \implies P(A \cap B) = P(A | B) P(B) \implies P(A ,B) = P(B | A) P(A)
\]</div>
<p>and if we set <span class="math notranslate nohighlight">\(A = x^{(n)}\)</span> and <span class="math notranslate nohighlight">\(B = z^{(n)} = k\)</span>, then we have:</p>
<div class="math notranslate nohighlight">
\[
P(x^{(n)} | z^{(n)} = k) = \frac{P(x^{(n)}, z^{(n)} = k)}{P(z^{(n)} = k)} \implies P(x^{(n)} ,z^{(n)} = k) = P(x^{(n)} | z^{(n)} = k) P(z^{(n)} = k)
\]</div>
<p>and <span class="math notranslate nohighlight">\(\cap\)</span> is the intersection symbol, so we have the joint probability of the data point <span class="math notranslate nohighlight">\(x^{(n)}\)</span> and the latent label <span class="math notranslate nohighlight">\(z^{(n)} = k\)</span>.</p>
</section>
<section id="weighted-likelihood">
<h4><a class="toc-backref" href="#id52" role="doc-backlink">Weighted Likelihood</a><a class="headerlink" href="#weighted-likelihood" title="Link to this heading">#</a></h4>
<p>Recall that we defined the prior, <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>, as the probability of a data point belonging to a Gaussian component, and the likelihood as the probability of a data point given the Gaussian component it belongs to, <span class="math notranslate nohighlight">\(\mathcal{N}(\boldsymbol{x} ; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)\)</span>.</p>
<p>Then we have just seen that the joint distribution of the data and the cluster assignment is the product of the prior and the likelihood. This is also known as the <strong>weighted likelihood</strong>.</p>
<p>The weighted likelihood is the joint probability of the data point <span class="math notranslate nohighlight">\(x^{(n)}\)</span> and the latent label <span class="math notranslate nohighlight">\(z^{(n)} = k\)</span>. Basically it answers the question: â€œWhat is the probability of observing the data point <span class="math notranslate nohighlight">\(x^{(n)}\)</span> and the latent label <span class="math notranslate nohighlight">\(z^{(n)} = k\)</span>?â€</p>
<p>To see why, first consider how we define the <strong>weighted likelihood</strong>:</p>
<div class="math notranslate nohighlight">
\[
P(x^{(n)}, z^{(n)} = k) = P(x^{(n)} | z^{(n)} = k) P(z^{(n)} = k)
\]</div>
<p>where <span class="math notranslate nohighlight">\(P(x^{(n)} | z^{(n)} = k)\)</span> is the likelihood of the <span class="math notranslate nohighlight">\(n\)</span>-th data point belonging to the <span class="math notranslate nohighlight">\(k\)</span>-th Gaussian component, and <span class="math notranslate nohighlight">\(P(z^{(n)} = k)\)</span> is the prior probability of the <span class="math notranslate nohighlight">\(k\)</span>-th Gaussian component.</p>
<p>First, some intuition, it is called weighted because the likelihood is weighted by the prior probability of the latent label <span class="math notranslate nohighlight">\(z^{(n)} = k\)</span>. In other words, if we have likelihoods <span class="math notranslate nohighlight">\(P(x^{(n)} | z^{(n)} = 2)\)</span> for <span class="math notranslate nohighlight">\(k=2\)</span>
to be say <span class="math notranslate nohighlight">\(0.2\)</span> and the prior probability of <span class="math notranslate nohighlight">\(z^{(n)} = 2\)</span> to be <span class="math notranslate nohighlight">\(0.9\)</span>, then the weighted likelihood is <span class="math notranslate nohighlight">\(0.2 \times 0.9 = 0.18\)</span> because we have super high confidence that the data point <span class="math notranslate nohighlight">\(x^{(n)}\)</span> belongs to the <span class="math notranslate nohighlight">\(k=2\)</span> Gaussian component. However, if the prior probability of <span class="math notranslate nohighlight">\(z^{(n)} = 2\)</span> is <span class="math notranslate nohighlight">\(0.1\)</span>, then the weighted likelihood is <span class="math notranslate nohighlight">\(0.2 \times 0.1 = 0.02\)</span> because we have low confidence that the data point <span class="math notranslate nohighlight">\(x^{(n)}\)</span> belongs to the <span class="math notranslate nohighlight">\(k=2\)</span> Gaussian component so the â€œlikelihoodâ€ got weighed down by the low prior probability.</p>
</section>
<section id="weighted-likelihood-of-one-single-data-point-boldsymbol-x-n">
<h4><a class="toc-backref" href="#id53" role="doc-backlink">Weighted Likelihood of One Single Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span></a><a class="headerlink" href="#weighted-likelihood-of-one-single-data-point-boldsymbol-x-n" title="Link to this heading">#</a></h4>
<p>The weighted likelihood of a single data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> is obtained by multiplying the likelihood of the data point belonging to each Gaussian component by the corresponding mixing coefficient (weight) of that component. Let <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> be the vector of mixing coefficients, with <span class="math notranslate nohighlight">\(\pi_k\)</span> representing the weight of the <span class="math notranslate nohighlight">\(k\)</span>-th Gaussian component. Then, the weighted likelihood of the data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> can be written as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{W}^{(n)} &amp;= \begin{bmatrix} \pi_1 p(x^{(n)} | z^{(n)} = 1) \\ \pi_2 p(x^{(n)} | z^{(n)} = 2) \\ \vdots \\ \pi_K p(x^{(n)} | z^{(n)} = K) \end{bmatrix}_{K \times 1} \\
&amp;= \begin{bmatrix} \pi_1 \mathcal{N}(x^{(n)} | \boldsymbol{\mu}_1, \boldsymbol{\Sigma}_1) \\ \pi_2 \mathcal{N}(x^{(n)} | \boldsymbol{\mu}_2, \boldsymbol{\Sigma}_2) \\ \vdots \\ \pi_K \mathcal{N}(x^{(n)} | \boldsymbol{\mu}_K, \boldsymbol{\Sigma}_K) \end{bmatrix}_{K \times 1} \\
&amp;= \begin{bmatrix} W_1^{(n)} \\ W_2^{(n)} \\ \vdots \\ W_K^{(n)} \end{bmatrix}_{K \times 1}
\end{aligned}
\end{split}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\boldsymbol{W}^{(n)}\)</span> is the vector of weighted likelihoods of the data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> belonging to each of the <span class="math notranslate nohighlight">\(K\)</span> Gaussian components, and <span class="math notranslate nohighlight">\(W_k^{(n)}\)</span> represents the weighted likelihood of the data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> belonging to the <span class="math notranslate nohighlight">\(k\)</span>-th Gaussian component.</p>
</section>
<section id="weighted-likelihood-of-the-entire-dataset-boldsymbol-x">
<h4><a class="toc-backref" href="#id54" role="doc-backlink">Weighted Likelihood of the Entire Dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span></a><a class="headerlink" href="#weighted-likelihood-of-the-entire-dataset-boldsymbol-x" title="Link to this heading">#</a></h4>
<p>To compute the weighted likelihood of the entire dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, we need to calculate the weighted likelihood for each data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> and then combine them. For this purpose, we can represent the weighted likelihood of the entire dataset as a matrix <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span> of size <span class="math notranslate nohighlight">\(N \times K\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the number of data points and <span class="math notranslate nohighlight">\(K\)</span> is the number of Gaussian components:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{W} = \begin{bmatrix}
W_1^{(1)} &amp; W_2^{(1)} &amp; \cdots &amp; W_K^{(1)} \\
W_1^{(2)} &amp; W_2^{(2)} &amp; \cdots &amp; W_K^{(2)} \\
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
W_1^{(N)} &amp; W_2^{(N)} &amp; \cdots &amp; W_K^{(N)}
\end{bmatrix}_{N \times K}
\end{split}\]</div>
<p>Each row of the matrix <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span> corresponds to the weighted likelihood vector <span class="math notranslate nohighlight">\(\boldsymbol{W}^{(n)}\)</span> for a data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span>. To obtain the weighted likelihood of the entire dataset, we can either sum or compute the product of all elements in the matrix <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span>, depending on the desired objective (e.g., maximizing the log-likelihood).</p>
<p>Now the returned is a matrix of shape <span class="math notranslate nohighlight">\((N, K)\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the number of data points and <span class="math notranslate nohighlight">\(K\)</span> is the number of Gaussian components. The <span class="math notranslate nohighlight">\(n\)</span>-th row and <span class="math notranslate nohighlight">\(k\)</span>-th column element is the weighted likelihood of the <span class="math notranslate nohighlight">\(n\)</span>-th data point belonging to the <span class="math notranslate nohighlight">\(k\)</span>-th Gaussian component.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{W} &amp;= \begin{bmatrix} p(z^{(1)} = 1) p(x^{(1)} \mid z^{(1)} = 1) &amp; p(z^{(1)} = 2) p(x^{(1)} \mid z^{(1)} = 2) &amp; \cdots &amp; p(z^{(1)} = K) p(x^{(1)} \mid z^{(1)} = K) \\ p(z^{(2)} = 1) p(x^{(2)} \mid z^{(2)} = 1) &amp; p(z^{(2)} = 2) p(x^{(2)} \mid z^{(2)} = 2) &amp; \cdots &amp; p(z^{(2)} = K) p(x^{(2)} \mid z^{(2)} = K) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ p(z^{(N)} = 1) p(x^{(N)} \mid z^{(N)} = 1) &amp; p(z^{(N)} = 2) p(x^{(N)} \mid z^{(N)} = 2) &amp; \cdots &amp; p(z^{(N)} = K) p(x^{(N)} \mid z^{(N)} = K) \end{bmatrix}_{N \times K} \\
&amp;= \begin{bmatrix} \pi_1 \mathcal{N}(x^{(1)} | \boldsymbol{\mu}_1, \boldsymbol{\Sigma}_1) &amp; \pi_2 \mathcal{N}(x^{(1)} | \boldsymbol{\mu}_2, \boldsymbol{\Sigma}_2) &amp; \cdots &amp; \pi_K \mathcal{N}(x^{(1)} | \boldsymbol{\mu}_K, \boldsymbol{\Sigma}_K) \\ \pi_1 \mathcal{N}(x^{(2)} | \boldsymbol{\mu}_1, \boldsymbol{\Sigma}_1) &amp; \pi_2 \mathcal{N}(x^{(2)} | \boldsymbol{\mu}_2, \boldsymbol{\Sigma}_2) &amp; \cdots &amp; \pi_K \mathcal{N}(x^{(2)} | \boldsymbol{\mu}_K, \boldsymbol{\Sigma}_K) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \pi_1 \mathcal{N}(x^{(N)} | \boldsymbol{\mu}_1, \boldsymbol{\Sigma}_1) &amp; \pi_2 \mathcal{N}(x^{(N)} | \boldsymbol{\mu}_2, \boldsymbol{\Sigma}_2) &amp; \cdots &amp; \pi_K \mathcal{N}(x^{(N)} | \boldsymbol{\mu}_K, \boldsymbol{\Sigma}_K) \end{bmatrix}_{N \times K} \\
&amp;= \begin{bmatrix} p(x^{(1)} ,z^{(1)} = 1) &amp; p(x^{(1)} ,z^{(1)} = 2) &amp; \cdots &amp; p(x^{(1)} ,z^{(1)} = K) \\ p(x^{(2)} ,z^{(2)} = 1) &amp; p(x^{(2)} ,z^{(2)} = 2) &amp; \cdots &amp; p(x^{(2)} ,z^{(2)} = K) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ p(x^{(N)} ,z^{(N)} = 1) &amp; p(x^{(N)} ,z^{(N)} = 2) &amp; \cdots &amp; p(x^{(N)} ,z^{(N)} = K) \end{bmatrix}_{N \times K}
\end{aligned}
\end{split}\]</div>
<p>In code, we need to separate the weighted likelihood matrix <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span> into two matrices,
as follows:</p>
<ol class="arabic simple">
<li><p>Mixing coefficients matrix, <span class="math notranslate nohighlight">\(\boldsymbol{\Pi}\)</span>, of shape <span class="math notranslate nohighlight">\((N \times K)\)</span>, where each row contains the mixing coefficients <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> repeated for each data point:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\Pi} = \begin{bmatrix} \pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_K \\ \pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_K \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_K \end{bmatrix}_{N \times K}
\end{split}\]</div>
<ol class="arabic simple" start="2">
<li><p>Likelihood matrix, <span class="math notranslate nohighlight">\(\boldsymbol{L}\)</span>, of shape <span class="math notranslate nohighlight">\((N \times K)\)</span>, where each element <span class="math notranslate nohighlight">\((i, j)\)</span> represents the likelihood of the <span class="math notranslate nohighlight">\(i\)</span>-th data point belonging to the <span class="math notranslate nohighlight">\(j\)</span>-th Gaussian component:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{L} = \begin{bmatrix} \mathcal{N}(x^{(1)} | \boldsymbol{\mu}_1, \boldsymbol{\Sigma}_1) &amp; \mathcal{N}(x^{(1)} | \boldsymbol{\mu}_2, \boldsymbol{\Sigma}_2) &amp; \cdots &amp; \mathcal{N}(x^{(1)} | \boldsymbol{\mu}_K, \boldsymbol{\Sigma}_K) \\ \mathcal{N}(x^{(2)} | \boldsymbol{\mu}_1, \boldsymbol{\Sigma}_1) &amp; \mathcal{N}(x^{(2)} | \boldsymbol{\mu}_2, \boldsymbol{\Sigma}_2) &amp; \cdots &amp; \mathcal{N}(x^{(2)} | \boldsymbol{\mu}_K, \boldsymbol{\Sigma}_K) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \mathcal{N}(x^{(N)} | \boldsymbol{\mu}_1, \boldsymbol{\Sigma}_1) &amp; \mathcal{N}(x^{(N)} | \boldsymbol{\mu}_2, \boldsymbol{\Sigma}_2) &amp; \cdots &amp; \mathcal{N}(x^{(N)} | \boldsymbol{\mu}_K, \boldsymbol{\Sigma}_K) \end{bmatrix}_{N \times K}
\end{split}\]</div>
<p>Now, you can obtain the weighted likelihood matrix <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span> by performing element-wise multiplication (Hadamard product) of the mixing coefficients matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Pi}\)</span> and the likelihood matrix <span class="math notranslate nohighlight">\(\boldsymbol{L}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{W} = \boldsymbol{\Pi} \odot \boldsymbol{L}
\]</div>
</section>
</section>
<section id="joint-distribution-fully-determines-the-model">
<h3><a class="toc-backref" href="#id55" role="doc-backlink">Joint Distribution Fully Determines the Model</a><a class="headerlink" href="#joint-distribution-fully-determines-the-model" title="Link to this heading">#</a></h3>
<p>With the joint distribution defined, the model is fully determined. Why do we say so?
Because the joint distribution of the data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> and the latent variable <span class="math notranslate nohighlight">\(z^{(n)}\)</span> is fully determined by the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, which are the model parameters.</p>
<p>Consequently, if we want to find the marginal, we need to integrate out the latent variable <span class="math notranslate nohighlight">\(z^{(n)}\)</span> from the joint distribution. Then subsequently, we can also find the posterior distribution of the latent variable <span class="math notranslate nohighlight">\(z^{(n)}\)</span> by using Bayesâ€™ rule. Therefore, when we say the joint distribution fully determines
the model, what is really means is that we have all the necessary tools to find anything related
to the random variables <span class="math notranslate nohighlight">\(z^{(n)}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span>.</p>
</section>
<section id="the-gaussian-mixture-model-and-the-marginal-distribution">
<h3><a class="toc-backref" href="#id56" role="doc-backlink">The Gaussian Mixture Model and the Marginal Distribution</a><a class="headerlink" href="#the-gaussian-mixture-model-and-the-marginal-distribution" title="Link to this heading">#</a></h3>
<section id="id9">
<h4><a class="toc-backref" href="#id57" role="doc-backlink">The Gaussian Mixture Model</a><a class="headerlink" href="#id9" title="Link to this heading">#</a></h4>
<p>Recall that we defined our Gaussian Mixture Model as a linear combination of <span class="math notranslate nohighlight">\(K\)</span> multivariate Gaussian distributions:</p>
<div class="math notranslate nohighlight">
\[
\overbrace{\mathbb{P}\left(\boldsymbol{X}^{(n)} = \boldsymbol{x}^{(n)} ; \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}\right)}^{\text{marginal distribution}} = \sum_{k=1}^K \overbrace{\mathbb{P}\left(Z^{(n)} = k ; \boldsymbol{\pi}\right)}^{\text{prior}=\text{Cat}(\boldsymbol{\pi})}\overbrace{\mathbb{P}\left(\boldsymbol{X}^{(n)} = \boldsymbol{x}^{(n)} \mid Z^{(n)} = k ; \boldsymbol{\mu}, \boldsymbol{\Sigma}\right)}^{\text{likelihood}=\mathcal{N}(\boldsymbol{x} ; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}
\]</div>
<p>We now claim that the mixture of <span class="math notranslate nohighlight">\(K\)</span> multivariate Gaussian distributions is a valid distribution,
and it is none other than the marginal distribution of <span class="math notranslate nohighlight">\(\boldsymbol{X}^{(n)}\)</span>.</p>
</section>
<section id="the-marginal-distribution">
<h4><a class="toc-backref" href="#id58" role="doc-backlink">The Marginal Distribution</a><a class="headerlink" href="#the-marginal-distribution" title="Link to this heading">#</a></h4>
<p>We go back to fundamentals and ask what is the marginal distribution of a random variable <span class="math notranslate nohighlight">\(X\)</span>?</p>
<p>In our setting, it is the probability of the data point <span class="math notranslate nohighlight">\(x^{(n)}\)</span>. Basically it
answers the question: â€œWhat is the probability of observing the data point <span class="math notranslate nohighlight">\(x^{(n)}\)</span>?â€</p>
<p>Since <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> is a <span class="math notranslate nohighlight">\(D\)</span>-dimensional vector, we can think of it as a point in a <span class="math notranslate nohighlight">\(D\)</span>-dimensional space. The marginal distribution is the probability of observing this point in this space.
Since it is in high dimensions usually, <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> is usually a point in a high-dimensional space,
and hence follow a multi-variate distribution.</p>
<p>The <strong>marginal distribution</strong> of the data points <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-cluster-gmm-final">
<span class="eqno">(98)<a class="headerlink" href="#equation-eq-cluster-gmm-final" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\overbrace{p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}\right)}^{\text{marginal distribution}} &amp;= \sum_{k=1}^K p\left(z^{(n)} = k ; \boldsymbol{\pi}\right) p\left(\boldsymbol{x}^{(n)} \mid z^{(n)} = k ; \boldsymbol{\mu}, \boldsymbol{\Sigma}\right) \\
&amp;= \sum_{k=1}^K \pi_k \mathcal{N}(\boldsymbol{x}^{(n)} ; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \\
\end{aligned}
\end{split}\]</div>
<p>where this is the actual <strong>Gaussian Mixture Model</strong> that we are trying to fit to the data.</p>
<p>One question is how do we get this marginal distribution? We can get it by marginalizing out the latent variable <span class="math notranslate nohighlight">\(z^{(n)}\)</span> from the joint distribution. And what does it mean by â€œmarginalizing outâ€ the latent variable <span class="math notranslate nohighlight">\(z^{(n)}\)</span>? This concept is tied to the concept of conditional probability and the law of total probability.</p>
</section>
<section id="marginalizing-out-the-latent-variable">
<h4><a class="toc-backref" href="#id59" role="doc-backlink">Marginalizing Out the Latent Variable</a><a class="headerlink" href="#marginalizing-out-the-latent-variable" title="Link to this heading">#</a></h4>
<p>Recall marginal distribution is none other than the denominator of the posterior distribution in Bayesâ€™ rule:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbb{P}(Y \mid X) = \frac{\mathbb{P}(X \mid Y) \mathbb{P}(Y)}{\mathbb{P}(X)}.
\end{aligned}
\]</div>
<p>and the denominator is called the marginal distribution. The expansion of the denominator
as a summation of the numerator uses the <a class="reference external" href="https://en.wikipedia.org/wiki/Law_of_total_probability">law of total probability</a>.</p>
<p>In other words, to marginalize out the latent variable <span class="math notranslate nohighlight">\(Z\)</span>, we can simply sum over all possible values of <span class="math notranslate nohighlight">\(Z\)</span>.
As a result, we get a mixture of <span class="math notranslate nohighlight">\(K\)</span> multivariate Gaussian distributions, where
each Gaussian distribution is defined by the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_k\)</span>
and its corresponding mixing coefficient <span class="math notranslate nohighlight">\(\pi_k\)</span>.</p>
<p>Thus, we have concluded in defining a systematic distribution in which our data points <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span>
come from. This is the <strong>Gaussian Mixture Model</strong>.</p>
<p>But more is to come, because estimating the parameters is not simple, there is no closed-form solution
to the problem. And if you look closely enough, the marginal distribution depends on
both the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>.
However, unlike our classification problem with true labels <span class="math notranslate nohighlight">\(y\)</span>, we do not have
access to the true labels <span class="math notranslate nohighlight">\(z\)</span> in this case. We only have access to the data points <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.
This is a problem because we can no longer â€œestimateâ€ the empirical distribution of
<span class="math notranslate nohighlight">\(z\)</span> by simply counting the number of occurrences of each <span class="math notranslate nohighlight">\(z\)</span> in the dataset. But hope is not lose
as we can make use of the <strong>expectation-maximization (EM) algorithm</strong> to solve this problem.</p>
</section>
<section id="marginal-of-one-single-data-point-boldsymbol-x-n">
<h4><a class="toc-backref" href="#id60" role="doc-backlink">Marginal of One Single Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span></a><a class="headerlink" href="#marginal-of-one-single-data-point-boldsymbol-x-n" title="Link to this heading">#</a></h4>
<p>The marginal of a single data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> is obtained by summing the weighted likelihoods of the data point belonging to each Gaussian component. Mathematically, it can be written as:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{M}^{(n)} = \begin{bmatrix} p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}\right) \end{bmatrix} = \begin{bmatrix} \sum_{k=1}^K \pi_k \mathcal{N}(\boldsymbol{x}^{(n)} ; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \end{bmatrix}
\]</div>
</section>
<section id="marginal-of-the-entire-dataset-boldsymbol-x">
<h4><a class="toc-backref" href="#id61" role="doc-backlink">Marginal of the Entire Dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span></a><a class="headerlink" href="#marginal-of-the-entire-dataset-boldsymbol-x" title="Link to this heading">#</a></h4>
<p>The marginal of the entire dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> is collated as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{M} = \begin{bmatrix} \boldsymbol{M}^{(1)} \\ \vdots \\ \boldsymbol{M}^{(N)} \end{bmatrix} = \begin{bmatrix} \sum_{k=1}^K \pi_k \mathcal{N}(\boldsymbol{x}^{(1)} ; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \\ \vdots \\ \sum_{k=1}^K \pi_k \mathcal{N}(\boldsymbol{x}^{(N)} ; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \end{bmatrix}
\end{split}\]</div>
</section>
</section>
<section id="the-posterior-distribution">
<h3><a class="toc-backref" href="#id62" role="doc-backlink">The Posterior Distribution</a><a class="headerlink" href="#the-posterior-distribution" title="Link to this heading">#</a></h3>
<p>Now we can answer the posterior distribution of the cluster assignment <span class="math notranslate nohighlight">\(z^{(n)}\)</span> given the data points <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-cluster-gmm-posterior">
<span class="eqno">(99)<a class="headerlink" href="#equation-eq-cluster-gmm-posterior" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\overbrace{p\left(z^{(n)}=k \mid \mathbf{x}^{(n)} ; \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}\right)}^{\text {posterior }} &amp;= \frac{\overbrace{p\left(z^{(n)}=k ; \boldsymbol{\pi}\right)}^{\text {prior }} \cdot \overbrace{p\left(\mathbf{x}^{(n)} \mid z^{(n)}=k ; \boldsymbol{\mu}, \boldsymbol{\Sigma}\right)}^{\text {likelihood }}}{\underbrace{p\left(\mathbf{x}^{(n)} ; \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi}\right)}_{\text {marginal }}} \\
&amp;= \frac{\pi_k \mathcal{N}(\boldsymbol{x}^{(n)} ; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{k=1}^K \pi_k \mathcal{N}(\boldsymbol{x}^{(n)} ; \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)} \\
\end{aligned}
\end{split}\]</div>
<p>This is the degree of belonging is none other than the posterior distribution!</p>
<section id="posterior-of-one-single-data-point-boldsymbol-x-n">
<h4><a class="toc-backref" href="#id63" role="doc-backlink">Posterior of One Single Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span></a><a class="headerlink" href="#posterior-of-one-single-data-point-boldsymbol-x-n" title="Link to this heading">#</a></h4>
<p>Using the posterior distribution equation <a class="reference internal" href="#equation-eq-cluster-gmm-posterior">(99)</a>, we can calculate the posterior probability of a single data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> belonging to each of the <span class="math notranslate nohighlight">\(K\)</span> Gaussian components. The result is a vector of size <span class="math notranslate nohighlight">\(K \times 1\)</span>, where the <span class="math notranslate nohighlight">\(k\)</span>-th element represents the posterior probability of the data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> belonging to the <span class="math notranslate nohighlight">\(k\)</span>-th Gaussian component:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{R}^{(n)}=\left[\begin{array}{c}
p\left(z^{(n)}=1 \mid \boldsymbol{x}^{(n)}\right) \\
p\left(z^{(n)}=2 \mid \boldsymbol{x}^{(n)}\right) \\
\vdots \\
p\left(z^{(n)}=K \mid \boldsymbol{x}^{(n)}\right)
\end{array}\right]_{K \times 1}
\end{split}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\boldsymbol{R}^{(n)}\)</span> is the posterior probability vector for the data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span>.</p>
<p>There is a reason we use <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span> instead of say <span class="math notranslate nohighlight">\(\boldsymbol{P}\)</span> for shorthand
representation of the posterior probability vector. The reason is that <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span> is
also known as the <strong>responsibility</strong> of the <span class="math notranslate nohighlight">\(k\)</span>-th Gaussian component for the data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span>, which we will see later.</p>
</section>
<section id="posterior-of-the-entire-dataset-boldsymbol-x">
<h4><a class="toc-backref" href="#id64" role="doc-backlink">Posterior of the Entire Dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span></a><a class="headerlink" href="#posterior-of-the-entire-dataset-boldsymbol-x" title="Link to this heading">#</a></h4>
<p>To compute the posterior probability of the entire dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, we need to calculate the posterior probability for each data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> and then combine them. For this purpose, we can represent the posterior probability of the entire dataset as a matrix <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span> of size <span class="math notranslate nohighlight">\(N \times K\)</span>, where <span class="math notranslate nohighlight">\(N\)</span> is the number of data points and <span class="math notranslate nohighlight">\(K\)</span> is the number of Gaussian components:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{R} &amp;= \begin{bmatrix} p\left(z^{(1)}=1 \mid \boldsymbol{x}^{(1)}\right) &amp; p\left(z^{(1)}=2 \mid \boldsymbol{x}^{(1)}\right) &amp; \cdots &amp; p\left(z^{(1)}=K \mid \boldsymbol{x}^{(1)}\right) \\ p\left(z^{(2)}=1 \mid \boldsymbol{x}^{(2)}\right) &amp; p\left(z^{(2)}=2 \mid \boldsymbol{x}^{(2)}\right) &amp; \cdots &amp; p\left(z^{(2)}=K \mid \boldsymbol{x}^{(2)}\right) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ p\left(z^{(N)}=1 \mid \boldsymbol{x}^{(N)}\right) &amp; p\left(z^{(N)}=2 \mid \boldsymbol{x}^{(N)}\right) &amp; \cdots &amp; p\left(z^{(N)}=K \mid \boldsymbol{x}^{(N)}\right) \end{bmatrix} \\
\end{aligned}
\end{split}\]</div>
<p>Each row of the matrix <span class="math notranslate nohighlight">\(\boldsymbol{P}\)</span> corresponds to the posterior probability vector <span class="math notranslate nohighlight">\(\boldsymbol{P}^{(n)}\)</span> for a data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span>. The posterior probability of the entire dataset can be used to assess the overall clustering quality, assign data points to the most probable cluster, or update the model parameters in an iterative manner (e.g., using the Expectation-Maximization algorithm).</p>
</section>
</section>
</section>
<section id="parameter-estimation-mixture-model-perspective">
<h2><a class="toc-backref" href="#id65" role="doc-backlink">Parameter Estimation (Mixture Model Perspective)</a><a class="headerlink" href="#parameter-estimation-mixture-model-perspective" title="Link to this heading">#</a></h2>
<p>Assume we are given a dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span></p>
<div class="math notranslate nohighlight">
\[
\mathcal{S}=\left\{\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(N)}\right\}
\]</div>
<p>where each data point <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span> are drawn i.i.d. from an unknown distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{D} &amp;= \mathbb{P}\left(\mathcal{X}, \mathcal{Z} ; \boldsymbol{\theta} \right) \\
            &amp;= \mathbb{P}_{\boldsymbol{\theta}}\left(\mathcal{X}, \mathcal{Z} \right) \\
            &amp;= \mathbb{P}_{\left\{\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}\right\}}\left(\mathcal{X}, \mathcal{Z} \right) \\
\end{aligned}
\end{split}\]</div>
<p>but since <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> is treated as a latent variable, we only have information to:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{D} &amp;= \mathbb{P}\left(\mathcal{X} ; \boldsymbol{\theta} \right) \\
            &amp;= \mathbb{P}_{\boldsymbol{\theta}}\left(\mathcal{X} \right) \\
            &amp;= \mathbb{P}_{\left\{\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}\right\}}\left(\mathcal{X} \right) \\
\end{aligned}
\end{split}\]</div>
<p>Our objective is to find a good approximation/representation of this unknown distribution <span class="math notranslate nohighlight">\(\mathbb{P}_{\left\{\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}\right\}}\left(\mathcal{X} \right)\)</span> by means of a GMM with <span class="math notranslate nohighlight">\(K\)</span> mixture components. The parameters of the GMM are the <span class="math notranslate nohighlight">\(K\)</span> means <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span>, the covariances <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span>, and mixture weights <span class="math notranslate nohighlight">\(\pi_{k}\)</span>. We summarize all these free parameters in the symbol <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}:=\left\{\pi_{k}, \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}: k=1, \ldots, K\right\}
\]</div>
<p>See <a class="reference external" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#The_symbols">the section here</a> for more information.</p>
<section id="the-vectorized-parameters">
<h3><a class="toc-backref" href="#id66" role="doc-backlink">The Vectorized Parameters</a><a class="headerlink" href="#the-vectorized-parameters" title="Link to this heading">#</a></h3>
<p>However, to facilitate the notation, we will use the following vectorized representation of the parameters:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}:=\left\{\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}\right\}
\]</div>
<section id="the-mixture-weights-boldsymbol-pi">
<h4><a class="toc-backref" href="#id67" role="doc-backlink">The Mixture Weights <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span></a><a class="headerlink" href="#the-mixture-weights-boldsymbol-pi" title="Link to this heading">#</a></h4>
<p><span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> is a <span class="math notranslate nohighlight">\(K\)</span>-dimensional vector of mixture weights:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\pi} = \begin{bmatrix} \pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_K \end{bmatrix}^{\mathrm{T}} \in \mathbb{R}^K
\]</div>
<p>and can be broadcasted to</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\pi} = \begin{bmatrix} \pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_K \\ \pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_K \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_K \end{bmatrix} \in \mathbb{R}^{N \times K}
\end{split}\]</div>
<p>for Hamadard product with <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span>.</p>
</section>
<section id="the-means-boldsymbol-mu">
<h4><a class="toc-backref" href="#id68" role="doc-backlink">The Means <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span></a><a class="headerlink" href="#the-means-boldsymbol-mu" title="Link to this heading">#</a></h4>
<p>These are the means of the Gaussian components in the Gaussian Mixture Model.</p>
<p>Let</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\mu}_1, \boldsymbol{\mu}_2, \dots, \boldsymbol{\mu}_K
\]</div>
<p>be the mean vectors of the Gaussian components, with</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\mu}_k = \begin{bmatrix} \mu_{k1} \\ \mu_{k2} \\ \vdots \\ \mu_{kD} \end{bmatrix}_{D \times 1} \in \mathbb{R}^D
\end{split}\]</div>
<p>being a column vector representing the mean of the <span class="math notranslate nohighlight">\(k\)</span>-th Gaussian component and <span class="math notranslate nohighlight">\(D\)</span> being the number of features.</p>
<p>Thus collating all <span class="math notranslate nohighlight">\(K\)</span> mean vectors <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_1, \boldsymbol{\mu}_2, \dots, \boldsymbol{\mu}_K\)</span> into a matrix <span class="math notranslate nohighlight">\(M\)</span> of dimensions <span class="math notranslate nohighlight">\((K, D)\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{M} = \begin{bmatrix} \boldsymbol{\mu}_1^T \\ \boldsymbol{\mu}_2^T \\ \vdots \\ \boldsymbol{\mu}_K^T \end{bmatrix}_{K \times D} = \begin{bmatrix} \mu_{11} &amp; \mu_{12} &amp; \cdots &amp; \mu_{1D} \\ \mu_{21} &amp; \mu_{22} &amp; \cdots &amp; \mu_{2D} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \mu_{K1} &amp; \mu_{K2} &amp; \cdots &amp; \mu_{KD} \end{bmatrix}_{K \times D}.
\end{split}\]</div>
</section>
<section id="the-covariances-boldsymbol-sigma">
<h4><a class="toc-backref" href="#id69" role="doc-backlink">The Covariances <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span></a><a class="headerlink" href="#the-covariances-boldsymbol-sigma" title="Link to this heading">#</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">self.covariances_</span></code>: These are the covariance matrices of the Gaussian components in the Gaussian Mixture Model. In the context of GMM, <code class="docutils literal notranslate"><span class="pre">self.covariances_</span></code> is a 3D array of shape <code class="docutils literal notranslate"><span class="pre">(num_components,</span> <span class="pre">num_features,</span> <span class="pre">num_features)</span></code>, where each â€œsliceâ€ along the first axis represents the covariance matrix of the corresponding Gaussian component. Let</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\Sigma}_1, \boldsymbol{\Sigma}_2, \dots, \boldsymbol{\Sigma}_K
\]</div>
<p>be the covariance matrices of the Gaussian components, with each <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_k\)</span> being a symmetric positive-definite matrix of dimensions <span class="math notranslate nohighlight">\((D, D)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\Sigma}_k = \begin{bmatrix} \sigma_{k11} &amp; \sigma_{k12} &amp; \cdots &amp; \sigma_{k1D} \\ \sigma_{k21} &amp; \sigma_{k22} &amp; \cdots &amp; \sigma_{k2D} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \sigma_{kD1} &amp; \sigma_{kD2} &amp; \cdots &amp; \sigma_{kDD} \end{bmatrix}.
\end{split}\]</div>
<p>The <code class="docutils literal notranslate"><span class="pre">self.covariances_</span></code> array can be represented as a tensor <span class="math notranslate nohighlight">\(\boldsymbol{C}\)</span> with dimensions <span class="math notranslate nohighlight">\((K, D, D)\)</span>, where the <span class="math notranslate nohighlight">\(k\)</span>-th â€œsliceâ€ is the covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_k\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{C} = \begin{bmatrix} \boldsymbol{\Sigma}_1 \\ \boldsymbol{\Sigma}_2 \\ \vdots \\ \boldsymbol{\Sigma}_K \end{bmatrix}_{K \times D \times D}
\end{split}\]</div>
</section>
</section>
<section id="likelihood-and-log-likelihood-of-marginal-distribution">
<h3><a class="toc-backref" href="#id70" role="doc-backlink">Likelihood and Log-Likelihood of Marginal Distribution</a><a class="headerlink" href="#likelihood-and-log-likelihood-of-marginal-distribution" title="Link to this heading">#</a></h3>
<p>As with any probabilistic model that requires parameter estimation, we need to define a likelihood function for the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>.</p>
<p>In the following, we detail how to obtain a maximum likelihood estimate <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\theta}}\)</span> of the model parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. We start by writing down the likelihood of the <strong>marginal likelihood of the observing the data</strong>, i.e., the predictive distribution of the training data given the parameters. We exploit our i.i.d. assumption, which leads to the factorized likelihood</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-likelihood-1">
<span class="eqno">(100)<a class="headerlink" href="#equation-eq-gmm-likelihood-1" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
&amp; \overbrace{p\left(\mathcal{S} = \left\{\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(n)}\right\};\boldsymbol{\theta}\right)}^{\mathcal{L}\left(\boldsymbol{\theta} \mid \mathcal{S} = \left\{\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(n)}\right\}\right)} &amp;&amp;= \prod_{n=1}^{N} p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right) \\
&amp; p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right) &amp;&amp;= \sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right),
\end{aligned}
\end{split}\]</div>
<p>where every individual likelihood term <span class="math notranslate nohighlight">\(p\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\theta}\right)\)</span> is a Gaussian mixture density.</p>
<p>Then we obtain the log-likelihood as</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-log-likelihood-1">
<span class="eqno">(101)<a class="headerlink" href="#equation-eq-gmm-log-likelihood-1" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\log p\left(\mathcal{S} = \left\{\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(n)}\right\};\boldsymbol{\theta}\right) &amp;= \log \prod_{n=1}^{N} p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right) \\
&amp;= \sum_{n=1}^{N} \log p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right) \\
&amp;= \underbrace{\sum_{n=1}^{N} \log \sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)}_{\log\mathcal{L}\left(\boldsymbol{\theta} \mid \mathcal{S} = \left\{\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(n)}\right\}\right)} .
\end{aligned}
\end{split}\]</div>
<p>We will abbreviate the log-likelihood as <span class="math notranslate nohighlight">\(\mathcal{L}\left(\boldsymbol{\theta} \mid \mathcal{S} = \left\{\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(n)}\right\}\right)\)</span> when the context is clear.</p>
</section>
<section id="no-closed-form-solution">
<h3><a class="toc-backref" href="#id71" role="doc-backlink">No Closed-Form Solution</a><a class="headerlink" href="#no-closed-form-solution" title="Link to this heading">#</a></h3>
<p>We aim to find parameters <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\theta}}^{*}\)</span> that maximize the log-likelihood <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> defined in <a class="reference internal" href="#equation-eq-gmm-log-likelihood-1">(101)</a>. Our â€œnormalâ€ procedure would be to compute the gradient <span class="math notranslate nohighlight">\(\mathrm{d} \mathcal{L} / \mathrm{d} \boldsymbol{\theta}\)</span> of the <span class="math notranslate nohighlight">\(\log\)</span>-likelihood with respect to the model parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, set it to <span class="math notranslate nohighlight">\(\mathbf{0}\)</span>, and solve for <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. However, unlike our previous examples for maximum likelihood estimation (e.g., when we discussed <a class="reference internal" href="#../../linear_models/linear_regression/concept.md"><span class="xref myst">linear regression</span></a>), we cannot obtain a closed-form solution. However, we can exploit an iterative scheme to find good model parameters <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\theta}}\)</span>, which will turn out to be the EM algorithm for GMMs. The key idea is to update one model parameter at a time while keeping the others fixed <span id="id10">[<a class="reference internal" href="../../bibliography.html#id9" title="Marc Peter Deisenroth, Cheng Soon Ong, and Aldo A. Faisal. Mathematics for Machine Learning. Cambridge University Press, 2021.">Deisenroth <em>et al.</em>, 2021</a>]</span>.</p>
<div class="proof remark admonition" id="prf:remark-gmm-closed-form">
<p class="admonition-title"><span class="caption-number">Remark 32 </span> (Remark: Closed-Form Solution for Single Gaussian)</p>
<section class="remark-content" id="proof-content">
<p>If we were to consider a single Gaussian as the desired density, the sum over <span class="math notranslate nohighlight">\(k\)</span> in <a class="reference internal" href="#equation-eq-gmm-log-likelihood-1">(101)</a> vanishes, and the log can be applied directly to the Gaussian component, such that we get</p>
<div class="math notranslate nohighlight">
\[
\log \mathcal{N}(\boldsymbol{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})=-\frac{D}{2} \log (2 \pi)-\frac{1}{2} \log \operatorname{det}(\boldsymbol{\Sigma})-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})
\]</div>
<p>This simple form allows us to find closed-form maximum likelihood estimates of <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>, as discussed in <a class="reference internal" href="../../probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html#estimation-theory-mle-common-distributions"><span class="std std-ref">the chapter on maximum likelihood estimation</span></a>. In <a class="reference internal" href="#equation-eq-gmm-log-likelihood-1">(101)</a>, we cannot move the log into the sum over <span class="math notranslate nohighlight">\(k\)</span> so that we cannot obtain a simple closed-form maximum likelihood solution <span id="id11">[<a class="reference internal" href="../../bibliography.html#id9" title="Marc Peter Deisenroth, Cheng Soon Ong, and Aldo A. Faisal. Mathematics for Machine Learning. Cambridge University Press, 2021.">Deisenroth <em>et al.</em>, 2021</a>]</span>.</p>
<p>Overall, what this means is that we can obtain a closed-form solution for the parameters of a single Gaussian, but not for a mixture of Gaussians.</p>
</section>
</div><p>To find out more why this constitutes a problem, one can read
section 9.2.1 in Bishop, Christopher M.â€™s book â€œPattern Recognition and Machine Learningâ€.</p>
</section>
<section id="parameter-estimation-the-necessary-conditions">
<h3><a class="toc-backref" href="#id72" role="doc-backlink">Parameter Estimation (The Necessary Conditions)</a><a class="headerlink" href="#parameter-estimation-the-necessary-conditions" title="Link to this heading">#</a></h3>
<p>Even though there is no closed form solution, we can still use iterative gradient-based optimization to find good model parameters <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\theta}}\)</span>. Consequently,
Any local optimum of a function exhibits the property that its gradient with respect to the parameters must vanish (necessary condition).</p>
<p>In our case, we obtain the following necessary conditions when we optimize the log-likelihood in <a class="reference internal" href="#equation-eq-gmm-log-likelihood-1">(101)</a> with respect to the GMM parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}, \pi_{k}\)</span> :</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\mu}_{k}} &amp; =\mathbf{0}^{\top} &amp;&amp;\Longleftrightarrow&amp;&amp; \sum_{n=1}^{N} \frac{\partial \log p\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\theta}\right)}{\partial \boldsymbol{\mu}_{k}}=\mathbf{0}^{\top}, \\
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\Sigma}_{k}} &amp; =\mathbf{0} &amp;&amp;\Longleftrightarrow&amp;&amp; \sum_{n=1}^{N} \frac{\partial \log p\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\theta}\right)}{\partial \boldsymbol{\Sigma}_{k}}=\mathbf{0}, \\
\frac{\partial \mathcal{L}}{\partial \pi_{k}} &amp; =0 &amp;&amp;\Longleftrightarrow&amp;&amp; \sum_{n=1}^{N} \frac{\partial \log p\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\theta}\right)}{\partial \pi_{k}}=0 .
\end{aligned}
\end{split}\]</div>
<p>In matrix/vector form, we have</p>
<p>the derivative of the log-likelihood with respect to the mean parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_1, \boldsymbol{\mu}_2 \ldots, \boldsymbol{\mu}_K\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla_{\boldsymbol{\mu}_1, \boldsymbol{\mu}_2 \ldots, \boldsymbol{\mu}_K} \mathcal{L} = \mathbf{0}_{K \times D} \Longleftrightarrow \begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\mu}_1} \\
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\mu}_2} \\
\vdots \\
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\mu}_K}
\end{bmatrix} = \mathbf{0}_{K \times D}
\end{split}\]</div>
<p>the derivative of the log-likelihood with respect to the covariance parameters <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_1, \boldsymbol{\Sigma}_2 \ldots, \boldsymbol{\Sigma}_K\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla_{\boldsymbol{\Sigma}_1, \boldsymbol{\Sigma}_2 \ldots, \boldsymbol{\Sigma}_K} \mathcal{L} = \mathbf{0}_{K \times D \times D} \Longleftrightarrow \begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\Sigma}_1} \\
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\Sigma}_2} \\
\vdots \\
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\Sigma}_K}
\end{bmatrix} = \mathbf{0}_{K \times D \times D}
\end{split}\]</div>
<p>the derivative of the log-likelihood with respect to the mixing coefficients <span class="math notranslate nohighlight">\(\pi_1, \pi_2 \ldots, \pi_K\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\nabla_{\pi_1, \pi_2 \ldots, \pi_K} \mathcal{L} = \mathbf{0}_{K} \Longleftrightarrow \begin{bmatrix}
\frac{\partial \mathcal{L}}{\partial \pi_1} \\
\frac{\partial \mathcal{L}}{\partial \pi_2} \\
\vdots \\
\frac{\partial \mathcal{L}}{\partial \pi_K}
\end{bmatrix} = \mathbf{0}_{K}
\end{split}\]</div>
</section>
<section id="the-chain-rule-matrix-calculus">
<h3><a class="toc-backref" href="#id73" role="doc-backlink">The Chain Rule (Matrix Calculus)</a><a class="headerlink" href="#the-chain-rule-matrix-calculus" title="Link to this heading">#</a></h3>
<p>See section 5.2.2. Chain Rule of Mathematics for Machine Learning, written by Deisenroth, Marc Peter, Faisal, A. Aldo and Ong, Cheng Soon.</p>
<p>For all three necessary conditions, by applying the chain rule, we require partial derivatives of the form</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-chain-rule-1">
<span class="eqno">(102)<a class="headerlink" href="#equation-eq-gmm-chain-rule-1" title="Link to this equation">#</a></span>\[
\frac{\partial \log p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right)}{\partial \boldsymbol{\theta}}=\textcolor{orange}{\frac{1}{p\left(\boldsymbol{x}^{(n)} ; \theta\right)} } \textcolor{blue}{\frac{\partial p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right)}{\partial \boldsymbol{\theta}}} ,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\theta}=\left\{\boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}, \pi_{k}, k=1, \ldots, K\right\}\)</span> are the model parameters and</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-chain-rule-2">
<span class="eqno">(103)<a class="headerlink" href="#equation-eq-gmm-chain-rule-2" title="Link to this equation">#</a></span>\[
\textcolor{orange}{\frac{1}{p\left(\boldsymbol{x}^{(n)} ; \theta\right)}=\frac{1}{\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\boldsymbol{x}^{(n)} ; \mu_{k}, \Sigma_{k}\right)}} .
\]</div>
</section>
<section id="running-example">
<h3><a class="toc-backref" href="#id74" role="doc-backlink">Running Example</a><a class="headerlink" href="#running-example" title="Link to this heading">#</a></h3>
<p>We will use the following running example to illustrate the GMM. This example
is from <span id="id12">[<a class="reference internal" href="../../bibliography.html#id9" title="Marc Peter Deisenroth, Cheng Soon Ong, and Aldo A. Faisal. Mathematics for Machine Learning. Cambridge University Press, 2021.">Deisenroth <em>et al.</em>, 2021</a>]</span>.</p>
<div class="proof example admonition" id="example-gmm-initialization">
<p class="admonition-title"><span class="caption-number">Example 15 </span> (Running Example (Initialization))</p>
<section class="example-content" id="proof-content">
<p>We consider a one-dimensional dataset <span class="math notranslate nohighlight">\(\mathcal{S}=\{-3,-2.5,-1,0,2,4,5\}\)</span> consisting of seven data points and wish to find a GMM with <span class="math notranslate nohighlight">\(K=3\)</span> components that models the density of the data. We initialize the mixture components as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp; p_{1}(x)=\mathcal{N}(x \mid-4,1) \\
&amp; p_{2}(x)=\mathcal{N}(x \mid 0,0.2) \\
&amp; p_{3}(x)=\mathcal{N}(x \mid 8,3)
\end{aligned}
\end{split}\]</div>
<p>and assign them equal weights <span class="math notranslate nohighlight">\(\pi_{1}=\pi_{2}=\pi_{3}=\frac{1}{3}\)</span>. The corresponding model (and the data points) are shown below.</p>
<p>Note we have not yet explained why are we â€œrandomlyâ€ initializing the mixture components
and the mixture weights. This is part of the EM algorithm and will be explained in the next sections.</p>
</section>
</div><div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="linenos"> 2</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="linenos"> 3</span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="k">def</span> <span class="nf">create_gmm</span><span class="p">(</span><span class="n">mus</span><span class="p">,</span> <span class="n">sigmas</span><span class="p">,</span> <span class="n">pis</span><span class="p">,</span> <span class="n">x_range</span><span class="p">):</span>
<span class="linenos"> 6</span>    <span class="n">pdfs</span> <span class="o">=</span> <span class="p">[</span><span class="n">pi</span> <span class="o">*</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="k">for</span> <span class="n">pi</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pis</span><span class="p">,</span> <span class="n">mus</span><span class="p">,</span> <span class="n">sigmas</span><span class="p">)]</span>
<span class="linenos"> 7</span>    <span class="n">gmm_pdf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pdfs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="linenos"> 8</span>    <span class="k">return</span> <span class="n">pdfs</span><span class="p">,</span> <span class="n">gmm_pdf</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="k">def</span> <span class="nf">plot_gmm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mus</span><span class="p">,</span> <span class="n">sigmas</span><span class="p">,</span> <span class="n">pis</span><span class="p">,</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">pdfs</span><span class="p">,</span> <span class="n">gmm_pdf</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="linenos">11</span>    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="linenos">12</span>    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data points&#39;</span><span class="p">)</span>
<span class="linenos">13</span>    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">pdf</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pdfs</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="linenos">14</span>        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">pdf</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;$\mathcal</span><span class="se">{{</span><span class="s1">N</span><span class="se">}}</span><span class="s1">(x \mid </span><span class="si">{</span><span class="n">mus</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">sigmas</span><span class="p">[</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s1">)$&#39;</span><span class="p">)</span>
<span class="linenos">15</span>    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">gmm_pdf</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;GMM&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="linenos">16</span>    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="linenos">17</span>    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="linenos">18</span>    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="linenos">19</span>    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
<span class="linenos">20</span>
<span class="linenos">21</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="linenos">22</span><span class="n">K</span> <span class="o">=</span> <span class="mi">3</span>
<span class="linenos">23</span>
<span class="linenos">24</span><span class="c1"># Initialize the mixture components</span>
<span class="linenos">25</span><span class="n">mus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="linenos">26</span><span class="n">sigmas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="linenos">27</span><span class="n">pis</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">])</span>
<span class="linenos">28</span>
<span class="linenos">29</span><span class="n">x_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">7.5</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="linenos">30</span><span class="n">pdfs</span><span class="p">,</span> <span class="n">gmm_pdf</span> <span class="o">=</span> <span class="n">create_gmm</span><span class="p">(</span><span class="n">mus</span><span class="p">,</span> <span class="n">sigmas</span><span class="p">,</span> <span class="n">pis</span><span class="p">,</span> <span class="n">x_range</span><span class="p">)</span>
<span class="linenos">31</span><span class="n">plot_gmm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mus</span><span class="p">,</span> <span class="n">sigmas</span><span class="p">,</span> <span class="n">pis</span><span class="p">,</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">pdfs</span><span class="p">,</span> <span class="n">gmm_pdf</span><span class="p">,</span> <span class="s1">&#39;Initial Gaussian Mixture Model and Data Points&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:42,786 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:42,793 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:42,799 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:42,850 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:42,859 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:42,866 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:42,960 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:42,970 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:42,978 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<img alt="../../_images/12feb4118d928f54ae9317af336a74164b43e27c8e27ead4cb80adee85e6f48d.svg" src="../../_images/12feb4118d928f54ae9317af336a74164b43e27c8e27ead4cb80adee85e6f48d.svg" />
</div>
</div>
<p>Next, we can calculate the responsibilities <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span> for each data point <span class="math notranslate nohighlight">\(x^{(n)}\)</span> and each mixture component <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>For our example from <a class="reference internal" href="#example-gmm-initialization">Example 15</a>, we compute the responsibilities <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{R} = \left[\begin{array}{ccc}
1.0 &amp; 0.0 &amp; 0.0 \\
1.0 &amp; 0.0 &amp; 0.0 \\
0.057 &amp; 0.943 &amp; 0.0 \\
0.001 &amp; 0.999 &amp; 0.0 \\
0.0 &amp; 0.066 &amp; 0.934 \\
0.0 &amp; 0.0 &amp; 1.0 \\
0.0 &amp; 0.0 &amp; 1.0
\end{array}\right] \in \mathbb{R}^{N \times K} \text {. }
\end{split}\]</div>
<p>Here the <span class="math notranslate nohighlight">\(n\)</span>th row tells us the responsibilities of all mixture components for <span class="math notranslate nohighlight">\(x^{(n)}\)</span>. The sum of all <span class="math notranslate nohighlight">\(K\)</span> responsibilities for a data point (sum of every row) is 1 . The <span class="math notranslate nohighlight">\(k\)</span> th column gives us an overview of the responsibility of the <span class="math notranslate nohighlight">\(k\)</span> th mixture component. We can see that the third mixture component (third column) is not responsible for any of the first four data points, but takes much responsibility of the remaining data points. The sum of all entries of a column gives us the values <span class="math notranslate nohighlight">\(N_{k}\)</span>, i.e., the total responsibility of the <span class="math notranslate nohighlight">\(k\)</span> th mixture component. In our example, we get <span class="math notranslate nohighlight">\(N_{1}=2.058, N_{2}=\)</span> <span class="math notranslate nohighlight">\(2.008, N_{3}=2.934\)</span> <span id="id13">[<a class="reference internal" href="../../bibliography.html#id9" title="Marc Peter Deisenroth, Cheng Soon Ong, and Aldo A. Faisal. Mathematics for Machine Learning. Cambridge University Press, 2021.">Deisenroth <em>et al.</em>, 2021</a>]</span>.</p>
</section>
<section id="estimating-the-mean-parameters-boldsymbol-mu-k">
<h3><a class="toc-backref" href="#id75" role="doc-backlink">Estimating the Mean Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span></a><a class="headerlink" href="#estimating-the-mean-parameters-boldsymbol-mu-k" title="Link to this heading">#</a></h3>
<div class="proof theorem admonition" id="theorem-gmm-update-means">
<p class="admonition-title"><span class="caption-number">Theorem 11 </span> (Update of the GMM Means)</p>
<section class="theorem-content" id="proof-content">
<p>The update of the mean parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}, k=1, \ldots, K\)</span>, of the <span class="math notranslate nohighlight">\(G M M\)</span> is given by</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-update-means-1">
<span class="eqno">(104)<a class="headerlink" href="#equation-eq-gmm-update-means-1" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\boldsymbol{\mu}_{k}^{n e w} &amp;= \frac{\sum_{n=1}^{N} r^{(n)}_{k} \boldsymbol{x}^{(n)}}{\sum_{n=1}^{N} r^{(n)}_{k}} \\
&amp;= \frac{1}{N_{k}} \sum_{n=1}^{N} r^{(n)}_{k} \boldsymbol{x}^{(n)}
\end{aligned}
\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p>the responsibilities <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span> are defined in <a class="reference internal" href="#equation-eq-gmm-responsibility">(93)</a>, which is the probability that the <span class="math notranslate nohighlight">\(k\)</span> th mixture component generated the <span class="math notranslate nohighlight">\(n\)</span>-th data point.</p></li>
<li><p><span class="math notranslate nohighlight">\(N_{k}=\sum_{n=1}^{N} r^{(n)}_{k}\)</span> can be interpreted the number of data points assigned to the <span class="math notranslate nohighlight">\(k\)</span> th mixture component.</p></li>
</ul>
</section>
</div><div class="proof remark admonition" id="remark-gmm-update-means">
<p class="admonition-title"><span class="caption-number">Remark 33 </span> (The update of the GMM Means depends on the responsibilities)</p>
<section class="remark-content" id="proof-content">
<p>The update of the means <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span> of the individual mixture components in <a class="reference internal" href="#equation-eq-gmm-update-means-1">(104)</a> depends on all means, covariance matrices <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span>, and mixture weights <span class="math notranslate nohighlight">\(\pi_{k}\)</span> via <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span> given in <a class="reference internal" href="#equation-eq-gmm-responsibility">(93)</a>. Therefore, we cannot obtain a closed-form solution for all <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span> at once.</p>
<p>What this means is that in order to update the means, we need to first compute the responsibilities <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span>, but computing the responsibilities requires us to know the means <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span>, which we want to update. This is a typical problem in iterative algorithms, which we will discuss in more detail in the next section.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. <strong>The proof is taken from <span id="id14">[<a class="reference internal" href="../../bibliography.html#id9" title="Marc Peter Deisenroth, Cheng Soon Ong, and Aldo A. Faisal. Mathematics for Machine Learning. Cambridge University Press, 2021.">Deisenroth <em>et al.</em>, 2021</a>]</span>.</strong></p>
<p>From <a class="reference internal" href="#equation-eq-gmm-chain-rule-1">(102)</a> we see that the gradient of the log-likelihood with respect to the mean parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}, k=1, \ldots, K\)</span>, requires us to compute the partial derivative</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\textcolor{blue}{\frac{\partial p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right)}{\partial \boldsymbol{\mu}_{k}}} &amp; =\sum_{j=1}^{K} \pi_{j} \frac{\partial \mathcal{N}\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}\right)}{\partial \boldsymbol{\mu}_{k}}=\pi_{k} \frac{\partial \mathcal{N}\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)}{\partial \boldsymbol{\mu}_{k}} &amp;&amp; (a) \\
&amp; = \textcolor{blue}{\pi_{k}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)^{\top} \boldsymbol{\Sigma}_{k}^{-1} \mathcal{N}\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)} &amp;&amp; (b) \\
\end{aligned}
\end{split}\]</div>
<p>where we exploited that only the <span class="math notranslate nohighlight">\(k\)</span> th mixture component depends on <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span>.</p>
<p>We use our result from (b) in <a class="reference internal" href="#equation-eq-gmm-chain-rule-1">(102)</a> and put everything together so that the desired partial derivative of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span> is given as</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-update-means-2">
<span class="eqno">(105)<a class="headerlink" href="#equation-eq-gmm-update-means-2" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\mu}_{k}} &amp; =\sum_{n=1}^{N} \frac{\partial \log p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right)}{\partial \boldsymbol{\mu}_{k}}=\sum_{n=1}^{N} \textcolor{orange}{\frac{1}{p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right)}}  \textcolor{blue}{\frac{\partial p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right)}{\partial \boldsymbol{\mu}_{k}}} &amp;&amp; (c) \\
&amp; =\sum_{n=1}^{N} \textcolor{blue}{\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)^{\top} \boldsymbol{\Sigma}_{k}^{-1}} \underbrace{\boxed{\frac{\textcolor{blue}{\pi_{k} \mathcal{N}\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)}}{\textcolor{orange}{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(\boldsymbol{x}^{(n)} \mid \mu_{j}, \boldsymbol{\Sigma}_{j}\right)}}}}_{=r^{(n)}_{k}} &amp;&amp; (d) \\
&amp; =\sum_{n=1}^{N} r^{(n)}_{k}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)^{\top} \boldsymbol{\Sigma}_{k}^{-1} &amp;&amp; (e)  \\
\end{aligned}
\end{split}\]</div>
<p>Here we used the identity from <a class="reference internal" href="#equation-eq-gmm-chain-rule-2">(103)</a> and the result of the partial derivative in (b) to get to (d). The values <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span> are the responsibilities we defined in <a class="reference internal" href="#equation-eq-gmm-responsibility">(93)</a>.</p>
<p>We now solve (e) for <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}^{\text {new }}\)</span> so that <span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}\left(\boldsymbol{\mu}_{k}^{\mathrm{new}}\right)}{\partial \boldsymbol{\mu}_{k}}=\mathbf{0}^{\top}\)</span> and obtain</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-update-means-3">
<span class="eqno">(106)<a class="headerlink" href="#equation-eq-gmm-update-means-3" title="Link to this equation">#</a></span>\[
\sum_{n=1}^{N} r^{(n)}_{k} \boldsymbol{x}^{(n)}=\sum_{n=1}^{N} r^{(n)}_{k} \boldsymbol{\mu}_{k}^{\mathrm{new}} \Longleftrightarrow \boldsymbol{\mu}_{k}^{\mathrm{new}}=\frac{\sum_{n=1}^{N} r^{(n)}_{k} \boldsymbol{x}^{(n)}}{\boxed{\sum_{n=1}^{N} r^{(n)}_{k}}}=\frac{1}{\boxed{N_{k}}} \sum_{n=1}^{N} r^{(n)}_{k} \boldsymbol{x}^{(n)},
\]</div>
<p>where we defined</p>
<div class="math notranslate nohighlight" id="equation-eq-nk-1">
<span class="eqno">(107)<a class="headerlink" href="#equation-eq-nk-1" title="Link to this equation">#</a></span>\[
N_{k}:=\sum_{n=1}^{N} r^{(n)}_{k}
\]</div>
<p>as the total responsibility of the <span class="math notranslate nohighlight">\(k\)</span> th mixture component for the entire dataset. This concludes the proof of <a class="reference internal" href="#theorem-gmm-update-means">Theorem 11</a>.</p>
</div>
<section id="some-intuition">
<h4><a class="toc-backref" href="#id76" role="doc-backlink">Some Intuition</a><a class="headerlink" href="#some-intuition" title="Link to this heading">#</a></h4>
<p>Intuitively, <a class="reference internal" href="#equation-eq-gmm-update-means-1">(104)</a> can be interpreted as an importance-weighted Monte Carlo estimate of the mean, where the importance weights of data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> are the responsibilities <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span> of the <span class="math notranslate nohighlight">\(k\)</span> th cluster for <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}, k=1, \ldots, K\)</span>. Therefore, the mean <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span> is pulled toward a data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> with strength given by <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span>. The means are pulled stronger toward data points for which the corresponding mixture component has a high responsibility, i.e., a high likelihood. Figure <a class="reference internal" href="#fig-gmm-mean-updates"><span class="std std-numref">Fig. 17</span></a> illustrates this.</p>
<figure class="align-default" id="fig-gmm-mean-updates">
<img alt="../../_images/mml-11.4.png" src="../../_images/mml-11.4.png" />
<figcaption>
<p><span class="caption-number">Fig. 17 </span><span class="caption-text">Update of the mean parameter of mixture component in a GMM. The mean <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> is being pulled toward individual data points with the weights given by the corresponding responsibilities. Image Credit: <span id="id15">[<a class="reference internal" href="../../bibliography.html#id9" title="Marc Peter Deisenroth, Cheng Soon Ong, and Aldo A. Faisal. Mathematics for Machine Learning. Cambridge University Press, 2021.">Deisenroth <em>et al.</em>, 2021</a>]</span>.</span><a class="headerlink" href="#fig-gmm-mean-updates" title="Link to this image">#</a></p>
</figcaption>
</figure>
<hr class="docutils" />
<p>We can also interpret the mean update in <a class="reference internal" href="#equation-eq-gmm-update-means-1">(104)</a> as the expected value of all data points under the distribution given by</p>
<div class="math notranslate nohighlight" id="equation-eq-responsibility-vector-1">
<span class="eqno">(108)<a class="headerlink" href="#equation-eq-responsibility-vector-1" title="Link to this equation">#</a></span>\[
\boldsymbol{r}_{k}:=\left[r_{1 k}, \ldots, r_{N k}\right]^{\top} / N_{k}
\]</div>
<p>which is a normalized probability vector, i.e.,</p>
<div class="math notranslate nohighlight" id="equation-eq-responsibility-vector-2">
<span class="eqno">(109)<a class="headerlink" href="#equation-eq-responsibility-vector-2" title="Link to this equation">#</a></span>\[
\boldsymbol{\mu}_{k} \leftarrow \mathbb{E}_{\boldsymbol{r}_{k}}[\mathcal{S}]
\]</div>
</section>
<section id="update-mean-of-running-example">
<h4><a class="toc-backref" href="#id77" role="doc-backlink">Update Mean of Running Example</a><a class="headerlink" href="#update-mean-of-running-example" title="Link to this heading">#</a></h4>
<div class="proof example admonition" id="prf-gmm-update-means-example">
<p class="admonition-title"><span class="caption-number">Example 16 </span> (Running Example: Update Mean)</p>
<section class="example-content" id="proof-content">
<p>In our example from <a class="reference internal" href="#example-gmm-initialization">Example 15</a>, the mean values are updated as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp; \mu_{1}:-4 \rightarrow-2.7 \\
&amp; \mu_{2}: 0 \rightarrow-0.4 \\
&amp; \mu_{3}: 8 \rightarrow 3.7
\end{aligned}
\end{split}\]</div>
<p>Here we see that the means of the first and third mixture component move toward the regime of the data, whereas the mean of the second component does not change so dramatically. Figure 11.3 illustrates this change, where Figure 11.3(a) shows the GMM density prior to updating the means and Figure 11.3(b) shows the GMM density after updating the mean values <span class="math notranslate nohighlight">\(\mu_{k}\)</span>.</p>
</section>
</div><div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1"># Update the means</span>
<span class="linenos"> 2</span><span class="n">mus_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.7</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">3.7</span><span class="p">])</span>
<span class="linenos"> 3</span>
<span class="linenos"> 4</span><span class="c1"># Create a new GMM with the updated means</span>
<span class="linenos"> 5</span><span class="n">pdfs_new</span><span class="p">,</span> <span class="n">gmm_pdf_new</span> <span class="o">=</span> <span class="n">create_gmm</span><span class="p">(</span><span class="n">mus_new</span><span class="p">,</span> <span class="n">sigmas</span><span class="p">,</span> <span class="n">pis</span><span class="p">,</span> <span class="n">x_range</span><span class="p">)</span>
<span class="linenos"> 6</span>
<span class="linenos"> 7</span><span class="c1"># Create the original GMM with the initial means</span>
<span class="linenos"> 8</span><span class="n">pdfs_original</span><span class="p">,</span> <span class="n">gmm_pdf_original</span> <span class="o">=</span> <span class="n">create_gmm</span><span class="p">(</span><span class="n">mus</span><span class="p">,</span> <span class="n">sigmas</span><span class="p">,</span> <span class="n">pis</span><span class="p">,</span> <span class="n">x_range</span><span class="p">)</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="linenos">11</span>
<span class="linenos">12</span><span class="c1"># Plot the original GMM</span>
<span class="linenos">13</span><span class="n">plot_gmm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mus</span><span class="p">,</span> <span class="n">sigmas</span><span class="p">,</span> <span class="n">pis</span><span class="p">,</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">pdfs_original</span><span class="p">,</span> <span class="n">gmm_pdf_original</span><span class="p">,</span> <span class="s1">&#39;Initial Gaussian Mixture Model and Data Points&#39;</span><span class="p">,</span> <span class="n">ax1</span><span class="p">)</span>
<span class="linenos">14</span>
<span class="linenos">15</span><span class="c1"># Plot the updated GMM</span>
<span class="linenos">16</span><span class="n">plot_gmm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mus_new</span><span class="p">,</span> <span class="n">sigmas</span><span class="p">,</span> <span class="n">pis</span><span class="p">,</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">pdfs_new</span><span class="p">,</span> <span class="n">gmm_pdf_new</span><span class="p">,</span> <span class="s1">&#39;Updated Gaussian Mixture Model and Data Points with New Mean.&#39;</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span>
<span class="linenos">17</span>
<span class="linenos">18</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,098 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,106 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,114 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,180 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,187 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,193 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,297 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,305 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,312 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,375 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,382 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,389 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<img alt="../../_images/c788c125d3629b5a4d7aa86b53c96f19b192bc24c3a2bb376ebb58feb8e232fb.svg" src="../../_images/c788c125d3629b5a4d7aa86b53c96f19b192bc24c3a2bb376ebb58feb8e232fb.svg" />
</div>
</div>
<p>The update of the mean parameters in <a class="reference internal" href="#equation-eq-gmm-update-means-1">(104)</a> look fairly straightforward. However, note that the responsibilities <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span> are a function of <span class="math notranslate nohighlight">\(\pi_{j}, \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}\)</span> for all <span class="math notranslate nohighlight">\(j=1, \ldots, K\)</span>, such that the updates in <a class="reference internal" href="#equation-eq-gmm-update-means-1">(104)</a> depend on all parameters of the GMM, and a closed-form solution, which we obtained for linear regression, cannot be obtained.</p>
<p>Another important thing one needs to realize is that the update of the meansâ€™s
right hand sideâ€™s <span class="math notranslate nohighlight">\(N_k\)</span> and <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span> are all based on the previous iterationâ€™s parameters (or current depending on how you term it). See code for concrete logical flow.</p>
</section>
<section id="estimating-the-mean-parameters-boldsymbol-mu-k-in-python">
<h4><a class="toc-backref" href="#id78" role="doc-backlink">Estimating the Mean Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span> in Python</a><a class="headerlink" href="#estimating-the-mean-parameters-boldsymbol-mu-k-in-python" title="Link to this heading">#</a></h4>
<p>We can estimate the mean parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span> in Python using the following code snippet:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">means</span> <span class="o">=</span> <span class="n">responsibilities</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span> <span class="o">/</span> <span class="n">nk</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="c1"># (K, D)</span>
</pre></div>
</div>
<p>where</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">responsibilities</span></code> is a <span class="math notranslate nohighlight">\(N \times K\)</span> matrix of responsibilities <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span>, and</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nk</span></code> is a <span class="math notranslate nohighlight">\(K\)</span>-dimensional vector of <span class="math notranslate nohighlight">\(N_{k}\)</span> values.</p></li>
</ul>
<p>Why? Because if you look at equation <a class="reference internal" href="#equation-eq-gmm-update-means-1">(104)</a>:</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-update-means-1-repeated">
<span class="eqno">(110)<a class="headerlink" href="#equation-eq-gmm-update-means-1-repeated" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\boldsymbol{\mu}_{k}^{n e w} &amp;= \frac{1}{N_{k}} \sum_{n=1}^{N} r^{(n)}_{k} \boldsymbol{x}^{(n)} \\
&amp;= \frac{1}{N_{k}} \left[r^{(1)}_{k} \boldsymbol{x}^{(1)} + \ldots + r^{(N)}_{k} \boldsymbol{x}^{(N)}\right] \\
&amp;= \frac{1}{N_{k}} \underbrace{\begin{bmatrix} r^{(1)}_{k} &amp; \ldots &amp; r^{(N)}_{k} \end{bmatrix}}_{\left(\boldsymbol{r}_k\right)^{T}} \underbrace{\begin{bmatrix} \boldsymbol{x}^{(1)} \\ \vdots \\ \boldsymbol{x}^{(N)} \end{bmatrix}}_{\boldsymbol{X}} \\
\end{aligned}
\end{split}\]</div>
<p>where the last equality leads is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">responsibilities</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span>
</pre></div>
</div>
<p>so in order to find all <span class="math notranslate nohighlight">\(K\)</span> mean parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span>, we just need to repeat the above code snippet for all <span class="math notranslate nohighlight">\(k=1, \ldots, K\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{M} = \begin{bmatrix} \boldsymbol{\mu}_{1}^{n e w} \\ \vdots \\ \boldsymbol{\mu}_{K}^{n e w} \end{bmatrix} &amp;= \frac{1}{N_{k}} \begin{bmatrix} \boldsymbol{r}_{1}^{T} \boldsymbol{X} \\ \vdots \\ \boldsymbol{r}_{K}^{T} \boldsymbol{X} \end{bmatrix} \\
&amp;= \frac{1}{N_k} \boldsymbol{R}^{T} \boldsymbol{X} \\
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{M}\)</span> is a <span class="math notranslate nohighlight">\(K \times D\)</span> matrix of mean parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span>
and <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span> is a <span class="math notranslate nohighlight">\(N \times K\)</span> matrix of responsibilities <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span>.</p>
<p>So we update our code snippet to:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">responsibilities</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span>
</pre></div>
</div>
<p>and to divide by <span class="math notranslate nohighlight">\(N_{k}\)</span>, we just need to broadcast the <code class="docutils literal notranslate"><span class="pre">nk</span></code> vector to the shape of the matrix obtained from the previous code snippet:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">responsibilities</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span> <span class="o">/</span> <span class="n">nk</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
</pre></div>
</div>
<p>and we are done finding the new mean parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span> in Python code (for all <span class="math notranslate nohighlight">\(k=1, \ldots, K\)</span>).</p>
</section>
</section>
<section id="estimating-the-covariance-parameters-boldsymbol-sigma-k">
<h3><a class="toc-backref" href="#id79" role="doc-backlink">Estimating the Covariance Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span></a><a class="headerlink" href="#estimating-the-covariance-parameters-boldsymbol-sigma-k" title="Link to this heading">#</a></h3>
<div class="proof theorem admonition" id="theorem-gmm-update-covariance">
<p class="admonition-title"><span class="caption-number">Theorem 12 </span> (Update of the GMM Covariances)</p>
<section class="theorem-content" id="proof-content">
<p><strong>The proof is taken from <span id="id16">[<a class="reference internal" href="../../bibliography.html#id9" title="Marc Peter Deisenroth, Cheng Soon Ong, and Aldo A. Faisal. Mathematics for Machine Learning. Cambridge University Press, 2021.">Deisenroth <em>et al.</em>, 2021</a>]</span>.</strong></p>
<p>The update of the covariance parameters <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}, k=1, \ldots, K\)</span> of the <span class="math notranslate nohighlight">\(G M M\)</span> is given by</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-update-covariance-1">
<span class="eqno">(111)<a class="headerlink" href="#equation-eq-gmm-update-covariance-1" title="Link to this equation">#</a></span>\[
\boldsymbol{\Sigma}_{k}^{n e w}=\frac{1}{N_{k}} \sum_{n=1}^{N} r^{(n)}_{k}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)^{\top}
\]</div>
<p>where <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span> and <span class="math notranslate nohighlight">\(N_{k}\)</span> are defined in <a class="reference internal" href="#equation-eq-gmm-responsibility">(93)</a> and <a class="reference internal" href="#equation-eq-nk-1">(107)</a>, respectively.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. To prove <a class="reference internal" href="#theorem-gmm-update-covariance">Theorem 12</a>, our approach is to compute the partial derivatives of the log-likelihood <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> with respect to the covariances <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span>, set them to <span class="math notranslate nohighlight">\(\mathbf{0}\)</span>, and solve for <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span>. We start with our general approach</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-update-covariance-2">
<span class="eqno">(112)<a class="headerlink" href="#equation-eq-gmm-update-covariance-2" title="Link to this equation">#</a></span>\[
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\Sigma}_{k}} = \sum_{n=1}^{N} \frac{\partial \log p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right)}{\partial \boldsymbol{\Sigma}_{k}}=\sum_{n=1}^{N} \textcolor{orange}{\frac{1}{p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right)}} \textcolor{blue}{\frac{\partial p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right)}{\partial \boldsymbol{\Sigma}_{k}}} .
\end{aligned}
\]</div>
<p>We already know <span class="math notranslate nohighlight">\(1 / p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right)\)</span> from (11.16). To obtain the remaining partial derivative <span class="math notranslate nohighlight">\(\partial p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right) / \partial \boldsymbol{\Sigma}_{k}\)</span>, we write down the definition of the Gaussian distribution <span class="math notranslate nohighlight">\(p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right)\)</span> (see (11.9)) and drop all terms but the <span class="math notranslate nohighlight">\(k\)</span> th. We then obtain</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-update-covariance-3">
<span class="eqno">(113)<a class="headerlink" href="#equation-eq-gmm-update-covariance-3" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
&amp; \textcolor{blue}{\frac{\partial p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right)}{\partial \boldsymbol{\Sigma}_{k}}} &amp;&amp; (a) \\
&amp; =\frac{\partial}{\partial \boldsymbol{\Sigma}_{k}}\left(\pi_{k}(2 \pi)^{-\frac{D}{2}} \operatorname{det}\left(\boldsymbol{\Sigma}_{k}\right)^{-\frac{1}{2}} \exp \left(-\frac{1}{2}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)^{\top} \boldsymbol{\Sigma}_{k}^{-1}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)\right)\right) &amp;&amp; (b) \\
&amp; =\pi_{k}(2 \pi)^{-\frac{D}{2}}\left[\textcolor{red}{\frac{\partial}{\partial \boldsymbol{\Sigma}_{k}} \operatorname{det}\left(\boldsymbol{\Sigma}_{k}\right)^{-\frac{1}{2}}} \exp \left(-\frac{1}{2}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)^{\top} \boldsymbol{\Sigma}_{k}^{-1}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)\right)\right. &amp;&amp; (c) \\
&amp; \left.+\operatorname{det}\left(\boldsymbol{\Sigma}_{k}\right)^{-\frac{1}{2}} \frac{\partial}{\partial \boldsymbol{\Sigma}_{k}} \exp \left(-\frac{1}{2}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)^{\top} \boldsymbol{\Sigma}_{k}^{-1}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)\right)\right] \text {. } &amp;&amp; (d)
\end{aligned}
\end{split}\]</div>
<p>We now use the identities</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp; \textcolor{red}{\frac{\partial}{\partial \boldsymbol{\Sigma}_{k}} \operatorname{det}\left(\boldsymbol{\Sigma}_{k}\right)^{-\frac{1}{2}}}  \stackrel{(5.101)}{=} \textcolor{red}{-\frac{1}{2} \operatorname{det}\left(\boldsymbol{\Sigma}_{k}\right)^{-\frac{1}{2}} \boldsymbol{\Sigma}_{k}^{-1}} \\
&amp; \frac{\partial}{\partial \boldsymbol{\Sigma}_{k}}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)^{\top} \boldsymbol{\Sigma}_{k}^{-1}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right) \stackrel{(5.103)}{=}-\boldsymbol{\Sigma}_{k}^{-1}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)^{\top} \boldsymbol{\Sigma}_{k}^{-1}
\end{aligned}
\end{split}\]</div>
<p>and obtain (after some rearranging) the desired partial derivative required in <a class="reference internal" href="#equation-eq-gmm-update-covariance-2">(112)</a> as</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-update-covariance-4">
<span class="eqno">(114)<a class="headerlink" href="#equation-eq-gmm-update-covariance-4" title="Link to this equation">#</a></span>\[
\begin{aligned}
\textcolor{blue}{\frac{\partial p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right)}{\partial \boldsymbol{\Sigma}_{k}}}= &amp; \textcolor{blue}{\pi_{k} \mathcal{N}\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)  \cdot\left[-\frac{1}{2}\left(\boldsymbol{\Sigma}_{k}^{-1}-\boldsymbol{\Sigma}_{k}^{-1}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)^{\top} \boldsymbol{\Sigma}_{k}^{-1}\right)\right]}
\end{aligned}
\]</div>
<p>Putting everything together, the partial derivative of the log-likelihood with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span> is given by (do pay attention to color coding):</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-update-covariance-5">
<span class="eqno">(115)<a class="headerlink" href="#equation-eq-gmm-update-covariance-5" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\Sigma}_{k}} &amp; =\sum_{n=1}^{N} \frac{\partial \log p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right)}{\partial \boldsymbol{\Sigma}_{k}} = \sum_{n=1}^{N} \textcolor{orange}{\frac{1}{p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right)}} \textcolor{blue}{\frac{\partial p\left(\boldsymbol{x}^{(n)} ; \boldsymbol{\theta}\right)}{\partial \boldsymbol{\Sigma}_{k}}} \\
&amp; =\sum_{n=1}^{N} \underbrace{\frac{\textcolor{blue}{\pi_{k} \mathcal{N}\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)}}{\textcolor{orange}{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(x_{n} \mid \mu_{j}, \boldsymbol{\Sigma}_{j}\right)}}}_{=r^{(n)}_{k}}  \cdot \textcolor{blue}{\left[-\frac{1}{2}\left(\boldsymbol{\Sigma}_{k}^{-1}-\boldsymbol{\Sigma}_{k}^{-1}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)^{\top} \boldsymbol{\Sigma}_{k}^{-1}\right)\right]} \\
&amp; =-\frac{1}{2} \sum_{n=1}^{N} r^{(n)}_{k}\left(\boldsymbol{\Sigma}_{k}^{-1}-\boldsymbol{\Sigma}_{k}^{-1}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)^{\top} \boldsymbol{\Sigma}_{k}^{-1}\right) \\
= &amp; -\frac{1}{2} \boldsymbol{\Sigma}_{k}^{-1} \underbrace{\sum_{n=1}^{N} r^{(n)}_{k}}_{=N_{k}}+\frac{1}{2} \boldsymbol{\Sigma}_{k}^{-1}\left(\sum_{n=1}^{N} r^{(n)}_{k}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)^{\top}\right) \boldsymbol{\Sigma}_{k}^{-1} .
\end{aligned}
\end{split}\]</div>
<p>We see that the responsibilities <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span> also appear in this partial derivative. Setting this partial derivative to <span class="math notranslate nohighlight">\(\mathbf{0}\)</span>, we obtain the necessary optimality condition</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-update-covariance-6">
<span class="eqno">(116)<a class="headerlink" href="#equation-eq-gmm-update-covariance-6" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
&amp; N_{k} \boldsymbol{\Sigma}_{k}^{-1}=\boldsymbol{\Sigma}_{k}^{-1}\left(\sum_{n=1}^{N} r^{(n)}_{k}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)^{\top}\right) \boldsymbol{\Sigma}_{k}^{-1} \\
&amp; \Longleftrightarrow N_{k} \boldsymbol{I}=\left(\sum_{n=1}^{N} r^{(n)}_{k}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)^{\top}\right) \boldsymbol{\Sigma}_{k}^{-1}
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{I}\)</span> is the identity matrix.</p>
<p>By solving for <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span>, we obtain</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\Sigma}_{k}^{\mathrm{new}}=\frac{1}{N_{k}} \sum_{n=1}^{N} r^{(n)}_{k}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)^{\top},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{r}_{k}\)</span> is the probability vector defined in <a class="reference internal" href="#equation-eq-responsibility-vector-1">(108)</a>. This gives us a simple update rule for <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span> for <span class="math notranslate nohighlight">\(k=1, \ldots, K\)</span> and proves <a class="reference internal" href="#theorem-gmm-update-covariance">Theorem 12</a>.</p>
<p>Similar to the update of <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span> in <a class="reference internal" href="#equation-eq-gmm-update-means-1">(104)</a>, we can interpret the update of the covariance in <a class="reference internal" href="#equation-eq-gmm-update-covariance-1">(111)</a> as an importance-weighted expected value of the square of the centered data <span class="math notranslate nohighlight">\(\tilde{\mathcal{S}}_{k}:=\left\{\boldsymbol{x}^{(1)}-\boldsymbol{\mu}_{k}, \ldots, \boldsymbol{x}^{(N)}-\boldsymbol{\mu}_{k}\right\}\)</span></p>
</div>
<section id="update-covariance-matrix-of-running-example">
<h4><a class="toc-backref" href="#id80" role="doc-backlink">Update Covariance Matrix of Running Example</a><a class="headerlink" href="#update-covariance-matrix-of-running-example" title="Link to this heading">#</a></h4>
<div class="proof example admonition" id="prf-gmm-update-covariance-example">
<p class="admonition-title"><span class="caption-number">Example 17 </span> (Running Example: Update Covariance Matrix)</p>
<section class="example-content" id="proof-content">
<p>In our example from <a class="reference internal" href="#example-gmm-initialization">Example 15</a>, the (co)variance values are updated as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp; \sigma_{1}^{2}: 1 \rightarrow 0.14 \\
&amp; \sigma_{2}^{2}: 0.2 \rightarrow 0.44 \\
&amp; \sigma_{3}^{2}: 3 \rightarrow 1.53
\end{aligned}
\end{split}\]</div>
<p>Here we see that the means of the first and third mixture component move toward the regime of the data, whereas the mean of the second component does not change so dramatically.</p>
<p>The figure below illustrates the change in the (co)variance values. The figure on the left
shows the GMM density prior to updating the (co)variance values, whereas the figure on the right shows the GMM density after updating the (co)variance values.</p>
</section>
</div><div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1"># Update the variances</span>
<span class="linenos"> 2</span><span class="n">sigmas_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.14</span><span class="p">,</span> <span class="mf">0.44</span><span class="p">,</span> <span class="mf">1.53</span><span class="p">]))</span>
<span class="linenos"> 3</span>
<span class="linenos"> 4</span><span class="c1"># Create the updated GMM with new means and variances</span>
<span class="linenos"> 5</span><span class="n">pdfs_new2</span><span class="p">,</span> <span class="n">gmm_pdf_new2</span> <span class="o">=</span> <span class="n">create_gmm</span><span class="p">(</span><span class="n">mus_new</span><span class="p">,</span> <span class="n">sigmas_new</span><span class="p">,</span> <span class="n">pis</span><span class="p">,</span> <span class="n">x_range</span><span class="p">)</span>
<span class="linenos"> 6</span>
<span class="linenos"> 7</span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="c1"># Plot the GMM with updated means</span>
<span class="linenos">10</span><span class="n">plot_gmm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mus_new</span><span class="p">,</span> <span class="n">sigmas</span><span class="p">,</span> <span class="n">pis</span><span class="p">,</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">pdfs_new</span><span class="p">,</span> <span class="n">gmm_pdf_new</span><span class="p">,</span> <span class="s1">&#39;Updated Gaussian Mixture Model and Data Points with New Mean.&#39;</span><span class="p">,</span> <span class="n">ax1</span><span class="p">)</span>
<span class="linenos">11</span>
<span class="linenos">12</span><span class="c1"># Plot the updated GMM with new means and variances</span>
<span class="linenos">13</span><span class="n">plot_gmm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mus_new</span><span class="p">,</span> <span class="n">sigmas_new</span><span class="p">,</span> <span class="n">pis</span><span class="p">,</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">pdfs_new2</span><span class="p">,</span> <span class="n">gmm_pdf_new2</span><span class="p">,</span> <span class="s1">&#39;Updated Gaussian Mixture Model and Data Points with New Variances&#39;</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span>
<span class="linenos">14</span>
<span class="linenos">15</span><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="linenos">16</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,493 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,687 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,693 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,733 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,743 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,754 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,791 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,798 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,805 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,847 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,858 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,868 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,967 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,974 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:43,981 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,035 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,045 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,056 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<img alt="../../_images/c8981cc1b328476818904dbe5af2438b35382a93f963d9a612282abbd4b6b892.svg" src="../../_images/c8981cc1b328476818904dbe5af2438b35382a93f963d9a612282abbd4b6b892.svg" />
</div>
</div>
</section>
<section id="id17">
<h4><a class="toc-backref" href="#id81" role="doc-backlink">Some Intuition</a><a class="headerlink" href="#id17" title="Link to this heading">#</a></h4>
<p>Similar to the update of the mean parameters, we can interpret <a class="reference internal" href="#equation-eq-gmm-update-means-1">(104)</a> as a Monte Carlo estimate of the weighted covariance of data points <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> associated with the <span class="math notranslate nohighlight">\(k\)</span> th mixture component, where the weights are the responsibilities <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span>. As with the updates of the mean parameters, this update depends on all <span class="math notranslate nohighlight">\(\pi_{j}, \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}, j=1, \ldots, K\)</span>, through the responsibilities <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span>, which prohibits a closed-form solution <span id="id18">[<a class="reference internal" href="../../bibliography.html#id9" title="Marc Peter Deisenroth, Cheng Soon Ong, and Aldo A. Faisal. Mathematics for Machine Learning. Cambridge University Press, 2021.">Deisenroth <em>et al.</em>, 2021</a>]</span>.</p>
</section>
<section id="estimating-the-covariance-matrix-boldsymbol-sigma-k-in-python">
<h4><a class="toc-backref" href="#id82" role="doc-backlink">Estimating the Covariance Matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span> in Python</a><a class="headerlink" href="#estimating-the-covariance-matrix-boldsymbol-sigma-k-in-python" title="Link to this heading">#</a></h4>
<p>We can estimate the covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span> in Python
using the following code snippet:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">covariances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>                                                           <span class="c1"># (K, D, D)</span>
    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_components</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_features</span><span class="p">)</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_components</span><span class="p">):</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">means</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>                                                           <span class="c1"># (N, D)</span>
    <span class="n">weighted_diff</span> <span class="o">=</span> <span class="n">responsibilities</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">diff</span>                  <span class="c1"># (N, D)</span>
    <span class="n">cov_k</span> <span class="o">=</span> <span class="n">weighted_diff</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">diff</span> <span class="o">/</span> <span class="n">nk</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>                                        <span class="c1"># (D, D)</span>
    <span class="n">covariances</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">cov_k</span>
</pre></div>
</div>
<p>where</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">responsibilities</span></code> is a <span class="math notranslate nohighlight">\(N \times K\)</span> matrix of responsibilities <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nk</span></code> is a <span class="math notranslate nohighlight">\(K\)</span>-dimensional vector of <span class="math notranslate nohighlight">\(N_{k}\)</span> values, and</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">means</span></code> is a <span class="math notranslate nohighlight">\(K \times D\)</span> matrix of mean parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span>.</p></li>
</ul>
<p>Why? Because if you look at the equation for updating the covariance matrices:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\Sigma}_k^{n e w}=\frac{1}{N_k} \sum_{n=1}^N r_k^{(n)}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_k^{\text {new }}\right)\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_k^{\text {new }}\right)^T
\]</div>
<p>For each Gaussian component <span class="math notranslate nohighlight">\(k\)</span>, we compute the difference between the data points <code class="docutils literal notranslate"><span class="pre">X</span></code> and the updated mean <code class="docutils literal notranslate"><span class="pre">means[k]</span></code>. This results in a <span class="math notranslate nohighlight">\((N, D)\)</span> matrix <code class="docutils literal notranslate"><span class="pre">diff</span></code>, where <span class="math notranslate nohighlight">\(N\)</span> is the number of data points.</p>
<p>To obtain the element-wise product of the responsibilities <code class="docutils literal notranslate"><span class="pre">responsibilities[:,</span> <span class="pre">k]</span></code> with the differences <code class="docutils literal notranslate"><span class="pre">diff</span></code>, we reshape the responsibilities to a column vector of shape <span class="math notranslate nohighlight">\((N, 1)\)</span> and multiply it element-wise with <code class="docutils literal notranslate"><span class="pre">diff</span></code>. This results in a <span class="math notranslate nohighlight">\((N, D)\)</span> matrix <code class="docutils literal notranslate"><span class="pre">weighted_diff</span></code>.</p>
<p>We then compute the covariance matrix for the <span class="math notranslate nohighlight">\(k\)</span>-th component by calculating the matrix product of the transpose of <code class="docutils literal notranslate"><span class="pre">weighted_diff</span></code> with <code class="docutils literal notranslate"><span class="pre">diff</span></code>, and then dividing the result by the <span class="math notranslate nohighlight">\(k\)</span>-th element of <code class="docutils literal notranslate"><span class="pre">nk</span></code>. This gives us a <span class="math notranslate nohighlight">\((D, D)\)</span> matrix <code class="docutils literal notranslate"><span class="pre">cov_k</span></code>.</p>
<p>Finally, we store the computed covariance matrix <code class="docutils literal notranslate"><span class="pre">cov_k</span></code> in the <code class="docutils literal notranslate"><span class="pre">covariances</span></code> array at the index <code class="docutils literal notranslate"><span class="pre">k</span></code>.</p>
<p>This Python code snippet computes the updated covariance matrices <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span> for all Gaussian components <span class="math notranslate nohighlight">\(k=1, \ldots, K\)</span>.</p>
<hr class="docutils" />
<p>We will derive the matrix justification for updating the covariance matrices <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{\Sigma}_{k}^{new} &amp;= \frac{1}{N_k}\sum_{n=1}^N r^{(n)}_{k}\left(\boldsymbol{x}^{(n)} - \boldsymbol{\mu}_{k}^{new}\right)\left(\boldsymbol{x}^{(n)} - \boldsymbol{\mu}_{k}^{new}\right)^T \\
\end{aligned}
\end{split}\]</div>
<p>We will rewrite the summation as a matrix product. First, letâ€™s define the difference matrix <span class="math notranslate nohighlight">\(\boldsymbol{D}_k\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{D}_k = \begin{bmatrix} \boldsymbol{x}^{(1)} - \boldsymbol{\mu}_{k}^{new} \\ \vdots \\ \boldsymbol{x}^{(N)} - \boldsymbol{\mu}_{k}^{new} \end{bmatrix} \in \mathbb{R}^{N \times D}
\end{split}\]</div>
<p>Now, letâ€™s define a diagonal matrix <span class="math notranslate nohighlight">\(\boldsymbol{W}_k\)</span> with the <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span> values on its diagonal:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{W}_k = \begin{bmatrix} r^{(1)}_{k} &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; r^{(2)}_{k} &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp; r^{(N)}_{k} \end{bmatrix} \in \mathbb{R}^{N \times N}
\end{split}\]</div>
<p>Now we can rewrite the covariance matrix update equation as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{\Sigma}_{k}^{new} &amp;= \frac{1}{N_k} \boldsymbol{D}_k^T \boldsymbol{W}_k \boldsymbol{D}_k \\
\end{aligned}
\end{split}\]</div>
<p>The Python code snippet computes the same equation as described above:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">covariances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">D</span><span class="p">,</span> <span class="n">D</span><span class="p">))</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">means</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>                                                           <span class="c1"># (N, D)</span>
    <span class="n">weighted_diff</span> <span class="o">=</span> <span class="n">responsibilities</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">diff</span>                  <span class="c1"># (N, D)</span>
    <span class="n">cov_k</span> <span class="o">=</span> <span class="n">weighted_diff</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">diff</span> <span class="o">/</span> <span class="n">nk</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>                                        <span class="c1"># (D, D)</span>
    <span class="n">covariances</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">cov_k</span>
</pre></div>
</div>
<p>The only slighly difference is the code uses <code class="docutils literal notranslate"><span class="pre">*</span></code> which is the Hadamard product (element-wise product) instead of <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> which is the matrix product. They will result in the same result.</p>
</section>
</section>
<section id="estimating-the-mixing-coefficients-prior-weights-pi-k">
<h3><a class="toc-backref" href="#id83" role="doc-backlink">Estimating the Mixing Coefficients (Prior/Weights) <span class="math notranslate nohighlight">\(\pi_{k}\)</span></a><a class="headerlink" href="#estimating-the-mixing-coefficients-prior-weights-pi-k" title="Link to this heading">#</a></h3>
<div class="proof theorem admonition" id="thm:gmm-update-mixture-weights">
<p class="admonition-title"><span class="caption-number">Theorem 13 </span> (Update of the GMM Mixture Weights)</p>
<section class="theorem-content" id="proof-content">
<p>The mixture weights of the GMM are updated as</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-update-mixture-weights-1">
<span class="eqno">(117)<a class="headerlink" href="#equation-eq-gmm-update-mixture-weights-1" title="Link to this equation">#</a></span>\[
\pi_{k}^{\text {new }}=\frac{N_{k}}{N}, \quad k=1, \ldots, K
\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the number of data points and <span class="math notranslate nohighlight">\(N_{k}\)</span> is defined in <a class="reference internal" href="#equation-eq-nk-1">(107)</a>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. To find the partial derivative of the log-likelihood with respect to the weight parameters <span class="math notranslate nohighlight">\(\pi_{k}, k=1, \ldots, K\)</span>, we account for the constraint <span class="math notranslate nohighlight">\(\sum_{k} \pi_{k}=1\)</span> by using Lagrange multipliers (see Section 7.2 of Mathematics for Machine Learning <span id="id19">[<a class="reference internal" href="../../bibliography.html#id9" title="Marc Peter Deisenroth, Cheng Soon Ong, and Aldo A. Faisal. Mathematics for Machine Learning. Cambridge University Press, 2021.">Deisenroth <em>et al.</em>, 2021</a>]</span>). The Lagrangian is</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-update-mixture-weights-2">
<span class="eqno">(118)<a class="headerlink" href="#equation-eq-gmm-update-mixture-weights-2" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathfrak{L}&amp;=\mathcal{L}+\lambda\left(\sum_{k=1}^{K} \pi_{k}-1\right) \\
&amp;=\sum_{n=1}^{N} \log \sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)+\lambda\left(\sum_{k=1}^{K} \pi_{k}-1\right) \\
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is the log-likelihood from <a class="reference internal" href="#equation-eq-gmm-log-likelihood-1">(101)</a> and the second term encodes for the equality constraint that all the mixture weights need to sum up to 1. We obtain the partial derivative with respect to <span class="math notranslate nohighlight">\(\pi_{k}\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-update-mixture-weights-3">
<span class="eqno">(119)<a class="headerlink" href="#equation-eq-gmm-update-mixture-weights-3" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\frac{\partial \mathfrak{L}}{\partial \pi_{k}} &amp; =\sum_{n=1}^{N} \frac{\mathcal{N}\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)}{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}\right)}+\lambda \\
&amp; =\frac{1}{\pi_{k}} \underbrace{\sum_{n=1}^{N} \frac{\pi_{k} \mathcal{N}\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)}{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}\right)}}_{=N_{k}}+\lambda=\frac{N_{k}}{\pi_{k}}+\lambda,
\end{aligned}
\end{split}\]</div>
<p>and the partial derivative with respect to the Lagrange multiplier <span class="math notranslate nohighlight">\(\lambda\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-update-mixture-weights-4">
<span class="eqno">(120)<a class="headerlink" href="#equation-eq-gmm-update-mixture-weights-4" title="Link to this equation">#</a></span>\[
\frac{\partial \mathfrak{L}}{\partial \lambda}=\sum_{k=1}^{K} \pi_{k}-1
\]</div>
<p>Setting both partial derivatives to <span class="math notranslate nohighlight">\(\mathbf{0}\)</span> (necessary condition for optimum) yields the system of equations</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-update-mixture-weights-5">
<span class="eqno">(121)<a class="headerlink" href="#equation-eq-gmm-update-mixture-weights-5" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
&amp; \pi_{k}=-\frac{N_{k}}{\lambda} &amp;&amp; (a)\\
&amp; 1=\sum_{k=1}^{K} \pi_{k} &amp;&amp; (b)
\end{aligned}
\end{split}\]</div>
<p>Using (a) in (b) and solving for <span class="math notranslate nohighlight">\(\pi_{k}\)</span>, we obtain</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-update-mixture-weights-6">
<span class="eqno">(122)<a class="headerlink" href="#equation-eq-gmm-update-mixture-weights-6" title="Link to this equation">#</a></span>\[
\sum_{k=1}^{K} \pi_{k}=1 \Longleftrightarrow-\sum_{k=1}^{K} \frac{N_{k}}{\lambda}=1 \Longleftrightarrow-\frac{N}{\lambda}=1 \Longleftrightarrow \lambda=-N .
\]</div>
<p>This allows us to substitute <span class="math notranslate nohighlight">\(-N\)</span> for <span class="math notranslate nohighlight">\(\lambda\)</span> in (a) to obtain</p>
<div class="math notranslate nohighlight" id="equation-eq-gmm-update-mixture-weights-7">
<span class="eqno">(123)<a class="headerlink" href="#equation-eq-gmm-update-mixture-weights-7" title="Link to this equation">#</a></span>\[
\pi_{k}^{\mathrm{new}}=\frac{N_{k}}{N}
\]</div>
<p>which gives us the update for the weight parameters <span class="math notranslate nohighlight">\(\pi_{k}\)</span> and proves <a class="reference internal" href="#thm:gmm-update-mixture-weights">Theorem 13</a>.</p>
</div>
<section id="id20">
<h4><a class="toc-backref" href="#id84" role="doc-backlink">Some Intuition</a><a class="headerlink" href="#id20" title="Link to this heading">#</a></h4>
<p>We can identify the mixture weight in <a class="reference internal" href="#equation-eq-gmm-update-mixture-weights-1">(117)</a> as the ratio of the total responsibility of the <span class="math notranslate nohighlight">\(k\)</span> th cluster and the number of data points. Since <span class="math notranslate nohighlight">\(N=\sum_{k} N_{k}\)</span>, the number of data points can also be interpreted as the total responsibility of all mixture components together, such that <span class="math notranslate nohighlight">\(\pi_{k}\)</span> is the relative importance of the <span class="math notranslate nohighlight">\(k\)</span> th mixture component for the dataset.</p>
<div class="proof remark admonition" id="rem:gmm-update-mixture-weights-depends-on-all-parameters">
<p class="admonition-title"><span class="caption-number">Remark 34 </span> (Update of the GMM Mixture Weights depends on all Parameters)</p>
<section class="remark-content" id="proof-content">
<p>Since <span class="math notranslate nohighlight">\(N_{k}=\sum_{i=1}^{N} r^{(n)}_{k}\)</span>, the update equation (11.42) for the mixture weights <span class="math notranslate nohighlight">\(\pi_{k}\)</span> also depends on all <span class="math notranslate nohighlight">\(\pi_{j}, \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}, j=1, \ldots, K\)</span> via the responsibilities <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span>.</p>
</section>
</div></section>
<section id="update-weight-prior-of-running-example">
<h4><a class="toc-backref" href="#id85" role="doc-backlink">Update Weight/Prior of Running Example</a><a class="headerlink" href="#update-weight-prior-of-running-example" title="Link to this heading">#</a></h4>
<div class="proof example admonition" id="ex:gmm-update-mixture-weights">
<p class="admonition-title"><span class="caption-number">Example 18 </span> (Running Example: Update of the GMM Mixture Weights)</p>
<section class="example-content" id="proof-content">
<p>In our running example from Figure 11.1, the mixture weights are updated as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp; \pi_{1}: \frac{1}{3} \rightarrow 0.29 \\
&amp; \pi_{2}: \frac{1}{3} \rightarrow 0.29 \\
&amp; \pi_{3}: \frac{1}{3} \rightarrow 0.42
\end{aligned}
\end{split}\]</div>
<p>Here we see that the third component gets more weight/importance, while the other components become slightly less important. The figure below illustrates the effect of updating the mixture weights. The left figure below shows the GMM density and its individual components prior to updating the mixture weights. The right figure shows the GMM density after updating the mixture weights.</p>
<p>Overall, having updated the means, the variances, and the weights once, we obtain the GMM shown in the figure below. Compared with the initialization shown in the very original, we can see that the parameter updates caused the GMM density to shift some of its mass toward the data points.</p>
<p>After updating the means, variances, and weights once, the GMM fit in the updated one is already remarkably better than its initialization from the original. This is also evidenced by the log-likelihood values, which increased from 28.3 (initialization) to 14.4 after one complete update cycle (you can verify this by hand or code).</p>
</section>
</div><div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1"># Update the mixture weights</span>
<span class="linenos"> 2</span><span class="n">pis_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.29</span><span class="p">,</span> <span class="mf">0.29</span><span class="p">,</span> <span class="mf">0.42</span><span class="p">])</span>
<span class="linenos"> 3</span>
<span class="linenos"> 4</span><span class="c1"># Create a new GMM with the updated mixture weights</span>
<span class="linenos"> 5</span><span class="n">pdfs_updated_pis</span><span class="p">,</span> <span class="n">gmm_pdf_updated_pis</span> <span class="o">=</span> <span class="n">create_gmm</span><span class="p">(</span><span class="n">mus_new</span><span class="p">,</span> <span class="n">sigmas_new</span><span class="p">,</span> <span class="n">pis_new</span><span class="p">,</span> <span class="n">x_range</span><span class="p">)</span>
<span class="linenos"> 6</span>
<span class="linenos"> 7</span><span class="c1"># Create the original GMM with the initial means, sigmas, and pis</span>
<span class="linenos"> 8</span><span class="n">pdfs_original</span><span class="p">,</span> <span class="n">gmm_pdf_original</span> <span class="o">=</span> <span class="n">create_gmm</span><span class="p">(</span><span class="n">mus</span><span class="p">,</span> <span class="n">sigmas</span><span class="p">,</span> <span class="n">pis</span><span class="p">,</span> <span class="n">x_range</span><span class="p">)</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="linenos">11</span>
<span class="linenos">12</span><span class="c1"># Plot the updated GMM with new means and variances</span>
<span class="linenos">13</span><span class="n">plot_gmm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mus_new</span><span class="p">,</span> <span class="n">sigmas_new</span><span class="p">,</span> <span class="n">pis</span><span class="p">,</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">pdfs_new2</span><span class="p">,</span> <span class="n">gmm_pdf_new2</span><span class="p">,</span> <span class="s1">&#39;Updated Gaussian Mixture Model and Data Points with New Variances&#39;</span><span class="p">,</span> <span class="n">ax1</span><span class="p">)</span>
<span class="linenos">14</span>
<span class="linenos">15</span><span class="c1"># Plot the updated GMM with new means, sigmas, and pis</span>
<span class="linenos">16</span><span class="n">plot_gmm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mus_new</span><span class="p">,</span> <span class="n">sigmas_new</span><span class="p">,</span> <span class="n">pis_new</span><span class="p">,</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">pdfs_updated_pis</span><span class="p">,</span> <span class="n">gmm_pdf_updated_pis</span><span class="p">,</span> <span class="s1">&#39;Updated Gaussian Mixture Model and Data Points with new Weights.&#39;</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span>
<span class="linenos">17</span>
<span class="linenos">18</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,157 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,171 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,186 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,354 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,367 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,381 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<img alt="../../_images/6597a26ce5d13cfc81cca2cbaeb10e28fbdbac7057708944490a6ab17492ed28.svg" src="../../_images/6597a26ce5d13cfc81cca2cbaeb10e28fbdbac7057708944490a6ab17492ed28.svg" />
</div>
</div>
<p>We see the full cycle of updates below:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="linenos"> 2</span><span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">),</span> <span class="p">(</span><span class="n">ax3</span><span class="p">,</span> <span class="n">ax4</span><span class="p">)</span> <span class="o">=</span> <span class="n">axes</span>
<span class="linenos"> 3</span>
<span class="linenos"> 4</span><span class="c1"># Plot the original GMM</span>
<span class="linenos"> 5</span><span class="n">plot_gmm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mus</span><span class="p">,</span> <span class="n">sigmas</span><span class="p">,</span> <span class="n">pis</span><span class="p">,</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">pdfs_original</span><span class="p">,</span> <span class="n">gmm_pdf_original</span><span class="p">,</span> <span class="s1">&#39;Initial Gaussian Mixture Model and Data Points&#39;</span><span class="p">,</span> <span class="n">ax1</span><span class="p">)</span>
<span class="linenos"> 6</span>
<span class="linenos"> 7</span><span class="c1"># Plot the updated GMM with new means</span>
<span class="linenos"> 8</span><span class="n">pdfs_updated_means</span><span class="p">,</span> <span class="n">gmm_pdf_updated_means</span> <span class="o">=</span> <span class="n">create_gmm</span><span class="p">(</span><span class="n">mus_new</span><span class="p">,</span> <span class="n">sigmas</span><span class="p">,</span> <span class="n">pis</span><span class="p">,</span> <span class="n">x_range</span><span class="p">)</span>
<span class="linenos"> 9</span><span class="n">plot_gmm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mus_new</span><span class="p">,</span> <span class="n">sigmas</span><span class="p">,</span> <span class="n">pis</span><span class="p">,</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">pdfs_updated_means</span><span class="p">,</span> <span class="n">gmm_pdf_updated_means</span><span class="p">,</span> <span class="s1">&#39;Updated Means&#39;</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span>
<span class="linenos">10</span>
<span class="linenos">11</span><span class="c1"># Plot the updated GMM with new sigmas</span>
<span class="linenos">12</span><span class="n">pdfs_updated_sigmas</span><span class="p">,</span> <span class="n">gmm_pdf_updated_sigmas</span> <span class="o">=</span> <span class="n">create_gmm</span><span class="p">(</span><span class="n">mus_new</span><span class="p">,</span> <span class="n">sigmas_new</span><span class="p">,</span> <span class="n">pis</span><span class="p">,</span> <span class="n">x_range</span><span class="p">)</span>
<span class="linenos">13</span><span class="n">plot_gmm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mus_new</span><span class="p">,</span> <span class="n">sigmas_new</span><span class="p">,</span> <span class="n">pis</span><span class="p">,</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">pdfs_updated_sigmas</span><span class="p">,</span> <span class="n">gmm_pdf_updated_sigmas</span><span class="p">,</span> <span class="s1">&#39;Updated Sigmas&#39;</span><span class="p">,</span> <span class="n">ax3</span><span class="p">)</span>
<span class="linenos">14</span>
<span class="linenos">15</span><span class="c1"># Plot the updated GMM with new pis</span>
<span class="linenos">16</span><span class="n">pdfs_updated_pis</span><span class="p">,</span> <span class="n">gmm_pdf_updated_pis</span> <span class="o">=</span> <span class="n">create_gmm</span><span class="p">(</span><span class="n">mus_new</span><span class="p">,</span> <span class="n">sigmas_new</span><span class="p">,</span> <span class="n">pis_new</span><span class="p">,</span> <span class="n">x_range</span><span class="p">)</span>
<span class="linenos">17</span><span class="n">plot_gmm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">mus_new</span><span class="p">,</span> <span class="n">sigmas_new</span><span class="p">,</span> <span class="n">pis_new</span><span class="p">,</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">pdfs_updated_pis</span><span class="p">,</span> <span class="n">gmm_pdf_updated_pis</span><span class="p">,</span> <span class="s1">&#39;Updated Mixture Weights&#39;</span><span class="p">,</span> <span class="n">ax4</span><span class="p">)</span>
<span class="linenos">18</span>
<span class="linenos">19</span><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="linenos">20</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,579 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,588 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,595 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,646 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,658 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,667 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,719 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,729 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,740 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,816 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,823 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,829 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,870 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,878 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,887 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,930 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,941 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:44,951 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:45,134 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:45,145 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:45,154 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:45,228 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:45,235 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:45,241 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:45,295 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:45,307 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-09-20 12:43:45,316 - INFO - Substituting symbol N from STIXNonUnicode
</pre></div>
</div>
<img alt="../../_images/d52e718224693c1f5d5faec33b4ff4708a02111a64451d891ad1bed5f98f65c4.svg" src="../../_images/d52e718224693c1f5d5faec33b4ff4708a02111a64451d891ad1bed5f98f65c4.svg" />
</div>
</div>
</section>
</section>
</section>
<section id="why-gmm-has-no-closed-form-solution">
<h2><a class="toc-backref" href="#id86" role="doc-backlink">Why GMM has no Closed-Form Solution</a><a class="headerlink" href="#why-gmm-has-no-closed-form-solution" title="Link to this heading">#</a></h2>
<p>We have emphasized along the way that the estimation of the parameters of a Gaussian Mixture Model is a difficult problem and has no closed-form solution. However, it may be
confusing to one why this is the case.</p>
<p>Since we seemingly have found estimates for the mean, covariance, and mixture weights of the GMM in the previous 3 sections, why is it that we cannot find a closed-form solution?</p>
<p>Well, there is some intricacy to this question. First, we clear the confusiong:</p>
<ol class="arabic">
<li><p>In the context of Gaussian Mixture Models, a closed-form solution refers to a single expression that can simultaneously provide the optimal values of all the parameters without any iterative steps or dependencies between the parameters. However, in the case of GMM, the responsibilities, which are crucial for updating the parameters, depend on the parameters themselves in a complex manner (more on this in Bishopâ€™s book).</p></li>
<li><p>Although we can optimally update each of the parameters <strong>given</strong> the other parameters, we cannot compute all the parameters at once because of their interdependence. What does this mean? This means when we update the means, we need to know the responsibilities, which depend on the means. However, when we update the responsibilities, we need to know the means, which depend on the responsibilities. This is a circular dependency, which means we cannot simultaneously update the means and the responsibilities (easily).</p>
<p>Moreover, when we update the covariance matrices, we need to know <strong>both</strong> the means and the responsibilities! Again, <em><strong>simultaneous</strong></em> updates of the means, responsibilities, and covariance matrices are not possible.</p>
<p>Finally, when we update the mixture weights, we need to know <strong>all</strong> the parameters, including the means, responsibilities, and covariance matrices. Again, <em><strong>simultaneous</strong></em> updates of all the parameters are not possible.</p>
</li>
</ol>
<p>So the reason we can â€œfind the estimatesâ€ just now is because we are not
simultaneously finding the optimal values of all the parameters. Instead, we are finding the optimal values of the parameters <strong>given</strong> the other parameters. This is a very important distinction.</p>
<p>This situation is similar to hard clustering in K-Means, where the ultimate goal is to jointly optimize cluster means and assignments, which is a NP-hard problem. However, we can optimize the cluster means <strong>given</strong> the cluster assignments, and then optimize the cluster assignments <strong>given</strong> the cluster means. This is a two-step process, which is not a closed-form solution, but gives local optima for each step. The same is true for GMM.</p>
<p>This interesting result gives rise to iterative methods like <a class="reference external" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation-Maximization (EM)</a>.</p>
</section>
<section id="the-expectation-maximization-em-algorithm">
<h2><a class="toc-backref" href="#id87" role="doc-backlink">The Expectation-Maximization (EM) Algorithm</a><a class="headerlink" href="#the-expectation-maximization-em-algorithm" title="Link to this heading">#</a></h2>
<p>We are now ready to introduce the Expectation-Maximization (EM) algorithm, which is a popular algorithm for estimating the parameters of a Gaussian Mixture Model.</p>
<p>As it stands, we will start by introducing the EM algorithm in the context of Gaussian Mixture Models. This is an slightly more <em>informal</em> treatment of the EM algorithm, which is meant to give you a general idea of how the algorithm works.</p>
<section id="expectation-maximization-em-gaussion-mixture-model-perspective">
<h3><a class="toc-backref" href="#id88" role="doc-backlink">Expectation-Maximization (EM) (Gaussion Mixture Model Perspective)</a><a class="headerlink" href="#expectation-maximization-em-gaussion-mixture-model-perspective" title="Link to this heading">#</a></h3>
<p><strong>This section below is from chapter 11.3. EM Algorithm, Mathematics for Machine Learning.</strong></p>
<p>Unfortunately, the updates in <a class="reference internal" href="#equation-eq-gmm-update-means-1">(104)</a>, <a class="reference internal" href="#equation-eq-gmm-update-covariance-1">(111)</a>, and <a class="reference internal" href="#equation-eq-gmm-update-mixture-weights-1">(117)</a> do not constitute a closed-form solution for the updates of the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}, \pi_{k}\)</span> of the mixture model because the responsibilities <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span> depend on those parameters in a complex way. However, the results suggest a simple iterative scheme for finding a solution to the parameters estimation problem via maximum likelihood. The expectation maximization algorithm was proposed by Dempster et al. (1977) and is a general iterative scheme for learning parameters (maximum likelihood or MAP) in mixture models and, more generally, latent-variable models.</p>
<p>In our example of the Gaussian mixture model, we choose initial values for <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}, \pi_{k}\)</span> and alternate until convergence between</p>
<ul class="simple">
<li><p>E-step: Evaluate the responsibilities <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span> (posterior probability of data point <span class="math notranslate nohighlight">\(n\)</span> belonging to mixture component <span class="math notranslate nohighlight">\(k\)</span> ).</p></li>
<li><p>M-step: Use the updated responsibilities to reestimate the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}, \pi_{k}\)</span>.</p></li>
</ul>
<p>Every step in the EM algorithm increases the log-likelihood function (Neal and Hinton, 1999). For convergence, we can check the log-likelihood or the parameters directly. A concrete instantiation of the EM algorithm for estimating the parameters of a GMM is as follows.</p>
<div class="proof algorithm admonition" id="alg:em-gmm-1">
<p class="admonition-title"><span class="caption-number">Algorithm 7 </span> (EM Algorithm for GMM)</p>
<section class="algorithm-content" id="proof-content">
<ol class="arabic simple">
<li><p>Initialize <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}, \pi_{k}\)</span>.</p></li>
<li><p>E-step: Evaluate responsibilities <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span> for every data point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> using current parameters <span class="math notranslate nohighlight">\(\pi_{k}, \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\)</span> :</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
r^{(n)}_{k}=\frac{\pi_{k} \mathcal{N}\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)}{\sum_{j} \pi_{j} \mathcal{N}\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\mu}_{j}, \boldsymbol{\Sigma}_{j}\right)} .
\]</div>
<ol class="arabic simple" start="3">
<li><p>M-step: Reestimate parameters <span class="math notranslate nohighlight">\(\pi_{k}, \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\)</span> using the current responsibilities <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span> (from E-step):</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{\mu}_{k} &amp; =\frac{1}{N_{k}} \sum_{n=1}^{N} r^{(n)}_{k} \boldsymbol{x}^{(n)}, \\
\boldsymbol{\Sigma}_{k} &amp; =\frac{1}{N_{k}} \sum_{n=1}^{N} r^{(n)}_{k}\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)\left(\boldsymbol{x}^{(n)}-\boldsymbol{\mu}_{k}\right)^{\top}, \\
\pi_{k} &amp; =\frac{N_{k}}{N} .
\end{aligned}
\end{split}\]</div>
<ol class="arabic simple" start="4">
<li><p><strong>Convergence</strong>: Check if the log-likelihood has increased. More concretely,
you can also check if the mean/sum of the marginal log-likelihood has increased.
See my code for this.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\frac{1}{N} \sum_{n=1}^{N} \log \sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\boldsymbol{x}^{(n)} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)
\]</div>
</section>
</div></section>
<section id="expectation-maximization-em-latent-variable-perspective">
<h3><a class="toc-backref" href="#id89" role="doc-backlink">Expectation-Maximization (EM) (Latent Variable Perspective)</a><a class="headerlink" href="#expectation-maximization-em-latent-variable-perspective" title="Link to this heading">#</a></h3>
<p>Given a joint distribution <span class="math notranslate nohighlight">\(p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})\)</span> over observed variables <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and latent variables <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>, governed by parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, the goal is to maximize the likelihood function <span class="math notranslate nohighlight">\(p(\mathbf{X} \mid \boldsymbol{\theta})\)</span> with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.</p>
<ol class="arabic simple">
<li><p>Choose an initial setting for the parameters <span class="math notranslate nohighlight">\(\theta^{\text {old }}\)</span>.</p></li>
<li><p>E step Evaluate <span class="math notranslate nohighlight">\(p\left(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}^{\text {old }}\right)\)</span>.</p></li>
<li><p>M step Evaluate <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{\text {new }}\)</span> given by</p></li>
<li></li>
</ol>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}^{\text {new }}=\underset{\boldsymbol{\theta}}{\arg \max } \mathcal{Q}\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old }}\right)
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\mathcal{Q}\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old }}\right)=\sum_{\mathbf{Z}} p\left(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}^{\text {old }}\right) \ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta}) .
\]</div>
<p>and in a more general sense, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{Q}\left(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{\text{old}}\right) &amp; =\mathbb{E}_{\boldsymbol{z} \mid \boldsymbol{x}, \boldsymbol{\theta}^{\text{old}}}[\log p(\boldsymbol{x}, \boldsymbol{z} \mid \boldsymbol{\theta})] \\
&amp; =\int \log p(\boldsymbol{x}, \boldsymbol{z} \mid \boldsymbol{\theta}) p\left(\boldsymbol{z} \mid \boldsymbol{x}, \boldsymbol{\theta}^{\text{old}}\right) \mathrm{d} \boldsymbol{z}
\end{aligned}
\end{split}\]</div>
<p>for the continuous case.</p>
<ol class="arabic simple" start="5">
<li><p>Check for convergence of either the log likelihood or the parameter values. If the convergence criterion is not satisfied, then let</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}^{\text {old }} \leftarrow \boldsymbol{\theta}^{\text {new }}
\]</div>
<p>and return to step 2 .</p>
</section>
<section id="the-expectation-step-posterior-inference-and-responsibilities">
<h3><a class="toc-backref" href="#id90" role="doc-backlink">The Expectation Step (Posterior Inference and Responsibilities)</a><a class="headerlink" href="#the-expectation-step-posterior-inference-and-responsibilities" title="Link to this heading">#</a></h3>
<p>See Chapter 9.3. An Alternative View of EM, Pattern Recognition and Machine Learning.</p>
</section>
<section id="the-maximization-step-parameter-estimation">
<h3><a class="toc-backref" href="#id91" role="doc-backlink">The Maximization Step (Parameter Estimation)</a><a class="headerlink" href="#the-maximization-step-parameter-estimation" title="Link to this heading">#</a></h3>
<p>See Chapter 9.3. An Alternative View of EM, Pattern Recognition and Machine Learning.</p>
</section>
</section>
<section id="gmm-and-its-relation-to-k-means">
<h2><a class="toc-backref" href="#id92" role="doc-backlink">GMM and its Relation to K-Means</a><a class="headerlink" href="#gmm-and-its-relation-to-k-means" title="Link to this heading">#</a></h2>
<p>Gaussian mixture models are closely related to the <span class="math notranslate nohighlight">\(K\)</span>-means clustering algorithm. <span class="math notranslate nohighlight">\(K\)</span>-means also uses the EM algorithm to assign data points to clusters. If we treat the means in the GMM as cluster centers and ignore the covariances (or set them to <span class="math notranslate nohighlight">\(I\)</span> ), we arrive at <span class="math notranslate nohighlight">\(K\)</span>-means. As also nicely described by MacKay (2003), <span class="math notranslate nohighlight">\(K\)</span>-means makes a â€œhardâ€ assignment of data points to cluster centers <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span>, whereas a GMM makes a â€œsoftâ€ assignment via the responsibilities.</p>
<p>More information can be found in chapter 9.3.2 of Bishopâ€™s book, Pattern Recognition and Machine Learning.</p>
</section>
<section id="a-small-example">
<h2><a class="toc-backref" href="#id93" role="doc-backlink">A Small Example</a><a class="headerlink" href="#a-small-example" title="Link to this heading">#</a></h2>
<p>Consider a scenario where <span class="math notranslate nohighlight">\(N = 10\)</span>, <span class="math notranslate nohighlight">\(D = 2\)</span>, and <span class="math notranslate nohighlight">\(K = 3\)</span> where the data points are as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X} = \begin{bmatrix}  1 &amp;  3 \\ 2 &amp;  4 \\ 3 &amp;  5 \\ 4 &amp;  6 \\ 5 &amp;  7 \\ 6 &amp;  8 \\ 7 &amp;  9 \\ 8 &amp; 10 \\ 9 &amp; 11 \\ 10 &amp; 12 \end{bmatrix}_{10 \times 2}
\end{split}\]</div>
<section id="random-initialization">
<h3><a class="toc-backref" href="#id94" role="doc-backlink">Random Initialization</a><a class="headerlink" href="#random-initialization" title="Link to this heading">#</a></h3>
<p>To initialize the GMM, we randomly initialize some means <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span>, covariances <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span>, and mixing coefficients <span class="math notranslate nohighlight">\(\pi_{k}\)</span>. The means
can be the data points themselves, and the covariances can be set to <span class="math notranslate nohighlight">\(\sigma^{2} \boldsymbol{I}\)</span> where <span class="math notranslate nohighlight">\(\sigma^{2}\)</span> is the variance of the data points. The mixing coefficients can be set to <span class="math notranslate nohighlight">\(\frac{1}{K}\)</span>.</p>
<p>This is evident from the code in our implementation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">weights_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_components</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_components</span><span class="p">)</span>         <span class="c1"># (K,)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">means_</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_components</span><span class="p">)]</span>      <span class="c1"># (K, D)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">covariances_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>                                                 <span class="c1"># (K, D, D)</span>
    <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_components</span><span class="p">)]</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="k-means-and-hard-assignments">
<h3><a class="toc-backref" href="#id95" role="doc-backlink">K-Means and Hard Assignments</a><a class="headerlink" href="#k-means-and-hard-assignments" title="Link to this heading">#</a></h3>
<section id="e-step">
<h4><a class="toc-backref" href="#id96" role="doc-backlink">E-Step</a><a class="headerlink" href="#e-step" title="Link to this heading">#</a></h4>
<p>Now in K-Means, the E-Step involves assigning each data point to the closest cluster center,
defined by the mean <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span>.</p>
<p>For example, letâ€™s assume a hypothetical where we have assigned the data points to the following clusters:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
C_1 &amp;= \{1, 2, 3, 4, 5\}, \\
C_2 &amp;= \{6, 7, 8\}, \\
C_3 &amp;= \{9, 10\}.
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(C_k\)</span> is the set of data points in cluster <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(N_k\)</span> is the number of data points in cluster <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>This is a hard assignment, where each data point is assigned to one and only one cluster.</p>
<p>We can define the assignments above as a matrix of posterior probabilities, where each row sums to 1 and each column <span class="math notranslate nohighlight">\(k\)</span> sums to <span class="math notranslate nohighlight">\(N_k\)</span>.</p>
<p>What the posterior means is the probability that a data point <span class="math notranslate nohighlight">\(n\)</span> belongs to cluster <span class="math notranslate nohighlight">\(k\)</span> given the data <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. In this case, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{R} = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}_{10 \times 3}
\end{split}\]</div>
<p>where each row sums to 1 and each column <span class="math notranslate nohighlight">\(k\)</span> sums to <span class="math notranslate nohighlight">\(N_k\)</span>.</p>
<p>So we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
N_1 &amp;= 5, \\
N_2 &amp;= 3, \\
N_3 &amp;= 2.
\end{aligned}
\end{split}\]</div>
<p>Then the next step that follows is the M-Step, where we need to update the parameters of the K-Means model, which are the means <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span>.</p>
<p>The key here is we are updating the means of the clusters, not the means of the data points.
And it turns out our mean update is very similar, just the mean of the data points in each cluster.</p>
</section>
<section id="m-step">
<h4><a class="toc-backref" href="#id97" role="doc-backlink">M-Step</a><a class="headerlink" href="#m-step" title="Link to this heading">#</a></h4>
<p>Therefore, computing the mean of each component/cluster is as follows:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\boldsymbol{\mu}_k = \frac{1}{N_k} \sum_{n \in C_k} \boldsymbol{x}^{(n)}
\end{aligned}
\]</div>
<p>where <span class="math notranslate nohighlight">\(N_k\)</span> is the number of data points in cluster <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{\mu}_1 &amp;= \frac{1}{5} \sum_{n=1}^5 \boldsymbol{x}_n = \frac{1}{5} \begin{bmatrix} 1 + 2 + 3 + 4 + 5 \\ 3 + 4 + 5 + 6 + 7 \end{bmatrix} = \begin{bmatrix} 3 \\ 5 \end{bmatrix} \\
\boldsymbol{\mu}_2 &amp;= \frac{1}{3} \sum_{n=6}^8 \boldsymbol{x}_n = \frac{1}{3} \begin{bmatrix} 6 + 7 + 8 \\ 8 + 9 + 10 \end{bmatrix} = \begin{bmatrix} 7 \\ 9 \end{bmatrix} \\
\boldsymbol{\mu}_3 &amp;= \frac{1}{2} \sum_{n=9}^{10} \boldsymbol{x}_n = \frac{1}{2} \begin{bmatrix} 9 + 10 \\ 11 + 12 \end{bmatrix} = \begin{bmatrix} 9.5 \\ 11.5 \end{bmatrix}
\end{aligned}
\end{split}\]</div>
<p>where each <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span> is a <span class="math notranslate nohighlight">\(D \times 1\)</span> vector.</p>
<p>This mean formula is intuitive because we are taking the average of the data points in each cluster. What is not so intuitive is the mean formula for the GMM, which is the weighted average of the data points in each cluster, where the weights are the posterior probabilities.</p>
</section>
</section>
<section id="gmm-and-soft-assignments">
<h3><a class="toc-backref" href="#id98" role="doc-backlink">GMM and Soft Assignments</a><a class="headerlink" href="#gmm-and-soft-assignments" title="Link to this heading">#</a></h3>
<p>Recall the formula for updating the means, covariances and weights of Gaussian mixture model,
as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{\mu}_k &amp; =\frac{1}{N_k} \sum_{n=1}^N r^{(n)}_{k} \boldsymbol{x}_n, \\
\boldsymbol{\Sigma}_k &amp; =\frac{1}{N_k} \sum_{n=1}^N r^{(n)}_{k}\left(\boldsymbol{x}_n-\boldsymbol{\mu}_k\right)\left(\boldsymbol{x}_n-\boldsymbol{\mu}_k\right)^{\top}, \\
\pi_k &amp; =\frac{N_k}{N} .
\end{aligned}
\end{split}\]</div>
<p>Now, we only talk about the mean update, to have a comparison with K-Meansâ€™s mean update.
We notice there is an additional term <span class="math notranslate nohighlight">\(r^{(n)}_{k}\)</span> in the mean update formula. We have
already known what they are, but letâ€™s view it as an extension of the posterior probabilities
matrix defined just now.</p>
<section id="id21">
<h4><a class="toc-backref" href="#id99" role="doc-backlink">E-Step</a><a class="headerlink" href="#id21" title="Link to this heading">#</a></h4>
<p>Given some random initial parameters, we can compute the posterior probabilities matrix <span class="math notranslate nohighlight">\(\mathbf{R}\)</span>. Say</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{R} = \begin{bmatrix} 0.9 &amp; 0.1 &amp; 0 \\ 0.8 &amp; 0.2 &amp; 0 \\ 0.7 &amp; 0.3 &amp; 0 \\ 0.75 &amp; 0.25 &amp; 0 \\ 0.85 &amp; 0.15 &amp; 0 \\ 0 &amp; 0.9 &amp; 0.1 \\ 0 &amp; 0.8 &amp; 0.2 \\ 0 &amp; 0.85 &amp; 0.15 \\ 0 &amp; 0 &amp; 1 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}_{10 \times 3}
\end{split}\]</div>
<p>where each row sums to 1 and each column <span class="math notranslate nohighlight">\(k\)</span> sums to <span class="math notranslate nohighlight">\(N_k\)</span> but this <span class="math notranslate nohighlight">\(N_k\)</span> is not the number of data points in cluster <span class="math notranslate nohighlight">\(k\)</span> but the sum of the soft assignments of all data points to cluster <span class="math notranslate nohighlight">\(k\)</span>. Note each row is just the <span class="math notranslate nohighlight">\(p(\boldsymbol{z}^{(n)}| \boldsymbol{x}^{(n)}, \boldsymbol{\pi})\)</span> over all <span class="math notranslate nohighlight">\(K\)</span> components.</p>
<p>In other words, we have the following equation for the posterior probability of the latent label <span class="math notranslate nohighlight">\(z^{(n)} = k\)</span> given the data point <span class="math notranslate nohighlight">\(x^{(n)}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{R} &amp;= \begin{bmatrix} P(z^{(1)} = 1 \mid x^{(1)}) &amp; P(z^{(1)} = 2 \mid x^{(1)}) &amp; \cdots &amp; P(z^{(1)} = K \mid x^{(1)}) \\ P(z^{(2)} = 1 \mid x^{(2)}) &amp; P(z^{(2)} = 2 \mid x^{(2)}) &amp; \cdots &amp; P(z^{(2)} = K \mid x^{(2)}) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ P(z^{(N)} = 1 \mid x^{(N)}) &amp; P(z^{(N)} = 2 \mid x^{(N)}) &amp; \cdots &amp; P(z^{(N)} = K \mid x^{(N)}) \end{bmatrix}_{N \times K} \\
\end{aligned}
\end{split}\]</div>
<p>and each row of <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span> sums to 1. Why?</p>
<p>Because <span class="math notranslate nohighlight">\(P(z^{(n)} = k \mid x^{(n)})\)</span> is a probability distribution over the <span class="math notranslate nohighlight">\(K\)</span> components. Therefore, the sum of the probabilities over all <span class="math notranslate nohighlight">\(K\)</span> components must be 1.</p>
<p>Consequently, when we sum up all elements in <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span> we recover the total number of data points <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>The major difference between the matrix here versus the one we had in K-Means is that the matrix here is not binary, but it is continuous. This is because we are dealing with soft assignments, not hard assignments. In each row, the sum of the elements is still 1,
but it is no longer a scenario where each data point can only belong to one and only one cluster.</p>
<p>To this end, our <span class="math notranslate nohighlight">\(N_k\)</span> is also different from the one we had in K-Means.</p>
<p><span class="math notranslate nohighlight">\(N_k\)</span> is not the number of data points in cluster <span class="math notranslate nohighlight">\(k\)</span> but the sum of the soft assignments of all data points to cluster <span class="math notranslate nohighlight">\(k\)</span>. Letâ€™s see the <span class="math notranslate nohighlight">\(N_k\)</span> for each cluster:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
N_1 &amp;= \sum_{n=1}^5 r^{(n)}_{1} = 0.9 + 0.8 + 0.7 + 0.75 + 0.85 + 0 + 0 + 0 + 0 + 0 = 4 \\
N_2 &amp;= \sum_{n=6}^8 r^{(n)}_{2} = 0.1 + 0.2 + 0.3 + 0.25 + 0.15 + 0.9 + 0.8 + 0.85 + 0 + 0 = 3.55 \\
N_3 &amp;= \sum_{n=9}^{10} r^{(n)}_{3} = 0 + 0 + 0 + 0 + 0 + 0.1 + 0.2 + 0.15 + 1 + 1 = 2.45
\end{aligned}
\end{split}\]</div>
<p>Notice that <span class="math notranslate nohighlight">\(N_1 + N_2 + N_3 = 10\)</span> but intuitively each <span class="math notranslate nohighlight">\(N_k\)</span> is a soft representation of the number of data points in cluster <span class="math notranslate nohighlight">\(k\)</span>. So you can still interpret <span class="math notranslate nohighlight">\(N_k\)</span> as the number of data points in cluster <span class="math notranslate nohighlight">\(k\)</span> if
you want to for the sake of intuition.</p>
</section>
<section id="id22">
<h4><a class="toc-backref" href="#id100" role="doc-backlink">M-Step</a><a class="headerlink" href="#id22" title="Link to this heading">#</a></h4>
<p>Now we can update the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_k\)</span> and <span class="math notranslate nohighlight">\(\pi_k\)</span>.</p>
<p>We only focus on the mean update here. The other two are similar.</p>
<p>The mean update is given by the following formula:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\mu}_k =\frac{1}{N_k} \sum_{n=1}^N r^{(n)}_{k} \boldsymbol{x}^{(n)}
\]</div>
<p>We have all the ingredients to compute the mean update. But we note to readers
that in K-Means, the mean formula is easy, the numerator is just the sum of all the data points in cluster <span class="math notranslate nohighlight">\(k\)</span> and the denominator is just the total number of data points in cluster <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>Here the denominator is <span class="math notranslate nohighlight">\(N_k\)</span>, which we have talked about.</p>
<p>Now there is one more thing to note, the numerator is also not the sum of all the data
points <span class="math notranslate nohighlight">\(x^{(n)}\)</span> in cluster <span class="math notranslate nohighlight">\(k\)</span> but the sum of the soft assignments of <strong>all</strong> data points to cluster <span class="math notranslate nohighlight">\(k\)</span>.
It is now the weighted sum of all the data points <span class="math notranslate nohighlight">\(x^{(n)}\)</span> in cluster <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{\mu}_1 &amp;= \frac{1}{N_1} \sum_{n=1}^{10} r^{(n)}_{1} \boldsymbol{x}^{(n)} = \frac{1}{N_1} \begin{bmatrix} 0.9 \times 1 + 0.8 \times 2 + 0.7 \times 3 + 0.75 \times 4 + 0.85 \times 5 + 0 \times 6 + 0 \times 7 + 0 \times 8 + 0 \times 9 + 0 \times 10 \\ 0.9 \times 3 + 0.8 \times 4 + 0.7 \times 5 + 0.75 \times 6 + 0.85 \times 7 + 0 \times 8 + 0 \times 9 + 0 \times 10 + 0 \times 11 + 0 \times 12 \end{bmatrix} = \begin{bmatrix} 3.75 \\ 5.75 \end{bmatrix} \\
\boldsymbol{\mu}_2 &amp;= \frac{1}{N_2} \sum_{n=1}^{10} r^{(n)}_{2} \boldsymbol{x}^{(n)} = \frac{1}{N_2} \begin{bmatrix} 0.1 \times 1 + 0.2 \times 2 + 0.3 \times 3 + 0.25 \times 4 + 0.15 \times 5 + 0.9 \times 6 + 0.8 \times 7 + 0.85 \times 8 + 0 \times 9 + 0 \times 10 \\ 0.1 \times 3 + 0.2 \times 4 + 0.3 \times 5 + 0.25 \times 6 + 0.15 \times 7 + 0.9 \times 8 + 0.8 \times 9 + 0.85 \times 10 + 0 \times 11 + 0 \times 12 \end{bmatrix} = \begin{bmatrix} 6.95 \\ 8.95 \end{bmatrix} \\
\boldsymbol{\mu}_3 &amp;= \frac{1}{N_3} \sum_{n=1}^{10} r^{(n)}_{3} \boldsymbol{x}^{(n)} = \frac{1}{N_3} \begin{bmatrix} 0 \times 1 + 0 \times 2 + 0 \times 3 + 0 \times 4 + 0 \times 5 + 0.1 \times 6 + 0.2 \times 7 + 0.15 \times 8 + 1 \times 9 + 1 \times 10 \\ 0 \times 3 + 0 \times 4 + 0 \times 5 + 0 \times 6 + 0 \times 7 + 0.1 \times 8 + 0.2 \times 9 + 0.15 \times 10 + 1 \times 11 + 1 \times 12 \end{bmatrix} = \begin{bmatrix} 9.5 \\ 11.5 \end{bmatrix}
\end{aligned}
\end{split}\]</div>
<p>so you can treat the numerator as the weighted sum of all the data points <span class="math notranslate nohighlight">\(x^{(n)}\)</span> in cluster <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>And thus when you divide the weighted sum of all the data points <span class="math notranslate nohighlight">\(x^{(n)}\)</span> in cluster <span class="math notranslate nohighlight">\(k\)</span> by the sum of the soft assignments of <strong>all</strong> data points to cluster <span class="math notranslate nohighlight">\(k\)</span>, you get the weighted mean of all the data points <span class="math notranslate nohighlight">\(x^{(n)}\)</span> in cluster <span class="math notranslate nohighlight">\(k\)</span>. This is the same as the weighted mean of all the data points <span class="math notranslate nohighlight">\(x^{(n)}\)</span> in cluster <span class="math notranslate nohighlight">\(k\)</span>.</p>
</section>
</section>
</section>
<section id="murphy-s-plots">
<h2><a class="toc-backref" href="#id101" role="doc-backlink">Murphyâ€™s Plots</a><a class="headerlink" href="#murphy-s-plots" title="Link to this heading">#</a></h2>
<section id="gmm-demo">
<h3><a class="toc-backref" href="#id102" role="doc-backlink">GMM Demo</a><a class="headerlink" href="#gmm-demo" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/book1/03/gmm_plot_demo.ipynb">Link here</a>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="linenos">  2</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="linenos">  3</span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">style</span>
<span class="linenos">  4</span><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">linalg</span>
<span class="linenos">  5</span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>
<span class="linenos">  6</span>
<span class="linenos">  7</span><span class="c1"># try:</span>
<span class="linenos">  8</span><span class="c1">#     import probml_utils as pml</span>
<span class="linenos">  9</span><span class="c1"># except ModuleNotFoundError:</span>
<span class="linenos"> 10</span><span class="c1">#     %pip install -qq git+https://github.com/probml/probml-utils.git</span>
<span class="linenos"> 11</span><span class="c1">#     import probml_utils as pml</span>
<span class="linenos"> 12</span>
<span class="linenos"> 13</span><span class="n">mu_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.22</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.45</span><span class="p">]])</span>
<span class="linenos"> 14</span><span class="n">mu_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]])</span>
<span class="linenos"> 15</span><span class="n">mu_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.77</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.55</span><span class="p">]])</span>
<span class="linenos"> 16</span><span class="n">Mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">mu_1</span><span class="p">,</span> <span class="n">mu_2</span><span class="p">,</span> <span class="n">mu_3</span><span class="p">])</span>
<span class="linenos"> 17</span>
<span class="linenos"> 18</span><span class="n">Sigma1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.011</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.018</span><span class="p">]])</span>
<span class="linenos"> 19</span><span class="n">Sigma2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.018</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.011</span><span class="p">]])</span>
<span class="linenos"> 20</span><span class="n">Sigma3</span> <span class="o">=</span> <span class="n">Sigma1</span>
<span class="linenos"> 21</span><span class="n">Sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">Sigma1</span><span class="p">,</span> <span class="n">Sigma2</span><span class="p">,</span> <span class="n">Sigma3</span><span class="p">])</span>
<span class="linenos"> 22</span><span class="n">mixmat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">]])</span>
<span class="linenos"> 23</span>
<span class="linenos"> 24</span>
<span class="linenos"> 25</span><span class="k">def</span> <span class="nf">sigmaEllipse2D</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">Sigma</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">npoints</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
<span class="linenos"> 26</span><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos"> 27</span><span class="sd">    SIGMAELLIPSE2D generates x,y-points which lie on the ellipse describing</span>
<span class="linenos"> 28</span><span class="sd">    a sigma level in the Gaussian density defined by mean and covariance.</span>
<span class="linenos"> 29</span>
<span class="linenos"> 30</span><span class="sd">    Input:</span>
<span class="linenos"> 31</span><span class="sd">        MU          [2 x 1] Mean of the Gaussian density</span>
<span class="linenos"> 32</span><span class="sd">        SIGMA       [2 x 2] Covariance matrix of the Gaussian density</span>
<span class="linenos"> 33</span><span class="sd">        LEVEL       Which sigma level curve to plot. Can take any positive value,</span>
<span class="linenos"> 34</span><span class="sd">                    but common choices are 1, 2 or 3. Default = 3.</span>
<span class="linenos"> 35</span><span class="sd">        NPOINTS     Number of points on the ellipse to generate. Default = 32.</span>
<span class="linenos"> 36</span>
<span class="linenos"> 37</span><span class="sd">    Output:</span>
<span class="linenos"> 38</span><span class="sd">        XY          [2 x npoints] matrix. First row holds x-coordinates, second</span>
<span class="linenos"> 39</span><span class="sd">                    row holds the y-coordinates. First and last columns should</span>
<span class="linenos"> 40</span><span class="sd">                    be the same point, to create a closed curve.</span>
<span class="linenos"> 41</span><span class="sd">    &quot;&quot;&quot;</span>
<span class="linenos"> 42</span>    <span class="n">phi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">npoints</span><span class="p">)</span>
<span class="linenos"> 43</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">phi</span><span class="p">)</span>
<span class="linenos"> 44</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">phi</span><span class="p">)</span>
<span class="linenos"> 45</span>    <span class="n">z</span> <span class="o">=</span> <span class="n">level</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="linenos"> 46</span>    <span class="n">xy</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">linalg</span><span class="o">.</span><span class="n">sqrtm</span><span class="p">(</span><span class="n">Sigma</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="linenos"> 47</span>    <span class="k">return</span> <span class="n">xy</span>
<span class="linenos"> 48</span>
<span class="linenos"> 49</span>
<span class="linenos"> 50</span><span class="k">def</span> <span class="nf">plot_sigma_levels</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">P</span><span class="p">):</span>
<span class="linenos"> 51</span>    <span class="n">xy_1</span> <span class="o">=</span> <span class="n">sigmaEllipse2D</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">)</span>
<span class="linenos"> 52</span>    <span class="n">xy_2</span> <span class="o">=</span> <span class="n">sigmaEllipse2D</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="linenos"> 53</span>    <span class="n">xy_3</span> <span class="o">=</span> <span class="n">sigmaEllipse2D</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="linenos"> 54</span>    <span class="n">xy_4</span> <span class="o">=</span> <span class="n">sigmaEllipse2D</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="linenos"> 55</span>    <span class="n">xy_5</span> <span class="o">=</span> <span class="n">sigmaEllipse2D</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="mf">1.25</span><span class="p">)</span>
<span class="linenos"> 56</span>    <span class="n">xy_6</span> <span class="o">=</span> <span class="n">sigmaEllipse2D</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
<span class="linenos"> 57</span>
<span class="linenos"> 58</span>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xy_1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xy_1</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="linenos"> 59</span>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xy_2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xy_2</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="linenos"> 60</span>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xy_3</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xy_3</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="linenos"> 61</span>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xy_4</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xy_4</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="linenos"> 62</span>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xy_5</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xy_5</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="linenos"> 63</span>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xy_6</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xy_6</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="linenos"> 64</span>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;ro&quot;</span><span class="p">)</span>
<span class="linenos"> 65</span>
<span class="linenos"> 66</span>
<span class="linenos"> 67</span><span class="k">def</span> <span class="nf">plot_sigma_vector</span><span class="p">(</span><span class="n">Mu</span><span class="p">,</span> <span class="n">Sigma</span><span class="p">):</span>
<span class="linenos"> 68</span>    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Mu</span><span class="p">)</span>
<span class="linenos"> 69</span>    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="linenos"> 70</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
<span class="linenos"> 71</span>        <span class="n">plot_sigma_levels</span><span class="p">(</span><span class="n">Mu</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">Sigma</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="linenos"> 72</span>    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="linenos"> 73</span>    <span class="c1"># pml.savefig(&quot;mixgaussSurface.pdf&quot;)</span>
<span class="linenos"> 74</span>    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="linenos"> 75</span>
<span class="linenos"> 76</span>
<span class="linenos"> 77</span><span class="n">plot_sigma_vector</span><span class="p">(</span><span class="n">Mu</span><span class="p">,</span> <span class="n">Sigma</span><span class="p">)</span>
<span class="linenos"> 78</span>
<span class="linenos"> 79</span>
<span class="linenos"> 80</span><span class="k">def</span> <span class="nf">plot_gaussian_mixture</span><span class="p">(</span><span class="n">Mu</span><span class="p">,</span> <span class="n">Sigma</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="linenos"> 81</span>    <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span>
<span class="linenos"> 82</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="linenos"> 83</span>    <span class="k">if</span> <span class="n">y</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span>
<span class="linenos"> 84</span>        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="linenos"> 85</span>
<span class="linenos"> 86</span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">Mu</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">Sigma</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span>
<span class="linenos"> 87</span>        <span class="k">pass</span>
<span class="linenos"> 88</span>    <span class="k">else</span><span class="p">:</span>
<span class="linenos"> 89</span>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error: Mu, Sigma and weights must have the same dimension&quot;</span><span class="p">)</span>
<span class="linenos"> 90</span>        <span class="k">return</span>
<span class="linenos"> 91</span>
<span class="linenos"> 92</span>    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="linenos"> 93</span>    <span class="n">Pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">))</span>
<span class="linenos"> 94</span>    <span class="n">Z</span> <span class="o">=</span> <span class="mi">0</span>
<span class="linenos"> 95</span>
<span class="linenos"> 96</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Mu</span><span class="p">)):</span>
<span class="linenos"> 97</span>        <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">Mu</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">Sigma</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">Pos</span><span class="p">)</span>
<span class="linenos"> 98</span>
<span class="linenos"> 99</span>    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="linenos">100</span>    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s2">&quot;3d&quot;</span><span class="p">)</span>
<span class="linenos">101</span>    <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;copper&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos">102</span>    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;X axis&quot;</span><span class="p">)</span>
<span class="linenos">103</span>    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Y axis&quot;</span><span class="p">)</span>
<span class="linenos">104</span>    <span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">&quot;Z axis&quot;</span><span class="p">)</span>
<span class="linenos">105</span>    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="linenos">106</span>    <span class="c1"># pml.savefig(&quot;mixgaussSurface.pdf&quot;)</span>
<span class="linenos">107</span>    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="linenos">108</span>
<span class="linenos">109</span>
<span class="linenos">110</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">]</span>
<span class="linenos">111</span><span class="c1"># plot_gaussian_mixture(Mu, Sigma, weights=weights)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/495388e18d9df8246759c293130b8d284eb292d85260b59f47a4e8fd0c0af692.svg" src="../../_images/495388e18d9df8246759c293130b8d284eb292d85260b59f47a4e8fd0c0af692.svg" />
</div>
</div>
</section>
<section id="gmm-2d-sklearn">
<h3><a class="toc-backref" href="#id103" role="doc-backlink">GMM 2D (sklearn)</a><a class="headerlink" href="#gmm-2d-sklearn" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://colab.research.google.com/github/probml/pyprobml/blob/master/notebooks/book1/03/gmm_2d.ipynb">Link here</a></p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">  1</span><span class="c1"># K-means clustering for semisupervised learning</span>
<span class="linenos">  2</span><span class="c1"># Code is from chapter 9 of</span>
<span class="linenos">  3</span><span class="c1"># https://github.com/ageron/handson-ml2</span>
<span class="linenos">  4</span>
<span class="linenos">  5</span>
<span class="linenos">  6</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="linenos">  7</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="linenos">  8</span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="linenos">  9</span><span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="linenos"> 10</span>
<span class="linenos"> 11</span><span class="kn">import</span> <span class="nn">itertools</span>
<span class="linenos"> 12</span><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">linalg</span>
<span class="linenos"> 13</span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="linenos"> 14</span>
<span class="linenos"> 15</span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="linenos"> 16</span><span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>
<span class="linenos"> 17</span>
<span class="linenos"> 18</span><span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">LogNorm</span>
<span class="linenos"> 19</span>
<span class="linenos"> 20</span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="linenos"> 21</span><span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
<span class="linenos"> 22</span>
<span class="linenos"> 23</span><span class="c1"># color_iter = itertools.cycle([&#39;navy&#39;, &#39;c&#39;, &#39;cornflowerblue&#39;, &#39;darkorange&#39;])</span>
<span class="linenos"> 24</span><span class="n">color_iter</span> <span class="o">=</span> <span class="n">itertools</span><span class="o">.</span><span class="n">cycle</span><span class="p">([</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">])</span>
<span class="linenos"> 25</span><span class="n">prop_cycle</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;axes.prop_cycle&quot;</span><span class="p">]</span>
<span class="linenos"> 26</span><span class="n">color_iter</span> <span class="o">=</span> <span class="n">prop_cycle</span><span class="o">.</span><span class="n">by_key</span><span class="p">()[</span><span class="s2">&quot;color&quot;</span><span class="p">]</span>
<span class="linenos"> 27</span>
<span class="linenos"> 28</span>
<span class="linenos"> 29</span><span class="k">if</span> <span class="mi">0</span><span class="p">:</span>
<span class="linenos"> 30</span>    <span class="n">K</span> <span class="o">=</span> <span class="mi">5</span>
<span class="linenos"> 31</span>    <span class="n">blob_centers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
<span class="linenos"> 32</span>        <span class="p">[[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">2.8</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">2.8</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">2.8</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">]]</span>
<span class="linenos"> 33</span>    <span class="p">)</span>
<span class="linenos"> 34</span>    <span class="n">blob_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">])</span>
<span class="linenos"> 35</span>    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span>
<span class="linenos"> 36</span>        <span class="n">n_samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="n">blob_centers</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="n">blob_std</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">7</span>
<span class="linenos"> 37</span>    <span class="p">)</span>
<span class="linenos"> 38</span>
<span class="linenos"> 39</span><span class="k">if</span> <span class="mi">0</span><span class="p">:</span>
<span class="linenos"> 40</span>    <span class="n">X1</span><span class="p">,</span> <span class="n">y1</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="linenos"> 41</span>    <span class="n">X1</span> <span class="o">=</span> <span class="n">X1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.374</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.732</span><span class="p">,</span> <span class="mf">0.598</span><span class="p">]]))</span>
<span class="linenos"> 42</span>    <span class="n">X2</span><span class="p">,</span> <span class="n">y2</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="linenos"> 43</span>    <span class="n">X2</span> <span class="o">=</span> <span class="n">X2</span> <span class="o">+</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">8</span><span class="p">]</span>
<span class="linenos"> 44</span>    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">]</span>
<span class="linenos"> 45</span>    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">]</span>
<span class="linenos"> 46</span>    <span class="n">K</span> <span class="o">=</span> <span class="mi">3</span>
<span class="linenos"> 47</span>
<span class="linenos"> 48</span><span class="k">if</span> <span class="mi">1</span><span class="p">:</span>
<span class="linenos"> 49</span>    <span class="c1"># two off-diagonal blobs</span>
<span class="linenos"> 50</span>    <span class="n">X1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="linenos"> 51</span>    <span class="n">X1</span> <span class="o">=</span> <span class="n">X1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.374</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.732</span><span class="p">,</span> <span class="mf">0.598</span><span class="p">]]))</span>
<span class="linenos"> 52</span>    <span class="c1"># three spherical blobs</span>
<span class="linenos"> 53</span>    <span class="n">blob_centers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]])</span>
<span class="linenos"> 54</span>    <span class="n">s</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="linenos"> 55</span>    <span class="n">blob_std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">s</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">s</span><span class="p">])</span>
<span class="linenos"> 56</span>    <span class="n">X2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span>
<span class="linenos"> 57</span>        <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="n">blob_centers</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="n">blob_std</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">7</span>
<span class="linenos"> 58</span>    <span class="p">)</span>
<span class="linenos"> 59</span>
<span class="linenos"> 60</span>    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">]</span>
<span class="linenos"> 61</span>    <span class="n">K</span> <span class="o">=</span> <span class="mi">5</span>
<span class="linenos"> 62</span>
<span class="linenos"> 63</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="linenos"> 64</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="mf">0.8</span><span class="p">)</span>
<span class="linenos"> 65</span><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="linenos"> 66</span><span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
<span class="linenos"> 67</span><span class="c1"># plt.savefig(&quot;figures/gmm_2d_data.pdf&quot;, dpi=300)</span>
<span class="linenos"> 68</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="linenos"> 69</span>
<span class="linenos"> 70</span><span class="n">gm</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">K</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="linenos"> 71</span><span class="n">gm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="linenos"> 72</span>
<span class="linenos"> 73</span><span class="n">w</span> <span class="o">=</span> <span class="n">gm</span><span class="o">.</span><span class="n">weights_</span>
<span class="linenos"> 74</span><span class="n">mu</span> <span class="o">=</span> <span class="n">gm</span><span class="o">.</span><span class="n">means_</span>
<span class="linenos"> 75</span><span class="n">Sigma</span> <span class="o">=</span> <span class="n">gm</span><span class="o">.</span><span class="n">covariances_</span>
<span class="linenos"> 76</span>
<span class="linenos"> 77</span><span class="n">resolution</span> <span class="o">=</span> <span class="mi">100</span>
<span class="linenos"> 78</span><span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">resolution</span><span class="p">)</span>
<span class="linenos"> 79</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
<span class="linenos"> 80</span><span class="n">X_full</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span><span class="o">.</span><span class="n">T</span>
<span class="linenos"> 81</span>
<span class="linenos"> 82</span><span class="c1"># score_samples is the log pdf</span>
<span class="linenos"> 83</span><span class="n">pdf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">gm</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">X_full</span><span class="p">))</span>
<span class="linenos"> 84</span><span class="n">pdf_probas</span> <span class="o">=</span> <span class="n">pdf</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">resolution</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
<span class="linenos"> 85</span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;integral of pdf </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pdf_probas</span><span class="o">.</span><span class="n">sum</span><span class="p">()))</span>
<span class="linenos"> 86</span>
<span class="linenos"> 87</span>
<span class="linenos"> 88</span><span class="c1"># https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm.html</span>
<span class="linenos"> 89</span><span class="k">def</span> <span class="nf">plot_gaussian_ellipse</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="linenos"> 90</span>    <span class="n">Y</span> <span class="o">=</span> <span class="n">gm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="linenos"> 91</span>    <span class="n">means</span> <span class="o">=</span> <span class="n">gm</span><span class="o">.</span><span class="n">means_</span>
<span class="linenos"> 92</span>    <span class="n">covariances</span> <span class="o">=</span> <span class="n">gm</span><span class="o">.</span><span class="n">covariances_</span>
<span class="linenos"> 93</span>    <span class="n">K</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">means</span><span class="o">.</span><span class="n">shape</span>
<span class="linenos"> 94</span>    <span class="k">if</span> <span class="n">gm</span><span class="o">.</span><span class="n">covariance_type</span> <span class="o">==</span> <span class="s2">&quot;tied&quot;</span><span class="p">:</span>
<span class="linenos"> 95</span>        <span class="n">covariances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">covariances</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="linenos"> 96</span>    <span class="n">splot</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="linenos"> 97</span>    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">covar</span><span class="p">,</span> <span class="n">color</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">covariances</span><span class="p">,</span> <span class="n">color_iter</span><span class="p">)):</span>
<span class="linenos"> 98</span>        <span class="k">if</span> <span class="n">gm</span><span class="o">.</span><span class="n">covariance_type</span> <span class="o">==</span> <span class="s2">&quot;spherical&quot;</span><span class="p">:</span>
<span class="linenos"> 99</span>            <span class="n">covar</span> <span class="o">=</span> <span class="n">covar</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
<span class="linenos">100</span>        <span class="k">if</span> <span class="n">gm</span><span class="o">.</span><span class="n">covariance_type</span> <span class="o">==</span> <span class="s2">&quot;diag&quot;</span><span class="p">:</span>
<span class="linenos">101</span>            <span class="n">covar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">covar</span><span class="p">)</span>
<span class="linenos">102</span>        <span class="n">v</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">covar</span><span class="p">)</span>
<span class="linenos">103</span>        <span class="n">v</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
<span class="linenos">104</span>        <span class="n">u</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="linenos">105</span>        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">Y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">Y</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="mf">0.8</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
<span class="linenos">106</span>        <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="linenos">107</span>        <span class="n">angle</span> <span class="o">=</span> <span class="mf">180.0</span> <span class="o">*</span> <span class="n">angle</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>  <span class="c1"># convert to degrees</span>
<span class="linenos">108</span>        <span class="n">ell</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">patches</span><span class="o">.</span><span class="n">Ellipse</span><span class="p">(</span><span class="n">xy</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">height</span><span class="o">=</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">angle</span><span class="o">=</span><span class="mf">180.0</span> <span class="o">+</span> <span class="n">angle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
<span class="linenos">109</span>        <span class="n">ell</span><span class="o">.</span><span class="n">set_clip_box</span><span class="p">(</span><span class="n">splot</span><span class="o">.</span><span class="n">bbox</span><span class="p">)</span>
<span class="linenos">110</span>        <span class="n">ell</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span>
<span class="linenos">111</span>        <span class="n">splot</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>
<span class="linenos">112</span>
<span class="linenos">113</span>
<span class="linenos">114</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="linenos">115</span><span class="c1"># plot_assignment(gm_full, X)</span>
<span class="linenos">116</span><span class="n">plot_gaussian_ellipse</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="linenos">117</span><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="linenos">118</span><span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
<span class="linenos">119</span><span class="c1"># plt.savefig(&quot;figures/gmm_2d_clustering.pdf&quot;, dpi=300)</span>
<span class="linenos">120</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="linenos">121</span>
<span class="linenos">122</span>
<span class="linenos">123</span><span class="k">def</span> <span class="nf">plot_data</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
<span class="linenos">124</span>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;k.&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="linenos">125</span>
<span class="linenos">126</span>
<span class="linenos">127</span><span class="k">def</span> <span class="nf">plot_centroids</span><span class="p">(</span><span class="n">centroids</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">circle_color</span><span class="o">=</span><span class="s2">&quot;w&quot;</span><span class="p">,</span> <span class="n">cross_color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">):</span>
<span class="linenos">128</span>    <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="linenos">129</span>        <span class="n">centroids</span> <span class="o">=</span> <span class="n">centroids</span><span class="p">[</span><span class="n">weights</span> <span class="o">&gt;</span> <span class="n">weights</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">/</span> <span class="mi">10</span><span class="p">]</span>
<span class="linenos">130</span>    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
<span class="linenos">131</span>        <span class="n">centroids</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
<span class="linenos">132</span>        <span class="n">centroids</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
<span class="linenos">133</span>        <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
<span class="linenos">134</span>        <span class="n">s</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
<span class="linenos">135</span>        <span class="n">linewidths</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
<span class="linenos">136</span>        <span class="n">color</span><span class="o">=</span><span class="n">circle_color</span><span class="p">,</span>
<span class="linenos">137</span>        <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="linenos">138</span>        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
<span class="linenos">139</span>    <span class="p">)</span>
<span class="linenos">140</span>    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
<span class="linenos">141</span>        <span class="n">centroids</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
<span class="linenos">142</span>        <span class="n">centroids</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
<span class="linenos">143</span>        <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span>
<span class="linenos">144</span>        <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
<span class="linenos">145</span>        <span class="n">linewidths</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
<span class="linenos">146</span>        <span class="n">color</span><span class="o">=</span><span class="n">cross_color</span><span class="p">,</span>
<span class="linenos">147</span>        <span class="n">zorder</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span>
<span class="linenos">148</span>        <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="linenos">149</span>    <span class="p">)</span>
<span class="linenos">150</span>
<span class="linenos">151</span>
<span class="linenos">152</span><span class="k">def</span> <span class="nf">plot_gaussian_mixture</span><span class="p">(</span><span class="n">clusterer</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">resolution</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">show_ylabels</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="linenos">153</span>    <span class="n">mins</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.1</span>
<span class="linenos">154</span>    <span class="n">maxs</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span>
<span class="linenos">155</span>    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span>
<span class="linenos">156</span>        <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">mins</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">maxs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">resolution</span><span class="p">),</span>
<span class="linenos">157</span>        <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">mins</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">maxs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">resolution</span><span class="p">),</span>
<span class="linenos">158</span>    <span class="p">)</span>
<span class="linenos">159</span>
<span class="linenos">160</span>    <span class="n">Z</span> <span class="o">=</span> <span class="o">-</span><span class="n">clusterer</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
<span class="linenos">161</span>    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="linenos">162</span>
<span class="linenos">163</span>    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span>
<span class="linenos">164</span>        <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">LogNorm</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">30.0</span><span class="p">),</span> <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="linenos">165</span>    <span class="p">)</span>
<span class="linenos">166</span>    <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span>
<span class="linenos">167</span>        <span class="n">xx</span><span class="p">,</span>
<span class="linenos">168</span>        <span class="n">yy</span><span class="p">,</span>
<span class="linenos">169</span>        <span class="n">Z</span><span class="p">,</span>
<span class="linenos">170</span>        <span class="n">norm</span><span class="o">=</span><span class="n">LogNorm</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">30.0</span><span class="p">),</span>
<span class="linenos">171</span>        <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span>
<span class="linenos">172</span>        <span class="n">linewidths</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="linenos">173</span>        <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span>
<span class="linenos">174</span>    <span class="p">)</span>
<span class="linenos">175</span>
<span class="linenos">176</span>    <span class="c1"># plot decision boundaries</span>
<span class="linenos">177</span>    <span class="k">if</span> <span class="mi">0</span><span class="p">:</span>
<span class="linenos">178</span>        <span class="n">Z</span> <span class="o">=</span> <span class="n">clusterer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
<span class="linenos">179</span>        <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="linenos">180</span>        <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">)</span>
<span class="linenos">181</span>
<span class="linenos">182</span>    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;k.&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="linenos">183</span>    <span class="c1"># plot_centroids(clusterer.means_, clusterer.weights_)</span>
<span class="linenos">184</span>
<span class="linenos">185</span>    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="linenos">186</span>    <span class="k">if</span> <span class="n">show_ylabels</span><span class="p">:</span>
<span class="linenos">187</span>        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$x_2$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="linenos">188</span>    <span class="k">else</span><span class="p">:</span>
<span class="linenos">189</span>        <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">labelleft</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="linenos">190</span>
<span class="linenos">191</span>
<span class="linenos">192</span><span class="k">def</span> <span class="nf">plot_assignment</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="linenos">193</span>    <span class="c1"># plt.figure(figsize=(8, 4))</span>
<span class="linenos">194</span>    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="linenos">195</span>    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="linenos">196</span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">gm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="linenos">197</span>    <span class="n">K</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">gm</span><span class="o">.</span><span class="n">means_</span><span class="o">.</span><span class="n">shape</span>
<span class="linenos">198</span>    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
<span class="linenos">199</span>        <span class="n">color</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">color_iter</span><span class="p">)</span>
<span class="linenos">200</span>        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">y_pred</span> <span class="o">==</span> <span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">y_pred</span> <span class="o">==</span> <span class="n">k</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
<span class="linenos">201</span>
<span class="linenos">202</span>
<span class="linenos">203</span><span class="n">gm_full</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span>
<span class="linenos">204</span>    <span class="n">n_components</span><span class="o">=</span><span class="n">K</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="linenos">205</span><span class="p">)</span>
<span class="linenos">206</span><span class="n">gm_tied</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span>
<span class="linenos">207</span>    <span class="n">n_components</span><span class="o">=</span><span class="n">K</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s2">&quot;tied&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="linenos">208</span><span class="p">)</span>
<span class="linenos">209</span><span class="n">gm_spherical</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span>
<span class="linenos">210</span>    <span class="n">n_components</span><span class="o">=</span><span class="n">K</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s2">&quot;spherical&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="linenos">211</span><span class="p">)</span>
<span class="linenos">212</span><span class="n">gm_diag</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span>
<span class="linenos">213</span>    <span class="n">n_components</span><span class="o">=</span><span class="n">K</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">covariance_type</span><span class="o">=</span><span class="s2">&quot;diag&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="linenos">214</span><span class="p">)</span>
<span class="linenos">215</span><span class="n">gm_full</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="linenos">216</span><span class="n">gm_tied</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="linenos">217</span><span class="n">gm_spherical</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="linenos">218</span><span class="n">gm_diag</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="linenos">219</span>
<span class="linenos">220</span>
<span class="linenos">221</span><span class="k">def</span> <span class="nf">make_plot</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
<span class="linenos">222</span>    <span class="n">ttl</span> <span class="o">=</span> <span class="n">name</span>
<span class="linenos">223</span>    <span class="c1"># plt.figure(figsize=(8, 4))</span>
<span class="linenos">224</span>    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="linenos">225</span>    <span class="n">plot_gaussian_mixture</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="linenos">226</span>    <span class="n">fname</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;figures/gmm_2d_</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">_contours.pdf&quot;</span>
<span class="linenos">227</span>    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">ttl</span><span class="p">)</span>
<span class="linenos">228</span>    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="linenos">229</span>    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
<span class="linenos">230</span>    <span class="c1"># plt.savefig(fname, dpi=300)</span>
<span class="linenos">231</span>    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="linenos">232</span>
<span class="linenos">233</span>    <span class="c1"># plt.figure(figsize=(8, 4))</span>
<span class="linenos">234</span>    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="linenos">235</span>    <span class="c1"># plot_assignment(gm, X)</span>
<span class="linenos">236</span>    <span class="n">plot_gaussian_ellipse</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="linenos">237</span>    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">ttl</span><span class="p">)</span>
<span class="linenos">238</span>    <span class="n">fname</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;figures/gmm_2d_</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">_components.pdf&quot;</span>
<span class="linenos">239</span>    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="linenos">240</span>    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
<span class="linenos">241</span>    <span class="c1"># plt.savefig(fname, dpi=300)</span>
<span class="linenos">242</span>    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="linenos">243</span>
<span class="linenos">244</span>
<span class="linenos">245</span><span class="k">if</span> <span class="mi">1</span><span class="p">:</span>
<span class="linenos">246</span>    <span class="n">make_plot</span><span class="p">(</span><span class="n">gm_full</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="s2">&quot;full&quot;</span><span class="p">)</span>
<span class="linenos">247</span>    <span class="n">make_plot</span><span class="p">(</span><span class="n">gm_tied</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="s2">&quot;tied&quot;</span><span class="p">)</span>
<span class="linenos">248</span>    <span class="n">make_plot</span><span class="p">(</span><span class="n">gm_spherical</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="s2">&quot;spherical&quot;</span><span class="p">)</span>
<span class="linenos">249</span>    <span class="n">make_plot</span><span class="p">(</span><span class="n">gm_diag</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="s2">&quot;diag&quot;</span><span class="p">)</span>
<span class="linenos">250</span>
<span class="linenos">251</span>
<span class="linenos">252</span><span class="c1"># Choosing K. Co,mpare to kmeans_silhouette</span>
<span class="linenos">253</span><span class="n">Ks</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="linenos">254</span><span class="n">gms_per_k</span> <span class="o">=</span> <span class="p">[</span>
<span class="linenos">255</span>    <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">Ks</span>
<span class="linenos">256</span><span class="p">]</span>
<span class="linenos">257</span>
<span class="linenos">258</span><span class="n">bics</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">bic</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">gms_per_k</span><span class="p">]</span>
<span class="linenos">259</span><span class="n">aics</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">aic</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">gms_per_k</span><span class="p">]</span>
<span class="linenos">260</span>
<span class="linenos">261</span>
<span class="linenos">262</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="linenos">263</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Ks</span><span class="p">,</span> <span class="n">bics</span><span class="p">,</span> <span class="s2">&quot;bo-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;BIC&quot;</span><span class="p">)</span>
<span class="linenos">264</span><span class="c1"># plt.plot(Ks, aics, &quot;go--&quot;, label=&quot;AIC&quot;)</span>
<span class="linenos">265</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$k$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="linenos">266</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Information Criterion&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="linenos">267</span><span class="c1"># plt.axis([1, 9.5, np.min(aics) - 50, np.max(aics) + 50])</span>
<span class="linenos">268</span><span class="k">if</span> <span class="mi">0</span><span class="p">:</span>
<span class="linenos">269</span>    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span>
<span class="linenos">270</span>        <span class="s2">&quot;Minimum&quot;</span><span class="p">,</span>
<span class="linenos">271</span>        <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">bics</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span>
<span class="linenos">272</span>        <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">),</span>
<span class="linenos">273</span>        <span class="n">textcoords</span><span class="o">=</span><span class="s2">&quot;figure fraction&quot;</span><span class="p">,</span>
<span class="linenos">274</span>        <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span>
<span class="linenos">275</span>        <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
<span class="linenos">276</span>    <span class="p">)</span>
<span class="linenos">277</span><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="linenos">278</span><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="linenos">279</span><span class="c1"># plt.savefig(&quot;figures/gmm_2d_bic_vs_k.pdf&quot;, dpi=300)</span>
<span class="linenos">280</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/02a26e9eb231d9a445a358b254746b721313788f6ab11a64478d9365f43464e2.svg" src="../../_images/02a26e9eb231d9a445a358b254746b721313788f6ab11a64478d9365f43464e2.svg" />
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">integral of pdf <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0000000000000429</span>
</pre>
</div><img alt="../../_images/a1f504c0f409d8d48dc91c3e50583bb5f88a5bbffc3505233f80adbb8f1d79d1.svg" src="../../_images/a1f504c0f409d8d48dc91c3e50583bb5f88a5bbffc3505233f80adbb8f1d79d1.svg" />
<img alt="../../_images/a222fcc60da9d4a9e379ac24bbdfc6a7d6a97b7f370ab23dcf9b7415c4538385.svg" src="../../_images/a222fcc60da9d4a9e379ac24bbdfc6a7d6a97b7f370ab23dcf9b7415c4538385.svg" />
<img alt="../../_images/d29c39c3800b361cfceae6b37a8cc633b1d1e7a3940312cfb542661090331ca7.svg" src="../../_images/d29c39c3800b361cfceae6b37a8cc633b1d1e7a3940312cfb542661090331ca7.svg" />
<img alt="../../_images/4ce68d2676de38503d3109a752eb22b38b2eaf97d0ab40352b7462eb6abc0353.svg" src="../../_images/4ce68d2676de38503d3109a752eb22b38b2eaf97d0ab40352b7462eb6abc0353.svg" />
<img alt="../../_images/8726ce458f6a7062d8d8a8b0e0f01275b337a61af62b219c96701f1732dff9d9.svg" src="../../_images/8726ce458f6a7062d8d8a8b0e0f01275b337a61af62b219c96701f1732dff9d9.svg" />
<img alt="../../_images/fc598c880a937a636c9dc26a1c4a6a230d7474f4b341ae253c2f4cd1e3509fab.svg" src="../../_images/fc598c880a937a636c9dc26a1c4a6a230d7474f4b341ae253c2f4cd1e3509fab.svg" />
<img alt="../../_images/4a7517774f23a0e7dc448770e9945af7f5b2bb74559ae372aababba867286c5d.svg" src="../../_images/4a7517774f23a0e7dc448770e9945af7f5b2bb74559ae372aababba867286c5d.svg" />
<img alt="../../_images/8dc86f6551b07a6602644b9658d6f76cdc36b30815da6484f0506521d73fc7e9.svg" src="../../_images/8dc86f6551b07a6602644b9658d6f76cdc36b30815da6484f0506521d73fc7e9.svg" />
<img alt="../../_images/082a307a7de1e6aac86e5335f27500cb3a7d9b5198f07ce6427d2dbf7cb659d7.svg" src="../../_images/082a307a7de1e6aac86e5335f27500cb3a7d9b5198f07ce6427d2dbf7cb659d7.svg" />
<img alt="../../_images/1d80fb4d467d2fc19594aff765a8416f845ca23eb0907ff3d33335680da1a227.svg" src="../../_images/1d80fb4d467d2fc19594aff765a8416f845ca23eb0907ff3d33335680da1a227.svg" />
</div>
</div>
</section>
</section>
<section id="references-and-further-readings">
<h2><a class="toc-backref" href="#id104" role="doc-backlink">References and Further Readings</a><a class="headerlink" href="#references-and-further-readings" title="Link to this heading">#</a></h2>
<p>I would say that the most important reference for this chapter is Bishopâ€™s book,
and the mathematics for machine learning book by Deisenroth et al. is also very
detailed for the derivations.</p>
<p>However, what is worth looking at is the code snippets from Murphyâ€™s book, he has a
wide array of examples in python code and it is very easy to follow.</p>
<ul class="simple">
<li><p>Bishop, Christopher M. â€œChapter 2.3.9. Mixture of Gaussians.â€ and â€œChapter 9. Mixture Models and EM.â€ In Pattern Recognition and Machine Learning. New York: Springer-Verlag, 2016.</p></li>
<li><p>Deisenroth, Marc Peter, Faisal, A. Aldo and Ong, Cheng Soon. â€œChapter  11.1 Gaussian Mixture Models.â€ In Mathematics for Machine Learning. : Cambridge University Press, 2020.</p></li>
<li><p>Jung, Alexander. â€œChapter 8.2. Soft Clustering with Gaussian Mixture Models.â€ In Machine Learning: The Basics. Singapore: Springer Nature Singapore, 2023.</p></li>
<li><p>Murphy, Kevin P. â€œChapter 3.5 Mixture Modelsâ€ and â€œChapter 21.4 Clustering using mixture models.â€ In Probabilistic Machine Learning: An Introduction. MIT Press, 2022.</p></li>
<li><p>Vincent Tan, â€œLecture 14-16.â€ In Data Modelling and Computation (MA4270).</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./influential/gaussian_mixture_models"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="01_intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Mixture Models</p>
      </div>
    </a>
    <a class="right-next"
       href="03_implementation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Gaussian Mixture Models Implementation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">Intuition</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-bi-modal-distribution">Simple Bi-Modal Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-story">Generative Story</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-story">Inference Story</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-with-2d-data">Inference with 2D Data</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-is-a-latent-variable">Prior is a Latent Variable</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-formulation">Problem Formulation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-primer">A Primer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-mixture-model">Gaussian Mixture Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-perpectives">The Perpectives</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mixture-model-perspective">The Mixture Model Perspective</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-latent-variable-perspective">The Latent Variable Perspective</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">The Mixture Model Perspective</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gaussian-mixture-model">The Gaussian Mixture Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-responsibilities">The Responsibilities</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">The Latent Variable Perspective</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-generative-process">The Generative Process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-1-the-distribution-of-the-data-point-boldsymbol-x-n-given-the-latent-variable-boldsymbol-z-n">Assumption 1: The Distribution of the Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> given the Latent Variable <span class="math notranslate nohighlight">\(\boldsymbol{z}^{(n)}\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-latent-clusters">The Latent Clusters</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-data-points-boldsymbol-x-n-is-the-likelihood">The Data Points <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> is the Likelihood</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-likelihood-of-one-single-data-point-boldsymbol-x-n">The Likelihood of One Single Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-likelihood-of-the-entire-dataset-boldsymbol-x">The Likelihood of the Entire Dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-2-the-latent-variable-boldsymbol-z">Assumption 2: The Latent Variable <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-prior-distribution-of-boldsymbol-z">The Prior Distribution of <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-categorical-distribution">The Categorical Distribution</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-distribution-of-the-entire-dataset-mathcal-s">Prior Distribution of the Entire Dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-3-the-joint-distribution-of-boldsymbol-x-n-and-boldsymbol-z-n">Assumption 3: The Joint Distribution of <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{z}^{(n)}\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-the-joint-distribution-the-product-of-the-likelihood-and-prior">Why is the Joint Distribution the Product of the Likelihood and Prior?</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weighted-likelihood">Weighted Likelihood</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weighted-likelihood-of-one-single-data-point-boldsymbol-x-n">Weighted Likelihood of One Single Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#weighted-likelihood-of-the-entire-dataset-boldsymbol-x">Weighted Likelihood of the Entire Dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-distribution-fully-determines-the-model">Joint Distribution Fully Determines the Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gaussian-mixture-model-and-the-marginal-distribution">The Gaussian Mixture Model and the Marginal Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">The Gaussian Mixture Model</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-marginal-distribution">The Marginal Distribution</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#marginalizing-out-the-latent-variable">Marginalizing Out the Latent Variable</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-of-one-single-data-point-boldsymbol-x-n">Marginal of One Single Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#marginal-of-the-entire-dataset-boldsymbol-x">Marginal of the Entire Dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-posterior-distribution">The Posterior Distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-of-one-single-data-point-boldsymbol-x-n">Posterior of One Single Data Point <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-of-the-entire-dataset-boldsymbol-x">Posterior of the Entire Dataset <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-estimation-mixture-model-perspective">Parameter Estimation (Mixture Model Perspective)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-vectorized-parameters">The Vectorized Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-mixture-weights-boldsymbol-pi">The Mixture Weights <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-means-boldsymbol-mu">The Means <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-covariances-boldsymbol-sigma">The Covariances <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-and-log-likelihood-of-marginal-distribution">Likelihood and Log-Likelihood of Marginal Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#no-closed-form-solution">No Closed-Form Solution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-estimation-the-necessary-conditions">Parameter Estimation (The Necessary Conditions)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-chain-rule-matrix-calculus">The Chain Rule (Matrix Calculus)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#running-example">Running Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-mean-parameters-boldsymbol-mu-k">Estimating the Mean Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#some-intuition">Some Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#update-mean-of-running-example">Update Mean of Running Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-mean-parameters-boldsymbol-mu-k-in-python">Estimating the Mean Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_{k}\)</span> in Python</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-covariance-parameters-boldsymbol-sigma-k">Estimating the Covariance Parameters <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#update-covariance-matrix-of-running-example">Update Covariance Matrix of Running Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Some Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-covariance-matrix-boldsymbol-sigma-k-in-python">Estimating the Covariance Matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{k}\)</span> in Python</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-mixing-coefficients-prior-weights-pi-k">Estimating the Mixing Coefficients (Prior/Weights) <span class="math notranslate nohighlight">\(\pi_{k}\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">Some Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#update-weight-prior-of-running-example">Update Weight/Prior of Running Example</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-gmm-has-no-closed-form-solution">Why GMM has no Closed-Form Solution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-expectation-maximization-em-algorithm">The Expectation-Maximization (EM) Algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-maximization-em-gaussion-mixture-model-perspective">Expectation-Maximization (EM) (Gaussion Mixture Model Perspective)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expectation-maximization-em-latent-variable-perspective">Expectation-Maximization (EM) (Latent Variable Perspective)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-expectation-step-posterior-inference-and-responsibilities">The Expectation Step (Posterior Inference and Responsibilities)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-maximization-step-parameter-estimation">The Maximization Step (Parameter Estimation)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gmm-and-its-relation-to-k-means">GMM and its Relation to K-Means</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-small-example">A Small Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#random-initialization">Random Initialization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-and-hard-assignments">K-Means and Hard Assignments</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#e-step">E-Step</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#m-step">M-Step</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gmm-and-soft-assignments">GMM and Soft Assignments</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">E-Step</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">M-Step</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#murphy-s-plots">Murphyâ€™s Plots</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gmm-demo">GMM Demo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gmm-2d-sklearn">GMM 2D (sklearn)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>