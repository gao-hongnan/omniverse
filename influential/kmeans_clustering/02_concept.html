
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Concept: K-Means Clustering &#8212; Omniverse</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=bb35926c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MYW5YKC2WF"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"defeq": "\\overset{\\text{def}}{=}", "defa": "\\overset{\\text{(a)}}{=}", "defb": "\\overset{\\text{(b)}}{=}", "defc": "\\overset{\\text{(c)}}{=}", "defd": "\\overset{\\text{(d)}}{=}", "st": "\\mid", "mod": "\\mid", "S": "\\Omega", "s": "\\omega", "e": "\\exp", "P": "\\mathbb{P}", "R": "\\mathbb{R}", "expectation": "\\mathbb{E}", "v": "\\mathbf{v}", "a": "\\mathbf{a}", "b": "\\mathbf{b}", "c": "\\mathbf{c}", "u": "\\mathbf{u}", "w": "\\mathbf{w}", "x": "\\mathbf{x}", "y": "\\mathbf{y}", "z": "\\mathbf{z}", "0": "\\mathbf{0}", "1": "\\mathbf{1}", "A": "\\mathbf{A}", "B": "\\mathbf{B}", "C": "\\mathbf{C}", "E": "\\mathcal{F}", "eventA": "\\mathcal{A}", "lset": "\\left\\{", "rset": "\\right\\}", "lsq": "\\left[", "rsq": "\\right]", "lpar": "\\left(", "rpar": "\\right)", "lcurl": "\\left\\{", "rcurl": "\\right\\}", "pmf": "p_X", "pdf": "f_X", "pdftwo": "f_{X,Y}", "pdfjoint": "f_{\\mathbf{X}}", "pmfjointxy": "p_{X, Y}", "pdfjointxy": "f_{X, Y}", "cdf": "F_X", "pspace": "(\\Omega, \\mathcal{F}, \\mathbb{P})", "var": "\\operatorname{Var}", "std": "\\operatorname{Std}", "bern": "\\operatorname{Bernoulli}", "binomial": "\\operatorname{Binomial}", "geometric": "\\operatorname{Geometric}", "poisson": "\\operatorname{Poisson}", "uniform": "\\operatorname{Uniform}", "normal": "\\operatorname{Normal}", "gaussian": "\\operatorname{Gaussian}", "gaussiansymbol": "\\mathcal{N}", "exponential": "\\operatorname{Exponential}", "iid": "\\textbf{i.i.d.}", "and": "\\text{and}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'influential/kmeans_clustering/02_concept';</script>
    <link rel="canonical" href="https://www.gaohongnan.com/influential/kmeans_clustering/02_concept.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Implementation: K-Means (Lloyd)" href="03_implementation.html" />
    <link rel="prev" title="K-Means" href="01_intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Omniverse - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Omniverse - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    🌌 Omniverse: A Journey Through Knowledge
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notations/machine_learning.html">Machine Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Influential Ideas and Papers</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../generative_pretrained_transformer/01_intro.html">Generative Pre-trained Transformers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../generative_pretrained_transformer/02_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../generative_pretrained_transformer/03_concept.html">The Concept of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../generative_pretrained_transformer/04_implementation.html">The Implementation of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../generative_pretrained_transformer/05_adder.html">Training a Mini-GPT to Learn Two-Digit Addition</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../low_rank_adaptation/01_intro.html">Low-Rank Adaptation Of Large Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../low_rank_adaptation/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../low_rank_adaptation/03_implementation.html">Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../empirical_risk_minimization/01_intro.html">Empirical Risk Minimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../empirical_risk_minimization/02_concept.html">Concept: Empirical Risk Minimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../empirical_risk_minimization/03_bayes_optimal_classifier.html">Bayes Optimal Classifier</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../learning_theory/01_intro.html">Is The Learning Problem Solvable?</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../learning_theory/02_concept.html">Concept: Learning Theory</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="01_intro.html">K-Means</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Concept: K-Means Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_implementation.html">Implementation: K-Means (Lloyd)</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_image_segmentation.html">Application: Image Compression and Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_conceptual_questions.html">Conceptual Questions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../naive_bayes/01_intro.html">Naive Bayes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/03_implementation.html">Naives Bayes Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/04_example_penguins.html">Naive Bayes Application: Penguins</a></li>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/05_application_mnist.html">Naive Bayes Application (MNIST)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../gaussian_mixture_models/01_intro.html">Mixture Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../gaussian_mixture_models/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gaussian_mixture_models/03_implementation.html">Gaussian Mixture Models Implementation</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Playbook</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_calculate_flops_in_transformer_based_models.html">How to Calculate the Number of FLOPs in Transformer Based Models?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_inspect_function_and_class_signatures.html">How to Inspect Function and Class Signatures in Python?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/why_cosine_annealing_warmup_stabilize_training.html">Why Does Cosine Annealing With Warmup Stabilize Training?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html">Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling.</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_finetune_decoder_with_last_token_pooling.html">How To Fine-Tune Decoder-Only Models For Sequence Classification Using Last Token Pooling?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_finetune_decoder_with_cross_attention.html">How To Fine-Tune Decoder-Only Models For Sequence Classification With Cross-Attention?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_teacher_student_knowledge_distillation.html">How To Do Teacher-Student Knowledge Distillation?</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Probability Theory</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/intro.html">Chapter 1. Mathematical Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/01_combinatorics.html">Permutations and Combinations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/02_calculus.html">Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/03_contours.html">Contour Maps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/02_probability/intro.html">Chapter 2. Probability</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0202_probability_space.html">Probability Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0203_probability_axioms.html">Probability Axioms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0204_conditional_probability.html">Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0205_independence.html">Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0206_bayes_theorem.html">Baye’s Theorem and the Law of Total Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/summary.html">Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/intro.html">Chapter 3. Discrete Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0301_random_variables.html">Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0302_discrete_random_variables.html">Discrete Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0303_probability_mass_function.html">Probability Mass Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0304_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0305_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0306_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/intro.html">Discrete Uniform Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/intro.html">Bernoulli Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/iid.html">Independent and Identically Distributed (IID)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/intro.html">Binomial Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_implementation.html">Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_application.html">Real World Examples</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/geometric/intro.html">Geometric Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/geometric/0310_geometric_distribution_concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/intro.html">Poisson Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_implementation.html">Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/summary.html">Important</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/intro.html">Chapter 4. Continuous Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/from_discrete_to_continuous.html">From Discrete to Continuous</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0401_continuous_random_variables.html">Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0402_probability_density_function.html">Probability Density Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0403_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0404_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0405_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0406_mean_median_mode.html">Mean, Median and Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0407_continuous_uniform_distribution.html">Continuous Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0408_exponential_distribution.html">Exponential Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0409_gaussian_distribution.html">Gaussian Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0410_skewness_and_kurtosis.html">Skewness and Kurtosis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0411_convolve_and_sum_of_random_variables.html">Convolution and Sum of Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0412_functions_of_random_variables.html">Functions of Random Variables</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/intro.html">Chapter 5. Joint Distributions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/05_joint_distributions/from_single_variable_to_joint_distributions.html">From Single Variable to Joint Distributions</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/intro.html">Joint PMF and PDF</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/intro.html">Joint Expectation and Correlation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/intro.html">Conditional PMF and PDF</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/intro.html">Conditional Expectation and Variance</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0505_sum_of_random_variables/intro.html">Sum of Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0505_sum_of_random_variables/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0506_random_vectors/intro.html">Random Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0506_random_vectors/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/intro.html">Multivariate Gaussian Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/application_transformation.html">Application: Plots and Transformations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/psd.html">Covariance Matrix is Positive Semi-Definite</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/eigendecomposition.html">Eigendecomposition and Covariance Matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/geometry_of_multivariate_gaussian.html">The Geometry of Multivariate Gaussians</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/intro.html">Chapter 6. Sample Statistics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/intro.html">Moment Generating and Characteristic Functions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function.html">Moment Generating Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function_application_sum_of_rv.html">Application: Moment Generating Function and the Sum of Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/characteristic_function.html">Characteristic Function</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/intro.html">Probability Inequalities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/concept.html">Probability Inequalities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/application.html">Application: Learning Theory</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/intro.html">Law of Large Numbers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/convergence.html">Convergence of Sample Average</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/application.html">Application: Learning Theory</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/08_estimation_theory/intro.html">Chapter 8. Estimation Theory</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/08_estimation_theory/maximum_likelihood_estimation/intro.html">Maximum Likelihood Estimation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html">Concept</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Operations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../operations/distributed/intro.html">Distributed Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/01_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/02_basics.html">Basics Of Distributed Data Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/03_how_to_setup_slurm_in_aws.html">How to Setup SLURM and ParallelCluster in AWS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/04_ablation.html">Ablations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../operations/profiling/intro.html">Profiling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/01_synchronize.html">Synchronize CUDA To Time CUDA Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/02_timeit.html">Profiling Code With Timeit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/03_time_profiler.html">PyTorch’s Event And Profiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/04_small_gpt_profile.html">Profile GPT Small Time And Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/05_memory_leak.html">CUDA Memory Allocations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../operations/machine_learning_lifecycle/00_intro.html">The Lifecycle of an AIOps System</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/01_problem_formulation.html">Stage 1. Problem Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/02_project_scoping.html">Stage 2. Project Scoping And Framing The Problem</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/03_dataops_pipeline.html">Stage 3. Data Pipeline (Data Engineering and DataOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/031_data_source_and_format.html">Stage 3.1. Data Source and Formats</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/032_data_model_and_storage.html">Stage 3.2. Data Model and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/033_etl.html">Stage 3.3. Extract, Transform, Load (ETL)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/04_mlops_data_pipeline.html">Stage 4. Data Extraction (MLOps), Data Analysis (Data Science), Data Preparation (Data Science)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/05_ml_training_pipeline.html">Stage 5. Model Development and Training (MLOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/051_model_selection.html">Stage 5.1. Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/052_metric_selection.html">Stage 5.2. Metric Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/053_experiment_tracking.html">Stage 5.3. Experiment Tracking And Versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/054_model_testing.html">Stage 5.4. Model Testing</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/06_model_evaluation.html">Stage 6. Model Evaluation (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/07_model_validation_registry_and_pushing_model_to_production.html">Stage 7. Model Validation, Registry and Pushing Model to Production (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/08_model_deployment_and_serving.html">Stage 8. Model Serving (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/09_model_monitoring.html">Stage 9. Model Monitoring (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/010_continuous_integration_deployment_learning_and_training.html">Stage 10. Continuous Integration, Deployment, Learning and Training (DevOps, DataOps, MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/011_infrastructure_and_tooling_for_mlops.html">Stage 11. Infrastructure and Tooling for MLOps</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/config_management/concept.html">Configuration Management</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/config_management/01-pydra.html">Pydantic And Hydra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/config_management/02-state.html">State And Metadata Management</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/design_patterns/intro.html">Design Patterns</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/dependency-inversion-principle.html">Dependency Inversion Principle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/strategy.html">Strategy Pattern</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/registry.html">Registry Design Pattern</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/python/intro.html">Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/decorator.html">Decorator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/pydantic.html">Pydantic Is All You Need - Jason Liu</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/intro.html">Concurrency, Parallelism and Asynchronous Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/generator_yield.html">A Rudimentary Introduction to Generator and Yield in Python</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../computer_science/type_theory/intro.html">Type Theory, A Very Rudimentary Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/01-subtypes.html">Subtypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/02-type-safety.html">Type Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/03-subsumption.html">Subsumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/04-generics.html">Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/05-typevar-bound-constraints.html">Bound and Constraint in Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/06-invariance-covariance-contravariance.html">Invariance, Covariance and Contravariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/07-pep-3124-overloading.html">Function Overloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/08-pep-661-sentinel-values.html">Sentinel Types</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Structures and Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/complexity_analysis/intro.html">Complexity Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/complexity_analysis/master_theorem.html">Master Theorem </a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/stack/intro.html">Stack</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/stack/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/intro.html">Linear Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/intro.html">Binary Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/problems/875-koko-eating-bananas.html">Koko Eating Bananas</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/01_preliminaries/intro.html">Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/01-fields.html">Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/02-systems-of-linear-equations.html">Systems of Linear Equations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/02_vectors/intro.html">Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/01-vector-definition.html">Vector and Its Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/02-vector-operation.html">Vector and Its Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/03-vector-norm.html">Vector Norm and Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/04-vector-products.html">A First Look at Vector Products</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../citations.html">IEEE (Style) Citations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../api/reproducibility.html">API Reference</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/gao-hongnan/omniverse/blob/main/omniverse/influential/kmeans_clustering/02_concept.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse/issues/new?title=Issue%20on%20page%20%2Finfluential/kmeans_clustering/02_concept.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/influential/kmeans_clustering/02_concept.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../../_sources/influential/kmeans_clustering/02_concept.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Concept: K-Means Clustering</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">Intuition</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-hypothesis-space">The Hypothesis Space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-loss-cost-objective-function">The Loss/Cost/Objective Function</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-notion-of-similarity-and-closeness">The Notion of Similarity and Closeness</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-formulation">Problem Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partition-and-voronoi-regions">Partition and Voronoi Regions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assignment">Assignment</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assignments-are-equivalent-to-clusters">Assignments are Equivalent to Clusters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#centroids-representatives">Centroids (Representatives)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-space">Hypothesis Space</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">Loss Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-function">Cost Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#objective-function">Objective Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-necessary-conditions-to-minimize-the-objective-function">The Necessary Conditions to Minimize the Objective Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#condition-1-the-optimal-assignment">Condition 1: The Optimal Assignment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#condition-2-the-optimal-cluster-centers-centroids">Condition 2: The Optimal Cluster Centers (Centroids)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#objective-function-re-defined">Objective Function Re-defined</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithm">Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-fitting">Model Fitting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-inference">Model Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence">Convergence</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lemma-1-stirling-numbers-of-the-second-kind">Lemma 1: Stirling Numbers of the Second Kind</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lemma-2-cost-function-of-k-means-monotonically-decreases">Lemma 2: Cost Function of K-Means Monotonically Decreases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lemma-3-monotone-convergence-theorem">Lemma 3: Monotone Convergence Theorem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-converges-in-finite-steps">K-Means Converges in Finite Steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#local-minima">Local Minima</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-find-k">How to find <span class="math notranslate nohighlight">\(K\)</span>?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choose-k-that-minimizes-the-cost-function">Choose <span class="math notranslate nohighlight">\(K\)</span> that Minimizes the Cost Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elbow-method">Elbow Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-methods">Other Methods</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#time-and-space-complexity">Time and Space Complexity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#brute-force-search-and-global-minimum">Brute Force Search and Global Minimum</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lloyds-algorithm">Lloyd’s Algorithm</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-k-means">When to Use K-Means?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-can-k-means-fail">When can K-Means Fail?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means">K-Means++</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-medoids">K-Medoids</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="concept-k-means-clustering">
<h1>Concept: K-Means Clustering<a class="headerlink" href="#concept-k-means-clustering" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://twitter.com/gaohongnan"><img alt="Twitter Handle" src="https://img.shields.io/badge/Twitter-&#64;gaohongnan-blue?style=social&amp;logo=twitter" /></a>
<a class="reference external" href="https://linkedin.com/in/gao-hongnan"><img alt="LinkedIn Profile" src="https://img.shields.io/badge/&#64;gaohongnan-blue?style=social&amp;logo=linkedin" /></a>
<a class="reference external" href="https://github.com/gao-hongnan"><img alt="GitHub Profile" src="https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&amp;logo=github" /></a>
<img alt="Tag" src="https://img.shields.io/badge/Tag-Organized_Chaos-orange" /></p>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#intuition" id="id12">Intuition</a></p>
<ul>
<li><p><a class="reference internal" href="#the-hypothesis-space" id="id13">The Hypothesis Space</a></p></li>
<li><p><a class="reference internal" href="#the-loss-cost-objective-function" id="id14">The Loss/Cost/Objective Function</a></p>
<ul>
<li><p><a class="reference internal" href="#the-notion-of-similarity-and-closeness" id="id15">The Notion of Similarity and Closeness</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#problem-formulation" id="id16">Problem Formulation</a></p></li>
<li><p><a class="reference internal" href="#partition-and-voronoi-regions" id="id17">Partition and Voronoi Regions</a></p></li>
<li><p><a class="reference internal" href="#assignment" id="id18">Assignment</a></p>
<ul>
<li><p><a class="reference internal" href="#assignments-are-equivalent-to-clusters" id="id19">Assignments are Equivalent to Clusters</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#centroids-representatives" id="id20">Centroids (Representatives)</a></p></li>
<li><p><a class="reference internal" href="#hypothesis-space" id="id21">Hypothesis Space</a></p></li>
<li><p><a class="reference internal" href="#loss-function" id="id22">Loss Function</a></p></li>
<li><p><a class="reference internal" href="#cost-function" id="id23">Cost Function</a></p></li>
<li><p><a class="reference internal" href="#objective-function" id="id24">Objective Function</a></p></li>
<li><p><a class="reference internal" href="#the-necessary-conditions-to-minimize-the-objective-function" id="id25">The Necessary Conditions to Minimize the Objective Function</a></p>
<ul>
<li><p><a class="reference internal" href="#condition-1-the-optimal-assignment" id="id26">Condition 1: The Optimal Assignment</a></p></li>
<li><p><a class="reference internal" href="#condition-2-the-optimal-cluster-centers-centroids" id="id27">Condition 2: The Optimal Cluster Centers (Centroids)</a></p></li>
<li><p><a class="reference internal" href="#objective-function-re-defined" id="id28">Objective Function Re-defined</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#algorithm" id="id29">Algorithm</a></p></li>
<li><p><a class="reference internal" href="#model-fitting" id="id30">Model Fitting</a></p></li>
<li><p><a class="reference internal" href="#model-inference" id="id31">Model Inference</a></p></li>
<li><p><a class="reference internal" href="#convergence" id="id32">Convergence</a></p>
<ul>
<li><p><a class="reference internal" href="#lemma-1-stirling-numbers-of-the-second-kind" id="id33">Lemma 1: Stirling Numbers of the Second Kind</a></p></li>
<li><p><a class="reference internal" href="#lemma-2-cost-function-of-k-means-monotonically-decreases" id="id34">Lemma 2: Cost Function of K-Means Monotonically Decreases</a></p></li>
<li><p><a class="reference internal" href="#lemma-3-monotone-convergence-theorem" id="id35">Lemma 3: Monotone Convergence Theorem</a></p></li>
<li><p><a class="reference internal" href="#k-means-converges-in-finite-steps" id="id36">K-Means Converges in Finite Steps</a></p></li>
<li><p><a class="reference internal" href="#local-minima" id="id37">Local Minima</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#how-to-find-k" id="id38">How to find <span class="math notranslate nohighlight">\(K\)</span>?</a></p>
<ul>
<li><p><a class="reference internal" href="#choose-k-that-minimizes-the-cost-function" id="id39">Choose <span class="math notranslate nohighlight">\(K\)</span> that Minimizes the Cost Function</a></p></li>
<li><p><a class="reference internal" href="#elbow-method" id="id40">Elbow Method</a></p></li>
<li><p><a class="reference internal" href="#other-methods" id="id41">Other Methods</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#time-and-space-complexity" id="id42">Time and Space Complexity</a></p>
<ul>
<li><p><a class="reference internal" href="#brute-force-search-and-global-minimum" id="id43">Brute Force Search and Global Minimum</a></p></li>
<li><p><a class="reference internal" href="#lloyds-algorithm" id="id44">Lloyd’s Algorithm</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#when-to-use-k-means" id="id45">When to Use K-Means?</a></p></li>
<li><p><a class="reference internal" href="#when-can-k-means-fail" id="id46">When can K-Means Fail?</a></p></li>
<li><p><a class="reference internal" href="#k-means" id="id47">K-Means++</a></p></li>
<li><p><a class="reference internal" href="#k-medoids" id="id48">K-Medoids</a></p></li>
<li><p><a class="reference internal" href="#references-and-further-readings" id="id49">References and Further Readings</a></p></li>
</ul>
</nav>
<section id="intuition">
<h2><a class="toc-backref" href="#id12" role="doc-backlink">Intuition</a><a class="headerlink" href="#intuition" title="Link to this heading">#</a></h2>
<p>Let’s first look at an example by randomly generating data points<a class="footnote-reference brackets" href="#y" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> that can
be partitioned into 3 distinct clusters.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<img alt="../../_images/970444c150ffff4c770c56c63d6d4d4210ba921445f82da0b857e23b072d857e.svg" src="../../_images/970444c150ffff4c770c56c63d6d4d4210ba921445f82da0b857e23b072d857e.svg" />
</div>
</div>
<p>The question on hand is, if we are given data of this form, how do we cluster
them into <span class="math notranslate nohighlight">\(3\)</span> distinct clusters?</p>
<p>Visually, we can literally just circle out the <span class="math notranslate nohighlight">\(3\)</span> clusters. The luxury of such
simplicity is because we are working with <span class="math notranslate nohighlight">\(2\)</span> features, i.e.
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^{2}\)</span>. In addition, the dataset generated is
relatively simple to partition, i.e. the clusters are well separated.</p>
<p>However, in the real world, we are working with <span class="math notranslate nohighlight">\(D\)</span>-dimensional features where
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> resides in <span class="math notranslate nohighlight">\(\mathbb{R}^{D}\)</span>. <span class="math notranslate nohighlight">\(D\)</span> can be very large and we are
unable to visually inspect anymore.</p>
<p>In any case, even with such a simple dataset, how do we tell the machine to find
the <span class="math notranslate nohighlight">\(3\)</span> clusters that our visuals have identified?</p>
<section id="the-hypothesis-space">
<h3><a class="toc-backref" href="#id13" role="doc-backlink">The Hypothesis Space</a><a class="headerlink" href="#the-hypothesis-space" title="Link to this heading">#</a></h3>
<p>Formulating such a problem is not trivial is non-trivial. We first have to
formulate the problem in a way that the machine can understand, and that is done
mathematically.</p>
<p>For one, there is no ground truth labels as in the supervised setting, and
therefore our learner or hypothesis need not output a label, but rather a
cluster assignment.</p>
<p>Retrospectively, we can think of the learner as a function <span class="math notranslate nohighlight">\(h(\cdot)\)</span> that takes
in many data points
<span class="math notranslate nohighlight">\(\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(N)}\)</span> and outputs a
cluster assignment <span class="math notranslate nohighlight">\(\mathcal{A}(\mathbf{x})\)</span> for each data point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<p>For now, let’s informally define the hypothesis space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> to be the
set of all possible cluster assignments <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\mathcal{H} = \{\mathcal{A}(\cdot) \mid \mathcal{A}(\cdot) \text{ somehow assigns the data points to the correct cluster.}\}
\]</div>
<p>We will make this more precise later.</p>
</section>
<section id="the-loss-cost-objective-function">
<h3><a class="toc-backref" href="#id14" role="doc-backlink">The Loss/Cost/Objective Function</a><a class="headerlink" href="#the-loss-cost-objective-function" title="Link to this heading">#</a></h3>
<p>The second part of formulating a machine learning problem is to define the loss
function <span class="math notranslate nohighlight">\(\mathcal{L}(\cdot)\)</span> and subsequently the cost function
<span class="math notranslate nohighlight">\(\mathcal{J}(\cdot)\)</span>.</p>
<p>In supervised learning, we have our typical loss functions such as cross-entropy
loss (classification), and in regression, we have mean squared error. We also
have metrics like accuracy, precision, recall, etc to measure the performance of
the model.</p>
<p>This means, given a hypothesis <span class="math notranslate nohighlight">\(\hat{y}:=h(\mathbf{x})\)</span>, how close is it to the
true label <span class="math notranslate nohighlight">\(y\)</span>? In unsupervised, we do not have such ground truth label <span class="math notranslate nohighlight">\(y\)</span> to
compare with, but the notion of closeness is still there.</p>
<section id="the-notion-of-similarity-and-closeness">
<h4><a class="toc-backref" href="#id15" role="doc-backlink">The Notion of Similarity and Closeness</a><a class="headerlink" href="#the-notion-of-similarity-and-closeness" title="Link to this heading">#</a></h4>
<p>To define such a metric for unsupervised learning, we can fall back on our
intuition. The purpose of clustering is to group similar data points together.
So we seek to find a metric that measures the similarity between data points in
a dataset.</p>
<p>A very simple idea is to use
<a class="reference external" href="https://stats.stackexchange.com/questions/120509/inter-cluster-variance"><strong>intra-cluster variance</strong></a>.
For example, within a cluster, the data points are close to each other if the
variance is small.</p>
<p>Consequently, to make our intuition precise, we need to define a metric rule and
an assignment <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span> to assign data points to clusters. We also
need to define the notion of closeness and similarity between data points.</p>
<p>Lastly, such algorithms require an initial guess of the cluster centers, so that
eventually the algorithm can converge to the optimal cluster centers, since we
have no way of knowing the optimal cluster centers beforehand, especially in
high dimensional space.</p>
<p>More formally, the optimization problem requires us to minimize the sum of
squared distances between each data point and its cluster center. This is
equivalent to minimizing the variance within each cluster.</p>
<p>Let’s look at some definitions first that will gradually lead us to the
formulation of the objective function.</p>
</section>
</section>
</section>
<section id="problem-formulation">
<h2><a class="toc-backref" href="#id16" role="doc-backlink">Problem Formulation</a><a class="headerlink" href="#problem-formulation" title="Link to this heading">#</a></h2>
<div class="proof remark admonition" id="remark-kmeans-problem-statement">
<p class="admonition-title"><span class="caption-number">Remark 21 </span> (Remark)</p>
<section class="remark-content" id="proof-content">
<p>Although K-Means does not explicitly model the underlying distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>,
we can still apply the learning theory framework to K-Means.</p>
</section>
</div><p><strong>Given</strong> a set <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> containing <span class="math notranslate nohighlight">\(N\)</span> data points:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{S} = \left\{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(N)}\right\} \subset \mathcal{X} = \mathbb{R}^{D}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{X} = \mathbb{R}^{D}\)</span> and the vector <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> is the
<span class="math notranslate nohighlight">\(n\)</span>-th sample with <span class="math notranslate nohighlight">\(D\)</span> number of features, given by:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^{(n)} \in \mathbb{R}^{D} = \begin{bmatrix} x_1^{(n)} &amp; x_2^{(n)} &amp; \cdots &amp; x_D^{(n)} \end{bmatrix}^{\mathrm{T}} \quad \text{where } n = 1, \ldots, N.
\]</div>
<p>We can further write <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> as a disjoint union <a class="footnote-reference brackets" href="#disjoint-union" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> of <span class="math notranslate nohighlight">\(K\)</span>
sets, as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{S} &amp;:= \left\{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(N)}\right\} \subset \mathbb{R}^{D} = C_1 \sqcup C_2 \sqcup \cdots \sqcup C_K \\
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(C_k\)</span> is the set of data points that belong to cluster <span class="math notranslate nohighlight">\(k\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-cluster-def">
<span class="eqno">(19)<a class="headerlink" href="#equation-eq-cluster-def" title="Link to this equation">#</a></span>\[C_k = \left\{\mathbf{x}^{(n)} \in \mathbb{R}^{D} \middle\vert y^{(n)} = k\right\} .\]</div>
<p>The notation <span class="math notranslate nohighlight">\(y^{(n)} \in \{1, 2, \dots, K\}\)</span> may seem strange at first glance,
since we are not given the labels <span class="math notranslate nohighlight">\(y^{(n)}\)</span> in an unsupervised problem. Indeed,
this <span class="math notranslate nohighlight">\(y^{(n)}\)</span> (<strong>latent</strong>) is generally not known to us, but we can have a
mental model that for each data point, there is an underlying cluster label
<span class="math notranslate nohighlight">\(y^{(n)}\)</span> that it should belong to.</p>
<p>More concretely, we say that <span class="math notranslate nohighlight">\(y^{(n)} \in \{1, 2, \dots, K\}\)</span> in equation
<a class="reference internal" href="#equation-eq-cluster-def">(19)</a> refers to the cluster (ground truth) label of data point
<span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span>.</p>
<p>We further define <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> as the collection of these
clusters<a class="footnote-reference brackets" href="#collection-of-clusters" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{C} = \left\{C_1, C_2, \dots, C_K\right\}.
\]</div>
<p>To this end, we have decomposed the <span class="math notranslate nohighlight">\(N\)</span> data points into <span class="math notranslate nohighlight">\(K\)</span> clusters, where <span class="math notranslate nohighlight">\(K\)</span>
is a <a class="reference external" href="https://en.wikipedia.org/wiki/A_priori_and_a_posteriori"><em>priori</em></a>, a
pre-defined number.</p>
<hr class="docutils" />
<p>The <strong>K-Means</strong> algorithm aims to group the data points into a set
<span class="math notranslate nohighlight">\(\hat{\mathcal{C}}\)</span> containing <span class="math notranslate nohighlight">\(K\)</span> clusters:</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathcal{C}} = \left\{ \hat{C}_1, \hat{C}_2 \dots, \hat{C}_K \right\}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{C}_k\)</span> is the set of data points <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)} \in \mathcal{S}\)</span>
assigned by <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span> (explained shortly) to the <span class="math notranslate nohighlight">\( k \)</span>-th cluster:</p>
<div class="math notranslate nohighlight">
\[
\hat{C}_k = \left\{\mathbf{x}^{(n)} \in \mathbb{R}^{D} \middle\vert \mathcal{A}(n):= \hat{y}^{(n)} = k\right\}.
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span> is the assignment map that “predicts” and
“classifies” each data point into their respective clusters.</p>
<p>To this end, the <strong>goal</strong> of such an <strong>unsupervised problem</strong> is to find the
<strong><em>clusters</em></strong> <span class="math notranslate nohighlight">\(\hat{\mathcal{C}}\)</span>, the predicted clusters learnt by K-Means
that best approximate the ground truth clusters <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>.</p>
<p>In other words, we want to find the clusters <span class="math notranslate nohighlight">\(\hat{\mathcal{C}}\)</span> that are the
closest to the ground truth clusters <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>, we will make precise the
notion of <em>close</em> later.</p>
<p>It is also customary to denote <span class="math notranslate nohighlight">\(\hat{C}_k\)</span> to be the set that contains the
indices of the data points that belong to cluster <span class="math notranslate nohighlight">\(k\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\hat{C}_k = \left\{n \in \{1, 2, \dots, N\} \middle\vert \mathcal{A}(n):= \hat{y}^{(n)} = k\right\}.
\]</div>
<hr class="docutils" />
<p><a class="reference external" href="https://en.wikipedia.org/wiki/K-means_clustering">K-Means clustering</a>’s goal is
to find the clusters <span class="math notranslate nohighlight">\(\hat{\mathcal{C}}\)</span> that are the closest to the ground
truth clusters <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> (<strong>hard clustering</strong>). In other words, we aim to
partition <span class="math notranslate nohighlight">\(N\)</span> data points <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> into <span class="math notranslate nohighlight">\(K\)</span> clusters <span class="math notranslate nohighlight">\(\hat{\mathcal{C}}\)</span>.
The problem in itself seems manageable, since we can simply partition the data
points into <span class="math notranslate nohighlight">\(K\)</span> clusters and minimize the intra-cluster distance (variances).
However, it is computationally challenging to solve the problem
(<a class="reference external" href="https://en.wikipedia.org/wiki/NP-hardness">NP-hard</a>).</p>
<p>Consequently, there are many heuristics that are used to solve the problem. We
will talk about one of the most popular heuristics, the
<a class="reference external" href="https://en.wikipedia.org/wiki/Lloyd%27s_algorithm">Lloyd’s algorithm</a> in this
section.</p>
<p>In this algorithm, there exists <span class="math notranslate nohighlight">\(K\)</span> centroids (centers)</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{v}_1, \boldsymbol{v}_2, \dots, \boldsymbol{v}_K \in \mathbb{R}^{D}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{v}_k\)</span> is defined to be the centroid of cluster <span class="math notranslate nohighlight">\(C_k\)</span>. Each
centroid <span class="math notranslate nohighlight">\(\boldsymbol{v}_k\)</span> is a vector that has the same dimension as the data
points <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> and is the <strong>representative vector</strong> of the cluster
<span class="math notranslate nohighlight">\(C_k\)</span>.</p>
<p>By representative vector, we mean that <span class="math notranslate nohighlight">\(\boldsymbol{v}_k\)</span> is a vector that can
“describe” the cluster <span class="math notranslate nohighlight">\(C_k\)</span>. By construction, the centroids can be defined as
any vector <span class="math notranslate nohighlight">\(\boldsymbol{v}_k\)</span> that has the same dimension as the data points
<span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span>. However, an intuitive choice is to use the <strong>mean</strong> of the
data points <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span> in the cluster <span class="math notranslate nohighlight">\(\hat{C}_k\)</span> as the
representative vector <span class="math notranslate nohighlight">\(\boldsymbol{v}_k\)</span>.</p>
<p>Next, the formulation of the assignment rule <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span> can be made
clear by the intuition below:</p>
<blockquote>
<div><p>Since <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span> is an assignment rule, an intuitive way is to find
a representative vector <span class="math notranslate nohighlight">\(\boldsymbol{v}_k\)</span> in each cluster <span class="math notranslate nohighlight">\(\hat{C}_k\)</span>, and
assign every data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> that is closest to this
representative. This is similar to the
<a class="reference external" href="https://en.wikipedia.org/wiki/Nearest_neighbor_search#:~:text=Nearest%20neighbor%20search%20(NNS)%2C,the%20larger%20the%20function%20values.">nearest-neighbour search algorithm</a>.</p>
</div></blockquote>
<p>Consequently, given the representative vectors
<span class="math notranslate nohighlight">\(\left\{\boldsymbol{v}_k\right\}_{k=1}^K\)</span>, we need an assignment function
<span class="math notranslate nohighlight">\(\mathcal{A}(n) = \hat{y}^{(n)}\)</span> that assigns each data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span>
to the cluster <span class="math notranslate nohighlight">\(\hat{C}_k\)</span>. An intuitive choice is to compare “closeness” of
each <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> to the representative vectors
<span class="math notranslate nohighlight">\(\left\{\boldsymbol{v}_k\right\}_{k=1}^K\)</span> and assign it to the cluster
<span class="math notranslate nohighlight">\(\hat{C}_k\)</span> that is closest to the representative vector <span class="math notranslate nohighlight">\(\boldsymbol{v}_k\)</span>.</p>
<p>We will make these intuition more precise later by proving it.</p>
<hr class="docutils" />
<p>To this end, we have tidied up the flow of the Lloyd’s algorithm (more details
in subsequent sections), we now finalize the problem statement by defining an
appropriate <a class="reference external" href="https://en.wikipedia.org/wiki/Loss_function"><strong>loss</strong></a> and
<a class="reference external" href="https://en.wikipedia.org/wiki/Mathematical_optimization"><strong>objective</strong></a>
function. More formally, we want to find the assignment <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span> and
the cluster center <span class="math notranslate nohighlight">\(\boldsymbol{v}_k\)</span> such that the
<a class="reference external" href="https://en.wikipedia.org/wiki/Residual_sum_of_squares#:~:text=In%20statistics%2C%20the%20residual%20sum,such%20as%20a%20linear%20regression."><strong>sum of squared distances</strong></a>
between each data point and its cluster center is minimized. This means
partitioning the data points according to the
<a class="reference external" href="https://en.wikipedia.org/wiki/Voronoi_diagram"><strong>Voronoi Diagram</strong></a>.</p>
<p>To this end, we can define an <em>empirical</em> cost function that measures the
quality of the requirements listed earlier.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\widehat{\mathcal{J}}\left(\left\{\hat{y}^{(n)}\right\}_{n=1}^N,\left\{\boldsymbol{v}_{k}\right\}_{k=1}^k \middle \vert \mathcal{S}\right) &amp;= \sum_{n=1}^{N} \left\|\mathbf{x}^{(n)} - \boldsymbol{v}_{\hat{y}^{(n)}} \right\|^2 \\
\end{aligned}
\end{split}\]</div>
<p>Note that the clustering error <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span> depends on <strong>both</strong> the
<strong>cluster assignments</strong> <span class="math notranslate nohighlight">\(\hat{y}^{(n)}\)</span>, which define the clusters <span class="math notranslate nohighlight">\(\hat{C}_k\)</span>,
and the <strong>cluster representatives</strong> <span class="math notranslate nohighlight">\(\boldsymbol{v}_k\)</span>, for <span class="math notranslate nohighlight">\(k=1, \ldots, K\)</span>. As
mentioned earlier, finding the optimal cluster means
<span class="math notranslate nohighlight">\(\left\{\boldsymbol{v}_k\right\}_{k=1}^K\)</span> and cluster assignments
<span class="math notranslate nohighlight">\(\left\{\hat{y}^{(n)}\right\}_{n=1}^N\)</span> that minimize the clustering error
<span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span> is a
<a class="reference external" href="https://cseweb.ucsd.edu/~avattani/papers/kmeans_hardness.pdf">NP-hard problem</a>.
The difficulty stems from the fact that the clustering error <span class="math notranslate nohighlight">\(\mathcal{J}\)</span> is a
<a class="reference external" href="https://en.wikipedia.org/wiki/Convex_optimization">non-convex</a> function of the
cluster means and assignments. In other words, there are many local minima of
the clustering error <span class="math notranslate nohighlight">\(\mathcal{J}\)</span>, and finding the global minimum is hard.</p>
<p>While jointly optimizing the cluster means and assignments is
hard<a class="footnote-reference brackets" href="#jointly-optimizing" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>, separately optimizing either the cluster means for
given assignments or vice-versa is easy. In what follows, we present simple
closed-form solutions for these sub-problems. The <span class="math notranslate nohighlight">\(k\)</span>-means method simply
combines these solutions in an alternating fashion <span id="id5">[<a class="reference internal" href="../../bibliography.html#id7" title="A. Jung. Machine Learning: The Basics. Springer, Singapore, 2022.">Jung, 2022</a>]</span>.</p>
<p>More concretely, we want to show:</p>
<ul>
<li><p>For fixed cluster assignments <span class="math notranslate nohighlight">\(\mathcal{A}(n) = \hat{y}^{(n)}\)</span>, the
clustering error <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span> is minimized by setting the cluster
representatives <span class="math notranslate nohighlight">\(\boldsymbol{v}_k\)</span> equal to the cluster means, this means
the mean vector is the optimal choice for the cluster center.</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{\mu}_1, \boldsymbol{\mu}_2, \dots, \boldsymbol{\mu}_K \in \mathbb{R}^{D}
    \]</div>
<p>where each <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span> is the mean vector of the data points in
cluster <span class="math notranslate nohighlight">\(C_k\)</span>.</p>
</li>
<li><p>Furthermore, now when we obtain the cluster means <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span> (now
we fix <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span>, we can assign data points <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> to
the cluster <span class="math notranslate nohighlight">\(\hat{C}_k\)</span> that is closest to the cluster mean
<span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span>. This assignment action is called the <strong>assignment
function</strong> <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>, a function that does the assignment of data points
to clusters. We will show later that the clustering error
<span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span> is minimized when the assignment function is the
<strong>nearest neighbor assignment</strong> function <span class="math notranslate nohighlight">\(\mathcal{A}^{*}(\cdot)\)</span>,</p>
<div class="math notranslate nohighlight">
\[
    \mathcal{A}^{*}(n) = \underset{k}{\operatorname{argmin}} \left\|\mathbf{x}^{(n)} - \boldsymbol{\mu}_k \right\|^2
    \]</div>
<p>where it assigns data points <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> to the cluster <span class="math notranslate nohighlight">\(k\)</span> whose
center <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span> is closest.</p>
</li>
</ul>
<p>We see that instead of jointly optimizing the cluster means and assignments in
one step, we alternate between the two steps. We first fix the cluster
assignments and optimize the cluster means, and then we fix the cluster means
and optimize the cluster assignments. Readings who are familiar with data
structures and algorithms will notice this looks like a
<a class="reference external" href="https://en.wikipedia.org/wiki/Greedy_algorithm">greedy algorithm</a>, and those
who have learnt the
<a class="reference external" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">expectation maximization</a>
algorithm will notice that this is a special case of the expectation
maximization algorithm.</p>
<p>In the following sections, we will phrase K-Means (Lloyd’s algorithm) as an
optimization problem, in which the goal is to find the optimal cluster centers
and cluster assignments that minimize the clustering error. We will also prove
why this is the case.</p>
</section>
<section id="partition-and-voronoi-regions">
<h2><a class="toc-backref" href="#id17" role="doc-backlink">Partition and Voronoi Regions</a><a class="headerlink" href="#partition-and-voronoi-regions" title="Link to this heading">#</a></h2>
<p>First off, let’s see the definition of Voronoi regions (extracted from
<a class="reference external" href="https://en.wikipedia.org/wiki/Voronoi_diagram">Wikipedia</a>):</p>
<div class="proof definition admonition" id="def-voronoi-region">
<p class="admonition-title"><span class="caption-number">Definition 29 </span> (Voronoi Region)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a metric space with distance function <span class="math notranslate nohighlight">\(d\)</span>. Let <span class="math notranslate nohighlight">\(K\)</span> be a set of
indices and let <span class="math notranslate nohighlight">\(\left(P_k\right)_{k \in K}\)</span> be a tuple (ordered collection) of
nonempty subsets (the sites) in the space <span class="math notranslate nohighlight">\(X\)</span>. The Voronoi cell, or Voronoi
region, <span class="math notranslate nohighlight">\(R_k\)</span>, associated with the site <span class="math notranslate nohighlight">\(P_k\)</span> is the set of all points in <span class="math notranslate nohighlight">\(X\)</span>
whose distance to <span class="math notranslate nohighlight">\(P_k\)</span> is not greater than their distance to the other sites
<span class="math notranslate nohighlight">\(P_j\)</span>, where <span class="math notranslate nohighlight">\(j\)</span> is any index different from <span class="math notranslate nohighlight">\(k\)</span>. In other words, if
<span class="math notranslate nohighlight">\(d(x, A)=\inf \{d(x, a) \mid a \in A\}\)</span> denotes the distance between the point
<span class="math notranslate nohighlight">\(x\)</span> and the subset <span class="math notranslate nohighlight">\(A\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
R_k=\left\{x \in X \mid d\left(x, P_k\right) \leq d\left(x, P_j\right) \text { for all } j \neq k\right\}
\]</div>
<p>The Voronoi diagram is simply the tuple of cells <span class="math notranslate nohighlight">\(\left(R_k\right)_{k \in K}\)</span>.
In principle, some of the sites can intersect and even coincide (an application
is described below for sites representing shops), but usually they are assumed
to be disjoint. In addition, infinitely many sites are allowed in the definition
(this setting has applications in geometry of numbers and crystallography), but
again, in many cases only finitely many sites are considered.</p>
<p>In the particular case where the space is a finite-dimensional Euclidean space,
each site is a point, there are finitely many points and all of them are
different, then the Voronoi cells are convex polytopes and they can be
represented in a combinatorial way using their vertices, sides, two-dimensional
faces, etc. Sometimes the induced combinatorial structure is referred to as the
Voronoi diagram. In general however, the Voronoi cells may not be convex or even
connected.</p>
<p>In the usual Euclidean space, we can rewrite the formal definition in usual
terms. Each Voronoi polygon <span class="math notranslate nohighlight">\(R_k\)</span> is associated with a generator point <span class="math notranslate nohighlight">\(P_k\)</span>.
Let <span class="math notranslate nohighlight">\(X\)</span> be the set of all points in the Euclidean space. Let <span class="math notranslate nohighlight">\(P_1\)</span> be a point
that generates its Voronoi region <span class="math notranslate nohighlight">\(R_1, P_2\)</span> that generates <span class="math notranslate nohighlight">\(R_2\)</span>, and <span class="math notranslate nohighlight">\(P_3\)</span>
that generates <span class="math notranslate nohighlight">\(R_3\)</span>, and so on. Then, all locations in the Voronoi polygon are
closer to the generator point of that polygon than any other generator point in
the Voronoi diagram in Euclidean plane”.</p>
</section>
</div><p>K-Means can be formulated via the lens of
<a class="reference external" href="https://en.wikipedia.org/wiki/Voronoi_diagram"><strong>Voronoi regions</strong></a> where we
define <span class="math notranslate nohighlight">\(C_k \in \mathcal{C}\)</span> as the <strong>partition</strong> of the data set <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>,
where each subset is a cluster. We say that <span class="math notranslate nohighlight">\(C_k\)</span> is a representative of the
cluster <span class="math notranslate nohighlight">\(k\)</span> and induces a <strong>Voronoi partition</strong> of <span class="math notranslate nohighlight">\(\mathbb{R}^D\)</span>. More
formally, we define the Voronoi partition as follows:</p>
<div class="proof definition admonition" id="def:kmeans-voronoi-partition">
<p class="admonition-title"><span class="caption-number">Definition 30 </span> (K-Means Voronoi Partition)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\mathcal{C} = \{C_1, C_2, \ldots, C_K\}\)</span> be a partition of <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, where <span class="math notranslate nohighlight">\(C_k \in C\)</span> is a subset of <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>.
Then <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> induces a <strong>Voronoi partition</strong> of <span class="math notranslate nohighlight">\(\mathbb{R}^D\)</span>, which decomposes <span class="math notranslate nohighlight">\(\mathbb{R}^D\)</span> into <span class="math notranslate nohighlight">\(K\)</span> convex cells,
each corresponding to some <span class="math notranslate nohighlight">\(C_k \in \mathcal{C}\)</span> and containing the region of space whose nearest representative is <span class="math notranslate nohighlight">\(C_k\)</span>.</p>
<p>More concretely, the Voronoi region <span class="math notranslate nohighlight">\(C_k\)</span>, contains all points <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^D\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\left\|\mathbf{x} - \boldsymbol{v}_k \right\|^2 \leq \left\|\mathbf{x} - \boldsymbol{v}_j \right\|^2 \text{ for all } j \neq k
\end{aligned}
\]</div>
<p>which means that the distance between <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{v}_k\)</span> is less than or equal to the distance between <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and any other cluster center <span class="math notranslate nohighlight">\(\boldsymbol{v}_j\)</span>.</p>
<p>Also,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{S} = \bigsqcup_{k=1}^K C_k
\]</div>
</section>
</div><p>For a visual representation, see <span id="id6">[<a class="reference internal" href="../../bibliography.html#id3" title="Kevin P. Murphy. Probabilistic Machine Learning: An introduction. MIT Press, 2022. URL: probml.ai.">Murphy, 2022</a>]</span>’s
<a class="reference external" href="https://github.com/probml/pyprobml/blob/master/notebooks/book1/21/kmeans_voronoi.ipynb">figure</a>.</p>
</section>
<section id="assignment">
<h2><a class="toc-backref" href="#id18" role="doc-backlink">Assignment</a><a class="headerlink" href="#assignment" title="Link to this heading">#</a></h2>
<div class="proof definition admonition" id="def:assignment">
<p class="admonition-title"><span class="caption-number">Definition 31 </span> (Assignment)</p>
<section class="definition-content" id="proof-content">
<p>An assignment <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span> is a surjective map,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{A} : \mathbb{Z}^{+} &amp;\to \mathbb{Z}^{+} \\
\{1, 2, \dots, N\} &amp;\to \{1, 2, \dots, K\} .
\end{aligned}
\end{split}\]</div>
<p>In this case, <span class="math notranslate nohighlight">\(\mathcal{A}(n) = k\)</span> means that data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> is assigned to cluster <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>One should see that the assignment function <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span> gives rise to the prediction <span class="math notranslate nohighlight">\(\hat{y}^{(n)}\)</span>
for each data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-assignment-prediction">
<span class="eqno">(20)<a class="headerlink" href="#equation-eq-assignment-prediction" title="Link to this equation">#</a></span>\[\hat{y}^{(n)} = \mathcal{A}(n) \quad \text{for } n = 1, 2, \dots, N.\]</div>
</section>
</div><div class="proof example admonition" id="example:assignment">
<p class="admonition-title"><span class="caption-number">Example 8 </span> (Assignment)</p>
<section class="example-content" id="proof-content">
<p>For example, if we have 4 data points <span class="math notranslate nohighlight">\(\mathbf{x}^{(1)}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}^{(2)}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}^{(3)}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{x}^{(4)}\)</span>,
and we want to partition them into 3 clusters <span class="math notranslate nohighlight">\(\hat{C}_1\)</span>, <span class="math notranslate nohighlight">\(\hat{C}_2\)</span>, and <span class="math notranslate nohighlight">\(\hat{C}_3\)</span>, we can define an assignment as follows:</p>
<ul class="simple">
<li><p>Assign <span class="math notranslate nohighlight">\(\mathbf{x}^{(1)}\)</span> to <span class="math notranslate nohighlight">\(\hat{C}_1\)</span>, <span class="math notranslate nohighlight">\(\hat{C}_1 = \left\{\mathbf{x}^{(1)}\right\}\)</span>.</p></li>
<li><p>Assign <span class="math notranslate nohighlight">\(\mathbf{x}^{(3)}\)</span> to <span class="math notranslate nohighlight">\(\hat{C}_2\)</span>, <span class="math notranslate nohighlight">\(\hat{C}_2 = \left\{\mathbf{x}^{(3)}\right\}\)</span>.</p></li>
<li><p>Assign <span class="math notranslate nohighlight">\(\mathbf{x}^{(2)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}^{(4)}\)</span> to <span class="math notranslate nohighlight">\(\hat{C}_3\)</span>, <span class="math notranslate nohighlight">\(\hat{C}_3 = \left\{\mathbf{x}^{(2)}, \mathbf{x}^{(4)}\right\}\)</span>.</p></li>
</ul>
<p>We can make this more precise by defining an assignment function <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{A} : \mathbb{Z}^{+} &amp;\to \mathbb{Z}^{+} \\
\{1, 2, 3, 4\} &amp;\to \{1, 2, 3\}
\end{aligned}
\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{A}(1) = 1\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{A}(2) = 3\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{A}(3) = 2\)</span>, and</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{A}(4) = 3\)</span>.</p></li>
</ul>
</section>
</div><p>We have seen earlier that the assignment function of the K-Means algorithm
follows the nearest-neighbour rule, but we did not explicitly define it here
just yet. We will derive that the optimal assignment <span class="math notranslate nohighlight">\(\mathcal{A}^{*}(\cdot)\)</span> is
the one that minimizes the cost function:</p>
<div class="math notranslate nohighlight" id="equation-eq-assignment-optimal-1">
<span class="eqno">(21)<a class="headerlink" href="#equation-eq-assignment-optimal-1" title="Link to this equation">#</a></span>\[\mathcal{A}^{*}(n) = \underset{k}{\operatorname{argmin}} \left\|\mathbf{x}^{(n)} - \boldsymbol{\mu}_k \right\|^2\]</div>
<section id="assignments-are-equivalent-to-clusters">
<h3><a class="toc-backref" href="#id19" role="doc-backlink">Assignments are Equivalent to Clusters</a><a class="headerlink" href="#assignments-are-equivalent-to-clusters" title="Link to this heading">#</a></h3>
<p>Note that when we mention the assignment function <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span>, we are
implicitly referring to the clusters <span class="math notranslate nohighlight">\(\hat{C}_1\)</span>, <span class="math notranslate nohighlight">\(\hat{C}_2\)</span>, <span class="math notranslate nohighlight">\(\ldots\)</span>,
<span class="math notranslate nohighlight">\(\hat{C}_K\)</span>. They have equivalent meanings.</p>
</section>
</section>
<section id="centroids-representatives">
<h2><a class="toc-backref" href="#id20" role="doc-backlink">Centroids (Representatives)</a><a class="headerlink" href="#centroids-representatives" title="Link to this heading">#</a></h2>
<div class="proof definition admonition" id="def:centroids">
<p class="admonition-title"><span class="caption-number">Definition 32 </span> (Centroids)</p>
<section class="definition-content" id="proof-content">
<p>The centroids <span class="math notranslate nohighlight">\(\boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_K\)</span> of a partition <span class="math notranslate nohighlight">\(\hat{\mathcal{C}}\)</span> are the representatives of each cluster <span class="math notranslate nohighlight">\(C_k \in \hat{\mathcal{C}}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{v}_k \text{ represents cluster } C_k \text{ for } k = 1, 2, \ldots, K.
\]</div>
</section>
</div></section>
<section id="hypothesis-space">
<h2><a class="toc-backref" href="#id21" role="doc-backlink">Hypothesis Space</a><a class="headerlink" href="#hypothesis-space" title="Link to this heading">#</a></h2>
<p>For completeness sake, let’s define the hypothesis space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> for
K-Means.</p>
<p>Intuitively, the hypothesis space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is the set of all possible
clusterings of the data.</p>
<p>Formally, given a set of <span class="math notranslate nohighlight">\(N\)</span> data points
<span class="math notranslate nohighlight">\(\left\{\mathbf{x}^{(n)}\right\}_{n=1}^N\)</span>, let <span class="math notranslate nohighlight">\(C_k\)</span> be the Voronoi cell of the
<span class="math notranslate nohighlight">\(k\)</span>-th cluster center <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span>.</p>
<p>Then, we can write the class of functions <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{H} &amp;= \left\{\mathcal{A}: \mathbb{Z} \rightarrow \mathbb{Z} \mid \mathcal{A}(n) \in \{1, 2, \dots, K\} \text{ for all } n \in \{1, 2, \dots, N\}\right\} \\
\end{aligned}
\end{split}\]</div>
<p>This means the hypothesis space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is finite with cardinality <span class="math notranslate nohighlight">\(K^N\)</span>.</p>
<p>For more details, see <a class="reference external" href="https://stats.stackexchange.com/posts/502352/">here</a> and
<a class="reference external" href="https://courses.cs.washington.edu/courses/cse446/16sp/clustering_1.pdf">here</a>.</p>
</section>
<section id="loss-function">
<h2><a class="toc-backref" href="#id22" role="doc-backlink">Loss Function</a><a class="headerlink" href="#loss-function" title="Link to this heading">#</a></h2>
<p>We make precise the notion of closeness and similarity between data points by
defining a loss function utilizing the
<a class="reference external" href="https://en.wikipedia.org/wiki/Euclidean_distance"><strong>euclidean distance</strong></a>. In
practice, we can use other distance metrics such as
<a class="reference external" href="https://simple.wikipedia.org/wiki/Manhattan_distance"><strong>manhattan distance</strong></a>
that suits one’s needs.</p>
<div class="proof definition admonition" id="def:kmeans-loss">
<p class="admonition-title"><span class="caption-number">Definition 33 </span> (K-Means Loss Function)</p>
<section class="definition-content" id="proof-content">
<p>For any assignment <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span> that maps the set <span class="math notranslate nohighlight">\(\{1, 2, \ldots, N\}\)</span> to <span class="math notranslate nohighlight">\(\{1, 2, \ldots, K\}\)</span> and
any centroids <span class="math notranslate nohighlight">\(\boldsymbol{v}_1, \boldsymbol{v}_2, \dots, \boldsymbol{v}_K \in \mathbb{R}^{D}\)</span>,
we construct the loss function as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\widehat{\mathcal{L}}_{\mathcal{S}}\left(\mathcal{A}, \boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_K \right) &amp;= \left\|\mathbf{x}^{(n)} - \boldsymbol{v}_{\mathcal{A}(n)} \right\|^2 \\
\end{aligned}
\end{split}\]</div>
</section>
</div><p>This just means the loss for one single data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> is the
squared euclidean distance between the data point and its assigned centroid.</p>
<p>As a reminder, the hat symbol <span class="math notranslate nohighlight">\(\widehat{\cdot}\)</span> is used to denote an estimate of
a quantity or function. In this case, the loss function
<span class="math notranslate nohighlight">\(\widehat{\mathcal{L}}_{\mathcal{S}}\)</span> is an estimate of the loss function
<span class="math notranslate nohighlight">\(\mathcal{L}\)</span> since we do not have access to all the data points <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
</section>
<section id="cost-function">
<h2><a class="toc-backref" href="#id23" role="doc-backlink">Cost Function</a><a class="headerlink" href="#cost-function" title="Link to this heading">#</a></h2>
<p>However, we are not interested in the loss for a single data point, but rather
the loss for all data points in the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>.</p>
<p>To this end, the cost function in K-Means is the sum of the loss function over
all data points <span class="math notranslate nohighlight">\(\left\{\mathbf{x}^{(n)}\right\}_{n=1}^N\)</span>, defined as follows:</p>
<div class="proof definition admonition" id="def:kmeans-cost">
<p class="admonition-title"><span class="caption-number">Definition 34 </span> (K-Means Cost Function)</p>
<section class="definition-content" id="proof-content">
<p>For any assignment <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span> that maps the set <span class="math notranslate nohighlight">\(\{1, 2, \ldots, N\}\)</span> to <span class="math notranslate nohighlight">\(\{1, 2, \ldots, K\}\)</span> and
any centroids <span class="math notranslate nohighlight">\(\boldsymbol{v}_1, \boldsymbol{v}_2, \dots, \boldsymbol{v}_K \in \mathbb{R}^{D}\)</span>,
we construct the cost function as follows:</p>
<div class="math notranslate nohighlight" id="equation-eq-kmeans-cost-1">
<span class="eqno">(22)<a class="headerlink" href="#equation-eq-kmeans-cost-1" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
\widehat{\mathcal{J}}_{\mathcal{S}}(\mathcal{A}, \boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_K) &amp;:= \widehat{\mathcal{J}}\left(\left\{\hat{y}^{(n)}\right\}_{n=1}^N,\left\{\boldsymbol{v}_{k}\right\}_{k=1}^k \middle \vert \mathcal{S}\right) \\
&amp;\overset{\text{(a)}}{=} \sum_{n=1}^{N} \left\|\mathbf{x}^{(n)} - \boldsymbol{v}_{\mathcal{A}(n)} \right\|^2 \\
&amp;\overset{\text{(b)}}{=} \sum_{n=1}^{N} \sum_{\mathcal{A}(n) = k} \left\|\mathbf{x}^{(n)} - \boldsymbol{v}_k \right\|^2 \\
&amp;\overset{\text{(c)}}{=} \sum_{n=1}^{N} \sum_{k=1}^{K} r^{(n)}_k \left\|\mathbf{x}^{(n)} - \boldsymbol{v}_k \right\|^2 \\
&amp;\overset{\text{(d)}}{=} \sum_{n=1}^{N} \sum_{k=1}^{K} \mathbb{I}\left\{\mathcal{A}(n) = k\right\} \left\|\mathbf{x}^{(n)} - \boldsymbol{v}_k \right\|^2 \\
&amp;\overset{\text{(e)}}{=} \sum_{k=1}^K \sum_{\mathbf{x}^{(n)} \in \hat{C}_k} \left\|\mathbf{x}^{(n)} - \boldsymbol{v}_k \right\|^2 \\
\end{aligned}\end{split}\]</div>
</section>
</div><p>where</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathcal{A}(n) = k\)</span> means that data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> is assigned to
cluster <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(r^{(n)}_k\)</span> is an indicator function that is equal to 1 if
<span class="math notranslate nohighlight">\(\mathcal{A}(n) = k\)</span> and 0 otherwise.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
    r^{(n)}_k &amp;= \begin{cases} 1 &amp; \text{if } \mathcal{A}(n) = k \\ 0 &amp; \text{otherwise} \end{cases}
    \end{aligned}
    \end{split}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(\hat{C}_k\)</span> is the set of data points that are assigned to cluster <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\left\|\cdot\right\|\)</span> is the euclidean norm.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
    \left\|\mathbf{x} - \boldsymbol{v}\right\|^2 &amp;= \left(\mathbf{x} - \boldsymbol{v}\right)^{\top} \left(\mathbf{x} - \boldsymbol{v}\right) \\
    \end{aligned}
    \end{split}\]</div>
</li>
<li><p>All 5 forms are equivalent<a class="footnote-reference brackets" href="#equivalent-k-means-cost-function" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a>.</p></li>
</ul>
<p>It is worth a reminder that we have not formally defined what the assignment
<span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span> is, as well as the representative vectors (centroids)
<span class="math notranslate nohighlight">\(\boldsymbol{v}_1, \boldsymbol{v}_2, \dots, \boldsymbol{v}_K\)</span>. We will show
later that <span class="math notranslate nohighlight">\(\boldsymbol{v}_k\)</span> is the mean of the data points in cluster <span class="math notranslate nohighlight">\(k\)</span> and
that
<span class="math notranslate nohighlight">\(\mathcal{A}(n)=\underset{k}{\operatorname{argmin}} \left\|\mathbf{x}^{(n)} - \boldsymbol{\mu}_k \right\|^2\)</span>
is the assignment that minimizes the cost function
<span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_{\mathcal{S}}\)</span>.</p>
<div class="proof remark admonition" id="remark:cost-function-is-a-function-of-assignment-and-centroids">
<p class="admonition-title"><span class="caption-number">Remark 22 </span> (Cost Function is a Function of Assignment and Centroids)</p>
<section class="remark-content" id="proof-content">
<p>The cost function is a function <strong>both</strong> the assignment <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span> and the cluster centers <span class="math notranslate nohighlight">\(\boldsymbol{v}_1, \boldsymbol{v}_2, \dots, \boldsymbol{v}_K\)</span>,
which adds up the squared euclidean distance between each data point and its assigned cluster center. The
total cost is what we are minimizing. Note that the problem is equivalent to minimizing each
cluster’s cost individually.</p>
<p>We also call the loss sum of squared error (SSE) , which is just the intra-cluster variance, a measure of how spread out the data points are within a cluster.</p>
</section>
</div></section>
<section id="objective-function">
<h2><a class="toc-backref" href="#id24" role="doc-backlink">Objective Function</a><a class="headerlink" href="#objective-function" title="Link to this heading">#</a></h2>
<p>Finally, we define the objective function as the cost function
<span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_{\mathcal{S}}\)</span> that we are minimizing.</p>
<div class="proof definition admonition" id="def:kmeans-objective">
<p class="admonition-title"><span class="caption-number">Definition 35 </span> (K-Means Objective Function)</p>
<section class="definition-content" id="proof-content">
<p>The <strong>objective</strong> function is to <strong>minimize</strong> the above expression in equation <a class="reference internal" href="#equation-eq-kmeans-cost-1">(22)</a>:</p>
<div class="math notranslate nohighlight" id="equation-eq-k-means-objective-function-1">
<span class="eqno">(23)<a class="headerlink" href="#equation-eq-k-means-objective-function-1" title="Link to this equation">#</a></span>\[\begin{split}\begin{alignat}{3}
\underset{\mathcal{A}, \boldsymbol{v}_k}{\operatorname{argmin}} &amp;\quad&amp; \widehat{\mathcal{J}}_{\mathcal{S}}(\mathcal{A}, \boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_K) &amp;= \sum_{n=1}^{N} \sum_{\mathcal{A}(n) = k} \left\|\mathbf{x}^{(n)} - \boldsymbol{v}_k \right\|^2  \\
\text{subject to} &amp;\quad&amp; \hat{C}_1 \sqcup \hat{C}_2 \sqcup \cdots \sqcup \hat{C}_K &amp;= \mathcal{S} \\
&amp;\quad&amp; \boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_K &amp;\in \mathbb{R}^D \\
\end{alignat}\end{split}\]</div>
</section>
</div><p>This just means, for all possible assignments <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span> and cluster
centers <span class="math notranslate nohighlight">\(\boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_K\)</span>, we want
to find the assignment <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span> and cluster centers
<span class="math notranslate nohighlight">\(\boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_K\)</span> that minimize the
cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_{\mathcal{S}}\)</span>.</p>
<p>In other words, of all possible sets (there’s a lot, as we should see later)
<span class="math notranslate nohighlight">\(\hat{\mathcal{C}} = \left\{\hat{C}_1, \hat{C}_2, \ldots, \hat{C}_K\right\}\)</span>, we
want to find the set <span class="math notranslate nohighlight">\(\hat{\mathcal{C}}\)</span> that minimizes the cost function
<span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_{\mathcal{S}}\)</span>.</p>
<div class="proof theorem admonition" id="thm:minimizing-individual-clusters-cost-is-equivalent-to-minimizing-the-objective-function">
<p class="admonition-title"><span class="caption-number">Theorem 10 </span> (Minimizing Individual Cluster’s Cost is Equivalent to Minimizing the Objective Function)</p>
<section class="theorem-content" id="proof-content">
<p>The objective function is equivalent to minimizing each cluster’s cost individually.</p>
</section>
</div><p>Recall we mentioned that optimizing the objective function
<span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_{\mathcal{S}}\)</span> means we are finding the optimal
assignment <span class="math notranslate nohighlight">\(\mathcal{A}^*(\cdot)\)</span> and the optimal cluster centers
<span class="math notranslate nohighlight">\(\boldsymbol{v}_1^*, \boldsymbol{v}_2^*, \ldots, \boldsymbol{v}_K^*\)</span> at the same
time. This is challenging as <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_{\mathcal{S}}\)</span> is a
non-convex function of <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span> and
<span class="math notranslate nohighlight">\(\boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_K\)</span>. We will now fall
back on heuristics to find the local optimum. In what follows, we will list the
<em>necessary</em> conditions to minimize the objective function
<span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_{\mathcal{S}}\)</span>.</p>
</section>
<section id="the-necessary-conditions-to-minimize-the-objective-function">
<h2><a class="toc-backref" href="#id25" role="doc-backlink">The Necessary Conditions to Minimize the Objective Function</a><a class="headerlink" href="#the-necessary-conditions-to-minimize-the-objective-function" title="Link to this heading">#</a></h2>
<p>With all the definitions in place, we can now formally state the necessary
conditions to minimize the objective function.</p>
<p>Note a necessary condition only guarantees that if a solution is optimal, then
the conditions must be satisfied. However, if a solution does satisfy the
conditions, it does not necessarily mean that it is optimal. In short, we may
land ourselves with a <strong>local</strong> minimum that is not <strong>globally</strong> optimal.</p>
<section id="condition-1-the-optimal-assignment">
<h3><a class="toc-backref" href="#id26" role="doc-backlink">Condition 1: The Optimal Assignment</a><a class="headerlink" href="#condition-1-the-optimal-assignment" title="Link to this heading">#</a></h3>
<div class="proof criterion admonition" id="criterion:kmeans-optimal-assignment">
<p class="admonition-title"><span class="caption-number">Criterion 1 </span> (K-Means Optimal Assignment)</p>
<section class="criterion-content" id="proof-content">
<p>Fix the cluster centers <span class="math notranslate nohighlight">\(\boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_K\)</span>, we seek
the optimal assignment <span class="math notranslate nohighlight">\(\mathcal{A}^*(\cdot)\)</span> that minimizes the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_{\mathcal{S}}(\cdot)\)</span>.</p>
<p>We claim that the optimal assignment <span class="math notranslate nohighlight">\(\mathcal{A}^*(\cdot)\)</span> follows the <em>nearest neighbor</em> rule, which means that,</p>
<div class="math notranslate nohighlight" id="equation-eq-k-means-criterion-1-1">
<span class="eqno">(24)<a class="headerlink" href="#equation-eq-k-means-criterion-1-1" title="Link to this equation">#</a></span>\[\begin{aligned}
\mathcal{A}^*(n) = \underset{k \in \{1, 2, \ldots, K\}}{\operatorname{argmin}} \left\|\mathbf{x}^{(n)} - \boldsymbol{v}_k \right\|^2 .
\end{aligned}\]</div>
<p>Then the assignment <span class="math notranslate nohighlight">\(\mathcal{A}^*\)</span> is the optimal assignment that minimizes the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_{\mathcal{S}}\)</span>.</p>
<p>This is quite intuitive as we are merely assigning each data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> to cluster <span class="math notranslate nohighlight">\(k\)</span>
whose center <span class="math notranslate nohighlight">\(\boldsymbol{v}_k\)</span> is closest to <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span>.</p>
<p>We rephrase the claim by saying that for any assignment <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>, we have</p>
<div class="math notranslate nohighlight" id="equation-eq-k-means-criterion-1-2">
<span class="eqno">(25)<a class="headerlink" href="#equation-eq-k-means-criterion-1-2" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
\widehat{\mathcal{J}}_{\mathcal{S}}(\mathcal{A}, \boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_K) &amp;\geq \widehat{\mathcal{J}}_{\mathcal{S}}(\mathcal{A}^*, \boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_K) \\
\end{aligned}\end{split}\]</div>
<p>Let’s prove this claim.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. In equation <a class="reference internal" href="#equation-eq-k-means-criterion-1-2">(25)</a>, we have that <span class="math notranslate nohighlight">\(\boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_K\)</span> are fixed.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\widehat{\mathcal{J}}_{\mathcal{S}}(\mathcal{A}, \boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_K) &amp;= \sum_{n=1}^{N} \sum_{\mathcal{A}(n) = k} \left\|\mathbf{x}^{(n)} - \boldsymbol{v}_k \right\|^2 \\
&amp;\geq \sum_{n=1}^{N} \sum_{\mathcal{A}^*(n) = k} \left\|\mathbf{x}^{(n)} - \boldsymbol{v}_k \right\|^2 \\
&amp;= \widehat{\mathcal{J}}_{\mathcal{S}}(\mathcal{A}^*, \boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_K)
\end{aligned}
\end{split}\]</div>
<p>This is just a proof by definition of <span class="math notranslate nohighlight">\(\mathcal{A}^*\)</span> since</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\left\|\mathbf{x}^{(n)} - \boldsymbol{v}_{\mathcal{A}(n)} \right\|^2 \geq \left\|\mathbf{x}^{(n)} - \boldsymbol{v}_{\mathcal{A}^*(n)} \right\|^2 .
\end{aligned}
\]</div>
<p>If you look at it intuitively, it just means there does not exist any other arrangement/assignment <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>
that can reduce the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_{\mathcal{S}}\)</span> better than the optimal assignment <span class="math notranslate nohighlight">\(\mathcal{A}^*\)</span> (nearest neighbor rule).</p>
</div>
</section>
<section id="condition-2-the-optimal-cluster-centers-centroids">
<h3><a class="toc-backref" href="#id27" role="doc-backlink">Condition 2: The Optimal Cluster Centers (Centroids)</a><a class="headerlink" href="#condition-2-the-optimal-cluster-centers-centroids" title="Link to this heading">#</a></h3>
<div class="proof criterion admonition" id="criterion:kmeans-optimal-cluster-centers">
<p class="admonition-title"><span class="caption-number">Criterion 2 </span> (K-Means Optimal Cluster Centers)</p>
<section class="criterion-content" id="proof-content">
<p>Fix the assignment <span class="math notranslate nohighlight">\(\mathcal{A}^*(\cdot)\)</span>, we seek the optimal cluster centers <span class="math notranslate nohighlight">\(\boldsymbol{v}_1^*, \boldsymbol{v}_2^*, \ldots, \boldsymbol{v}_K^*\)</span> that minimize the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_{\mathcal{S}}\)</span>.</p>
<p>We claim that the optimal cluster centers is the mean of the data points assigned to each cluster.</p>
<div class="math notranslate nohighlight" id="equation-eq-k-means-criterion-2-1">
<span class="eqno">(26)<a class="headerlink" href="#equation-eq-k-means-criterion-2-1" title="Link to this equation">#</a></span>\[\begin{aligned}
\boldsymbol{v}_k^* = \frac{1}{\left|\hat{C}_k^*\right|} \sum_{\mathbf{x}^{(n)} \in \hat{C}_k^*} \mathbf{x}^{(n)}
\end{aligned}\]</div>
<p>where <span class="math notranslate nohighlight">\(\left|\hat{C}_k^*\right|\)</span> is the number of data points assigned to cluster <span class="math notranslate nohighlight">\(k\)</span>. We can denote it
as <span class="math notranslate nohighlight">\(N_k\)</span> for convenience.</p>
<p>We can also rephrase this claim by saying that for any cluster centers <span class="math notranslate nohighlight">\(\boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_K\)</span>, fixing the assignment <span class="math notranslate nohighlight">\(\mathcal{A}^*\)</span>, we have</p>
<div class="math notranslate nohighlight" id="equation-eq-k-means-criterion-2-2">
<span class="eqno">(27)<a class="headerlink" href="#equation-eq-k-means-criterion-2-2" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
\widehat{\mathcal{J}}_{\mathcal{S}}(\mathcal{A}^*, \boldsymbol{v}_1, \boldsymbol{v}_2, \ldots, \boldsymbol{v}_K) &amp;\geq \widehat{\mathcal{J}}_{\mathcal{S}}(\mathcal{A}^*, \boldsymbol{v}_1^*, \boldsymbol{v}_2^*, \ldots, \boldsymbol{v}_K^*) \\
\end{aligned}\end{split}\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. This proof in short just says that the mean minimizes the sum of squared distances.</p>
<p>Since we established (<a class="reference internal" href="#thm:minimizing-individual-clusters-cost-is-equivalent-to-minimizing-the-objective-function">Theorem 10</a>)
that minimizing each individual cluster <span class="math notranslate nohighlight">\(\hat{C}_k\)</span> is equivalent to minimizing the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_{\mathcal{S}}\)</span>,
we can now fix any cluster <span class="math notranslate nohighlight">\(\hat{C}_k\)</span> (i.e. also fixing the assignment <span class="math notranslate nohighlight">\(\mathcal{A}^*\)</span>) and seek the optimal cluster center <span class="math notranslate nohighlight">\(\boldsymbol{v}_k^*\)</span> that minimizes the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_{\hat{C}_k}\)</span>.</p>
<p>Note after fixing the assignment <span class="math notranslate nohighlight">\(\mathcal{A}^*(\cdot)\)</span>, <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_{\hat{C}_k}\)</span> is now just a
function of <span class="math notranslate nohighlight">\(\boldsymbol{v}_k\)</span> and is the cost for that cluster.</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\widehat{\mathcal{J}}_{\hat{C}_k}(\boldsymbol{v}_k) = \sum_{\mathbf{x}^{(n)} \in \hat{C}_k} \left\|\mathbf{x}^{(n)} - \boldsymbol{v}_k \right\|^2
\end{aligned}
\]</div>
<p>We can now take the derivative of <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_{\hat{C}_k}\)</span> with respect to <span class="math notranslate nohighlight">\(\boldsymbol{v}_k\)</span> and set it to zero to find the optimal cluster center <span class="math notranslate nohighlight">\(\boldsymbol{v}_k^*\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-derivative-of-k-means-cost-function">
<span class="eqno">(28)<a class="headerlink" href="#equation-eq-derivative-of-k-means-cost-function" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
\frac{\partial}{\partial \boldsymbol{v}_k} \widehat{\mathcal{J}}_{\hat{C}_k}(\boldsymbol{v}_k) &amp;= \frac{\partial}{\partial \boldsymbol{v}_k} \sum_{\mathbf{x}^{(n)} \in \hat{C}_k} \left\|\mathbf{x}^{(n)} - \boldsymbol{v}_k \right\|^2 \\
&amp;= 2 \sum_{\mathbf{x}^{(n)} \in \hat{C}_k} \left(\mathbf{x}^{(n)} - \boldsymbol{v}_k \right) \\
&amp;= 2 \left( \sum_{\mathbf{x}^{(n)} \in \hat{C}_k} \mathbf{x}^{(n)} - \sum_{\mathbf{x}^{(n)} \in \hat{C}_k} \boldsymbol{v}_k \right) \\
&amp;= 2 \left( \sum_{\mathbf{x}^{(n)} \in \hat{C}_k} \mathbf{x}^{(n)} - N_k \boldsymbol{v}_k \right) \\
\end{aligned}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(N_k\)</span> is the number of data points assigned to cluster <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>Now to minimize equation <a class="reference internal" href="#equation-eq-derivative-of-k-means-cost-function">(28)</a>, we set it to zero and solve for <span class="math notranslate nohighlight">\(\boldsymbol{v}_k\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial}{\partial \boldsymbol{v}_k} \widehat{\mathcal{J}}_{\hat{C}_k}(\boldsymbol{v}_k) = 0 &amp;\iff 2 \left( \sum_{\mathbf{x}^{(n)} \in \hat{C}_k} \mathbf{x}^{(n)} - N_k \boldsymbol{v}_k \right) = 0 \\
&amp;\iff \sum_{\mathbf{x}^{(n)} \in \hat{C}_k} \mathbf{x}^{(n)} - N_k \boldsymbol{v}_k = 0 \\
&amp;\iff \sum_{\mathbf{x}^{(n)} \in \hat{C}_k} \mathbf{x}^{(n)} = N_k \boldsymbol{v}_k \\
&amp;\iff \boldsymbol{v}_k = \frac{1}{N_k} \sum_{\mathbf{x}^{(n)} \in \hat{C}_k} \mathbf{x}^{(n)} \\
\end{aligned}
\end{split}\]</div>
<p>recovering <a class="reference internal" href="#criterion:kmeans-optimal-cluster-centers">Criterion 2</a>.</p>
<p>There are other variants of <a class="reference external" href="https://math.stackexchange.com/questions/967138/formal-proof-that-mean-minimize-squared-error-function">proof</a>.</p>
</div>
<div class="proof remark admonition" id="prf:remark:kmeans-optimal-cluster-centers-notation">
<p class="admonition-title"><span class="caption-number">Remark 23 </span> (Notation)</p>
<section class="remark-content" id="proof-content">
<p>We will now denote <span class="math notranslate nohighlight">\(\boldsymbol{v}_k^*\)</span> as <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span> in the following sections.</p>
</section>
</div></section>
<section id="objective-function-re-defined">
<h3><a class="toc-backref" href="#id28" role="doc-backlink">Objective Function Re-defined</a><a class="headerlink" href="#objective-function-re-defined" title="Link to this heading">#</a></h3>
<p>We can now re-define the objective function in equation
<a class="reference internal" href="#equation-eq-k-means-objective-function-1">(23)</a> in terms of the optimal cluster centers
and assignments.</p>
<div class="math notranslate nohighlight" id="equation-eq-k-means-objective-function-2">
<span class="eqno">(29)<a class="headerlink" href="#equation-eq-k-means-objective-function-2" title="Link to this equation">#</a></span>\[\begin{split}\begin{alignat}{4}
\underset{\mathcal{A}, \boldsymbol{\mu}_k}{\operatorname{argmin}} &amp;\quad&amp; \widehat{\mathcal{J}}_{\mathcal{S}}(\mathcal{A}, \boldsymbol{\mu}_1, \boldsymbol{\mu}_2, \ldots, \boldsymbol{\mu}_K) &amp;= \sum_{n=1}^{N} \sum_{\mathcal{A}(n) = k} \left\|\mathbf{x}^{(n)} - \boldsymbol{\mu}_k \right\|^2  \\
\text{subject to} &amp;\quad&amp; \hat{C}_1 \cup \hat{C}_2 \cup \cdots \cup \hat{C}_K &amp;= \mathcal{S} \\
&amp;\quad&amp; \boldsymbol{\mu}_1, \boldsymbol{\mu}_2, \ldots, \boldsymbol{\mu}_K &amp;\in \mathbb{R}^D \\
&amp;\quad&amp; \hat{y}^{(n)} := \mathcal{A}(n) &amp;= \underset{k}{\operatorname{argmin}} \left\|\mathbf{x}^{(n)} - \boldsymbol{\mu}_k \right\|^2 \\
\end{alignat}\end{split}\]</div>
<div class="proof remark admonition" id="remark:kmeans-cost-function-is-a-function-of-assignments-and-cluster-centers">
<p class="admonition-title"><span class="caption-number">Remark 24 </span> (Cost Function is a function of assignments and cluster centers)</p>
<section class="remark-content" id="proof-content">
<p>Reminder!</p>
<p>The cost function in equation <a class="reference internal" href="#equation-eq-k-means-objective-function-2">(29)</a> is a function of <strong>both</strong> the cluster assignments and cluster centers.
And therefore we are minimizing the cost function with respect to the cluster assignments and cluster centers. However,
jointly optimizing both the cluster assignments and cluster centers is computationally challenging, and therefore
we split to two steps, first optimizing the cluster assignments and then optimizing the cluster centers in a greedy manner.</p>
</section>
</div></section>
</section>
<section id="algorithm">
<h2><a class="toc-backref" href="#id29" role="doc-backlink">Algorithm</a><a class="headerlink" href="#algorithm" title="Link to this heading">#</a></h2>
<p>We are now ready to define the full Lloyd’s algorithm for K-Means.</p>
<div class="proof algorithm admonition" id="lloyd-kmeans-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 1 </span> (Lloyd’s Algorithm (K-Means))</p>
<section class="algorithm-content" id="proof-content">
<p>Given a set of data points (samples)</p>
<div class="math notranslate nohighlight">
\[
\mathcal{S} = \left\{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(N)}\right\}
\]</div>
<p>the K-Means algorithm aims to group the data points into <span class="math notranslate nohighlight">\(K\)</span> clusters</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathcal{C}} = \left\{ \hat{C}_1, \hat{C}_2, \dots, \hat{C}_K \right\}
\]</div>
<p>such that the sum of squared distances
between each data point and its cluster center is minimized.</p>
<p>In code, <span class="math notranslate nohighlight">\(\hat{\mathcal{C}}\)</span> can be treated as a dictionary/hash map,
where the <strong>key</strong> is the cluster number and the <strong>value</strong> is the set of data points assigned to that cluster.</p>
<ol class="arabic">
<li><p><strong>Initialization Step</strong>: Initialize <span class="math notranslate nohighlight">\(K\)</span> cluster centers <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_1^{[0]}, \boldsymbol{\mu}_2^{[0]}, \dots, \boldsymbol{\mu}_K^{[0]}\)</span> randomly (best to be far apart)
where the superscript <span class="math notranslate nohighlight">\([0]\)</span> denotes the iteration number <span class="math notranslate nohighlight">\(t=0\)</span>.</p>
<ul class="simple">
<li><p>In the very first iteration, there are no data points in any cluster <span class="math notranslate nohighlight">\(\hat{C}_k^{[0]} = \emptyset\)</span>. Therefore, the cluster centers are just randomly chosen for simplicity.</p></li>
<li><p>By random, we mean that the cluster centers are randomly chosen from the data points <span class="math notranslate nohighlight">\(\mathcal{S} = \left\{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(N)}\right\}\)</span>
and not randomly chosen from the feature space <span class="math notranslate nohighlight">\(\mathbb{R}^D\)</span>.</p></li>
<li><p>Subsequent iterations will have data points in the clusters <span class="math notranslate nohighlight">\(\hat{C}_k^{[t]} \neq \emptyset\)</span> and thus
<span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{[t]}\)</span> will be the mean of the data points in cluster <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
<li><p>Each <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{[0]} = \begin{bmatrix} \mu_{1k}^{[0]} &amp; \mu_{2k}^{[0]} &amp; \cdots &amp; \mu_{Dk}^{[0]} \end{bmatrix}^{\mathrm{T}}\)</span> is a <span class="math notranslate nohighlight">\(D\)</span>-dimensional vector, where <span class="math notranslate nohighlight">\(D\)</span> is the number of features, and represents the
mean vector of all the data points in cluster <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
<li><p>Note that <span class="math notranslate nohighlight">\(\mu_{dk}^{[0]}\)</span> is the mean value of the <span class="math notranslate nohighlight">\(d\)</span>-th feature in cluster <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
<li><p>We denote <span class="math notranslate nohighlight">\(\boldsymbol{\mu} = \begin{bmatrix} \boldsymbol{\mu}_1 &amp; \boldsymbol{\mu}_2 &amp; \cdots &amp; \boldsymbol{\mu}_K \end{bmatrix}_{K \times D}^{\mathrm{T}}\)</span> to be the collection of all <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_1, \boldsymbol{\mu}_2, \dots, \boldsymbol{\mu}_K\)</span>.</p></li>
</ul>
</li>
<li><p><strong>Assignment Step (E)</strong>: For <span class="math notranslate nohighlight">\(t=0, 1, 2, \dots\)</span>, assign each data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> to the closest cluster center <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{[t]}\)</span>,</p>
<div class="math notranslate nohighlight" id="equation-eq-kmeans-classify">
<span class="eqno">(30)<a class="headerlink" href="#equation-eq-kmeans-classify" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
\hat{y}^{(n)[t]} := \mathcal{A}^{*(n)[t]} &amp;= \underset{k \in \{1, 2, \ldots, K\}}{\operatorname{argmin}} \left\| \mathbf{x}^{(n)} - \boldsymbol{\mu}_k^{[t]} \right\|^2 \\
\end{aligned}\end{split}\]</div>
<p>In other words, <span class="math notranslate nohighlight">\(\hat{y}^{(n)[t]}\)</span> is the output of the optimal assignment rule at the <span class="math notranslate nohighlight">\(t\)</span>-th iteration
and is the index of the cluster center <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{[t]}\)</span> that is closest to <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span>.</p>
<p>For instance, if <span class="math notranslate nohighlight">\(K = 3\)</span>, and for the first sample point <span class="math notranslate nohighlight">\(n=1\)</span>,
assume the closest cluster center is <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_2^{[t]}\)</span>, then the assignment <span class="math notranslate nohighlight">\(\mathcal{A}^{*}\)</span> will
assign this point to cluster <span class="math notranslate nohighlight">\(k=2\)</span>, <span class="math notranslate nohighlight">\(\hat{y}^{(1)} = 2\)</span>. Note that <span class="math notranslate nohighlight">\(\hat{y}^{(n)}\)</span> is a scalar and has the same superscript as <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span>, indicating they belong to the same sample.</p>
<p>For notational convenience, we can also denote <span class="math notranslate nohighlight">\(\hat{C}_k^{[t]}\)</span> as the set of data points that are assigned to cluster <span class="math notranslate nohighlight">\(k\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    \begin{aligned}
    \hat{C}_k^{[t]} &amp;= \left\{ \mathbf{x}^{(n)} \mid \hat{y}^{(n)} = k \right\}
    \end{aligned}
    \]</div>
<p>Mathematically, this means partitioning the data points using <a class="reference external" href="https://en.wikipedia.org/wiki/Voronoi_diagram">Voronoi Diagram</a>,
as mentioned in the previous section <a class="reference internal" href="#def:kmeans-voronoi-partition">Definition 30</a>.</p>
<p><strong>Note, at this step, we obtain the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_{\mathcal{S}}^{[t]}\)</span> at iteration <span class="math notranslate nohighlight">\(t\)</span></strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
    \widehat{\mathcal{J}}_{\mathcal{S}}^{[t]}\left(\mathcal{A}^{*(1)[t]}, \mathcal{A}^{*(2)[t]}, \dots, \mathcal{A}^{*(N)[t]}, \boldsymbol{\mu}_1^{[t]}, \boldsymbol{\mu}_2^{[t]}, \dots, \boldsymbol{\mu}_K^{[t]}\right) &amp;= \widehat{\mathcal{J}}_{\mathcal{S}}^{[t]}\left(\hat{C}_1^{[t]}, \hat{C}_2^{[t]}, \dots, \hat{C}_K^{[t]}, \boldsymbol{\mu}_1^{[t]}, \boldsymbol{\mu}_2^{[t]}, \dots, \boldsymbol{\mu}_K^{[t]}\right) \\
      &amp;= \sum_{k=1}^K \sum_{\mathbf{x}^{(n)} \in \hat{C}_k^{[t]}} \left\| \mathbf{x}^{(n)} - \boldsymbol{\mu}_k^{[t]} \right\|^2 \\
    \end{aligned}
    \end{split}\]</div>
<p>This makes sense because once we obtain the cluster centers <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{[t]}\)</span> for the <span class="math notranslate nohighlight">\(t\)</span>-th iteration,
then it induces the partition <span class="math notranslate nohighlight">\(\hat{C}_k^{[t]}\)</span> and the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_{\mathcal{S}}^{[t]}\)</span>.
<strong>However, one important thing to realize is that the mean <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{[t]}\)</span> is not
necessarily the “best”, and hence we need to update the cluster centers <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{[t]}\)</span> in the next step.</strong></p>
</li>
<li><p><strong>Update Step (M)</strong>: Update the cluster centers for the next iteration.</p>
<div class="math notranslate nohighlight" id="equation-eq-kmeans-recenter">
<span class="eqno">(31)<a class="headerlink" href="#equation-eq-kmeans-recenter" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
\boldsymbol{\mu}_k^{[t+1]} &amp;= \frac{1}{|\hat{C}_k^{[t]}|} \sum_{\mathbf{x}^{(n)} \in \hat{C}_k^{[t]}} \mathbf{x}^{(n)} \\
\end{aligned}\end{split}\]</div>
<p>Notice that the cluster center <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{[t+1]}\)</span> is the mean of all data points that are assigned to cluster <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>It may be confusing to define the cluster center <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{[t+1]}\)</span> for the next iteration, but this will
be apparent later.</p>
</li>
<li><p>Repeat steps 2 and 3 until the centroids stop changing.</p>
<div class="math notranslate nohighlight" id="equation-eq-kmeans-convergence">
<span class="eqno">(32)<a class="headerlink" href="#equation-eq-kmeans-convergence" title="Link to this equation">#</a></span>\[\begin{aligned}
\boldsymbol{\mu}_k^{[t+1]} = \boldsymbol{\mu}_k^{[t]}
\end{aligned}\]</div>
<p>In other words,</p>
<div class="math notranslate nohighlight">
\[
    \begin{aligned}
    \widehat{\mathcal{J}}_{\mathcal{S}}^{[t+1]}\left(\mathcal{A}^{*[t+1]}, \boldsymbol{\mu}^{[t+1]} \right) = \widehat{\mathcal{J}}_{\mathcal{S}}^{[t]}\left(\mathcal{A}^{*[t]}, \boldsymbol{\mu}^{[t]} \right)
    \end{aligned}
    \]</div>
<p>This is the convergence condition.</p>
</li>
</ol>
</section>
</div><div class="proof remark admonition" id="remark-kmeans-greedy">
<p class="admonition-title"><span class="caption-number">Remark 25 </span> (K-Means is a Greedy Algorithm)</p>
<section class="remark-content" id="proof-content">
<p>It is important to recognize that the K-Means (Lloyd’s) Algorithm optimizes two objectives in an alternating fashion.
It alternatively changes both the assignment step <span class="math notranslate nohighlight">\(\mathcal{A}^{*}(\cdot)\)</span> and the update step <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{[t+1]}\)</span>
to greedily minimize the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}(\mathcal{A}, \boldsymbol{\mu})\)</span>.</p>
</section>
</div></section>
<section id="model-fitting">
<h2><a class="toc-backref" href="#id30" role="doc-backlink">Model Fitting</a><a class="headerlink" href="#model-fitting" title="Link to this heading">#</a></h2>
<p>The fitting of the K-Means model is straightforward, we directly apply the
K-Means Algorithm defined in <a class="reference internal" href="#lloyd-kmeans-algorithm">Algorithm 1</a> to the training
data <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> to obtain the optimal assignment <span class="math notranslate nohighlight">\(\mathcal{A}^{*}\)</span> and the
optimal cluster centers <span class="math notranslate nohighlight">\(\boldsymbol{\mu}^{*}\)</span>.</p>
</section>
<section id="model-inference">
<h2><a class="toc-backref" href="#id31" role="doc-backlink">Model Inference</a><a class="headerlink" href="#model-inference" title="Link to this heading">#</a></h2>
<p>The inference of the K-Means model is also straightforward, for the new data
point <span class="math notranslate nohighlight">\(\mathbf{x}^{(q)}\)</span>, we apply the assignment step <span class="math notranslate nohighlight">\(\mathcal{A}^{*}(\cdot)\)</span>
to it and find the closest cluster center <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{*}\)</span>, then we
assign it to cluster <span class="math notranslate nohighlight">\(k\)</span>. In other words, we assign <span class="math notranslate nohighlight">\(\mathbf{x}^{(q)}\)</span> to the
cluster that has the closest cluster center.</p>
</section>
<section id="convergence">
<h2><a class="toc-backref" href="#id32" role="doc-backlink">Convergence</a><a class="headerlink" href="#convergence" title="Link to this heading">#</a></h2>
<p>In this section, we will prove that the K-Means Algorithm converges to a local
minimum of the cost function
<span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}(\mathcal{A}, \boldsymbol{\mu})\)</span>.</p>
<section id="lemma-1-stirling-numbers-of-the-second-kind">
<h3><a class="toc-backref" href="#id33" role="doc-backlink">Lemma 1: Stirling Numbers of the Second Kind</a><a class="headerlink" href="#lemma-1-stirling-numbers-of-the-second-kind" title="Link to this heading">#</a></h3>
<div class="proof lemma admonition" id="stirling-numbers">
<p class="admonition-title"><span class="caption-number">Lemma 2 </span> (Stirling Numbers of the Second Kind)</p>
<section class="lemma-content" id="proof-content">
<p>The Stirling Numbers of the Second Kind <span class="math notranslate nohighlight">\(S(n, k)\)</span> are defined as the number of ways to partition a set of <span class="math notranslate nohighlight">\(n\)</span> elements into <span class="math notranslate nohighlight">\(k\)</span> non-empty subsets.</p>
<p>There are at most <span class="math notranslate nohighlight">\(k^n\)</span> ways to partition a set of <span class="math notranslate nohighlight">\(n\)</span> elements into <span class="math notranslate nohighlight">\(k\)</span> non-empty subsets.</p>
<p>In our case, since there are <span class="math notranslate nohighlight">\(N\)</span> data points, and we want to partition them into <span class="math notranslate nohighlight">\(K\)</span> clusters, there are at most <span class="math notranslate nohighlight">\(K^N\)</span> ways to partition the data points into <span class="math notranslate nohighlight">\(K\)</span> clusters.</p>
<p>In other words, the assignment step <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span> has at most <span class="math notranslate nohighlight">\(K^N\)</span> possible mappings.
The same applies to the update step <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span> since <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span> is dependent on the assignment step <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span>.</p>
</section>
</div></section>
<section id="lemma-2-cost-function-of-k-means-monotonically-decreases">
<span id="cost-function-monotically-decreases"></span><h3><a class="toc-backref" href="#id34" role="doc-backlink">Lemma 2: Cost Function of K-Means Monotonically Decreases</a><a class="headerlink" href="#lemma-2-cost-function-of-k-means-monotonically-decreases" title="Link to this heading">#</a></h3>
<div class="proof lemma admonition" id="kmeans-monotonic-decrease">
<p class="admonition-title"><span class="caption-number">Lemma 3 </span> (Cost Function of K-Means Monotonically Decreases)</p>
<section class="lemma-content" id="proof-content">
<p>The cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span> of K-Means monotonically decreases. This means</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\widehat{\mathcal{J}}_{\mathcal{S}}^{[t+1]}\left(\hat{C}_1^{[t+1]}, \ldots, \hat{C}_K^{[t+1]}, \boldsymbol{\mu}_1^{[t+1]}, \ldots, \boldsymbol{\mu}_K^{[t+1]} \right) \leq \widehat{\mathcal{J}}_{\mathcal{S}}^{[t]}\left(\hat{C}_1^{[t]}, \ldots, \hat{C}_K^{[t]}, \boldsymbol{\mu}_1^{[t]}, \ldots, \boldsymbol{\mu}_K^{[t]} \right)
\end{aligned}
\]</div>
<p>for each iteration <span class="math notranslate nohighlight">\(t\)</span>.</p>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. This is a consequence of <a class="reference internal" href="#criterion:kmeans-optimal-assignment">Criterion 1</a> and <a class="reference internal" href="#criterion:kmeans-optimal-cluster-centers">Criterion 2</a>.</p>
<p>In particular, the objective function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span> is made up of two steps, the assignment step and the update step. We minimize the assignment step by finding the optimal assignment <span class="math notranslate nohighlight">\(\mathcal{A}^{*}(\cdot)\)</span>, and we minimize the update step by finding the optimal cluster centers <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{*}\)</span> based on the optimal assignment <span class="math notranslate nohighlight">\(\mathcal{A}^{*}(\cdot)\)</span> at each iteration.</p>
<p>Consequently, if we can show that the following:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\widehat{\mathcal{J}}_{\mathcal{S}}\left(\hat{C}_1^{[t]}, \ldots, \hat{C}_K^{[t]}, \boldsymbol{\mu}_1^{[t+1]}, \ldots, \boldsymbol{\mu}_K^{[t+1]} \right) \leq \widehat{\mathcal{J}}_{\mathcal{S}}\left(\hat{C}_1^{[t]}, \ldots, \hat{C}_K^{[t]}, \boldsymbol{\mu}_1^{[t]}, \ldots, \boldsymbol{\mu}_K^{[t]} \right)
\end{aligned}
\]</div>
<p>and then,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\widehat{\mathcal{J}}_{\mathcal{S}}\left(\hat{C}_1^{[t+1]}, \ldots, \hat{C}_K^{[t+1]}, \boldsymbol{\mu}_1^{[t+1]}, \ldots, \boldsymbol{\mu}_K^{[t+1]} \right) \leq \widehat{\mathcal{J}}_{\mathcal{S}}\left(\hat{C}_1^{[t]}, \ldots, \hat{C}_K^{[t]}, \boldsymbol{\mu}_1^{[t+1]}, \ldots, \boldsymbol{\mu}_K^{[t+1]} \right)
\end{aligned}
\]</div>
<p>then we can easily show that the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span> monotonically decreases.</p>
<hr class="docutils" />
<p><strong>First</strong>, once all the samples <span class="math notranslate nohighlight">\(\left\{\mathbf{x}^{(n)}\right\}_{n=1}^N\)</span> are assigned to the clusters as
per the <strong>assignment step</strong> in <a class="reference internal" href="#equation-eq-kmeans-classify">(30)</a>, we will recover the cost at
the <span class="math notranslate nohighlight">\(t\)</span>-th iteration, defined as:</p>
<div class="math notranslate nohighlight" id="equation-eq-kmeans-convergence-1">
<span class="eqno">(33)<a class="headerlink" href="#equation-eq-kmeans-convergence-1" title="Link to this equation">#</a></span>\[\widehat{\mathcal{J}}_{\mathcal{S}}^{[t]}\left(\hat{C}_1^{[t]}, \ldots, \hat{C}_K^{[t]}, \boldsymbol{\mu}_1^{[t]}, \ldots, \boldsymbol{\mu}_K^{[t]} \right) = \sum_{k=1}^K \sum_{n \in \hat{C}_k^{[t]}} \left\|\mathbf{x}^{(n)} - \boldsymbol{\mu}_k^{[t]}\right\|^2\]</div>
</div>
<p>Note in particular that the base case is we initialized the cluster centers
<span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{[0]}\)</span> for the first iteration <span class="math notranslate nohighlight">\(t=0\)</span> and this induces the
clusters <span class="math notranslate nohighlight">\(\hat{C}_1^{[0]}, \ldots, \hat{C}_K^{[0]}\)</span> for which we assigned each
data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> to the closest cluster center
<span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{[0]}\)</span>. If we just look at the base case, the mean is
randomly initialized, and so there may be room of improvement, which is why we
need the <strong>update step</strong>.</p>
<hr class="docutils" />
<p><strong>Next</strong>, we recalculate the cluster centers <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{[t+1]}\)</span> based
on the clusters for the <span class="math notranslate nohighlight">\(t\)</span>-th iteration. In other words, we find the cluster
centers for the <span class="math notranslate nohighlight">\(t+1\)</span>-th iteration based on the cluster assignments for the
<span class="math notranslate nohighlight">\(t\)</span>-th iteration. We claim that this new cluster centers
<span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{[t+1]}\)</span> will minimize the cost function
<span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span>.</p>
<p>We <strong>fix</strong> the assignment <span class="math notranslate nohighlight">\(\hat{C}_1^{[t]}, \ldots, \hat{C}_K^{[t]}\)</span>, and then
show that the cluster centers
<span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{[t+1]} = \dfrac{1}{\left|\hat{C}_k^{[t]}\right|} \sum_{n \in \hat{C}_k^{[t]}} \mathbf{x}^{(n)}\)</span>
minimizes the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span>, which means:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\boldsymbol{\mu}_1^{[t+1]}, \ldots, \boldsymbol{\mu}_K^{[t+1]} = \underset{\boldsymbol{\mu}_1, \ldots, \boldsymbol{\mu}_K}{\operatorname{argmin}} \widehat{\mathcal{J}}_{\mathcal{S}}\left(\hat{C}_1^{[t]}, \ldots, \hat{C}_K^{[t]}, \boldsymbol{\mu}_1, \ldots, \boldsymbol{\mu}_K \right)
\end{aligned}
\]</div>
<p>and subsequently,</p>
<div class="math notranslate nohighlight" id="equation-eq-kmeans-convergence-2">
<span class="eqno">(34)<a class="headerlink" href="#equation-eq-kmeans-convergence-2" title="Link to this equation">#</a></span>\[\begin{aligned}
\widehat{\mathcal{J}}_{\mathcal{S}}\left(\hat{C}_1^{[t]}, \ldots, \hat{C}_K^{[t]}, \boldsymbol{\mu}_1^{[t+1]}, \ldots, \boldsymbol{\mu}_K^{[t+1]} \right) \leq \widehat{\mathcal{J}}_{\mathcal{S}}\left(\hat{C}_1^{[t]}, \ldots, \hat{C}_K^{[t]}, \boldsymbol{\mu}_1^{[t]}, \ldots, \boldsymbol{\mu}_K^{[t]} \right)
\end{aligned}\]</div>
<p>because this step cannot increase the cost
<span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\left(\hat{C}_1^{[t]}, \ldots, \hat{C}_K^{[t]}, \boldsymbol{\mu}_1^{[t]}, \ldots, \boldsymbol{\mu}_K^{[t]} \right)\)</span>
as the new cluster centers minimizes the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span>
when we replace the cluster centers
<span class="math notranslate nohighlight">\(\boldsymbol{\mu}_1^{[t]}, \ldots, \boldsymbol{\mu}_K^{[t]}\)</span> by the new cluster
centers <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_1^{[t+1]}, \ldots, \boldsymbol{\mu}_K^{[t+1]}\)</span>.</p>
<hr class="docutils" />
<p><strong>Next</strong>, as we have finished one cycle in the <span class="math notranslate nohighlight">\(t\)</span>-th iteration, now we turn our
attention to the <span class="math notranslate nohighlight">\(t+1\)</span>-th iteration. As usual, we look at the first step, which
is the <strong>assignment step</strong>. We <strong>fix</strong> the cluster centers
<span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{[t+1]}\)</span> found in the previous step, and then show that the
assignment <span class="math notranslate nohighlight">\(\hat{C}_1^{[t+1]}, \ldots, \hat{C}_K^{[t+1]}\)</span> will minimize the cost
function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span>, which means:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\hat{C}_1^{[t+1]}, \ldots, \hat{C}_K^{[t+1]} = \underset{\hat{C}_1, \ldots, \hat{C}_K}{\operatorname{argmin}} \widehat{\mathcal{J}}_{\mathcal{S}}\left(\hat{C}_1, \ldots, \hat{C}_K, \boldsymbol{\mu}_1^{[t+1]}, \ldots, \boldsymbol{\mu}_K^{[t+1]} \right)
\end{aligned}
\]</div>
<p>and subsequently,</p>
<div class="math notranslate nohighlight" id="equation-eq-kmeans-convergence-3">
<span class="eqno">(35)<a class="headerlink" href="#equation-eq-kmeans-convergence-3" title="Link to this equation">#</a></span>\[\begin{aligned}
\widehat{\mathcal{J}}_{\mathcal{S}}\left(\hat{C}_1^{[t+1]}, \ldots, \hat{C}_K^{[t+1]}, \boldsymbol{\mu}_1^{[t+1]}, \ldots, \boldsymbol{\mu}_K^{[t+1]} \right) \leq \widehat{\mathcal{J}}_{\mathcal{S}}\left(\hat{C}_1^{[t]}, \ldots, \hat{C}_K^{[t]}, \boldsymbol{\mu}_1^{[t+1]}, \ldots, \boldsymbol{\mu}_K^{[t+1]} \right)
\end{aligned}\]</div>
<p>because this step cannot increase the cost
<span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\left(\hat{C}_1^{[t]}, \ldots, \hat{C}_K^{[t]}, \boldsymbol{\mu}_1^{[t+1]}, \ldots, \boldsymbol{\mu}_K^{[t+1]} \right)\)</span>
as the new assignments minimizes the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span> when
we replace the cluster assignments <span class="math notranslate nohighlight">\(\hat{C}_1^{[t]}, \ldots, \hat{C}_K^{[t]}\)</span> by
the new assignments <span class="math notranslate nohighlight">\(\hat{C}_1^{[t+1]}, \ldots, \hat{C}_K^{[t+1]}\)</span>.</p>
<hr class="docutils" />
<p><strong>Finally</strong>, we can show that the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span> is
<strong>decreasing</strong> in each iteration.</p>
<p>Combining <a class="reference internal" href="#equation-eq-kmeans-convergence-2">(34)</a> and <a class="reference internal" href="#equation-eq-kmeans-convergence-3">(35)</a>, we
have the following inequality:</p>
<div class="math notranslate nohighlight" id="equation-eq-kmeans-convergence-4">
<span class="eqno">(36)<a class="headerlink" href="#equation-eq-kmeans-convergence-4" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
\widehat{\mathcal{J}}_{\mathcal{S}}^{[t+1]} &amp;= \widehat{\mathcal{J}}_{\mathcal{S}}\left(\hat{C}_1^{[t+1]}, \ldots, \hat{C}_K^{[t+1]}, \boldsymbol{\mu}_1^{[t+1]}, \ldots, \boldsymbol{\mu}_K^{[t+1]} \right) \\
&amp;\leq \widehat{\mathcal{J}}_{\mathcal{S}}\left(\hat{C}_1^{[t]}, \ldots, \hat{C}_K^{[t]}, \boldsymbol{\mu}_1^{[t + 1]}, \ldots, \boldsymbol{\mu}_K^{[t + 1]} \right) \\
&amp;\leq \widehat{\mathcal{J}}_{\mathcal{S}}\left(\hat{C}_1^{[t]}, \ldots, \hat{C}_K^{[t]}, \boldsymbol{\mu}_1^{[t]}, \ldots, \boldsymbol{\mu}_K^{[t]} \right) = \widehat{\mathcal{J}}_{\mathcal{S}}^{[t]}
\end{aligned}\end{split}\]</div>
</section>
<section id="lemma-3-monotone-convergence-theorem">
<h3><a class="toc-backref" href="#id35" role="doc-backlink">Lemma 3: Monotone Convergence Theorem</a><a class="headerlink" href="#lemma-3-monotone-convergence-theorem" title="Link to this heading">#</a></h3>
<div class="proof lemma admonition" id="monotone-convergence">
<p class="admonition-title"><span class="caption-number">Lemma 4 </span> (Monotone Convergence Theorem)</p>
<section class="lemma-content" id="proof-content">
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Monotone_convergence_theorem">Monotone Convergence Theorem</a> states
that if a sequence of functions <span class="math notranslate nohighlight">\(\{f_n\}\)</span> is non-decreasing and bounded, then the sequence <span class="math notranslate nohighlight">\(\{f_n\}\)</span> converges to a limit.</p>
<p>In our case, the sequence of functions <span class="math notranslate nohighlight">\(\{f_n\}\)</span> is the sequence of cost functions <span class="math notranslate nohighlight">\(\left\{\widehat{\mathcal{J}}^{[t]}\right\}\)</span>, and the limit is the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}^{*}\)</span>.</p>
<p>So it is guaranteed that the sequence of cost functions <span class="math notranslate nohighlight">\(\left\{\widehat{\mathcal{J}}^{[t]}\right\}\)</span> converges to the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}^{*}\)</span> locally.</p>
</section>
</div></section>
<section id="k-means-converges-in-finite-steps">
<h3><a class="toc-backref" href="#id36" role="doc-backlink">K-Means Converges in Finite Steps</a><a class="headerlink" href="#k-means-converges-in-finite-steps" title="Link to this heading">#</a></h3>
<p>We are now left to show that the sequence of cost functions
<span class="math notranslate nohighlight">\(\left\{\widehat{\mathcal{J}}^{[t]}\right\}\)</span> is finite, so that
<span class="math notranslate nohighlight">\(\left\{\widehat{\mathcal{J}}^{[t]}\right\}\)</span> converges in finite steps.</p>
<p>Since <a class="reference internal" href="#stirling-numbers">Lemma 2</a> states that there exists <span class="math notranslate nohighlight">\(K^N\)</span> possible
assignments <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span>, and simiarly exists <span class="math notranslate nohighlight">\(K^N\)</span> possible cluster
centers <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span>, then there exists <span class="math notranslate nohighlight">\(K^N\)</span> possible cost functions
<span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span>. Then,</p>
<ul class="simple">
<li><p>At each iteration <span class="math notranslate nohighlight">\(t\)</span>, the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}^{[t]}\)</span>
decreases monotonically.</p></li>
<li><p>This means at <span class="math notranslate nohighlight">\(t+1\)</span>, if the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}^{[t + 1]}\)</span>
decreases, then this means the assignments <span class="math notranslate nohighlight">\(\mathcal{A}^{*[t + 1]}\)</span> are
different from the assignments <span class="math notranslate nohighlight">\(\mathcal{A}^{*[t]}\)</span>. Consequently, the
partition never repeats if the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span>
decreases.</p></li>
<li><p>This means it will loop over each possible assignment <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>, and
eventually converge to the unique solution <span class="math notranslate nohighlight">\(\mathcal{A}^{*}(\cdot)\)</span>.</p></li>
</ul>
<p>For that specific initialization, the algorithm has an unique solution, and it
is guaranteed to converge to that solution.</p>
</section>
<section id="local-minima">
<h3><a class="toc-backref" href="#id37" role="doc-backlink">Local Minima</a><a class="headerlink" href="#local-minima" title="Link to this heading">#</a></h3>
<p>It is known that K-Means converges in finite steps but does not guarantee
convergence to the global minimum. This means that for different
initializations, K-Means can converge to different local minima.</p>
<p>We can initialize the algorithm with different initializations, and run the
algorithm multiple times. Then, we can choose the best solution among the
different local minima.</p>
</section>
</section>
<section id="how-to-find-k">
<h2><a class="toc-backref" href="#id38" role="doc-backlink">How to find <span class="math notranslate nohighlight">\(K\)</span>?</a><a class="headerlink" href="#how-to-find-k" title="Link to this heading">#</a></h2>
<p>Since <span class="math notranslate nohighlight">\(K\)</span> is a <em>priori</em>, we need to choose <span class="math notranslate nohighlight">\(K\)</span> before we run the algorithm.
Choosing the wrong <span class="math notranslate nohighlight">\(K\)</span> will result in a poor clustering. So, how do we choose
the right <span class="math notranslate nohighlight">\(K\)</span>?</p>
<section id="choose-k-that-minimizes-the-cost-function">
<h3><a class="toc-backref" href="#id39" role="doc-backlink">Choose <span class="math notranslate nohighlight">\(K\)</span> that Minimizes the Cost Function</a><a class="headerlink" href="#choose-k-that-minimizes-the-cost-function" title="Link to this heading">#</a></h3>
<p>In normal supervised problem, we usually run the algorithm on the train dataset
and choose the model that minimizes the cost function on the train dataset, or
one that maximizes the performance.</p>
<p>Can we do the same for K-Means? The answer is no, this is because our cost
funtion monotonically decreases with increasing <span class="math notranslate nohighlight">\(K\)</span>.</p>
<p>This is because we “cover” more input space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> with increasing <span class="math notranslate nohighlight">\(K\)</span>,
thus decreasing the cost function <span id="id8">[<a class="reference internal" href="../../bibliography.html#id3" title="Kevin P. Murphy. Probabilistic Machine Learning: An introduction. MIT Press, 2022. URL: probml.ai.">Murphy, 2022</a>]</span>.</p>
</section>
<section id="elbow-method">
<h3><a class="toc-backref" href="#id40" role="doc-backlink">Elbow Method</a><a class="headerlink" href="#elbow-method" title="Link to this heading">#</a></h3>
<p>While this may not be the best method, it is a simple and widely recognized one
to choose <span class="math notranslate nohighlight">\(K\)</span>.</p>
<p>The simple heuristic is described as follows:</p>
<div class="proof algorithm admonition" id="elbow-method">
<p class="admonition-title"><span class="caption-number">Algorithm 2 </span> (Elbow Method)</p>
<section class="algorithm-content" id="proof-content">
<ol class="arabic simple">
<li><p>Run K-Means with <span class="math notranslate nohighlight">\(K\)</span> from 1 to <span class="math notranslate nohighlight">\(K_{\max}\)</span>.</p></li>
<li><p>For each <span class="math notranslate nohighlight">\(k=0,1,\ldots, K_{\max}\)</span>, compute the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_k\)</span></p></li>
<li><p>Plot the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_k\)</span> against <span class="math notranslate nohighlight">\(k\)</span>.</p></li>
<li><p>Find the “elbow” of the curve, which is the point where the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}_k\)</span> starts to decrease more slowly.</p></li>
</ol>
</section>
</div></section>
<section id="other-methods">
<h3><a class="toc-backref" href="#id41" role="doc-backlink">Other Methods</a><a class="headerlink" href="#other-methods" title="Link to this heading">#</a></h3>
<p>See <span id="id9">[<a class="reference internal" href="../../bibliography.html#id3" title="Kevin P. Murphy. Probabilistic Machine Learning: An introduction. MIT Press, 2022. URL: probml.ai.">Murphy, 2022</a>]</span> for more methods.</p>
</section>
</section>
<section id="time-and-space-complexity">
<h2><a class="toc-backref" href="#id42" role="doc-backlink">Time and Space Complexity</a><a class="headerlink" href="#time-and-space-complexity" title="Link to this heading">#</a></h2>
<section id="brute-force-search-and-global-minimum">
<h3><a class="toc-backref" href="#id43" role="doc-backlink">Brute Force Search and Global Minimum</a><a class="headerlink" href="#brute-force-search-and-global-minimum" title="Link to this heading">#</a></h3>
<p>The hypothesis space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is finite, implying that if we do a brute
force search over all possible clusterings, we can find the global minimum.</p>
<p>Quoting from
<a class="reference external" href="http://www.cs.cmu.edu/~ninamf/courses/315sp19/homeworks/hw6.pdf">CMU 10-315</a>,
we consider the brute-force search to be the following:</p>
<div class="proof algorithm admonition" id="brute-force-search-kmeans">
<p class="admonition-title"><span class="caption-number">Algorithm 3 </span> (Brute Force Search for K-Means)</p>
<section class="algorithm-content" id="proof-content">
<p>For each possible cluster</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathcal{C}} = \left\{\hat{C}_1, \hat{C}_2, \dots, \hat{C}_K\right\}
\]</div>
<p>induced by the assignment <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> in <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, compute the
optimal centroids</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\mu}} = \left\{\hat{\boldsymbol{\mu}}_1, \hat{\boldsymbol{\mu}}_2, \dots, \hat{\boldsymbol{\mu}}_K\right\}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\mu}}_k = \frac{1}{|\hat{C}_k|} \sum_{\mathbf{x} \in \hat{C}_k} \mathbf{x}
\]</div>
<p>is the mean of the points in the <span class="math notranslate nohighlight">\(k\)</span>-th cluster.</p>
<p>Then, compute the cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span> centroids <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\mu}}\)</span>.</p>
<p>Repeat this for all possible clusterings <span class="math notranslate nohighlight">\(\mathcal{A}(\cdot)\)</span> in <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> and finally
return the clustering <span class="math notranslate nohighlight">\(\hat{C}\)</span> that gives the minimum cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span>.</p>
</section>
</div><p>Then the time complexity of the brute force search is exponential with respect
to the number of inputs since there are <span class="math notranslate nohighlight">\(K^N\)</span> possible clusterings and we are
looping over each possible clustering to find the global minimum. Indeed, this
has time complexity</p>
<div class="math notranslate nohighlight">
\[
\mathcal{O}\left(K^N\right)
\]</div>
</section>
<section id="lloyds-algorithm">
<h3><a class="toc-backref" href="#id44" role="doc-backlink">Lloyd’s Algorithm</a><a class="headerlink" href="#lloyds-algorithm" title="Link to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(T\)</span> denote the number of iterations of Lloyd’s algorithm.</p>
<p>Then, the average time complexity of Lloyd’s algorithm is</p>
<div class="math notranslate nohighlight">
\[
\mathcal{O}(T N K D)
\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the number of data points, <span class="math notranslate nohighlight">\(K\)</span> is the number of clusters, and <span class="math notranslate nohighlight">\(D\)</span>
is the number of features.</p>
<p>This can be easily seen in the python implementation written
<a class="reference external" href="https://github.com/gao-hongnan/gaohn-probability-stats/blob/machine-learning/src/clustering/kmeans/kmeans.py">here</a>.
We are essentially looping like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
    <span class="c1"># E Step: Assign each data point to the closest cluster center</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
      <span class="c1"># compute argmin distance O(KD) since we are looping over all</span>
      <span class="c1"># K cluster centers and each cluster center has D features</span>

      <span class="c1"># do assignment which requires you to loop over all</span>
      <span class="c1"># K cluster centers: O(N)</span>

    <span class="c1"># so total O(NKD) here already</span>

  <span class="c1"># M step: Update the cluster centers</span>
  <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">):</span>
    <span class="c1"># compute the mean of the points in the k-th cluster: O(KD)</span>
    <span class="c1"># since we are looping over all K cluster centers and each</span>
    <span class="c1"># cluster center has D features</span>
</pre></div>
</div>
<p>where the total time complexity approximately <span class="math notranslate nohighlight">\(\mathcal{O}(T N K D)\)</span>.</p>
<p>The worst case complexity is given by
<span class="math notranslate nohighlight">\(\mathcal{O}\left(N^{(K+2/D)}\right)\)</span><a class="footnote-reference brackets" href="#worst-case-time-complexity" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a>.</p>
<div class="pst-scrollable-table-container"><table class="table" id="time-complexity-kmeans">
<caption><span class="caption-number">Table 10 </span><span class="caption-text">Time Complexity of K-Means</span><a class="headerlink" href="#time-complexity-kmeans" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Train</p></th>
<th class="head"><p>Inference</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathcal{O}(NKD)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(KD)\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<p>For space complexity, we need to store the cluster centers <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span>
and the cluster assignments <span class="math notranslate nohighlight">\(\mathcal{A}(n)\)</span>, where the former is a <span class="math notranslate nohighlight">\(K \times D\)</span>
matrix and the latter is a <span class="math notranslate nohighlight">\(N\)</span>-dimensional vector. We typically do not include
the input data <span class="math notranslate nohighlight">\(\left\{\mathbf{x}^{(n)}\right\}_{n=1}^N\)</span> in the space complexity
since it is given. If included that is <span class="math notranslate nohighlight">\(\mathcal{O}(ND)\)</span>, totalling
<span class="math notranslate nohighlight">\(\mathcal{O}(N + KD + ND)\)</span>.</p>
<p>Inference wise, even for a single data point, we need to compute the distance to
all cluster centers, so you need to invoke the cluster centers
<span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span>, so roughly is <span class="math notranslate nohighlight">\(\mathcal{O}(KD)\)</span>.</p>
<div class="pst-scrollable-table-container"><table class="table" id="space-complexity-kmeans">
<caption><span class="caption-number">Table 11 </span><span class="caption-text">Space Complexity of K-Means</span><a class="headerlink" href="#space-complexity-kmeans" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Train</p></th>
<th class="head"><p>Inference</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathcal{O}(KD + N)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(KD)\)</span></p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="when-to-use-k-means">
<h2><a class="toc-backref" href="#id45" role="doc-backlink">When to Use K-Means?</a><a class="headerlink" href="#when-to-use-k-means" title="Link to this heading">#</a></h2>
<p>See
<a class="reference external" href="https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages">google’s article</a>.</p>
<ul class="simple">
<li><p>Relatively simple to implement.</p></li>
<li><p>Scales to large data sets.</p></li>
<li><p>Guarantees local convergence.</p></li>
<li><p>Can warm-start the positions of centroids.</p></li>
<li><p>Easily adapts to new examples.</p></li>
<li><p>Generalizes to clusters of different shapes and sizes, such as elliptical
clusters.</p></li>
</ul>
</section>
<section id="when-can-k-means-fail">
<h2><a class="toc-backref" href="#id46" role="doc-backlink">When can K-Means Fail?</a><a class="headerlink" href="#when-can-k-means-fail" title="Link to this heading">#</a></h2>
<ul>
<li><p>The number of clusters (<span class="math notranslate nohighlight">\(K\)</span>) is specified a <strong>priori</strong>, which means we need
to specify the number of clusters before running the algorithm. Choosing <span class="math notranslate nohighlight">\(K\)</span>
may not be straightforward, especially in the case of high-dimensional data.</p></li>
<li><p>The Lloyd’s algorithm is sensitive to the initial cluster centers. This
means that the algorithm may converge to a local minimum instead of the
global minimum. To remedy this, we can run the algorithm multiple times with
different initial cluster centers.</p></li>
<li><p>K-Means assumes spherical clusters. This is not obvious.</p>
<ul>
<li><p>K-Means assumes spherical shape because the algorithm uses the euclidean
distance metric to measure the similarity between observations and
centroids. euclidean distance is a measure of straight-line distance
between two points in a euclidean space, and it assumes that the data is
isotropic, meaning that the <strong>variance</strong> along all dimensions is equal,
or (the same in all directions) from the centroid of the cluster. Now
imagine a cluster with an elliptical shape. And imagine the principal
axis is quite long, then two points at the extreme ends of the cluster
will have a large euclidean distance. This means that the cluster may be
split into two clusters by K-Means, which is not what we want. On the
contrary, if the cluster is spherical, then the euclidean distance
between two points at the extreme ends of the cluster will be
equidistant to the centroid.</p></li>
<li><p>Further quoting
<a class="reference external" href="https://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means">the answer here</a>,
K-means is a special case of Gaussian Mixture Models (GMM). GMM assumes
that the data comes from a mixture of <span class="math notranslate nohighlight">\(K\)</span> Gaussian distributions. In
other words, there is a certain probability that the data comes from one
of <span class="math notranslate nohighlight">\(K\)</span> of the Gaussian distributions.</p>
<p>If we make the the probability to be in each of the <span class="math notranslate nohighlight">\(K\)</span> Gaussians equal
and make the covariance matrices to be <span class="math notranslate nohighlight">\(\sigma^2 \mathbf{I}\)</span>, where
<span class="math notranslate nohighlight">\(\sigma^2 \)</span> is the same fixed constant for each of the <span class="math notranslate nohighlight">\(K\)</span> Gaussians,
and take the limit when <span class="math notranslate nohighlight">\(\sigma^2 \rightarrow 0\)</span> then we get K-means.</p>
<p>So, what does this tell us about the drawbacks of K-means?</p>
<ol class="arabic simple">
<li><p>K-means leads to clusters that look multivariate Gaussian.</p></li>
<li><p>Since the variance across the variables is the same, K-means leads to
clusters that look spherical.</p></li>
<li><p>Not only do clusters look spherical, but since the covariance matrix
is the same across the <span class="math notranslate nohighlight">\(K\)</span> groups, K-means leads to clusters that
look like the same sphere.</p></li>
<li><p>K-means tends towards equal sized groups.</p></li>
</ol>
<p>Overall, if we interpret K-Means from the perspective of probabilistic
modeling, then we can see that K-Means is a special case of GMM. And
recall that
<a class="reference external" href="https://www.gaohongnan.com/probability_theory/05_joint_distributions/0507_multivariate_gaussian/geometry_of_multivariate_gaussian.html">in the geometry of multivariate gaussian</a>,
the shape of the multivariate gaussian is determined by the covariance
matrix. Since we have deduced that the covariance matrix is
<span class="math notranslate nohighlight">\(\sigma^2 \mathbf{I}\)</span>, a diagonal matrix with equal variance across all
features, then the shape is a sphere since the axis has equal length.</p>
</li>
</ul>
</li>
<li><p>Scaling with number of dimensions. As the number of dimensions increases, a
distance-based similarity measure converges to a constant value between any
given examples. Reduce dimensionality either by using PCA on the feature
data, or by using “spectral clustering” to modify the clustering algorithm.</p></li>
<li><p>Best to feature scale if we use euclidean distance as the distance metric.
This is because features with larger scale will dominate the distance
metric.</p></li>
<li><p>For categorical features, we no longer use mean as the cluster center.
Instead, we use the mode.</p></li>
</ul>
</section>
<section id="k-means">
<h2><a class="toc-backref" href="#id47" role="doc-backlink">K-Means++</a><a class="headerlink" href="#k-means" title="Link to this heading">#</a></h2>
<p>We have seen earlier that convergence can be an issue with K-Means, and it is
recommended to use different seed initializations to get better results.</p>
<p>We state a better initialization method, K-Means++. The intuition behind this
approach is that spreading out the <span class="math notranslate nohighlight">\(K\)</span> initial cluster centers is a good thing:
the first cluster center is chosen uniformly at random from the data points that
are being clustered, after which each subsequent cluster center is chosen from
the remaining data points with probability proportional to its squared distance
from the point’s closest existing cluster center.</p>
<p>As we have seen, K-Means is optimizing a non-convex objective function, which
means that it can get stuck in local optima. This means different
initializations of the cluster centers can lead to different final cluster
assignments. So initialization is usually key to convergence. We usually
initialize randomly by choosing <span class="math notranslate nohighlight">\(K\)</span> data points from the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>
at random and set these as the initial cluster centers. However, this can lead
to poor results, especially if the data is not well-separated. Furthermore, we
can do <strong>multiple restarts</strong> by running K-Means multiple times with different
random initializations and choosing the best solution. This can be slow, and
hence we can use a smarter initialization method.</p>
<p>K-Means++ is an improved initialization method for the K-Means algorithm, which
aims to choose the initial cluster centers in a smarter way. The goal is to
spread out the initial cluster centers more uniformly across the data space. The
K-Means++ initialization method is as follows:</p>
<ol class="arabic">
<li><p><strong>Initialization Step</strong>:</p>
<ul>
<li><p>Choose the first cluster center <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_1^{[0]}\)</span> uniformly at
random from the data points
<span class="math notranslate nohighlight">\(\mathcal{S} = \left\{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(N)}\right\}\)</span>.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(k = 2, 3, \dots, K\)</span>:</p>
<ul class="simple">
<li><p>Compute the squared distances <span class="math notranslate nohighlight">\(D(\mathbf{x}^{(n)})\)</span> for each data
point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)} \in \mathcal{S}\)</span> to the nearest cluster center
that has already been chosen:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
        \begin{aligned}
        D(\mathbf{x}^{(n)}) = \min_{1 \leq j \leq k-1} \left\| \mathbf{x}^{(n)} - \boldsymbol{\mu}_j^{[0]} \right\|^2
        \end{aligned}
        \]</div>
<ul class="simple">
<li><p>Choose the next cluster center <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k^{[0]}\)</span> with
probability proportional to the squared distance
<span class="math notranslate nohighlight">\(D(\mathbf{x}^{(n)})\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
        \begin{aligned}
        \mathbb{P}[\boldsymbol{\mu}_k^{[0]} = \mathbf{x}^{(n)}] = \frac{D(\mathbf{x}^{(n)})}{\sum_{i=1}^N D(\mathbf{x}^{(i)})}
        \end{aligned}
        \]</div>
</li>
</ul>
</li>
</ol>
<p>Once the initial cluster centers are chosen using the K-Means++ initialization,
the rest of the K-Means algorithm proceeds as usual, following the steps in
<a class="reference internal" href="#lloyd-kmeans-algorithm">Algorithm 1</a>.</p>
<p>The primary advantage of the K-Means++ initialization is that it tends to
provide better starting points for the K-Means algorithm, often leading to a
faster convergence and better final clustering results.</p>
<p>What is more surprising is that this method can be shown to guarantee that the
recontruction error is never more than <span class="math notranslate nohighlight">\(\mathcal{O}(\log K)\)</span> worse than optimal
<span id="id11">[<a class="reference internal" href="../../bibliography.html#id3" title="Kevin P. Murphy. Probabilistic Machine Learning: An introduction. MIT Press, 2022. URL: probml.ai.">Murphy, 2022</a>]</span>.</p>
</section>
<section id="k-medoids">
<h2><a class="toc-backref" href="#id48" role="doc-backlink">K-Medoids</a><a class="headerlink" href="#k-medoids" title="Link to this heading">#</a></h2>
<p>See section 21.3.5 of Probabilistic Machine Learning: An Introduction by Kevin
P. Murphy.</p>
</section>
<section id="references-and-further-readings">
<h2><a class="toc-backref" href="#id49" role="doc-backlink">References and Further Readings</a><a class="headerlink" href="#references-and-further-readings" title="Link to this heading">#</a></h2>
<p>I also highly recommend Nathaniel Dake’s
<a class="reference external" href="https://www.nathanieldake.com/Machine_Learning/04-Unsupervised_Learning_Cluster_Analysis-02-Cluster-Analysis-K-Means-Clustering.html">blog on K-Means</a>
here, he does a fantatic job in explaining the intuition behind K-Means and
provide visualizations to help you understand the algorithm, especially how
K-Means can fail.</p>
<ul class="simple">
<li><p>Murphy, Kevin P. “Chapter 21.3. K-Means Clustering.” In Probabilistic
Machine Learning: An Introduction. MIT Press, 2022.</p></li>
<li><p>Hal Daumé III. “Chapter 3.4. K-Means Clustering.” In A Course in Machine
Learning, January 2017.</p></li>
<li><p>Hal Daumé III. “Chapter 15.1. K-Means Clustering.” In A Course in Machine
Learning, January 2017.</p></li>
<li><p>Bishop, Christopher M. “Chapter 9.1. K-Means Clustering.” In Pattern
Recognition and Machine Learning. New York: Springer-Verlag, 2016.</p></li>
<li><p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.
“Chapter 12.4.1. K-Means Clustering.” In An Introduction to Statistical
Learning: With Applications in R. Boston: Springer, 2022.</p></li>
<li><p>Hastie, Trevor, Tibshirani, Robert and Friedman, Jerome. “Chapter 14.3.
Cluster Analysis.” In The Elements of Statistical Learning. New York, NY,
USA: Springer New York Inc., 2001.</p></li>
<li><p>Raschka, Sebastian. “Chapter 10.1. Grouping objects by similarity using
k-means.” In Machine Learning with PyTorch and Scikit-Learn.</p></li>
<li><p>Jung, Alexander. “Chapter 8.1. Hard Clustering with K-Means.” In Machine
Learning: The Basics. Singapore: Springer Nature Singapore, 2023.</p></li>
<li><p>Vincent, Tan. “Lecture 17a.” In Data Modelling and Computation (MA4270).</p></li>
</ul>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="y" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>There’s actually no ground truth target labels in unsupervised learning,
this is for education purposes.</p>
</aside>
<aside class="footnote brackets" id="disjoint-union" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>Disjoint union indicates that each data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> can be
assigned to one and only one cluster <span class="math notranslate nohighlight">\(C_k\)</span>.</p>
</aside>
<aside class="footnote brackets" id="collection-of-clusters" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>Note <span class="math notranslate nohighlight">\(C\)</span> is not the same as <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> even though they both represent
all samples. The cardinality of <span class="math notranslate nohighlight">\(C\)</span> is <span class="math notranslate nohighlight">\(K\)</span>, while the cardinality of
<span class="math notranslate nohighlight">\(\mathcal{S}\)</span> is <span class="math notranslate nohighlight">\(N\)</span>.</p>
</aside>
<aside class="footnote brackets" id="jointly-optimizing" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">4</a><span class="fn-bracket">]</span></span>
<p>This means that we are jointly optimizing the assignments <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> and
the cluster centers <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span>.</p>
</aside>
<aside class="footnote brackets" id="equivalent-k-means-cost-function" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">5</a><span class="fn-bracket">]</span></span>
<p>The reason of writing so many equivalent forms is because many textbooks use
different notations, so I tried to list a few common ones.</p>
</aside>
<aside class="footnote brackets" id="worst-case-time-complexity" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">6</a><span class="fn-bracket">]</span></span>
<p>Refer to “How slow is the k-means method?” D. Arthur and S. Vassilvitskii -
SoCG2006. for more details.</p>
</aside>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./influential/kmeans_clustering"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="01_intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">K-Means</p>
      </div>
    </a>
    <a class="right-next"
       href="03_implementation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Implementation: K-Means (Lloyd)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">Intuition</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-hypothesis-space">The Hypothesis Space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-loss-cost-objective-function">The Loss/Cost/Objective Function</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-notion-of-similarity-and-closeness">The Notion of Similarity and Closeness</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-formulation">Problem Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#partition-and-voronoi-regions">Partition and Voronoi Regions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assignment">Assignment</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assignments-are-equivalent-to-clusters">Assignments are Equivalent to Clusters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#centroids-representatives">Centroids (Representatives)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-space">Hypothesis Space</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">Loss Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-function">Cost Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#objective-function">Objective Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-necessary-conditions-to-minimize-the-objective-function">The Necessary Conditions to Minimize the Objective Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#condition-1-the-optimal-assignment">Condition 1: The Optimal Assignment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#condition-2-the-optimal-cluster-centers-centroids">Condition 2: The Optimal Cluster Centers (Centroids)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#objective-function-re-defined">Objective Function Re-defined</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithm">Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-fitting">Model Fitting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-inference">Model Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#convergence">Convergence</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lemma-1-stirling-numbers-of-the-second-kind">Lemma 1: Stirling Numbers of the Second Kind</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lemma-2-cost-function-of-k-means-monotonically-decreases">Lemma 2: Cost Function of K-Means Monotonically Decreases</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lemma-3-monotone-convergence-theorem">Lemma 3: Monotone Convergence Theorem</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-converges-in-finite-steps">K-Means Converges in Finite Steps</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#local-minima">Local Minima</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-find-k">How to find <span class="math notranslate nohighlight">\(K\)</span>?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choose-k-that-minimizes-the-cost-function">Choose <span class="math notranslate nohighlight">\(K\)</span> that Minimizes the Cost Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elbow-method">Elbow Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-methods">Other Methods</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#time-and-space-complexity">Time and Space Complexity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#brute-force-search-and-global-minimum">Brute Force Search and Global Minimum</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lloyds-algorithm">Lloyd’s Algorithm</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-k-means">When to Use K-Means?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-can-k-means-fail">When can K-Means Fail?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means">K-Means++</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-medoids">K-Medoids</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>