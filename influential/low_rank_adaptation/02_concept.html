
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Concept &#8212; Omniverse</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=bb35926c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MYW5YKC2WF"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"defeq": "\\overset{\\text{def}}{=}", "defa": "\\overset{\\text{(a)}}{=}", "defb": "\\overset{\\text{(b)}}{=}", "defc": "\\overset{\\text{(c)}}{=}", "defd": "\\overset{\\text{(d)}}{=}", "st": "\\mid", "mod": "\\mid", "S": "\\Omega", "s": "\\omega", "e": "\\exp", "P": "\\mathbb{P}", "R": "\\mathbb{R}", "expectation": "\\mathbb{E}", "v": "\\mathbf{v}", "a": "\\mathbf{a}", "b": "\\mathbf{b}", "c": "\\mathbf{c}", "u": "\\mathbf{u}", "w": "\\mathbf{w}", "x": "\\mathbf{x}", "y": "\\mathbf{y}", "z": "\\mathbf{z}", "0": "\\mathbf{0}", "1": "\\mathbf{1}", "A": "\\mathbf{A}", "B": "\\mathbf{B}", "C": "\\mathbf{C}", "E": "\\mathcal{F}", "eventA": "\\mathcal{A}", "lset": "\\left\\{", "rset": "\\right\\}", "lsq": "\\left[", "rsq": "\\right]", "lpar": "\\left(", "rpar": "\\right)", "lcurl": "\\left\\{", "rcurl": "\\right\\}", "pmf": "p_X", "pdf": "f_X", "pdftwo": "f_{X,Y}", "pdfjoint": "f_{\\mathbf{X}}", "pmfjointxy": "p_{X, Y}", "pdfjointxy": "f_{X, Y}", "cdf": "F_X", "pspace": "(\\Omega, \\mathcal{F}, \\mathbb{P})", "var": "\\operatorname{Var}", "std": "\\operatorname{Std}", "bern": "\\operatorname{Bernoulli}", "binomial": "\\operatorname{Binomial}", "geometric": "\\operatorname{Geometric}", "poisson": "\\operatorname{Poisson}", "uniform": "\\operatorname{Uniform}", "normal": "\\operatorname{Normal}", "gaussian": "\\operatorname{Gaussian}", "gaussiansymbol": "\\mathcal{N}", "exponential": "\\operatorname{Exponential}", "iid": "\\textbf{i.i.d.}", "and": "\\text{and}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'influential/low_rank_adaptation/02_concept';</script>
    <link rel="canonical" href="https://www.gaohongnan.com/influential/low_rank_adaptation/02_concept.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Implementation" href="03_implementation.html" />
    <link rel="prev" title="Low-Rank Adaptation Of Large Language Models" href="01_intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Omniverse - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Omniverse - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Omniverse
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notations/machine_learning.html">Machine Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Influential Ideas and Papers</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../generative_pretrained_transformer/01_intro.html">Generative Pre-trained Transformers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../generative_pretrained_transformer/02_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../generative_pretrained_transformer/03_concept.html">The Concept of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../generative_pretrained_transformer/04_implementation.html">The Implementation of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../generative_pretrained_transformer/05_adder.html">Training a Mini-GPT to Learn Two-Digit Addition</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="01_intro.html">Low-Rank Adaptation Of Large Language Models</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_implementation.html">Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../empirical_risk_minimization/01_intro.html">Empirical Risk Minimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../empirical_risk_minimization/02_concept.html">Concept: Empirical Risk Minimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../empirical_risk_minimization/03_bayes_optimal_classifier.html">Bayes Optimal Classifier</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../learning_theory/01_intro.html">Is The Learning Problem Solvable?</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../learning_theory/02_concept.html">Concept: Learning Theory</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../kmeans_clustering/01_intro.html">Lloyd’s K-Means Clustering Algorithm</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/02_concept.html">Concept: K-Means Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/03_implementation.html">Implementation: K-Means (Lloyd)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/04_image_segmentation.html">Application: Image Compression and Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/05_conceptual_questions.html">Conceptual Questions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../naive_bayes/01_intro.html">Naive Bayes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/03_implementation.html">Naives Bayes Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/04_example_penguins.html">Naive Bayes Application: Penguins</a></li>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/05_application_mnist.html">Naive Bayes Application (MNIST)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../gaussian_mixture_models/01_intro.html">Mixture Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../gaussian_mixture_models/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gaussian_mixture_models/03_implementation.html">Gaussian Mixture Models Implementation</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Playbook</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../playbook/training/intro.html">Training Dynamics And Tricks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../playbook/training/how_to_calculate_flops_in_transformer_based_models.html">How to Calculate the Number of FLOPs in Transformer Based Models?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../playbook/training/why_cosine_annealing_warmup_stabilize_training.html">Why Does Cosine Annealing With Warmup Stabilize Training?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../playbook/training/how_to_finetune_decoder_with_last_token_pooling.html">How To Fine-Tune Decoder-Only Models For Sequence Classification Using Last Token Pooling?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../playbook/training/how_to_finetune_decoder_with_cross_attention.html">How To Fine-Tune Decoder-Only Models For Sequence Classification With Cross-Attention?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../playbook/training/how_to_teacher_student_knowledge_distillation.html">How To Do Teacher-Student Knowledge Distillation?</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html">Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling.</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_inspect_function_and_class_signatures.html">How to Inspect Function and Class Signatures in Python?</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Probability Theory</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/intro.html">Chapter 1. Mathematical Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/01_combinatorics.html">Permutations and Combinations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/02_calculus.html">Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/03_contours.html">Contour Maps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/02_probability/intro.html">Chapter 2. Probability</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0202_probability_space.html">Probability Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0203_probability_axioms.html">Probability Axioms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0204_conditional_probability.html">Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0205_independence.html">Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0206_bayes_theorem.html">Baye’s Theorem and the Law of Total Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/summary.html">Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/intro.html">Chapter 3. Discrete Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0301_random_variables.html">Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0302_discrete_random_variables.html">Discrete Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0303_probability_mass_function.html">Probability Mass Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0304_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0305_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0306_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/intro.html">Discrete Uniform Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/intro.html">Bernoulli Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/iid.html">Independent and Identically Distributed (IID)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/intro.html">Binomial Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_implementation.html">Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_application.html">Real World Examples</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/geometric/intro.html">Geometric Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/geometric/0310_geometric_distribution_concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/intro.html">Poisson Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_implementation.html">Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/summary.html">Important</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/intro.html">Chapter 4. Continuous Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/from_discrete_to_continuous.html">From Discrete to Continuous</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0401_continuous_random_variables.html">Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0402_probability_density_function.html">Probability Density Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0403_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0404_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0405_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0406_mean_median_mode.html">Mean, Median and Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0407_continuous_uniform_distribution.html">Continuous Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0408_exponential_distribution.html">Exponential Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0409_gaussian_distribution.html">Gaussian Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0410_skewness_and_kurtosis.html">Skewness and Kurtosis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0411_convolve_and_sum_of_random_variables.html">Convolution and Sum of Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0412_functions_of_random_variables.html">Functions of Random Variables</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/intro.html">Chapter 5. Joint Distributions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/05_joint_distributions/from_single_variable_to_joint_distributions.html">From Single Variable to Joint Distributions</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/intro.html">Joint PMF and PDF</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/intro.html">Joint Expectation and Correlation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/intro.html">Conditional PMF and PDF</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/intro.html">Conditional Expectation and Variance</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0505_sum_of_random_variables/intro.html">Sum of Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0505_sum_of_random_variables/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0506_random_vectors/intro.html">Random Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0506_random_vectors/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/intro.html">Multivariate Gaussian Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/application_transformation.html">Application: Plots and Transformations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/psd.html">Covariance Matrix is Positive Semi-Definite</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/eigendecomposition.html">Eigendecomposition and Covariance Matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/geometry_of_multivariate_gaussian.html">The Geometry of Multivariate Gaussians</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/intro.html">Chapter 6. Sample Statistics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/intro.html">Moment Generating and Characteristic Functions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function.html">Moment Generating Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function_application_sum_of_rv.html">Application: Moment Generating Function and the Sum of Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/characteristic_function.html">Characteristic Function</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/intro.html">Probability Inequalities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/concept.html">Probability Inequalities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/application.html">Application: Learning Theory</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/intro.html">Law of Large Numbers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/convergence.html">Convergence of Sample Average</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/application.html">Application: Learning Theory</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/08_estimation_theory/intro.html">Chapter 8. Estimation Theory</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/08_estimation_theory/maximum_likelihood_estimation/intro.html">Maximum Likelihood Estimation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html">Concept</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Operations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../operations/distributed/intro.html">Distributed Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/01_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/02_basics.html">Basics Of Distributed Data Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/03_how_to_setup_slurm_in_aws.html">How to Setup SLURM and ParallelCluster in AWS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/04_ablation.html">Ablations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../operations/profiling/intro.html">Profiling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/01_synchronize.html">Synchronize CUDA To Time CUDA Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/02_timeit.html">Profiling Code With Timeit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/03_time_profiler.html">PyTorch’s Event And Profiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/04_small_gpt_profile.html">Profile GPT Small Time And Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/05_memory_leak.html">CUDA Memory Allocations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../operations/machine_learning_lifecycle/00_intro.html">The Lifecycle of an AIOps System</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/01_problem_formulation.html">Stage 1. Problem Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/02_project_scoping.html">Stage 2. Project Scoping And Framing The Problem</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/03_dataops_pipeline.html">Stage 3. Data Pipeline (Data Engineering and DataOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/031_data_source_and_format.html">Stage 3.1. Data Source and Formats</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/032_data_model_and_storage.html">Stage 3.2. Data Model and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/033_etl.html">Stage 3.3. Extract, Transform, Load (ETL)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/04_mlops_data_pipeline.html">Stage 4. Data Extraction (MLOps), Data Analysis (Data Science), Data Preparation (Data Science)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/05_ml_training_pipeline.html">Stage 5. Model Development and Training (MLOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/051_model_selection.html">Stage 5.1. Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/052_metric_selection.html">Stage 5.2. Metric Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/053_experiment_tracking.html">Stage 5.3. Experiment Tracking And Versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/054_model_testing.html">Stage 5.4. Model Testing</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/06_model_evaluation.html">Stage 6. Model Evaluation (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/07_model_validation_registry_and_pushing_model_to_production.html">Stage 7. Model Validation, Registry and Pushing Model to Production (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/08_model_deployment_and_serving.html">Stage 8. Model Serving (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/09_model_monitoring.html">Stage 9. Model Monitoring (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/010_continuous_integration_deployment_learning_and_training.html">Stage 10. Continuous Integration, Deployment, Learning and Training (DevOps, DataOps, MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/011_infrastructure_and_tooling_for_mlops.html">Stage 11. Infrastructure and Tooling for MLOps</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/config_management/intro.html">Config, State, Metadata Management</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/config_management/concept.html">Configuration Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/config_management/01-pydra.html">Pydantic And Hydra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/config_management/02-state.html">State And Metadata Management</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/design_patterns/intro.html">Design Patterns</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/dependency_inversion_principle.html">Dependency Inversion Principle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/named_constructor.html">Named Constructor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/strategy.html">Strategy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/registry.html">Registry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/god_object_pattern.html">Context Object Pattern (God Object)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/factory_method.html">Factory Method</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/python/intro.html">Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/iterator_protocol.html">The Iterator Protocol</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/decorator.html">Decorator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/generators_over_lists.html">Generators Over Lists For Memory Efficiency</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/pydantic.html">Pydantic Is All You Need - Jason Liu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/mutable_default.html">Do Not Use Mutable Default Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/set_vs_list.html">Set Over List For Frequent Membership Tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/late_binding_closures.html">Late Binding Closures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/is_vs_equality.html">Is vs Equality</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/intro.html">Concurrency, Parallelism and Asynchronous Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/overview.html">Overview Of Concurrency, Parallelism, and Asynchronous Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/generator_yield.html">A Rudimentary Introduction to Generator and Yield in Python</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../computer_science/type_theory/intro.html">Type Theory, A Very Rudimentary Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/01-subtypes.html">Subtypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/02-type-safety.html">Type Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/03-subsumption.html">Subsumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/04-generics.html">Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/05-typevar-bound-constraints.html">Bound and Constraint in Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/06-invariance-covariance-contravariance.html">Invariance, Covariance and Contravariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/07-pep-3124-overloading.html">Function Overloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/08-pep-661-sentinel-values.html">Sentinel Types</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Structures and Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/complexity_analysis/intro.html">Complexity Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/complexity_analysis/master_theorem.html">Master Theorem </a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/stack/intro.html">Stack</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/stack/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/intro.html">Linear Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/intro.html">Binary Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/problems/875-koko-eating-bananas.html">Koko Eating Bananas</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/01_preliminaries/intro.html">Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/01-fields.html">Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/02-systems-of-linear-equations.html">Systems of Linear Equations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/02_vectors/intro.html">Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/01-vector-definition.html">Vector and Its Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/02-vector-operation.html">Vector and Its Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/03-vector-norm.html">Vector Norm and Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/04-vector-products.html">A First Look at Vector Products</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../citations.html">IEEE (Style) Citations</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/gao-hongnan/omniverse/blob/main/omniverse/influential/low_rank_adaptation/02_concept.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse/issues/new?title=Issue%20on%20page%20%2Finfluential/low_rank_adaptation/02_concept.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/influential/low_rank_adaptation/02_concept.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../../_sources/influential/low_rank_adaptation/02_concept.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Concept</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducibility">Reproducibility</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rank-and-low-rank-decomposition-via-matrix-factorization">Rank And Low-Rank Decomposition Via Matrix Factorization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-autoregressive-self-supervised-learning-paradigm">The Autoregressive Self-Supervised Learning Paradigm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#task-specific-fine-tuning">Task Specific Fine-Tuning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-update-weights-of-fine-tuning-has-a-low-intrinsic-rank">The Update Weights Of Fine-Tuning Has A Low Intrinsic Rank</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters-reduction-in-lora">Parameters Reduction In LoRA</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-low-rank-adaptation-lora-algorithm">The Low-Rank Adaptation (LoRA) Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#merge-no-additional-inference-latency">Merge - No Additional Inference Latency</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References And Further Readings</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="concept">
<h1>Concept<a class="headerlink" href="#concept" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://twitter.com/gaohongnan"><img alt="Twitter Handle" src="https://img.shields.io/badge/Twitter-&#64;gaohongnan-blue?style=social&amp;logo=twitter" /></a>
<a class="reference external" href="https://linkedin.com/in/gao-hongnan"><img alt="LinkedIn Profile" src="https://img.shields.io/badge/&#64;gaohongnan-blue?style=social&amp;logo=linkedin" /></a>
<a class="reference external" href="https://github.com/gao-hongnan"><img alt="GitHub Profile" src="https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&amp;logo=github" /></a>
<img alt="Tag" src="https://img.shields.io/badge/Tag-Organized_Chaos-orange" /></p>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#reproducibility" id="id3">Reproducibility</a></p></li>
<li><p><a class="reference internal" href="#motivation" id="id4">Motivation</a></p></li>
<li><p><a class="reference internal" href="#rank-and-low-rank-decomposition-via-matrix-factorization" id="id5">Rank And Low-Rank Decomposition Via Matrix Factorization</a></p></li>
<li><p><a class="reference internal" href="#the-autoregressive-self-supervised-learning-paradigm" id="id6">The Autoregressive Self-Supervised Learning Paradigm</a></p></li>
<li><p><a class="reference internal" href="#task-specific-fine-tuning" id="id7">Task Specific Fine-Tuning</a></p></li>
<li><p><a class="reference internal" href="#the-update-weights-of-fine-tuning-has-a-low-intrinsic-rank" id="id8">The Update Weights Of Fine-Tuning Has A Low Intrinsic Rank</a></p></li>
<li><p><a class="reference internal" href="#parameters-reduction-in-lora" id="id9">Parameters Reduction In LoRA</a></p></li>
<li><p><a class="reference internal" href="#the-low-rank-adaptation-lora-algorithm" id="id10">The Low-Rank Adaptation (LoRA) Algorithm</a></p></li>
<li><p><a class="reference internal" href="#merge-no-additional-inference-latency" id="id11">Merge - No Additional Inference Latency</a></p></li>
<li><p><a class="reference internal" href="#references-and-further-readings" id="id12">References And Further Readings</a></p></li>
</ul>
</nav>
<section id="reproducibility">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Reproducibility</a><a class="headerlink" href="#reproducibility" title="Link to this heading">#</a></h2>
<p>We first set the seed for reproducibility.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="kn">import</span> <span class="nn">os</span>
<span class="linenos"> 4</span><span class="kn">import</span> <span class="nn">random</span>
<span class="linenos"> 5</span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="linenos"> 6</span>
<span class="linenos"> 7</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="linenos"> 8</span><span class="kn">import</span> <span class="nn">torch</span>
<span class="linenos"> 9</span><span class="kn">import</span> <span class="nn">torch.backends.cudnn</span>
<span class="linenos">10</span>
<span class="linenos">11</span><span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
<span class="linenos">12</span>    <span class="s2">&quot;seed_all&quot;</span><span class="p">,</span>
<span class="linenos">13</span>    <span class="s2">&quot;seed_worker&quot;</span><span class="p">,</span>
<span class="linenos">14</span>    <span class="s2">&quot;configure_deterministic_mode&quot;</span><span class="p">,</span>
<span class="linenos">15</span>    <span class="s2">&quot;raise_error_if_seed_is_negative_or_outside_32_bit_unsigned_integer&quot;</span><span class="p">,</span>
<span class="linenos">16</span><span class="p">]</span>
<span class="linenos">17</span>
<span class="linenos">18</span><span class="n">max_seed_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span>
<span class="linenos">19</span><span class="n">min_seed_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">)</span><span class="o">.</span><span class="n">min</span>
<span class="linenos">20</span>
<span class="linenos">21</span>
<span class="linenos">22</span><span class="k">def</span> <span class="nf">raise_error_if_seed_is_negative_or_outside_32_bit_unsigned_integer</span><span class="p">(</span><span class="n">value</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="linenos">23</span>    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">min_seed_value</span> <span class="o">&lt;=</span> <span class="n">value</span> <span class="o">&lt;=</span> <span class="n">max_seed_value</span><span class="p">):</span>
<span class="linenos">24</span>        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Seed must be within the range [</span><span class="si">{</span><span class="n">min_seed_value</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">max_seed_value</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
<span class="linenos">25</span>
<span class="linenos">26</span>
<span class="linenos">27</span><span class="k">def</span> <span class="nf">configure_deterministic_mode</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="linenos">28</span><span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">29</span><span class="sd">    Activates deterministic mode in PyTorch and CUDA to ensure reproducible</span>
<span class="linenos">30</span><span class="sd">    results at the cost of performance and potentially higher CUDA memory usage.</span>
<span class="linenos">31</span><span class="sd">    It sets deterministic algorithms, disables cudnn benchmarking and enables,</span>
<span class="linenos">32</span><span class="sd">    and sets the CUBLAS workspace configuration.</span>
<span class="linenos">33</span>
<span class="linenos">34</span><span class="sd">    References</span>
<span class="linenos">35</span><span class="sd">    ----------</span>
<span class="linenos">36</span><span class="sd">    - `PyTorch Reproducibility &lt;https://pytorch.org/docs/stable/notes/randomness.html&gt;`_</span>
<span class="linenos">37</span><span class="sd">    - `PyTorch deterministic algorithms &lt;https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html&gt;`_</span>
<span class="linenos">38</span><span class="sd">    - `CUBLAS reproducibility &lt;https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility&gt;`_</span>
<span class="linenos">39</span><span class="sd">    &quot;&quot;&quot;</span>
<span class="linenos">40</span>
<span class="linenos">41</span>    <span class="c1"># fmt: off</span>
<span class="linenos">42</span>    <span class="n">torch</span><span class="o">.</span><span class="n">use_deterministic_algorithms</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">warn_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos">43</span>    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span>        <span class="o">=</span> <span class="kc">False</span>
<span class="linenos">44</span>    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span>    <span class="o">=</span> <span class="kc">True</span>
<span class="linenos">45</span>    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span>          <span class="o">=</span> <span class="kc">False</span>
<span class="linenos">46</span>
<span class="linenos">47</span>    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUBLAS_WORKSPACE_CONFIG&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;:4096:8&#39;</span>
<span class="linenos">48</span>    <span class="c1"># fmt: on</span>
<span class="linenos">49</span>    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
<span class="linenos">50</span>        <span class="s2">&quot;Deterministic mode is activated. This will negatively impact performance and may cause increase in CUDA memory footprint.&quot;</span><span class="p">,</span>
<span class="linenos">51</span>        <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">,</span>
<span class="linenos">52</span>        <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="linenos">53</span>    <span class="p">)</span>
<span class="linenos">54</span>
<span class="linenos">55</span>
<span class="linenos">56</span><span class="k">def</span> <span class="nf">seed_all</span><span class="p">(</span>
<span class="linenos">57</span>    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1992</span><span class="p">,</span>
<span class="linenos">58</span>    <span class="n">seed_torch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="linenos">59</span>    <span class="n">set_torch_deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="linenos">60</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="linenos">61</span><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">62</span><span class="sd">    Seeds all relevant random number generators to ensure reproducible</span>
<span class="linenos">63</span><span class="sd">    outcomes. Optionally seeds PyTorch and activates deterministic</span>
<span class="linenos">64</span><span class="sd">    behavior in PyTorch based on the flags provided.</span>
<span class="linenos">65</span>
<span class="linenos">66</span><span class="sd">    Parameters</span>
<span class="linenos">67</span><span class="sd">    ----------</span>
<span class="linenos">68</span><span class="sd">    seed : int, default=1992</span>
<span class="linenos">69</span><span class="sd">        The seed number for reproducibility.</span>
<span class="linenos">70</span><span class="sd">    seed_torch : bool, default=True</span>
<span class="linenos">71</span><span class="sd">        If True, seeds PyTorch&#39;s RNGs.</span>
<span class="linenos">72</span><span class="sd">    set_torch_deterministic : bool, default=True</span>
<span class="linenos">73</span><span class="sd">        If True, activates deterministic mode in PyTorch.</span>
<span class="linenos">74</span>
<span class="linenos">75</span><span class="sd">    Returns</span>
<span class="linenos">76</span><span class="sd">    -------</span>
<span class="linenos">77</span><span class="sd">    seed : int</span>
<span class="linenos">78</span><span class="sd">        The seed number used for reproducibility.</span>
<span class="linenos">79</span><span class="sd">    &quot;&quot;&quot;</span>
<span class="linenos">80</span>    <span class="n">raise_error_if_seed_is_negative_or_outside_32_bit_unsigned_integer</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="linenos">81</span>
<span class="linenos">82</span>    <span class="c1"># fmt: off</span>
<span class="linenos">83</span>    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PYTHONHASHSEED&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>       <span class="c1"># set PYTHONHASHSEED env var at fixed value</span>
<span class="linenos">84</span>    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>                           <span class="c1"># numpy pseudo-random generator</span>
<span class="linenos">85</span>    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>                              <span class="c1"># python&#39;s built-in pseudo-random generator</span>
<span class="linenos">86</span>
<span class="linenos">87</span>    <span class="k">if</span> <span class="n">seed_torch</span><span class="p">:</span>
<span class="linenos">88</span>        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="linenos">89</span>        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>           <span class="c1"># pytorch (both CPU and CUDA)</span>
<span class="linenos">90</span>        <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
<span class="linenos">91</span>        <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
<span class="linenos">92</span>        <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="kc">False</span>
<span class="linenos">93</span>
<span class="linenos">94</span>        <span class="k">if</span> <span class="n">set_torch_deterministic</span><span class="p">:</span>
<span class="linenos">95</span>            <span class="n">configure_deterministic_mode</span><span class="p">()</span>
<span class="linenos">96</span>    <span class="c1"># fmt: on</span>
<span class="linenos">97</span>    <span class="k">return</span> <span class="n">seed</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="motivation">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Motivation</a><a class="headerlink" href="#motivation" title="Link to this heading">#</a></h2>
<p>Consider that you have access to an open source large language model that is of
175 billion parameters, and you want to fine-tune it for your domain specific
task for your company. Let’s say you succeed in fine-tuning the model and it
works well for your task and you spent a lot of time and resources on it.
However your data used for fine-tuning is non-stationary and the model’s
performance degrades over time. So you have to retrain the model again and again
to keep up with the data distribution changes. To further exacerbate the
problem, your other departments across the company wants to also fine-tune the
large language model for their own domain specific tasks.</p>
<p>This will become a problem because performing a <em>full fine-tuning</em> for each
domain specific task will be computationally expensive and time consuming simply
because full fine-tuning requires <em>adjusting all the parameters</em> of the model -
which means if your model has 175 billion <em>trainable</em> parameters, you will have
to adjust all of them for each domain specific task. Such prohibitive
computational cost and time consumption is not feasible for most companies and
this is where <em>Low Rank Adaptation</em> comes in - in which it <em>freezes</em> the
<strong>backbone</strong> of the model weights and <em>inject trainable rank decomposition
matrices into selected layers</em> <span id="id1">[<a class="reference internal" href="../../bibliography.html#id32" title="Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: low-rank adaptation of large language models. 2021. URL: https://arxiv.org/abs/2106.09685, arXiv:2106.09685.">Hu <em>et al.</em>, 2021</a>]</span> of the
said model (often a transformer model).</p>
<p>To take things into perspective, GPT-3 175B fine-tuned with Adam (note adaptive
optimizer like this takes up a lot of memory because it stores the first and
second moments of the gradients) and LoRA can reduce the trainable parameters by
10,000 times and the GPU memory by 3 folds - all while maintaining the on-par
performance with suitable hyperparameters.</p>
</section>
<section id="rank-and-low-rank-decomposition-via-matrix-factorization">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Rank And Low-Rank Decomposition Via Matrix Factorization</a><a class="headerlink" href="#rank-and-low-rank-decomposition-via-matrix-factorization" title="Link to this heading">#</a></h2>
<p>Firstly, we assume that the reader has a basic understanding of the
<a class="reference external" href="https://arxiv.org/abs/1706.03762">transformer-based models</a> and how they work,
as well as the definition of
<a class="reference external" href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)"><em>rank</em></a> and
<a class="reference external" href="https://en.wikipedia.org/wiki/Low-rank_approximation"><em>low-rank decomposition</em> </a>in
linear algebra.</p>
<p>In simple terms, given a matrix <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> with dimensions <span class="math notranslate nohighlight">\(d \times k\)</span>, the
rank of a matrix is the number of linearly independent rows or columns it
contains, denoted as <span class="math notranslate nohighlight">\(r\)</span>, where <span class="math notranslate nohighlight">\(r \leq \min (d, k)\)</span>. Intuitively, if a matrix
is full rank, it represents a wider array of linear transformations, indicating
a diverse set of information. Conversely, a low-rank matrix, having fewer
linearly independent rows or columns, suggests that it contains redundant
information due to dependencies among its elements. For instance, an image of a
person can be represented as a low-rank matrix because the pixels in the image
often show strong spatial correlations. Techniques like principal component
analysis (PCA) exploit this property to compress images, reducing dimensionality
while retaining essential features.</p>
<p>A low-rank approximation of a matrix <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> is another matrix
<span class="math notranslate nohighlight">\(\overset{\sim}{\mathbf{W}}\)</span> with the same dimensions as <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>, which
approximates <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> while having a lower rank, aimed at minimizing the
approximation error <span class="math notranslate nohighlight">\(\| \mathbf{W} - \overset{\sim}{\mathbf{W}} \|\)</span> under a
specific norm. This is actually a minimization problem and is well-defined and
used commonly in various applications. A common way to find such an
approximation is to use the
<a class="reference external" href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a>
(SVD) of <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>, which decomposes <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> into three matrices
<span class="math notranslate nohighlight">\(\mathbf{U}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{V}^{T}\)</span> such that
<span class="math notranslate nohighlight">\(\mathbf{W} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^{T}\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{V}\)</span> are orthogonal matrices (assume
<span class="math notranslate nohighlight">\(\mathbf{W} \in \mathbb{R}^{d \times k}\)</span>) and <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span> is a diagonal
matrix with non-negative real numbers on the diagonal. However, for our specific
purpose, we will mention another form of low-rank decomposition via matrix
factorization. More concretely, we will use two matrices <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{B}\)</span> to approximate a given matrix <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\mathbf{A}, \mathbf{B}=\underset{\mathbf{A}, \mathbf{B}}{\operatorname{argmin}} \frac{1}{2}\|\mathbf{A} \mathbf{B}-\mathbf{W}\|_F^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\|\mathbf{B} \mathbf{A}-\mathbf{W}\|_F^2\)</span> is the objective function to
minimize. The Frobenius norm <span class="math notranslate nohighlight">\(\| \cdot \|_F\)</span> is used to measure the error
between the original matrix <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> and the approximation
<span class="math notranslate nohighlight">\(\mathbf{A} \mathbf{B}\)</span>. Lastly, it is important to note that if <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>
has rank <span class="math notranslate nohighlight">\(r\)</span>, then <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> will have dimensions
<span class="math notranslate nohighlight">\(d \times r\)</span> and <span class="math notranslate nohighlight">\(r \times k\)</span>, respectively, where <span class="math notranslate nohighlight">\(r \lt \min (d, k)\)</span> (or in
our LoRA case, <span class="math notranslate nohighlight">\(r \ll \min (d, k)\)</span> to emphasise that the <span class="math notranslate nohighlight">\(r\)</span> is much smaller
than <span class="math notranslate nohighlight">\(\min (d, k)\)</span>).</p>
</section>
<section id="the-autoregressive-self-supervised-learning-paradigm">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">The Autoregressive Self-Supervised Learning Paradigm</a><a class="headerlink" href="#the-autoregressive-self-supervised-learning-paradigm" title="Link to this heading">#</a></h2>
<p>The authors in LoRA mentioned that while our proposal is agnostic to training
objective, they focus on language modeling as our motivating use case. So we
detail a brief description of the language modeling problem.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> be the true but unknown distribution of the natural language
space. In the context of unsupervised learning with self-supervision, such as
language modeling, we consider both the inputs and the implicit labels derived
from the same data sequence. Thus, while traditionally we might decompose the
distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> of a supervised learning task into input space
<span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and label space <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>, in this scenario, <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and
<span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> are intrinsically linked, because <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> is a shifted
version of <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>, and so we can consider <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> as a distribution
over <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> only.</p>
<p>Since <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is a distribution, we also define it as a probability
distribution over <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>, and we can write it as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{D} &amp;= \mathbb{P}(\mathcal{X} ; \boldsymbol{\Theta}) \\
            &amp;= \mathbb{P}_{\{\mathcal{X} ; \boldsymbol{\Theta}\}}(\mathbf{x})
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\Theta}\)</span> is the parameter space that defines the distribution
<span class="math notranslate nohighlight">\(\mathbb{P}(\mathcal{X} ; \boldsymbol{\Theta})\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is a sample
from <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> generated by the distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. It is common to
treat <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> as a sequence of tokens (i.e. a sentence is a sequence of
tokens), and we can write <span class="math notranslate nohighlight">\(\mathbf{x} = \left(x_1, x_2, \ldots, x_T\right)\)</span>,
where <span class="math notranslate nohighlight">\(T\)</span> is the length of the sequence.</p>
<p>Given such a sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, the joint probability of the sequence can be
factorized into the product of the conditional probabilities of each token in
the sequence via the
<a class="reference external" href="https://en.wikipedia.org/wiki/Chain_rule_(probability)">chain rule of probability</a>:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(\mathbf{x} ; \boldsymbol{\Theta}) = \prod_{t=1}^T \mathbb{P}(x_t \mid x_1, x_2, \ldots, x_{t-1} ; \boldsymbol{\Theta})
\]</div>
<p>We can do this because natural language are <em>inherently ordered</em>. Such
decomposition allows for <em>tractable sampling</em> from and <em>estimation</em> of the
distribution <span class="math notranslate nohighlight">\(\mathbb{P}(\mathbf{x} ; \boldsymbol{\Theta})\)</span> as well as any
conditionals in the form of
<span class="math notranslate nohighlight">\(\mathbb{P}(x_{t-k}, x_{t-k+1}, \ldots, x_{t} \mid x_{1}, x_{2}, \ldots, x_{t-k-1} ; \boldsymbol{\Theta})\)</span>
<span id="id2">[<a class="reference internal" href="../../bibliography.html#id17" title="Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.">Radford <em>et al.</em>, 2019</a>]</span>.</p>
<p>To this end, consider a corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> with <span class="math notranslate nohighlight">\(N\)</span> sequences
<span class="math notranslate nohighlight">\(\left\{\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{N}\right\}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{S} = \left\{\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{N}\right\} \underset{\text{i.i.d.}}{\sim} \mathcal{D}
\]</div>
<p>where each sequence <span class="math notranslate nohighlight">\(\mathbf{x}_{n}\)</span> is a sequence of tokens that are sampled
<span class="math notranslate nohighlight">\(\text{i.i.d.}\)</span> from the distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</p>
<p>Then, we can frame the
<a class="reference external" href="https://gao-hongnan.github.io/gaohn-galaxy/probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html">likelihood function</a>
<span class="math notranslate nohighlight">\(\hat{\mathcal{L}}(\cdot)\)</span> as the likelihood of observing the sequences in the
corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathcal{L}}\left(\mathcal{S} ; \hat{\boldsymbol{\Theta}}\right) = \prod_{n=1}^N \mathbb{P}(\mathbf{x}_{n} ; \hat{\boldsymbol{\Theta}})
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Theta}}\)</span> is the estimated parameter space that
approximates the true parameter space <span class="math notranslate nohighlight">\(\boldsymbol{\Theta}\)</span>.</p>
<p>Subsequently, the objective function is now well-defined, to be the maximization
of the likelihood of the sequences in the corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\hat{\boldsymbol{\theta}}^{*} &amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmax}} \hat{\mathcal{L}}\left(\mathcal{S} ; \hat{\boldsymbol{\Theta}}\right) \\
                              &amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmax}} \prod_{n=1}^N \mathbb{P}(\mathbf{x}_{n} ; \hat{\boldsymbol{\Theta}}) \\
                              &amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmax}} \prod_{n=1}^N \prod_{t=1}^{T_n} \mathbb{P}(x_{n, t} \mid x_{n, 1}, x_{n, 2}, \ldots, x_{n, t-1} ; \hat{\boldsymbol{\Theta}}) \\
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(T_n\)</span> is the length of the sequence <span class="math notranslate nohighlight">\(\mathbf{x}_{n}\)</span>.</p>
<p>Owing to the fact that multiplying many probabilities together can lead to
<a class="reference external" href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html#numerical-optimization-and-the-negative-log-likelihood">numerical instability</a>
because the product of many probabilities can be very small, it is common and
necessary to use the log-likelihood as the objective function, because it can be
proven that maximizing the log-likelihood is equivalent to maximizing the
likelihood itself.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\hat{\boldsymbol{\theta}}^{*} &amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmax}} \log\left(\hat{\mathcal{L}}\left(\mathcal{S} ; \hat{\boldsymbol{\Theta}}\right)\right) \\
&amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmax}} \sum_{n=1}^N \sum_{t=1}^{T_n} \log \mathbb{P}(x_{n, t} \mid x_{n, 1}, x_{n, 2}, \ldots, x_{n, t-1} ; \hat{\boldsymbol{\Theta}}) \\
\end{aligned}
\end{split}\]</div>
<p>Furthermore, since we are treating the the loss function as a form of
minimization, we can simply negate the log-likelihood to obtain the negative
log-likelihood as the objective function to be minimized,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\hat{\boldsymbol{\theta}}^{*} &amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmin}} \left(-\log\left(\hat{\mathcal{L}}\left(\mathcal{S} ; \hat{\boldsymbol{\Theta}}\right)\right)\right) \\
&amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmin}} \left(-\sum_{n=1}^N \sum_{t=1}^{T_n} \log \mathbb{P}(x_{n, t} \mid x_{n, 1}, x_{n, 2}, \ldots, x_{n, t-1} ; \hat{\boldsymbol{\Theta}})\right) \\
\end{aligned}
\end{split}\]</div>
<p>It is worth noting that the objective function is a function of the parameter
space <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Theta}}\)</span>, and not the data <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, so all
analysis such as convergence and consistency will be with respect to the
parameter space <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Theta}}\)</span>.</p>
<p>To this end, we denote the model <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> to be an <em>autoregressive</em> and
<em>self-supervised learning</em> model that is trained to maximize the likelihood of
observing all data points <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{S}\)</span> via the objective
function <span class="math notranslate nohighlight">\(\hat{\mathcal{L}}\left(\mathcal{S} ; \hat{\boldsymbol{\Theta}}\right)\)</span>
by learning the conditional probability distribution
<span class="math notranslate nohighlight">\(\mathbb{P}(x_t \mid x_{&lt;t} ; \hat{\boldsymbol{\Theta}})\)</span> over the vocabulary
<span class="math notranslate nohighlight">\(\mathcal{V}\)</span> of tokens, conditioned on the contextual preciding tokens
<span class="math notranslate nohighlight">\(x_{&lt;t} = \left(x_1, x_2, \ldots, x_{t-1}\right)\)</span>. We are clear that although
the goal is to model the joint probability distribution of the token sequences,
we can do so by estimating the joint probability distribution via the
conditional probability distributions.</p>
</section>
<section id="task-specific-fine-tuning">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Task Specific Fine-Tuning</a><a class="headerlink" href="#task-specific-fine-tuning" title="Link to this heading">#</a></h2>
<p>We can now look at what the authors define next which is the maximization of
conditional probabilities given a task-specific prompt.</p>
<p>Suppose we are given a pre-trained autoregressive language model
<span class="math notranslate nohighlight">\(\mathcal{M}_{\Theta}(y \mid x)\)</span> parametrized by <span class="math notranslate nohighlight">\(\Theta\)</span>. For instance,
<span class="math notranslate nohighlight">\(\mathcal{M}_{\Theta}(y \mid x)\)</span> can be a generic multi-task learner such as GPT
based on the Transformer architecture. Consider adapting this pre-trained model
to downstream conditional text generation tasks, such as summarization, machine
reading comprehension (MRC), and natural language to SQL (NL2SQL). Each
downstream task is represented by a training dataset of context-target pairs:
<span class="math notranslate nohighlight">\(\mathcal{Z}=\left\{\left(x_i, y_i\right)\right\}_{i=1, \ldots, N}\)</span>, where both
<span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(y_i\)</span> are sequences of tokens. For example, in NL2SQL, <span class="math notranslate nohighlight">\(x_i\)</span> is a
natural language query and <span class="math notranslate nohighlight">\(y_i\)</span> its corresponding SQL command; for
summarization, <span class="math notranslate nohighlight">\(x_i\)</span> is the content of an article and <span class="math notranslate nohighlight">\(y_i\)</span> its summary.</p>
<p>During full fine-tuning, the model is initialized to pre-trained weights
<span class="math notranslate nohighlight">\(\Theta_{\mathcal{P}}\)</span> (where <span class="math notranslate nohighlight">\(\Theta_{\mathcal{P}}\)</span> just denotes the final
pretrained weights) and updated to <span class="math notranslate nohighlight">\(\Theta_{\mathcal{P}}+\Delta \Theta\)</span> by
repeatedly following the gradient to maximize the conditional language modeling
objective:</p>
<div class="math notranslate nohighlight">
\[
\max_{\Theta} \sum_{(x, y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log \left(\mathcal{M}_{\Theta}\left(y_t \mid x, y_{&lt;t}\right)\right)
\]</div>
<p>One of the main drawbacks for full fine-tuning is that for <em>each</em> downstream
task, we learn a different set of parameters <span class="math notranslate nohighlight">\(\Delta \Theta\)</span> whose dimension
<span class="math notranslate nohighlight">\(|\Delta \Theta|\)</span> equals <span class="math notranslate nohighlight">\(\left|\Theta_{\mathcal{P}}\right|\)</span>. Thus, if the
pre-trained model is large (such as GPT-3 with
<span class="math notranslate nohighlight">\(\left|\Theta_{\mathcal{P}}\right| \approx 175\)</span> Billion), storing and deploying
many independent instances of fine-tuned models can be challenging, if at all
feasible. In this paper, we adopt a more parameter-efficient approach, where the
task-specific parameter increment <span class="math notranslate nohighlight">\(\Delta \Theta=\Delta \Theta(\Phi)\)</span> is further
encoded by a much smaller-sized set of parameters <span class="math notranslate nohighlight">\(\Phi\)</span> with
<span class="math notranslate nohighlight">\(|\Phi| \ll \left|\Theta_{\mathcal{P}}\right|\)</span>. The task of finding
<span class="math notranslate nohighlight">\(\Delta \Theta\)</span> thus becomes optimizing over <span class="math notranslate nohighlight">\(\Phi\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\max _{\Theta} \sum_{(x, y) \in \mathcal{Z}} \sum_{t=1}^{|y|} \log \left(p_{\Theta_{\mathcal{P}}+\Delta \Theta(\Phi)}\left(y_t \mid x, y_{&lt;t}\right)\right)
\]</div>
<p>In the subsequent sections, we propose to use a low-rank representation to
encode <span class="math notranslate nohighlight">\(\Delta \Theta\)</span> that is both compute- and memory-efficient. When the
pre-trained model is GPT-3 175B, the number of trainable parameters <span class="math notranslate nohighlight">\(|\Phi|\)</span> can
be as small as <span class="math notranslate nohighlight">\(0.01 \%\)</span> of <span class="math notranslate nohighlight">\(\left|\Theta_{\mathcal{P}}\right|\)</span>. Note that you
can visualize the <span class="math notranslate nohighlight">\(\Delta \Theta(\Phi)\)</span> as the low-rank decomposition of the
update weights <span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span> in the fine-tuning process.</p>
</section>
<section id="the-update-weights-of-fine-tuning-has-a-low-intrinsic-rank">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">The Update Weights Of Fine-Tuning Has A Low Intrinsic Rank</a><a class="headerlink" href="#the-update-weights-of-fine-tuning-has-a-low-intrinsic-rank" title="Link to this heading">#</a></h2>
<p>We describe the author’s first big idea in this section, where they hypothesize
(with empirical evidence) that the update weights of a large language model
during fine-tuning reside in a low-dimensional subspace.</p>
<p>The image below illustrates and gives a very simplified visual representation of
a single weight update step from a full fine-tuning process (left) versus a
LoRA-based fine-tuning process (right). The matrices <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{B}\)</span> (which we explain shortly) are approximations of the update weights
<span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span> in the LoRA-based fine-tuning process.</p>
<figure class="align-default" id="low-rank-weights-visual-seb">
<img alt="../../_images/lora_weights_visual_seb.png" src="../../_images/lora_weights_visual_seb.png" />
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">Low-rank decomposition of the update weights <span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span> into two
matrices <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span>.</span><a class="headerlink" href="#low-rank-weights-visual-seb" title="Link to this image">#</a></p>
<div class="legend">
<p>Image Credit:
<a class="reference external" href="https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-E/01_main-chapter-code/appendix-E.ipynb">Sebastian Raschka</a></p>
</div>
</figcaption>
</figure>
<p>First, we use very rough notations to describe the update weights of fine-tuning
as a matrix <span class="math notranslate nohighlight">\(\mathbf{W} \in \mathbb{R}^{d \times k}\)</span> in a gradient-based
optimization process. For simplicity we call <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> as the <em>pre-trained
weights</em> and <span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span> as the <em>update weights</em> at a given iteration of
a gradient-based optimization process. More concretely, we have the below update
process:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbf{W}  &amp;\leftarrow \underbrace{\mathbf{W} - \alpha \nabla \mathbf{W}}_{\mathbf{W} - \alpha \frac{\partial \mathcal{J}}{\partial \mathbf{W}}}
\end{aligned}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{J}\)</span> is the objective function, <span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate,
and <span class="math notranslate nohighlight">\(\nabla \mathbf{W}\)</span> is the gradient of the objective function with respect
to the pre-trained weights <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> and collectively
<span class="math notranslate nohighlight">\(-\alpha \nabla \mathbf{W}\)</span> is the update weights <span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span> and that
both <span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span> and <span class="math notranslate nohighlight">\(\nabla \mathbf{W}\)</span> lies in the same subspace.</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\Delta \mathbf{W} := -\alpha \nabla \mathbf{W}
\end{aligned}
\]</div>
<p>To ease the notations, we further denote <span class="math notranslate nohighlight">\(\mathbf{W}^{(t)}\)</span> as the pre-trained
weights at iteration <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(\Delta \mathbf{W}^{(t)}\)</span> as the update weights at
iteration <span class="math notranslate nohighlight">\(t\)</span>. We can then rewrite the above equation as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{W}^{(t+1)} &amp;= \mathbf{W}^{(t)} + \Delta \mathbf{W}^{(t)} \\
\end{aligned}
\end{split}\]</div>
<p>to indicate that <span class="math notranslate nohighlight">\(\mathbf{W}^{(t+1)}\)</span> is the updated weights after
<span class="math notranslate nohighlight">\(\Delta \mathbf{W}^{(t)}\)</span> is added to the pre-trained weights
<span class="math notranslate nohighlight">\(\mathbf{W}^{(t)}\)</span>.</p>
<p>Empirical evidence suggests that deep learning models (often large language
models) <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> are over-parametrized with respect to their parameter
space <span class="math notranslate nohighlight">\(\Theta\)</span> (i.e. the weights of the model). This means the model contains
more parameters than are necessary to achieve the minimum error on the training
data. This redundancy often implies that many parameters are either not
essential or are correlated with others. While the “full space” offers maximal
degrees of freedom for the model parameters, allowing complex representations
and potentially capturing intricate patterns in the data, it can lead to
overfitting, and in our context, it can lead to high computational costs and
memory usage.</p>
<p>As a result, the authors hypothesize that <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> can operate within a
much lower-dimensional subspace <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> which means that we can reduce the
effective degrees of freedom of the model <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> by projecting the
weights of the model into a lower-dimensional subspace while <em>maintaining the
performance of the model</em> - and this is what the author mean by “model resides
in a low intrinsic dimension”.</p>
<p>More concretely, suppose the weight matrix
<span class="math notranslate nohighlight">\(\mathbf{W} \in \mathbb{R}^{d \times k}\)</span> has rank <span class="math notranslate nohighlight">\(r\)</span> meaning the maximum number
of linearly independent rows or columns it contains is <span class="math notranslate nohighlight">\(r\)</span>. We say that this
weight matrix reside in a subspace <span class="math notranslate nohighlight">\(\mathcal{S}_{W} \subset \mathbb{R}^{d}\)</span>
(column space/range of <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>) and the dimension of this subspace is <span class="math notranslate nohighlight">\(r\)</span>.
The authors argue that the update weights <span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span> of the model
<span class="math notranslate nohighlight">\(\mathcal{M}\)</span> at a given iteration of the optimization process also reside in a
low-dimensional subspace
<span class="math notranslate nohighlight">\(\mathcal{S}_{\Delta \mathbf{W}} \subset \mathbb{R}^{d}\)</span> with
<span class="math notranslate nohighlight">\(\dim\left(\mathcal{S}_{\Delta \mathbf{W}}\right) \ll r\)</span>. Note that without
LoRA, the update weights <span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span> typically have a high rank (not
guaranteed to be of same rank of <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>), and so the authors intelligently
proposed an approximation of the update weights <span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span> using a
low-rank decomposition because of the hypothesis that the update weights reside
in a low-dimensional subspace and is sufficient to represent the
over-parametrized model <span class="math notranslate nohighlight">\(\mathcal{M}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{W}^{(t+1)} &amp;= \mathbf{W}^{(t)} + \Delta \mathbf{W}^{(t)} \\
&amp;= \mathbf{W}^{(t)} + \mathbf{B}^{(t)} \mathbf{A}^{(t)}
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{A}^{(t)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{B}^{(t)}\)</span> are the low-rank decomposition
matrices of the update weights <span class="math notranslate nohighlight">\(\Delta \mathbf{W}^{(t)}\)</span> at iteration <span class="math notranslate nohighlight">\(t\)</span>. In
other words, the update weights <span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span> are approximated by the
product of two low-rank matrices <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{r \times k}\)</span> and
and <span class="math notranslate nohighlight">\(\mathbf{B} \in \mathbb{R}^{d \times r}\)</span>, where <span class="math notranslate nohighlight">\(r \ll \min (d, k)\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\Delta \mathbf{W} \approx \mathbf{B} \mathbf{A}
\end{aligned}
\]</div>
</section>
<section id="parameters-reduction-in-lora">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Parameters Reduction In LoRA</a><a class="headerlink" href="#parameters-reduction-in-lora" title="Link to this heading">#</a></h2>
<p>Now, let’s do some quick math. Earlier we said our model is of size
<span class="math notranslate nohighlight">\(\| \Theta_{\mathcal{M}} \| = 175,000,000,000\)</span> parameters. Then for simplicity
case we assume our weight <span class="math notranslate nohighlight">\(\mathbf{W} \in \mathbb{R}^{d \times k}\)</span> of the
pretrained model <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> to be of size
<span class="math notranslate nohighlight">\(d = k = \sqrt{175,000,000,000} = 418,330\)</span>. And if we do not do LoRA, the update
weights <span class="math notranslate nohighlight">\(\nabla \mathbf{W}\)</span> will also be of size
<span class="math notranslate nohighlight">\(d \times k = 418,330 \times 418,330 = 175,000,000,000\)</span> parameters. However, if
we decompose the update weights <span class="math notranslate nohighlight">\(\nabla \mathbf{W}\)</span> into two low-rank matrices
<span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span>, then the number of parameters in the low-rank
decomposition is <span class="math notranslate nohighlight">\(r \times (d + k)\)</span>. Suppose that we use a LoRA rank of <span class="math notranslate nohighlight">\(r = 8\)</span>,
then <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{8 \times 418,330}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{B} \in
\mathbb{R}^{418,330 \times 8}\)</span>, and the number of parameters in
the low-rank decomposition is <span class="math notranslate nohighlight">\(8 \times (418,330 + 418,330) = 6,693,280\)</span>
parameters. We do some quick calculations and see that the reduction in the
number of parameters is more than 26100 times.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="kn">import</span> <span class="nn">math</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="k">def</span> <span class="nf">compute_lora_parameters</span><span class="p">(</span><span class="n">d</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">r</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="linenos"> 4</span>    <span class="n">parameters_A</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="n">d</span>
<span class="linenos"> 5</span>    <span class="n">parameters_B</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="n">k</span>
<span class="linenos"> 6</span>    <span class="k">return</span> <span class="n">parameters_A</span> <span class="o">+</span> <span class="n">parameters_B</span>
<span class="linenos"> 7</span>
<span class="linenos"> 8</span><span class="n">total_trainable_parameters</span> <span class="o">=</span> <span class="mi">175_000_000_000</span>
<span class="linenos"> 9</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total trainable parameters: </span><span class="si">{</span><span class="n">total_trainable_parameters</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">10</span>
<span class="linenos">11</span><span class="n">d</span> <span class="o">=</span> <span class="n">k</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">total_trainable_parameters</span><span class="p">)</span>
<span class="linenos">12</span><span class="n">r</span> <span class="o">=</span> <span class="mi">8</span>
<span class="linenos">13</span><span class="n">lora_parameters</span> <span class="o">=</span> <span class="n">compute_lora_parameters</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
<span class="linenos">14</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LoRA parameters: </span><span class="si">{</span><span class="n">lora_parameters</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">15</span>
<span class="linenos">16</span><span class="n">reduction</span> <span class="o">=</span> <span class="p">(</span><span class="n">total_trainable_parameters</span> <span class="o">-</span> <span class="n">lora_parameters</span><span class="p">)</span> <span class="o">/</span> <span class="n">total_trainable_parameters</span>
<span class="linenos">17</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Reduction: </span><span class="si">{</span><span class="n">reduction</span><span class="si">:</span><span class="s2">.6%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">18</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">total_trainable_parameters</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">lora_parameters</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Total trainable parameters: 175000000000
LoRA parameters: 6693280.212272604
Reduction: 99.996175%
26145.625829189863
</pre></div>
</div>
</div>
</div>
<p>However, do note that there is no free lunch, we have to acknowledge that the
rank <span class="math notranslate nohighlight">\(r\)</span> of the low-rank decomposition is a hyperparameter that needs to be
tuned. Too small a rank can lead to underfitting, while too large a rank can
lead to overfitting. Furthermore, no one knows the underlying “true” rank of the
model and it may be well the case that the approximation <span class="math notranslate nohighlight">\(\mathbf{B} \mathbf{A}\)</span>
is not a good approximation of the update weights <span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span> and cannot
capture every nuance. That is fine, for one, during pretraining stage, there is
no low rank approximation and we hypothesize that the weight matrix <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>
is large and sufficient enough to capture all the nuances and knowledge in the
huge pretraining dataset. However, during the fine-tuning stage, we hypothesize
the domain specific task is not as complex as the pretraining task and that the
model has sufficient knowledge to <em>adapt</em> to the domain specific task with a
low-rank decomposition/approximation of the update weights <span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span>.
This brings to our second point, if the target domain specific task
<span class="math notranslate nohighlight">\(\mathcal{T}\)</span> is too drastically different from the pretraining task
<span class="math notranslate nohighlight">\(\mathcal{P}\)</span>, then the low-rank decomposition may not be able to capture the
necessary information for the adaptation and the model may not perform well - so
here we recommend increasing the rank <span class="math notranslate nohighlight">\(r\)</span> where appropriate.</p>
</section>
<section id="the-low-rank-adaptation-lora-algorithm">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">The Low-Rank Adaptation (LoRA) Algorithm</a><a class="headerlink" href="#the-low-rank-adaptation-lora-algorithm" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> be our model with some linear layer with weights
<span class="math notranslate nohighlight">\(\mathbf{W} \in \mathbb{R}^{d \times k}\)</span>, where <span class="math notranslate nohighlight">\(d\)</span> is the output dimension and
<span class="math notranslate nohighlight">\(k\)</span> is the input dimension (get used to this notation with PyTorch). In
particular <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> is the original pre-trained weights of the model
<span class="math notranslate nohighlight">\(\mathcal{M}\)</span> (correspond to our <span class="math notranslate nohighlight">\(\Theta_{\mathcal{P}}\)</span> earlier).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
</pre></div>
</div>
<p>We define the linear transformation
<span class="math notranslate nohighlight">\(f_{\mathbf{W}} : \mathbb{R}^k \rightarrow \mathbb{R}^d\)</span> by
<span class="math notranslate nohighlight">\(f_{\mathbf{W}}(\mathbf{x}) = \mathbf{x} &#64; \mathbf{W}^T\)</span> where
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^{1 \times k}\)</span> (assume batch size of <span class="math notranslate nohighlight">\(1\)</span> for
simplicity). Note a quirk here is that we usually define the input as
<span class="math notranslate nohighlight">\(\mathbb{R}^{\mathcal{B} \times k}\)</span> where <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> is the batch size and
transpose the weights from torch’s <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">x</span> <span class="o">@</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
<p>Next, we define two low rank matrices <span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{r \times k}\)</span>
and <span class="math notranslate nohighlight">\(\mathbf{B} \in \mathbb{R}^{d \times r}\)</span> where <span class="math notranslate nohighlight">\(r\)</span> is the rank of the
low-rank decomposition. We define transformations as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
f_{\mathbf{A}} &amp;: \mathbb{R}^k \rightarrow \mathbb{R}^r &amp; \quad f_{\mathbf{B}} &amp;: \mathbb{R}^r \rightarrow \mathbb{R}^d \\
f_{\mathbf{A}}(\mathbf{x}) &amp;= \mathbf{x} &#64; \mathbf{A}^T &amp; \quad f_{\mathbf{B}}(\mathbf{y}) &amp;= \mathbf{y} &#64; \mathbf{B}^T
\end{aligned}
\end{split}\]</div>
<p>For an input <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^k\)</span>, we technically have the following
update:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{y}  &amp;= \mathbf{x} &#64; \left(\mathbf{W}^T + \Delta \mathbf{W}^T\right) \\
            &amp;= \mathbf{x} &#64; \left(\mathbf{W}^T + (\mathbf{B} \mathbf{A})^T\right) \\
            &amp;= \mathbf{x} &#64; \mathbf{W}^T + \mathbf{x} &#64; \left(\mathbf{B} \mathbf{A}\right)^T \\
\end{aligned}
\end{split}\]</div>
<p>But we have our pretrained model weights <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> is <em>frozen</em> so we can
compute the frozen output first as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{y}_{\text{frozen}} = \mathbf{x} &#64; \mathbf{W}^T
\]</div>
<p>Why can we do this? Because of the distributive law of matrix multiplication. As
we will mention again later, this allows the weight to be updated on the fly
during inference, meaning we do not need to store the original pre-trained
weights <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> and only need to store the low-rank matrices <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>
and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> - which is much more tractable.</p>
<p>Then finally we have the following update:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{y}  &amp;= \mathbf{y}_{\text{frozen}} + \mathbf{x} &#64; \left(\mathbf{B} \mathbf{A}\right)^T \\
\end{aligned}
\end{split}\]</div>
<p>However, three nuances here:</p>
<ol class="arabic">
<li><p>The pretrained weights <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> of <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> is frozen during
training via <code class="docutils literal notranslate"><span class="pre">requires_grad=False</span></code>. This tells PyTorch not to update the
weights of the pretrained model during backpropagation. This is important
because we want to keep the pretrained weights fixed and only update the
low-rank matrices <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> - both of which are
trainable.</p></li>
<li><p>They use gaussian initialization for <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and zero initialization for
<span class="math notranslate nohighlight">\(\mathbf{B}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
     \begin{aligned}
     \mathbf{A} &amp;\sim \mathcal{N}(0, \sigma^2) \\
     \mathbf{B} &amp;= \mathbf{0}
     \end{aligned}
    \end{split}\]</div>
<p>One of the matrices must be zero at initialization to ensure that the
initial state of the adaptation <span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span> does not alter the
pre-trained weights <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>, allowing the training process to start
from the original pre-trained state. In simpler words, your first forward
pass of the model should be from the original pre-trained weights
<span class="math notranslate nohighlight">\(\mathbf{W}\)</span>, and not from some random lora weights.</p>
<p>As to why <span class="math notranslate nohighlight">\(\mathbf{A} \sim \mathcal{N}(0, \sigma^2)\)</span>, this is a common
initialization strategy for neural networks to break the symmetry and ensure
that the gradients are not too small or too large at the beginning of
training. Just remember, vanishing and exploding gradients are bad, and we
want to avoid them. How to avoid them is to make sure your initial
conditions are good, what it means by good is say each layer weights has
similar distribution (mean and variance) and so pertubations won’t be too
large or too small. If you ask “if <span class="math notranslate nohighlight">\(\mathbf{B} \mathbf{A}\)</span> is zero, why
don’t we just initialize <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> to zero as well?” - I think one needs
to know that the backpropagation process update both weights <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>
and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> <em>separately</em> and we want stable gradient flow, so
<span class="math notranslate nohighlight">\(\mathbf{A}\)</span> breaks the symmetry! If you initialize all weights with zeros
then every hidden unit will get zero independent of the input. So, when all
the hidden neurons start with the zero weights, then all of them will follow
the same gradient and for this reason “it affects only the scale of the
weight vector, not the direction”. See
<a class="reference external" href="https://datascience.stackexchange.com/questions/26134/initialize-perceptron-weights-with-zero">this very useful thread on the whys</a>.</p>
</li>
<li><p>They have a scaling factor, where they scale <span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span> by
<span class="math notranslate nohighlight">\(\frac{\alpha}{r}\)</span>. In LoRA paper, <span class="math notranslate nohighlight">\(\alpha\)</span> is constant in <span class="math notranslate nohighlight">\(r\)</span> means that if
once you fix a value of <span class="math notranslate nohighlight">\(r\)</span> in your initial experiments, you can keep
<span class="math notranslate nohighlight">\(\alpha\)</span> constant for all future experiments with different values of <span class="math notranslate nohighlight">\(r\)</span> -
because you can tune the learning rate scheduler’s <span class="math notranslate nohighlight">\(\eta\)</span> instead because yes
both <span class="math notranslate nohighlight">\(\alpha\)</span> is pretty similar in <em>scaling the gradients
<span class="math notranslate nohighlight">\(\nabla \mathbf{W}\)</span></em> because by the chain rule, any scaling of the weights
will proportionally affect the gradients! Note for less confusion, we use
<span class="math notranslate nohighlight">\(\eta\)</span> for the learning rate in the optimizer and <span class="math notranslate nohighlight">\(\alpha\)</span> for the scaling
factor in LoRA.</p>
<div class="math notranslate nohighlight">
\[
     \begin{aligned}
     \mathbf{y} &amp;= \mathbf{y}_{\text{frozen}} + \frac{\alpha}{r} \mathbf{x} &#64; \left(\mathbf{B} \mathbf{A}\right)^T
     \end{aligned}
    \]</div>
<p>Note that <span class="math notranslate nohighlight">\(\alpha\)</span> is generally understood as an amplification factor - and
if <span class="math notranslate nohighlight">\(\alpha\)</span> is large, it amplifies the contribution of the LoRA weights to
the final output of the adapter layer.</p>
<p>Now some quick and rough (read: non-rigorous) math here, suppose we keep
rank <span class="math notranslate nohighlight">\(r\)</span> fixed, and we increase <span class="math notranslate nohighlight">\(\alpha\)</span> (LoRA) by a factor of <span class="math notranslate nohighlight">\(c\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
    c \times \frac{\alpha}{r} \mathbf{x} &#64; \left(\mathbf{B} \mathbf{A}\right)^T &amp;= (c \times \eta) \times \mathbf{x} &#64; \left(\frac{\mathbf{B}}{\sqrt{c}} \frac{\mathbf{A}}{\sqrt{c}}\right)^T \\
    \end{aligned}
    \end{split}\]</div>
<p>So in other words, if you keep <span class="math notranslate nohighlight">\(r\)</span> fixed, when you increase <span class="math notranslate nohighlight">\(\alpha\)</span> by a
factor of <span class="math notranslate nohighlight">\(c\)</span> - this is <em>equivalent</em> to <em>increasing</em> the learning rate
<span class="math notranslate nohighlight">\(\eta\)</span> by a factor of <span class="math notranslate nohighlight">\(c\)</span> because in gradient updates we do
<span class="math notranslate nohighlight">\(-\eta \nabla \left(\mathbf{B}\mathbf{A}\right)\)</span> and so if you increase
<span class="math notranslate nohighlight">\(\alpha\)</span> by a factor of <span class="math notranslate nohighlight">\(c\)</span>, you are inevitably increasing the learning rate
<span class="math notranslate nohighlight">\(\eta\)</span> by a factor of <span class="math notranslate nohighlight">\(c\)</span>. To compensate for this, you can decrease the
initializations of <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> by a factor of <span class="math notranslate nohighlight">\(\sqrt{c}\)</span>
to keep to the same scale as before. Therefore, the authors recommend users
to (1) keep the rank <span class="math notranslate nohighlight">\(r\)</span> fixed and (2) tune the learning rate scheduler’s
<span class="math notranslate nohighlight">\(\eta\)</span> instead of <span class="math notranslate nohighlight">\(\alpha\)</span> (and maybe the weights as well). One can read a
thread on this
<a class="reference external" href="https://civitai.com/articles/2125/what-lora-alpha-actually-does-in-theory">here</a>.</p>
</li>
</ol>
</section>
<section id="merge-no-additional-inference-latency">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Merge - No Additional Inference Latency</a><a class="headerlink" href="#merge-no-additional-inference-latency" title="Link to this heading">#</a></h2>
<p>The distributive law of matrix multiplication we saw earlier ensures that the
update weights <span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span> can be applied on the fly during inference
without too much over memory overhead, and therefore not much additional
latency. Recall the equation below:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{y}  &amp;= \mathbf{x} &#64; \left(\mathbf{W}^T + \Delta \mathbf{W}^T\right) \\
            &amp;= \mathbf{x} &#64; \left(\mathbf{W}^T + (\mathbf{B} \mathbf{A})^T\right) \\
            &amp;= \mathbf{x} &#64; \mathbf{W}^T + \mathbf{x} &#64; \left(\mathbf{B} \mathbf{A}\right)^T \\
\end{aligned}
\end{split}\]</div>
<p>We easily see that once we obtain the trained low-rank matrices <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{B}\)</span>, we can apply the update weights <span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span> on the fly
during inference without having to store the original pre-trained weights by
just doing an element-wise addition of the frozen output
<span class="math notranslate nohighlight">\(\mathbf{y}_{\text{frozen}}\)</span> and the update
<span class="math notranslate nohighlight">\(\mathbf{x} &#64; \left(\mathbf{B} \mathbf{A}\right)^T\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\mathbf{y} = \mathbf{x} &#64; \mathbf{W}^T \oplus \mathbf{x} &#64; \left(\mathbf{B} \mathbf{A}\right)^T
\]</div>
<p>Again, we are reminded that this is a huge advantage because we need not store
<span class="math notranslate nohighlight">\(N\)</span> instances of the updated weights <span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span> for <span class="math notranslate nohighlight">\(N\)</span> different
tasks, but only the low-rank matrices <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span>. During
inference, just apply the low-rank matrices on the fly and you are good to go.</p>
<figure class="align-default" id="lora-weights-visual-paper">
<img alt="../../_images/lora_weights_visual.png" src="../../_images/lora_weights_visual.png" />
<figcaption>
<p><span class="caption-number">Fig. 6 </span><span class="caption-text">LoRA update weights <span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span> as a low-rank decomposition of two
matrices <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span>.</span><a class="headerlink" href="#lora-weights-visual-paper" title="Link to this image">#</a></p>
<div class="legend">
<p>Image Credit: <a class="reference external" href="https://arxiv.org/pdf/2106.09685">LoRA Paper</a></p>
</div>
</figcaption>
</figure>
<p>In practice, it is common to <em>merge and unload</em> the adapter layer after training
into the original pretrained base model. This merge operation is done by
updating the original pretrained weights <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> with the low-rank
decomposition matrices <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> to obtain the merged
weights <span class="math notranslate nohighlight">\(\mathbf{W}_{'}\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbf{W}^{'} &amp;= \mathbf{W} \oplus \mathbf{B} \mathbf{A}
\end{aligned}
\]</div>
<p>This way, during inference, given an input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, we can compute the
output <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> by applying the merged weights <span class="math notranslate nohighlight">\(\mathbf{W}^{'}\)</span> on the input
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbf{y} &amp;= \mathbf{x} &#64; \left(\mathbf{W}^{'}\right)^T
\end{aligned}
\]</div>
<p>This requires only a single matrix multiplication operation. In total, we can
loosely say that if <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is a <span class="math notranslate nohighlight">\(1 \times k\)</span> vector, and
<span class="math notranslate nohighlight">\(\mathbf{W}^{'} \in \mathbb{R}^{d \times k}\)</span>, then doing
<span class="math notranslate nohighlight">\(\mathbf{x} &#64; \left(\mathbf{W}^{'}\right)^T\)</span> costs <span class="math notranslate nohighlight">\(2 \times d \times k\)</span>
multiplications and <span class="math notranslate nohighlight">\(d \times k\)</span> additions. But if you do not merge, and keep
the weights separate, you get
<span class="math notranslate nohighlight">\(\mathbf{x} &#64; \left(\mathbf{W}^T\right) + \mathbf{x} &#64; \left(\mathbf{B} \mathbf{A}\right)^T\)</span>
which costs <span class="math notranslate nohighlight">\(2 \times d \times k\)</span> multiplications and <span class="math notranslate nohighlight">\(2 \times d \times k\)</span>
additions. So, in terms of computational complexity, even though they are on the
same order, the merged weights are slightly more efficient.</p>
</section>
<section id="references-and-further-readings">
<h2><a class="toc-backref" href="#id12" role="doc-backlink">References And Further Readings</a><a class="headerlink" href="#references-and-further-readings" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>[1] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
W. Chen, “LoRA: Low-Rank Adaptation of Large Language Models,” <em>arXiv
preprint arXiv:2106.09685</em>, submitted Jun. 17, 2021, revised Oct. 16, 2021.
[Online]. Available: <a class="reference external" href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a></p></li>
<li><p>[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin.
<a class="reference external" href="https://arxiv.org/abs/1706.03762">“Attention is all you need”</a>. In Advances
in Neural Information Processing Systems, pp. 5998–6008, 2017.</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Low-rank_approximation">Low Rank Approximation - Wikipedia</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)">Rank (Linear Algebra) - Wikipedia</a></p></li>
<li><p><a class="reference external" href="https://datascience.stackexchange.com/questions/26134/initialize-perceptron-weights-with-zero">Initialize perceptron weights with zero</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./influential/low_rank_adaptation"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="01_intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Low-Rank Adaptation Of Large Language Models</p>
      </div>
    </a>
    <a class="right-next"
       href="03_implementation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Implementation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducibility">Reproducibility</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rank-and-low-rank-decomposition-via-matrix-factorization">Rank And Low-Rank Decomposition Via Matrix Factorization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-autoregressive-self-supervised-learning-paradigm">The Autoregressive Self-Supervised Learning Paradigm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#task-specific-fine-tuning">Task Specific Fine-Tuning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-update-weights-of-fine-tuning-has-a-low-intrinsic-rank">The Update Weights Of Fine-Tuning Has A Low Intrinsic Rank</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters-reduction-in-lora">Parameters Reduction In LoRA</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-low-rank-adaptation-lora-algorithm">The Low-Rank Adaptation (LoRA) Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#merge-no-additional-inference-latency">Merge - No Additional Inference Latency</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References And Further Readings</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>