
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Concept &#8212; Omniverse</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=bb35926c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MYW5YKC2WF"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"defeq": "\\overset{\\text{def}}{=}", "defa": "\\overset{\\text{(a)}}{=}", "defb": "\\overset{\\text{(b)}}{=}", "defc": "\\overset{\\text{(c)}}{=}", "defd": "\\overset{\\text{(d)}}{=}", "st": "\\mid", "mod": "\\mid", "S": "\\Omega", "s": "\\omega", "e": "\\exp", "P": "\\mathbb{P}", "R": "\\mathbb{R}", "expectation": "\\mathbb{E}", "v": "\\mathbf{v}", "a": "\\mathbf{a}", "b": "\\mathbf{b}", "c": "\\mathbf{c}", "u": "\\mathbf{u}", "w": "\\mathbf{w}", "x": "\\mathbf{x}", "y": "\\mathbf{y}", "z": "\\mathbf{z}", "0": "\\mathbf{0}", "1": "\\mathbf{1}", "A": "\\mathbf{A}", "B": "\\mathbf{B}", "C": "\\mathbf{C}", "E": "\\mathcal{F}", "eventA": "\\mathcal{A}", "lset": "\\left\\{", "rset": "\\right\\}", "lsq": "\\left[", "rsq": "\\right]", "lpar": "\\left(", "rpar": "\\right)", "lcurl": "\\left\\{", "rcurl": "\\right\\}", "pmf": "p_X", "pdf": "f_X", "pdftwo": "f_{X,Y}", "pdfjoint": "f_{\\mathbf{X}}", "pmfjointxy": "p_{X, Y}", "pdfjointxy": "f_{X, Y}", "cdf": "F_X", "pspace": "(\\Omega, \\mathcal{F}, \\mathbb{P})", "var": "\\operatorname{Var}", "std": "\\operatorname{Std}", "bern": "\\operatorname{Bernoulli}", "binomial": "\\operatorname{Binomial}", "geometric": "\\operatorname{Geometric}", "poisson": "\\operatorname{Poisson}", "uniform": "\\operatorname{Uniform}", "normal": "\\operatorname{Normal}", "gaussian": "\\operatorname{Gaussian}", "gaussiansymbol": "\\mathcal{N}", "exponential": "\\operatorname{Exponential}", "iid": "\\textbf{i.i.d.}", "and": "\\text{and}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'influential/linear_regression/02_concept';</script>
    <link rel="canonical" href="https://www.gaohongnan.com/influential/linear_regression/02_concept.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Implementation" href="03_implementation.html" />
    <link rel="prev" title="Linear Regression" href="01_intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Omniverse - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Omniverse - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Omniverse
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notations/machine_learning.html">Machine Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Influential Ideas and Papers</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../generative_pretrained_transformer/01_intro.html">Generative Pre-trained Transformers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../generative_pretrained_transformer/02_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../generative_pretrained_transformer/03_concept.html">The Concept of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../generative_pretrained_transformer/04_implementation.html">The Implementation of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../generative_pretrained_transformer/05_adder.html">Training a Mini-GPT to Learn Two-Digit Addition</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../low_rank_adaptation/01_intro.html">Low-Rank Adaptation Of Large Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../low_rank_adaptation/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../low_rank_adaptation/03_implementation.html">Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../empirical_risk_minimization/01_intro.html">Empirical Risk Minimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../empirical_risk_minimization/02_concept.html">Concept: Empirical Risk Minimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../empirical_risk_minimization/03_bayes_optimal_classifier.html">Bayes Optimal Classifier</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../learning_theory/01_intro.html">Is The Learning Problem Solvable?</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../learning_theory/02_concept.html">Concept: Learning Theory</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../kmeans_clustering/01_intro.html">Lloyd’s K-Means Clustering Algorithm</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/02_concept.html">Concept: K-Means Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/03_implementation.html">Implementation: K-Means (Lloyd)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/04_image_segmentation.html">Application: Image Compression and Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/05_conceptual_questions.html">Conceptual Questions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../naive_bayes/01_intro.html">Naive Bayes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/03_implementation.html">Naives Bayes Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/04_example_penguins.html">Naive Bayes Application: Penguins</a></li>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/05_application_mnist.html">Naive Bayes Application (MNIST)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../gaussian_mixture_models/01_intro.html">Mixture Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../gaussian_mixture_models/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gaussian_mixture_models/03_implementation.html">Gaussian Mixture Models Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="01_intro.html">Linear Regression</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_implementation.html">Implementation</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Playbook</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../playbook/training/intro.html">Training Dynamics And Tricks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../playbook/training/how_to_calculate_flops_in_transformer_based_models.html">How to Calculate the Number of FLOPs in Transformer Based Models?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../playbook/training/why_cosine_annealing_warmup_stabilize_training.html">Why Does Cosine Annealing With Warmup Stabilize Training?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../playbook/training/how_to_finetune_decoder_with_last_token_pooling.html">How To Fine-Tune Decoder-Only Models For Sequence Classification Using Last Token Pooling?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../playbook/training/how_to_finetune_decoder_with_cross_attention.html">How To Fine-Tune Decoder-Only Models For Sequence Classification With Cross-Attention?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../playbook/training/how_to_teacher_student_knowledge_distillation.html">How To Do Teacher-Student Knowledge Distillation?</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html">Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling.</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_inspect_function_and_class_signatures.html">How to Inspect Function and Class Signatures in Python?</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Probability Theory</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/intro.html">Chapter 1. Mathematical Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/01_combinatorics.html">Permutations and Combinations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/02_calculus.html">Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/03_contours.html">Contour Maps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/02_probability/intro.html">Chapter 2. Probability</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0202_probability_space.html">Probability Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0203_probability_axioms.html">Probability Axioms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0204_conditional_probability.html">Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0205_independence.html">Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0206_bayes_theorem.html">Baye’s Theorem and the Law of Total Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/summary.html">Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/intro.html">Chapter 3. Discrete Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0301_random_variables.html">Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0302_discrete_random_variables.html">Discrete Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0303_probability_mass_function.html">Probability Mass Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0304_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0305_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0306_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/intro.html">Discrete Uniform Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/intro.html">Bernoulli Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/iid.html">Independent and Identically Distributed (IID)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/intro.html">Binomial Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_implementation.html">Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_application.html">Real World Examples</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/geometric/intro.html">Geometric Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/geometric/0310_geometric_distribution_concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/intro.html">Poisson Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_implementation.html">Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/summary.html">Important</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/intro.html">Chapter 4. Continuous Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/from_discrete_to_continuous.html">From Discrete to Continuous</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0401_continuous_random_variables.html">Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0402_probability_density_function.html">Probability Density Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0403_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0404_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0405_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0406_mean_median_mode.html">Mean, Median and Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0407_continuous_uniform_distribution.html">Continuous Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0408_exponential_distribution.html">Exponential Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0409_gaussian_distribution.html">Gaussian Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0410_skewness_and_kurtosis.html">Skewness and Kurtosis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0411_convolve_and_sum_of_random_variables.html">Convolution and Sum of Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0412_functions_of_random_variables.html">Functions of Random Variables</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/intro.html">Chapter 5. Joint Distributions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/05_joint_distributions/from_single_variable_to_joint_distributions.html">From Single Variable to Joint Distributions</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/intro.html">Joint PMF and PDF</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/intro.html">Joint Expectation and Correlation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/intro.html">Conditional PMF and PDF</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/intro.html">Conditional Expectation and Variance</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0505_sum_of_random_variables/intro.html">Sum of Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0505_sum_of_random_variables/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0506_random_vectors/intro.html">Random Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0506_random_vectors/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/intro.html">Multivariate Gaussian Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/application_transformation.html">Application: Plots and Transformations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/psd.html">Covariance Matrix is Positive Semi-Definite</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/eigendecomposition.html">Eigendecomposition and Covariance Matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/geometry_of_multivariate_gaussian.html">The Geometry of Multivariate Gaussians</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/intro.html">Chapter 6. Sample Statistics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/intro.html">Moment Generating and Characteristic Functions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function.html">Moment Generating Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function_application_sum_of_rv.html">Application: Moment Generating Function and the Sum of Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/characteristic_function.html">Characteristic Function</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/intro.html">Probability Inequalities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/concept.html">Probability Inequalities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/application.html">Application: Learning Theory</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/intro.html">Law of Large Numbers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/convergence.html">Convergence of Sample Average</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/application.html">Application: Learning Theory</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/08_estimation_theory/intro.html">Chapter 8. Estimation Theory</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/08_estimation_theory/maximum_likelihood_estimation/intro.html">Maximum Likelihood Estimation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html">Concept</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Operations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../operations/distributed/intro.html">Distributed Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/01_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/02_basics.html">Basics Of Distributed Data Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/03_how_to_setup_slurm_in_aws.html">How to Setup SLURM and ParallelCluster in AWS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/04_ablation.html">Ablations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../operations/profiling/intro.html">Profiling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/01_synchronize.html">Synchronize CUDA To Time CUDA Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/02_timeit.html">Profiling Code With Timeit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/03_time_profiler.html">PyTorch’s Event And Profiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/04_small_gpt_profile.html">Profile GPT Small Time And Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/05_memory_leak.html">CUDA Memory Allocations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../operations/machine_learning_lifecycle/00_intro.html">The Lifecycle of an AIOps System</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/01_problem_formulation.html">Stage 1. Problem Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/02_project_scoping.html">Stage 2. Project Scoping And Framing The Problem</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/03_dataops_pipeline.html">Stage 3. Data Pipeline (Data Engineering and DataOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/031_data_source_and_format.html">Stage 3.1. Data Source and Formats</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/032_data_model_and_storage.html">Stage 3.2. Data Model and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/033_etl.html">Stage 3.3. Extract, Transform, Load (ETL)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/04_mlops_data_pipeline.html">Stage 4. Data Extraction (MLOps), Data Analysis (Data Science), Data Preparation (Data Science)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/05_ml_training_pipeline.html">Stage 5. Model Development and Training (MLOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/051_model_selection.html">Stage 5.1. Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/052_metric_selection.html">Stage 5.2. Metric Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/053_experiment_tracking.html">Stage 5.3. Experiment Tracking And Versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/054_model_testing.html">Stage 5.4. Model Testing</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/06_model_evaluation.html">Stage 6. Model Evaluation (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/07_model_validation_registry_and_pushing_model_to_production.html">Stage 7. Model Validation, Registry and Pushing Model to Production (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/08_model_deployment_and_serving.html">Stage 8. Model Serving (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/09_model_monitoring.html">Stage 9. Model Monitoring (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/010_continuous_integration_deployment_learning_and_training.html">Stage 10. Continuous Integration, Deployment, Learning and Training (DevOps, DataOps, MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/011_infrastructure_and_tooling_for_mlops.html">Stage 11. Infrastructure and Tooling for MLOps</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/config_management/intro.html">Config, State, Metadata Management</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/config_management/concept.html">Configuration Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/config_management/01-pydra.html">Pydantic And Hydra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/config_management/02-state.html">State And Metadata Management</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/design_patterns/intro.html">Design Patterns</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/dependency_inversion_principle.html">Dependency Inversion Principle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/named_constructor.html">Named Constructor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/strategy.html">Strategy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/registry.html">Registry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/god_object_pattern.html">Context Object Pattern (God Object)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/factory_method.html">Factory Method</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/python/intro.html">Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/decorator.html">Decorator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/pydantic.html">Pydantic Is All You Need - Jason Liu</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/intro.html">Concurrency, Parallelism and Asynchronous Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/overview.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/generator_yield.html">A Rudimentary Introduction to Generator and Yield in Python</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../computer_science/type_theory/intro.html">Type Theory, A Very Rudimentary Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/01-subtypes.html">Subtypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/02-type-safety.html">Type Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/03-subsumption.html">Subsumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/04-generics.html">Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/05-typevar-bound-constraints.html">Bound and Constraint in Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/06-invariance-covariance-contravariance.html">Invariance, Covariance and Contravariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/07-pep-3124-overloading.html">Function Overloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/08-pep-661-sentinel-values.html">Sentinel Types</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Structures and Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/complexity_analysis/intro.html">Complexity Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/complexity_analysis/master_theorem.html">Master Theorem </a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/stack/intro.html">Stack</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/stack/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/intro.html">Linear Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/intro.html">Binary Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/problems/875-koko-eating-bananas.html">Koko Eating Bananas</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/01_preliminaries/intro.html">Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/01-fields.html">Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/02-systems-of-linear-equations.html">Systems of Linear Equations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/02_vectors/intro.html">Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/01-vector-definition.html">Vector and Its Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/02-vector-operation.html">Vector and Its Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/03-vector-norm.html">Vector Norm and Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/04-vector-products.html">A First Look at Vector Products</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../citations.html">IEEE (Style) Citations</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/gao-hongnan/omniverse/blob/main/omniverse/influential/linear_regression/02_concept.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse/issues/new?title=Issue%20on%20page%20%2Finfluential/linear_regression/02_concept.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/influential/linear_regression/02_concept.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../../_sources/influential/linear_regression/02_concept.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Concept</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">Intuition</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#univariate-linear-model">Univariate Linear Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-linear-model">Multivariate Linear Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-hypothesis-space">The Hypothesis Space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-loss-function">The Loss Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-formulation">Problem Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-normal-distribution-gaussian-noise-and-the-mean-squared-error">The Normal Distribution, Gaussian Noise and the Mean Squared Error</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#target-variable-is-normally-distributed">Target Variable is Normally Distributed</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-conditional-distribution-of-y-given-mathbf-x-is-normally-distributed">The Conditional Distribution of <span class="math notranslate nohighlight">\(y\)</span> Given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is Normally Distributed</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-estimates-the-conditional-expectation">Mean Squared Error estimates the Conditional Expectation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-in-parameters">Linear in Parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basis-function-and-non-linearity">Basis Function and Non-Linearity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-representation">Matrix Representation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#break-down-of-the-matrix-representation">Break down of the Matrix Representation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-space">Hypothesis Space</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">Loss Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-function">Cost Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convexity-and-differentiability">Convexity and Differentiability</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#objective-function">Objective Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-fitting-via-least-squares">Model Fitting via Least Squares</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-for-1-dimensional-case">Solving for 1-Dimensional Case</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-for-d-dimensional-case">Solving for <span class="math notranslate nohighlight">\(D\)</span>-Dimensional Case</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-fitting-via-maximum-likelihood-estimation">Model Fitting via Maximum Likelihood Estimation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iid-assumption">IID Assumption</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-likelihood-function">Conditional Likelihood Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-log-likelihood-function">Conditional Log-Likelihood Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation">Maximum Likelihood Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-estimates-the-conditional-expectation-of-the-target-variable-given-the-input">MLE estimates the conditional expectation of the target variable given the input</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-metrics">Performance Metrics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-mse">Mean Squared Error (MSE)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#r-squared">R-Squared</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adjusted-r-squared">Adjusted R-Squared</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions-of-linear-regression">Assumptions of Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linearity">Linearity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#homoscedasticity">Homoscedasticity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normality-of-the-error-terms">Normality of the Error Terms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#no-autocorrelation-between-error-terms">No Autocorrelation between Error Terms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multicollinearity-among-predictors">Multicollinearity among Predictors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-scaling">Feature Scaling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-inference">Model Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-is-an-unbiased-estimator-of-the-variance-of-the-error">Mean Squared Error is an Unbiased Estimator of the Variance of the Error</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimated-coefficients-are-multivariate-normally-distributed">Estimated Coefficients are Multivariate Normally Distributed</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assessing-the-accuracy-of-the-coefficient-estimates">Assessing the Accuracy of the Coefficient Estimates</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-error">Standard Error</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#t-statistics">T-Statistics</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-t-statistics">Implementing T-Statistics</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#t-distributions">T-Distributions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#p-values">P-Values</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-pandora-box-of-confidence-intervals-p-values-and-hypothesis-testing">The Pandora Box of Confidence Intervals, P-values and Hypothesis Testing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#time-complexity">Time Complexity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="concept">
<h1><a class="toc-backref" href="#id13" role="doc-backlink">Concept</a><a class="headerlink" href="#concept" title="Link to this heading">#</a></h1>
<nav class="contents" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#concept" id="id13">Concept</a></p>
<ul>
<li><p><a class="reference internal" href="#intuition" id="id14">Intuition</a></p>
<ul>
<li><p><a class="reference internal" href="#univariate-linear-model" id="id15">Univariate Linear Model</a></p></li>
<li><p><a class="reference internal" href="#multivariate-linear-model" id="id16">Multivariate Linear Model</a></p></li>
<li><p><a class="reference internal" href="#the-hypothesis-space" id="id17">The Hypothesis Space</a></p></li>
<li><p><a class="reference internal" href="#the-loss-function" id="id18">The Loss Function</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#problem-formulation" id="id19">Problem Formulation</a></p></li>
<li><p><a class="reference internal" href="#the-normal-distribution-gaussian-noise-and-the-mean-squared-error" id="id20">The Normal Distribution, Gaussian Noise and the Mean Squared Error</a></p>
<ul>
<li><p><a class="reference internal" href="#target-variable-is-normally-distributed" id="id21">Target Variable is Normally Distributed</a></p></li>
<li><p><a class="reference internal" href="#the-conditional-distribution-of-y-given-mathbf-x-is-normally-distributed" id="id22">The Conditional Distribution of <span class="math notranslate nohighlight">\(y\)</span> Given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is Normally Distributed</a></p></li>
<li><p><a class="reference internal" href="#mean-squared-error-estimates-the-conditional-expectation" id="id23">Mean Squared Error estimates the Conditional Expectation</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#linear-in-parameters" id="id24">Linear in Parameters</a></p></li>
<li><p><a class="reference internal" href="#basis-function-and-non-linearity" id="id25">Basis Function and Non-Linearity</a></p></li>
<li><p><a class="reference internal" href="#matrix-representation" id="id26">Matrix Representation</a></p>
<ul>
<li><p><a class="reference internal" href="#break-down-of-the-matrix-representation" id="id27">Break down of the Matrix Representation</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#hypothesis-space" id="id28">Hypothesis Space</a></p></li>
<li><p><a class="reference internal" href="#loss-function" id="id29">Loss Function</a></p></li>
<li><p><a class="reference internal" href="#cost-function" id="id30">Cost Function</a></p>
<ul>
<li><p><a class="reference internal" href="#convexity-and-differentiability" id="id31">Convexity and Differentiability</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#objective-function" id="id32">Objective Function</a></p></li>
<li><p><a class="reference internal" href="#model-fitting-via-least-squares" id="id33">Model Fitting via Least Squares</a></p>
<ul>
<li><p><a class="reference internal" href="#solving-for-1-dimensional-case" id="id34">Solving for 1-Dimensional Case</a></p></li>
<li><p><a class="reference internal" href="#solving-for-d-dimensional-case" id="id35">Solving for <span class="math notranslate nohighlight">\(D\)</span>-Dimensional Case</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#model-fitting-via-maximum-likelihood-estimation" id="id36">Model Fitting via Maximum Likelihood Estimation</a></p>
<ul>
<li><p><a class="reference internal" href="#iid-assumption" id="id37">IID Assumption</a></p></li>
<li><p><a class="reference internal" href="#conditional-likelihood-function" id="id38">Conditional Likelihood Function</a></p></li>
<li><p><a class="reference internal" href="#conditional-log-likelihood-function" id="id39">Conditional Log-Likelihood Function</a></p></li>
<li><p><a class="reference internal" href="#maximum-likelihood-estimation" id="id40">Maximum Likelihood Estimation</a></p></li>
<li><p><a class="reference internal" href="#mle-estimates-the-conditional-expectation-of-the-target-variable-given-the-input" id="id41">MLE estimates the conditional expectation of the target variable given the input</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#performance-metrics" id="id42">Performance Metrics</a></p>
<ul>
<li><p><a class="reference internal" href="#mean-squared-error-mse" id="id43">Mean Squared Error (MSE)</a></p></li>
<li><p><a class="reference internal" href="#r-squared" id="id44">R-Squared</a></p></li>
<li><p><a class="reference internal" href="#adjusted-r-squared" id="id45">Adjusted R-Squared</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#assumptions-of-linear-regression" id="id46">Assumptions of Linear Regression</a></p>
<ul>
<li><p><a class="reference internal" href="#linearity" id="id47">Linearity</a></p></li>
<li><p><a class="reference internal" href="#homoscedasticity" id="id48">Homoscedasticity</a></p></li>
<li><p><a class="reference internal" href="#normality-of-the-error-terms" id="id49">Normality of the Error Terms</a></p></li>
<li><p><a class="reference internal" href="#no-autocorrelation-between-error-terms" id="id50">No Autocorrelation between Error Terms</a></p></li>
<li><p><a class="reference internal" href="#multicollinearity-among-predictors" id="id51">Multicollinearity among Predictors</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#feature-scaling" id="id52">Feature Scaling</a></p></li>
<li><p><a class="reference internal" href="#model-inference" id="id53">Model Inference</a></p></li>
<li><p><a class="reference internal" href="#mean-squared-error-is-an-unbiased-estimator-of-the-variance-of-the-error" id="id54">Mean Squared Error is an Unbiased Estimator of the Variance of the Error</a></p></li>
<li><p><a class="reference internal" href="#estimated-coefficients-are-multivariate-normally-distributed" id="id55">Estimated Coefficients are Multivariate Normally Distributed</a></p></li>
<li><p><a class="reference internal" href="#assessing-the-accuracy-of-the-coefficient-estimates" id="id56">Assessing the Accuracy of the Coefficient Estimates</a></p>
<ul>
<li><p><a class="reference internal" href="#standard-error" id="id57">Standard Error</a></p></li>
<li><p><a class="reference internal" href="#t-statistics" id="id58">T-Statistics</a></p>
<ul>
<li><p><a class="reference internal" href="#implementing-t-statistics" id="id59">Implementing T-Statistics</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#t-distributions" id="id60">T-Distributions</a></p></li>
<li><p><a class="reference internal" href="#p-values" id="id61">P-Values</a></p></li>
<li><p><a class="reference internal" href="#the-pandora-box-of-confidence-intervals-p-values-and-hypothesis-testing" id="id62">The Pandora Box of Confidence Intervals, P-values and Hypothesis Testing</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#time-complexity" id="id63">Time Complexity</a></p></li>
<li><p><a class="reference internal" href="#references-and-further-readings" id="id64">References and Further Readings</a></p></li>
</ul>
</li>
</ul>
</nav>
<p>This section details the linear regression model from a probabilistic perspective.</p>
<blockquote>
<div><p>Notation is not consistent when using <span class="math notranslate nohighlight">\(\hat{h}\)</span> and <span class="math notranslate nohighlight">\(h\)</span> in the
problem formulation section. Same for <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> and
<span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>. I have mostly standardize the notation for the rest of
the sections but have not checked the usage of <span class="math notranslate nohighlight">\(\hat{h}\)</span>.</p>
</div></blockquote>
<section id="intuition">
<h2><a class="toc-backref" href="#id14" role="doc-backlink">Intuition</a><a class="headerlink" href="#intuition" title="Link to this heading">#</a></h2>
<p>As a running example, suppose that we wish
to estimate the prices of houses (in dollars)
based on their area (in square feet) and age (in years).
To develop a model for predicting house prices,
we need to get our hands on data consisting of sales,
including the sales price, area, and age for each home.
In the terminology of machine learning,
the dataset is called a <em>training dataset</em> or <em>training set</em>,
and each row (containing the data corresponding to one sale)
is called an <em>example</em> (or <em>data point</em>, <em>instance</em>, <em>sample</em>).
The thing we are trying to predict (price)
is called a <em>label</em> (or <em>target</em>).
The variables (age and area)
upon which the predictions are based
are called <em>features</em> (or <em>covariates</em>) <span id="id1">[]</span>.</p>
<p>Let’s create a housing dataset below with <span class="math notranslate nohighlight">\(100\)</span> samples.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>House Size (sqft)</th>
      <th>Age (years)</th>
      <th>Price ($)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>51</td>
      <td>25</td>
      <td>163551</td>
    </tr>
    <tr>
      <th>1</th>
      <td>92</td>
      <td>24</td>
      <td>201654</td>
    </tr>
    <tr>
      <th>2</th>
      <td>14</td>
      <td>44</td>
      <td>121846</td>
    </tr>
    <tr>
      <th>3</th>
      <td>71</td>
      <td>40</td>
      <td>186793</td>
    </tr>
    <tr>
      <th>4</th>
      <td>60</td>
      <td>28</td>
      <td>170572</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>95</th>
      <td>84</td>
      <td>0</td>
      <td>171542</td>
    </tr>
    <tr>
      <th>96</th>
      <td>79</td>
      <td>26</td>
      <td>193731</td>
    </tr>
    <tr>
      <th>97</th>
      <td>81</td>
      <td>12</td>
      <td>190853</td>
    </tr>
    <tr>
      <th>98</th>
      <td>52</td>
      <td>40</td>
      <td>163161</td>
    </tr>
    <tr>
      <th>99</th>
      <td>23</td>
      <td>2</td>
      <td>125537</td>
    </tr>
  </tbody>
</table>
<p>100 rows × 3 columns</p>
</div></div></div>
</div>
<p>The goal is to predict the price of a house given its size. We can visualize the
relationship between the house size and the price using a scatter plot.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;House Size (sqft)&#39;</span><span class="p">]</span>
<span class="linenos">2</span><span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Price ($)&#39;</span><span class="p">]</span>
<span class="linenos">3</span>
<span class="linenos">4</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="linenos">5</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;House Size (sqft)&#39;</span><span class="p">)</span>
<span class="linenos">6</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Price ($)&#39;</span><span class="p">)</span>
<span class="linenos">7</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/86bd5f68b1981738bca835fca7cc524180a06c6b169dbca85289ddefa12067f1.svg" src="../../_images/86bd5f68b1981738bca835fca7cc524180a06c6b169dbca85289ddefa12067f1.svg" />
</div>
</div>
<section id="univariate-linear-model">
<h3><a class="toc-backref" href="#id15" role="doc-backlink">Univariate Linear Model</a><a class="headerlink" href="#univariate-linear-model" title="Link to this heading">#</a></h3>
<p>Let’s try to model the relationship between the house size (sqft) and the price ($). The assumption of linearity means that the expected value of the target (price) can be expressed as a
linear function of the feature (sqft).</p>
<div class="math notranslate nohighlight" id="equation-eq-price-area">
<span class="eqno">(125)<a class="headerlink" href="#equation-eq-price-area" title="Link to this equation">#</a></span>\[
\text{price} = \beta_0 + \beta_1 \cdot \text{sqft} \iff y = \beta_0 + \beta_1 x
\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept, <span class="math notranslate nohighlight">\(\beta_1\)</span> the coefficient, <span class="math notranslate nohighlight">\(x\)</span> the feature (sqft) and <span class="math notranslate nohighlight">\(y\)</span> the target (price).</p>
<p>Here the coefficient <span class="math notranslate nohighlight">\(\beta_1\)</span> is called the <em>weights</em> and the intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> is called the <em>bias</em>.</p>
<p>The weights determine the influence of each feature on our prediction, this means that
a larger weight indicates that the feature is more important for predicting the target.</p>
<p>The bias determines the value of the estimate when all features are zero. Why do we need this?
Will there ever be a case where the house size (sqft) is <span class="math notranslate nohighlight">\(0\)</span>? No, but we still need the bias because it allows us
to express the functional/hypothesis space of linear functions (more on that later). Intuitively,
if there is no bias, the equation will always pass through the origin, which means that the model
can never vary up or down.</p>
<div class="proof remark admonition" id="prf:remark-bias">
<p class="admonition-title"><span class="caption-number">Remark 35 </span> (Why do we need a bias?)</p>
<section class="remark-content" id="proof-content">
<p>If there is no bias term in the model, you will lose the flexibility of your model. Imagine a simple linear regression model without a bias term, then your linear equation <span class="math notranslate nohighlight">\(y=mx\)</span> will only pass through the origin. Therefore, if your underlying data (pretend that we know that the underlying data’s actual function <span class="math notranslate nohighlight">\(y = 3x + 5\)</span>), then your Linear Regression model will never find the “best fit line” simply because we already assume that our prediction is governed by the slope <span class="math notranslate nohighlight">\(m\)</span>, and there does not exist our <span class="math notranslate nohighlight">\(c\)</span>, the intercept/bias term.</p>
</section>
</div><p>Therefore, it is usually the case whereby we always add in an intercept term. We intend to estimate the values of <span class="math notranslate nohighlight">\(y\)</span> <em><strong>given</strong></em> <span class="math notranslate nohighlight">\(x\)</span>. Each value of <span class="math notranslate nohighlight">\(x\)</span> is multiplied by <span class="math notranslate nohighlight">\(\beta_{1}\)</span>, with a constant intercept term <span class="math notranslate nohighlight">\(\beta_{0}\)</span>. We also note that <span class="math notranslate nohighlight">\(1\)</span> unit increase in <span class="math notranslate nohighlight">\(x\)</span> will correspond to a <span class="math notranslate nohighlight">\(\beta_1\)</span> unit increase in <span class="math notranslate nohighlight">\(y\)</span> according to the model, while always remebering to add the intercept term.</p>
<p>The equation <a class="reference internal" href="#equation-eq-price-area">(125)</a> is an <a class="reference external" href="https://en.wikipedia.org/wiki/Affine_transformation"><strong>affine transformation</strong></a> of the input features. The difference between an affine transformation and a linear transformation is that an affine transformation has a translation (bias) component. In other words,
an affine transformation is a linear transformation combined with a translation.</p>
<p>Our goal is to choose
the weights <span class="math notranslate nohighlight">\(\beta_1\)</span> and the bias <span class="math notranslate nohighlight">\(\beta_0\)</span> such that, on average, make our model’s predictions
fit the true prices observed in the data as closely as possible <span id="id2">[]</span>.
This also means being able to extrapolate to new data points that were not part of the training set.</p>
<p>For example, the best fit line for the data above is shown below.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;House Size (sqft)&#39;</span><span class="p">]</span>
<span class="linenos">2</span><span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Price ($)&#39;</span><span class="p">]</span>
<span class="linenos">3</span>
<span class="linenos">4</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="linenos">5</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="linenos">6</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;House Size (sqft)&#39;</span><span class="p">)</span>
<span class="linenos">7</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Price ($)&#39;</span><span class="p">)</span>
<span class="linenos">8</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/afc84f203bd0d601cb2ae4f850d20b4bfa91da023154dc129eb89e942d69b78a.svg" src="../../_images/afc84f203bd0d601cb2ae4f850d20b4bfa91da023154dc129eb89e942d69b78a.svg" />
</div>
</div>
<p>Our goal is to find that red line among all possible lines that best fits the data.</p>
</section>
<section id="multivariate-linear-model">
<h3><a class="toc-backref" href="#id16" role="doc-backlink">Multivariate Linear Model</a><a class="headerlink" href="#multivariate-linear-model" title="Link to this heading">#</a></h3>
<p>It is often the case that there are multiple
explanatory variables (features). As a continuation of the example above,
let’s say that we want to predict the price of a house given its size and age, we
would then have the linear equation below.</p>
<div class="math notranslate nohighlight" id="equation-eq-price-area-age">
<span class="eqno">(126)<a class="headerlink" href="#equation-eq-price-area-age" title="Link to this equation">#</a></span>\[
\text{price} = \beta_0 + \beta_1 \cdot \text{sqft} + \beta_2 \cdot \text{age} \iff y = \beta_0 + \beta_1 x_1 + \beta_2 x_2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept, <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_2\)</span> the coefficients, <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> the features (sqft and age) and <span class="math notranslate nohighlight">\(y\)</span> the target (price).</p>
<p>Then our task is to find the best values for the weights <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\beta_2\)</span> and the bias <span class="math notranslate nohighlight">\(\beta_0\)</span>
such that the model’s predictions are as close as possible to the true prices observed in the data.</p>
</section>
<section id="the-hypothesis-space">
<h3><a class="toc-backref" href="#id17" role="doc-backlink">The Hypothesis Space</a><a class="headerlink" href="#the-hypothesis-space" title="Link to this heading">#</a></h3>
<p>As with any machine learning problem, we need to define a hypothesis space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, which is the set of all possible models that we consider. In the case of linear regression, the hypothesis space is the set of all possible linear functions.</p>
<div class="math notranslate nohighlight">
\[
\mathcal{H} = \left\{ h \in \mathbb{R}^{D} \rightarrow \mathbb{R} \mid h\left(\mathbf{x}; \boldsymbol{\beta}\right) = \boldsymbol{\beta}^T \mathbf{x} + \beta_0 \text{ for some parameter/weight vector } \boldsymbol{\beta} \in \mathbb{R}^{D}\right\}
\]</div>
<p>where we specify</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^{D}\)</span> is the input vector,</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\beta} \in \mathbb{R}^{D}\)</span> is the weight vector,</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta_0 \in \mathbb{R}\)</span> is the bias,</p></li>
<li><p><span class="math notranslate nohighlight">\(h\)</span> as a function of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>,
and we put <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> after <span class="math notranslate nohighlight">\(;\)</span> to indicate that <span class="math notranslate nohighlight">\(h\)</span> is parameterized by <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p></li>
</ul>
<p>We will explain them in more detail in the next few sections.</p>
</section>
<section id="the-loss-function">
<h3><a class="toc-backref" href="#id18" role="doc-backlink">The Loss Function</a><a class="headerlink" href="#the-loss-function" title="Link to this heading">#</a></h3>
<p>Once we have defined our hypothesis, we need to define a loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> that measures how good a given hypothesis <span class="math notranslate nohighlight">\(h\)</span> is.</p>
<p>The loss in the case of linear regression is usually the <a class="reference external" href="https://en.wikipedia.org/wiki/Mean_squared_error"><strong>mean squared error loss</strong></a> (MSE),
which we will use in this section.</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L} = \mathbb{E}_{\mathbf{x}, y \sim \mathcal{D}}\left[\left(y - h\left(\mathbf{x}; \boldsymbol{\beta}\right)\right)^{2}\right]
\]</div>
<p>and via the ERM framework we deal with empirical loss <span class="math notranslate nohighlight">\(\widehat{\mathcal{L}}\)</span>
which we will make precise later.</p>
</section>
</section>
<section id="problem-formulation">
<h2><a class="toc-backref" href="#id19" role="doc-backlink">Problem Formulation</a><a class="headerlink" href="#problem-formulation" title="Link to this heading">#</a></h2>
<p>The main formulation is formulated based on Foundations of Machine Learning <span id="id3">[]</span>.</p>
<p>Consider <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> as our input space, usually <span class="math notranslate nohighlight">\(\mathbb{R}^{D}\)</span>, and <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> a measurable subset of <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>,
we adopt the probablistic framework and denote <span class="math notranslate nohighlight">\(\mathbb{P}_{\mathcal{D}}\)</span> as the probability distribution over <span class="math notranslate nohighlight">\(\mathcal{X} \times \mathcal{Y}\)</span>.
The deterministic scenario is merely a special case where <span class="math notranslate nohighlight">\(\mathbb{P}_{\mathcal{D}}\)</span> is a Dirac delta function<a class="footnote-reference brackets" href="#dirac-delta" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, and that we have
a ground truth function <span class="math notranslate nohighlight">\(f: \mathcal{X} \rightarrow \mathcal{Y}\)</span> that uniquely determines the
target <span class="math notranslate nohighlight">\(y \in \mathcal{Y}\)</span> for any input <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{X}\)</span>.</p>
<p>Then the linear regression model has the following form:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-1">
<span class="eqno">(127)<a class="headerlink" href="#equation-eq-linear-regression-1" title="Link to this equation">#</a></span>\[\begin{split}
\left|
\begin{array}{l}
y := f\left(\mathbf{x}\right) + \varepsilon = \beta_0 + \boldsymbol{\beta}^T \mathbf{x} + \varepsilon \\
 \boldsymbol{\varepsilon} \overset{\small{\text{i.i.d.}}}{\sim} \mathcal{N}(0, \sigma^2) \\
\end{array}
\right.
\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\varepsilon\)</span> is a random variable drawn <span class="math notranslate nohighlight">\(i.i.d.\)</span> from a zero-mean Gaussian distribution with variance <span class="math notranslate nohighlight">\(\sigma^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(f\)</span> is the <strong>unknown</strong> ground truth linear function of the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p></li>
</ul>
<p>To be more precise, the <span class="math notranslate nohighlight">\(\varepsilon^{(1)}, \ldots, \varepsilon^{(N)}\)</span> are
all conditionally <span class="math notranslate nohighlight">\(i.i.d.\)</span> given <span class="math notranslate nohighlight">\(\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(N)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\left\{\varepsilon^{(i)} \mid \mathbf{x}^{(n)}\right\}_{n=1}^{N} \overset{\small{\text{i.i.d.}}}{\sim} \mathcal{N}(0, \sigma^2)
\]</div>
<p>It follows that (explained later) that we can interpret <span class="math notranslate nohighlight">\(y\)</span> as a realization of a random variable
<span class="math notranslate nohighlight">\(Y\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-2">
<span class="eqno">(128)<a class="headerlink" href="#equation-eq-linear-regression-2" title="Link to this equation">#</a></span>\[
Y \overset{\small{\text{i.i.d.}}}{\sim} \mathcal{N}\left(\beta_0 + \boldsymbol{\beta}^T \mathbf{x}, \sigma^2\right)
\]</div>
<p>and whether <span class="math notranslate nohighlight">\(Y\)</span> is indeed <span class="math notranslate nohighlight">\(i.i.d.\)</span> depends on if we treat <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> as a realization of a random variable <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>
or not.</p>
<p>In any case, we are interested in finding <span class="math notranslate nohighlight">\(y \mid \mathbf{x}\)</span>, so we can also
say that:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-3">
<span class="eqno">(129)<a class="headerlink" href="#equation-eq-linear-regression-3" title="Link to this equation">#</a></span>\[
Y \mid \mathbf{X} \sim \mathcal{N}\left(\beta_0 + \boldsymbol{\beta}^T \mathbf{x}, \sigma^2\right)
\]</div>
<p>which we will show later.</p>
<p>Our goal is to learn a <strong>linear</strong> hypothesis <span class="math notranslate nohighlight">\(h\)</span> that approximates <span class="math notranslate nohighlight">\(f\)</span> as closely as possible, both in
terms of the generalization error and the training error. In other words, we want to find a good estimate
<span class="math notranslate nohighlight">\(\hat{h}\)</span> of the ground truth function <span class="math notranslate nohighlight">\(f\)</span> in <a class="reference internal" href="#equation-eq-linear-regression-1">(127)</a> such that the error between
<span class="math notranslate nohighlight">\(\hat{h}\)</span> and <span class="math notranslate nohighlight">\(f\)</span> is small.</p>
<hr class="docutils" />
<p>As in all supervised learning problems, the learner <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> receives a labeled sample dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>
containing <span class="math notranslate nohighlight">\(N\)</span> i.i.d. samples <span class="math notranslate nohighlight">\(\left(\mathbf{x}^{(n)}, y^{(n)}\right)\)</span> drawn from <span class="math notranslate nohighlight">\(\mathbb{P}_{\mathcal{D}}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-dataset-linear-regression">
<span class="eqno">(130)<a class="headerlink" href="#equation-eq-dataset-linear-regression" title="Link to this equation">#</a></span>\[
\mathcal{S} = \left\{\left(\mathbf{x}^{(1)}, y^{(1)}\right), \left(\mathbf{x}^{(2)}, y^{(2)}\right), \ldots, \left(\mathbf{x}^{(N)}, y^{(N)}\right)\right\} \subset \mathbb{R}^{D} \quad \overset{\small{\text{i.i.d.}}}{\sim} \quad \mathbb{P}_{\mathcal{D}}\left(\mathcal{X}, \mathcal{Y} ; \boldsymbol{\beta}\right)
\]</div>
<p>Subsequently, the learner <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> will learn a linear hypothesis (linear in the parameters)
<span class="math notranslate nohighlight">\(h \in \mathcal{H}\)</span> that minimizes a certain defined loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>.</p>
<div class="proof remark admonition" id="prf-remark-iid">
<p class="admonition-title"><span class="caption-number">Remark 36 </span> (The i.i.d. assumption)</p>
<section class="remark-content" id="proof-content">
<p>Something to note is that if the data points for <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is fixed and known, which is a common
assumption in many formulations, then only the error term <span class="math notranslate nohighlight">\(\varepsilon\)</span> is drawn <span class="math notranslate nohighlight">\(i.i.d.\)</span>.</p>
</section>
</div><hr class="docutils" />
<p>More concretely, define
the learned hypothesis <span class="math notranslate nohighlight">\(\hat{h}\)</span> to be of the following form:</p>
<div class="math notranslate nohighlight" id="equation-eq-hypothesis-1">
<span class="eqno">(131)<a class="headerlink" href="#equation-eq-hypothesis-1" title="Link to this equation">#</a></span>\[\begin{split}
\hat{y} := \hat{h}\left(\mathbf{x}; \hat{\boldsymbol{\beta}}\right) &amp;= \hat{\beta}_0 + \hat{\beta}_1 x_1 + \ldots + \hat{\beta}_D x_D \\
&amp;= \hat{\beta}_0 + \hat{\boldsymbol{\beta}}^T \mathbf{x}
\end{split}\]</div>
<p>where we collected all features <span class="math notranslate nohighlight">\(x_1, \ldots, x_D\)</span> into a vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and all weights <span class="math notranslate nohighlight">\(\hat{\beta}_1, \ldots, \hat{\beta}_D\)</span> into a vector <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>.
Note that the “hat” symbol is used to denote an estimated quantity.</p>
<p>It is also typical to add a bias term <span class="math notranslate nohighlight">\(1\)</span> to the input vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>
and a bias weight <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> to the weight vector <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>
for a more compact notation:</p>
<div class="math notranslate nohighlight" id="equation-eq-hypothesis-2">
<span class="eqno">(132)<a class="headerlink" href="#equation-eq-hypothesis-2" title="Link to this equation">#</a></span>\[
\hat{y} := \hat{h}\left(\mathbf{x}; \hat{\boldsymbol{\beta}}\right) = \hat{\boldsymbol{\beta}}^T \mathbf{x}
\]</div>
<p>where</p>
<ul>
<li><p>the vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is re-defined to be of the form below:</p>
<div class="math notranslate nohighlight" id="equation-eq-x">
<span class="eqno">(133)<a class="headerlink" href="#equation-eq-x" title="Link to this equation">#</a></span>\[\begin{split}
    \mathbf{x} = \begin{bmatrix}1 \\ x_1 \\ \vdots \\ x_D\end{bmatrix}_{(D+1) \times 1}
    \end{split}\]</div>
<p>and <span class="math notranslate nohighlight">\(x_1, \ldots, x_D\)</span> are the features of the input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(1\)</span> is the bias term.</p>
</li>
<li><p>the weight vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> is re-defined to be of the form below:</p>
<div class="math notranslate nohighlight" id="equation-eq-beta">
<span class="eqno">(134)<a class="headerlink" href="#equation-eq-beta" title="Link to this equation">#</a></span>\[\begin{split}
    \boldsymbol{\beta} = \begin{bmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_D\end{bmatrix}_{(D+1) \times 1}
    \end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta_0\)</span> is the bias term and <span class="math notranslate nohighlight">\(\beta_1, \ldots, \beta_D\)</span> are the weights of the features <span class="math notranslate nohighlight">\(x_1, \ldots, x_D\)</span>.</p>
</li>
</ul>
<p>Next, we define a loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> that measures the error between the true label
<span class="math notranslate nohighlight">\(y\)</span> and the predicted label <span class="math notranslate nohighlight">\(\hat{h}\left(\mathbf{x}; \boldsymbol{\beta}\right)\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-loss-function-1">
<span class="eqno">(135)<a class="headerlink" href="#equation-eq-linear-regression-loss-function-1" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathcal{L}: \mathcal{X} \times \mathcal{Y} \times \mathcal{H} &amp;\to \mathbb{R}_{+} \\
\left((\mathbf{x}, y), h\right) &amp;\mapsto \mathcal{L}\left(\left(\mathbf{x}, y\right), h\right)
\end{aligned}
\end{split}\]</div>
<p>We present a few equivalent notations for the loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-loss-function-2">
<span class="eqno">(136)<a class="headerlink" href="#equation-eq-linear-regression-loss-function-2" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathcal{L}\left(\left(\mathbf{x}, y\right), h\right) &amp;= \mathcal{L}(y, \hat{y}) \\
&amp;= \mathcal{L}\left(y, \hat{h}\left(\mathbf{x}; \hat{\boldsymbol{\beta}}\right)\right) \\
&amp;= \mathcal{L}\left(\hat{\boldsymbol{\beta}}\right)
\end{aligned}
\end{split}\]</div>
<p>where the last equation is to emphasize that the loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is a function of the weight vector <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>
and not of the input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> for the context of machine learning.</p>
<p>The choice of loss function in regression is often the <a class="reference external" href="https://en.wikipedia.org/wiki/Mean_squared_error"><strong>mean squared error loss</strong></a> (MSE)
defined by:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-loss-function-3">
<span class="eqno">(137)<a class="headerlink" href="#equation-eq-linear-regression-loss-function-3" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathcal{L}\left(\left(\mathbf{x}, y\right), h\right) &amp;= \left(y - \hat{y}\right)^2 \\
&amp;= \left(y - \hat{h}\left(\mathbf{x}; \hat{\boldsymbol{\beta}}\right)\right)^2 \\
&amp;= \left(y - \hat{\boldsymbol{\beta}}^T \mathbf{x} \right)^2 \\
\end{aligned}
\end{split}\]</div>
<p>where we used the fact that <span class="math notranslate nohighlight">\(\hat{y} = \hat{\boldsymbol{\beta}}^T \mathbf{x}\)</span>.</p>
<p>One should not be surprised that we use such a loss function, as compared to other loss functions
such as <a class="reference external" href="https://en.wikipedia.org/wiki/Cross_entropy">cross-entropy loss</a> or
<a class="reference external" href="https://en.wikipedia.org/wiki/Loss_functions_for_classification#Zero-one_loss_function">zero-one loss</a>
in classification. This is because the label <span class="math notranslate nohighlight">\(y\)</span> in regression is a real-valued number
and we are not measuring the error in terms of the equality or inequality of the predicted label <span class="math notranslate nohighlight">\(\hat{y}\)</span> and the true label <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>To this end, we will invoke the <a class="reference external" href="https://en.wikipedia.org/wiki/Empirical_risk_minimization"><strong>Empirical Risk Minimization (ERM)</strong></a>
framework mentioned <a class="reference internal" href="#../../fundamentals/empirical_risk_minimization/concept.md"><span class="xref myst">in our earlier section</span></a>.</p>
<p>Given a hypothesis set <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> of functions mapping <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> to <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>,
the regression problem consists of using the labeled sample <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> <a class="reference internal" href="#equation-eq-dataset-linear-regression">(130)</a>
to find a hypothesis <span class="math notranslate nohighlight">\(h \in \mathcal{H}\)</span> with small expected loss or generalization error <span class="math notranslate nohighlight">\(\mathcal{R}(\hat{h})\)</span> with respect to the target <span class="math notranslate nohighlight">\(f\)</span> :</p>
<div class="math notranslate nohighlight" id="equation-eq-generalization-error-1">
<span class="eqno">(138)<a class="headerlink" href="#equation-eq-generalization-error-1" title="Link to this equation">#</a></span>\[
\mathcal{R}_{\mathcal{D}}(\hat{h})=\underset{(x, y) \sim \mathcal{D}}{\mathbb{E}}[\mathcal{L}\left(y, \hat{h}\left(x\right)\right)]
\]</div>
<p>We will denote <span class="math notranslate nohighlight">\(\mathcal{R}_{\mathcal{D}}\)</span> as <span class="math notranslate nohighlight">\(\mathcal{R}\)</span> when the context is clear.</p>
<p>Then via the Empirical Risk Minimization (ERM) framework, we can define the empirical risk or empirical loss of <span class="math notranslate nohighlight">\(h\)</span> as:</p>
<div class="math notranslate nohighlight" id="equation-eq-empirical-risk-1">
<span class="eqno">(139)<a class="headerlink" href="#equation-eq-empirical-risk-1" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\widehat{\mathcal{R}}_{\mathcal{S}}(\hat{h}) &amp;= \frac{1}{N} \sum_{n=1}^{N} \mathcal{L}\left(y^{(n)}, \hat{h}\left(\mathbf{x}^{(n)}\right)\right) \\
&amp;= \frac{1}{N} \sum_{n=1}^{N} \left(y^{(n)} - \hat{h}\left(\mathbf{x}^{(n)}\right)\right)^2
\end{aligned}
\end{split}\]</div>
<p>It is also very common to denote the empirical risk as <span class="math notranslate nohighlight">\(\widehat{\mathcal{R}}_{\mathcal{S}}\)</span> as <span class="math notranslate nohighlight">\(\widehat{\mathcal{R}}\)</span> when the context is clear.
In addition, in some machine learning texts, the empirical risk is the <a class="reference external" href="https://en.wikipedia.org/wiki/Cost_function"><strong>cost function</strong></a>,
denoted as <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span>.</p>
<hr class="docutils" />
<p>And thus, we task a learner <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>, which can be any algorithm that takes as input the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> that
searches for the optimal parameters <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> that minimize the empirical risk <span class="math notranslate nohighlight">\(\widehat{\mathcal{R}}_{\mathcal{S}}(\hat{h})\)</span>.
We define the objective function as follows:</p>
<div class="math notranslate nohighlight" id="equation-eq-objective-function-1">
<span class="eqno">(140)<a class="headerlink" href="#equation-eq-objective-function-1" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\hat{h} := \underset{\hat{h} \in \mathcal{H}}{\operatorname{argmin}} \widehat{\mathcal{R}}_{\mathcal{S}}(\hat{h}) &amp;= \underset{\hat{h} \in \mathcal{H}}{\operatorname{argmin}} \frac{1}{N} \sum_{n=1}^{N} \left(y^{(n)} - \hat{h}\left(\mathbf{x}^{(n)}\right)\right)^2 \\
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{h}\)</span> is a of the form <span class="math notranslate nohighlight">\(\hat{h}(\mathbf{x}) = \hat{\boldsymbol{\beta}}^T \mathbf{x}\)</span>.</p>
<p>Since the hypothesis space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is <strong>parametrized</strong> by the weight vector <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>,
we can rewrite the objective function as follows:</p>
<div class="math notranslate nohighlight" id="equation-eq-objective-function-2">
<span class="eqno">(141)<a class="headerlink" href="#equation-eq-objective-function-2" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\hat{\boldsymbol{\beta}} := \underset{\hat{\boldsymbol{\beta}} \in \mathbb{R}^D}{\operatorname{argmin}} \widehat{\mathcal{R}}_{\mathcal{S}}(\hat{h}) &amp;= \underset{\hat{\boldsymbol{\beta}} \in \mathbb{R}^D}{\operatorname{argmin}} \frac{1}{N} \sum_{n=1}^{N} \left(y^{(n)} - \hat{\boldsymbol{\beta}}^T \mathbf{x}^{(n)}\right)^2 \\
\end{aligned}
\end{split}\]</div>
<p>It is worth mentioning that both <a class="reference internal" href="#equation-eq-objective-function-1">(140)</a> and <a class="reference internal" href="#equation-eq-objective-function-2">(141)</a> are equivalent in
the sense that solving for the optimal <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> in <a class="reference internal" href="#equation-eq-objective-function-2">(141)</a>
will in turn be used to construct an optimal hypothesis <span class="math notranslate nohighlight">\(\hat{h}\)</span> in <a class="reference internal" href="#equation-eq-objective-function-1">(140)</a> <span id="id5">[<a class="reference internal" href="../../bibliography.html#id40" title="Alexander Jung. Machine learning: The basics. Springer Nature Singapore, 2023.">Jung, 2023</a>]</span>.</p>
<div class="proof example admonition" id="prf-example-learner">
<p class="admonition-title"><span class="caption-number">Example 20 </span> (What is a learner?)</p>
<section class="example-content" id="proof-content">
<p>As an example, <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> can be a linear regression algorithm (gradient descent) that minimizes the empirical risk <span class="math notranslate nohighlight">\(\widehat{\mathcal{R}}_{\mathcal{S}}(\hat{h})\)</span>.</p>
</section>
</div><p>We have wrapped up the problem formulation, we will proceed to expand on a few details in the following sections.</p>
</section>
<section id="the-normal-distribution-gaussian-noise-and-the-mean-squared-error">
<h2><a class="toc-backref" href="#id20" role="doc-backlink">The Normal Distribution, Gaussian Noise and the Mean Squared Error</a><a class="headerlink" href="#the-normal-distribution-gaussian-noise-and-the-mean-squared-error" title="Link to this heading">#</a></h2>
<p>Previously, we claimed that we can interpret <span class="math notranslate nohighlight">\(y\)</span> as a realization of a random variable
<span class="math notranslate nohighlight">\(Y\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-4">
<span class="eqno">(142)<a class="headerlink" href="#equation-eq-linear-regression-4" title="Link to this equation">#</a></span>\[
Y \overset{\small{\text{i.i.d.}}}{\sim} \mathcal{N}\left(\beta_0 + \boldsymbol{\beta}^T \mathbf{x}, \sigma^2\right)
\]</div>
<p>But why is this the case?</p>
<section id="target-variable-is-normally-distributed">
<h3><a class="toc-backref" href="#id21" role="doc-backlink">Target Variable is Normally Distributed</a><a class="headerlink" href="#target-variable-is-normally-distributed" title="Link to this heading">#</a></h3>
<p>To see why this is the case, let’s consider the following linear regression model:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-5">
<span class="eqno">(143)<a class="headerlink" href="#equation-eq-linear-regression-5" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
y &amp;= f\left(\mathbf{x}; \boldsymbol{\beta}\right) \\
&amp;= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_D x_D + \varepsilon \\
\varepsilon &amp;\overset{\small{\text{i.i.d.}}}{\sim} \mathcal{N}(0, \sigma^2)
\end{aligned}
\end{split}\]</div>
<p>and note that we treat <span class="math notranslate nohighlight">\(f(\cdot)\)</span> as a <strong>deterministic</strong> function of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>
(see section 3.1.1 of <span id="id6">[]</span>). Under this assumption, then it is easy to see that
<span class="math notranslate nohighlight">\(y\)</span> must follow a normal distribution with mean <span class="math notranslate nohighlight">\(\beta_0 + \boldsymbol{\beta}^T \mathbf{x}\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>.</p>
<p>This is because the only source of randomness in <a class="reference internal" href="#equation-eq-linear-regression-5">(143)</a> is <span class="math notranslate nohighlight">\(\varepsilon\)</span>, which
follows a normal distribution with mean <span class="math notranslate nohighlight">\(0\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Then adding a constant
(since it is a deterministic function) to a normal distribution with mean <span class="math notranslate nohighlight">\(0\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>
will mean shifting the mean of the Gaussian distribution by the constant and the variance remains the same.</p>
<p>Thus, we have:</p>
<div class="math notranslate nohighlight" id="equation-eq-y-distribution-1">
<span class="eqno">(144)<a class="headerlink" href="#equation-eq-y-distribution-1" title="Link to this equation">#</a></span>\[
\begin{aligned}
y &amp;\sim \mathcal{N}\left(\beta_0 + \boldsymbol{\beta}^T \mathbf{x}, \sigma^2\right)
\end{aligned}
\]</div>
<p>However, we are not that interested in the distribution of <span class="math notranslate nohighlight">\(y\)</span> unconditioned. Why? If we model
the distribution of <span class="math notranslate nohighlight">\(y\)</span> unconditioned, then we are essentially saying that we do not care about
the input features <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, which is not true. We are interested in the distribution of <span class="math notranslate nohighlight">\(y\)</span>
<strong>given</strong> <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>!</p>
<div class="proof remark admonition" id="prf-remark-x-random">
<p class="admonition-title"><span class="caption-number">Remark 37 </span> (Is <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> a random variable?)</p>
<section class="remark-content" id="proof-content">
<p>We assumed earlier that <span class="math notranslate nohighlight">\(f(\cdot)\)</span> is deterministic and therefore
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is fixed and known. So in this scenario the “randomness” is removed.
How do we reconcile with our formulation that we are drawing <span class="math notranslate nohighlight">\(i.i.d.\)</span> samples from
<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>?</p>
<p>If <span class="math notranslate nohighlight">\(\mathbf{X} = \mathbf{x}\)</span> is assumed to be a random variable, then <span class="math notranslate nohighlight">\(f(\cdot)\)</span> is
random as well. And there is no guarantee that the unconditional distribution of <span class="math notranslate nohighlight">\(y\)</span> is normal,
however, we can prove instead that the conditional distribution of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is
normally distributed, which is what we are interested in.</p>
<p>I personally also have troubles reconciling the two formulations, but I think it requires a much
more rigourous treatment of probability theory to fully understand the difference.</p>
<p>See <a class="reference external" href="https://stats.stackexchange.com/questions/327427/how-is-y-normally-distributed-in-linear-regression">here</a>
and <a class="reference external" href="https://stats.stackexchange.com/questions/246047/independent-variable-random-variable">here</a>
for a related discussion.</p>
</section>
</div></section>
<section id="the-conditional-distribution-of-y-given-mathbf-x-is-normally-distributed">
<h3><a class="toc-backref" href="#id22" role="doc-backlink">The Conditional Distribution of <span class="math notranslate nohighlight">\(y\)</span> Given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is Normally Distributed</a><a class="headerlink" href="#the-conditional-distribution-of-y-given-mathbf-x-is-normally-distributed" title="Link to this heading">#</a></h3>
<p>It can be shown that the conditional distribution of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is also normally
distributed with the same vein of logic in the previous section.</p>
<p>That means the following:</p>
<div class="math notranslate nohighlight" id="equation-eq-y-distribution-2">
<span class="eqno">(145)<a class="headerlink" href="#equation-eq-y-distribution-2" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
y \mid \mathbf{x} &amp;\sim \mathcal{N}\left(y ; f\left(\mathbf{x}; \boldsymbol{\beta}\right), \sigma^2\right) \\
&amp;= \mathcal{N}\left(y ; f\left(\mathbf{x}; \boldsymbol{\beta}\right), \boldsymbol{\beta}^{-1}\right) \\
&amp;= \mathcal{N}\left(\beta_0 + \boldsymbol{\beta}^T \mathbf{x}, \boldsymbol{\beta}^{-1}\right) \\
\end{aligned}
\end{split}\]</div>
<p>where the notation <span class="math notranslate nohighlight">\(\mathcal{N}\left(y ; f\left(\mathbf{x}; \boldsymbol{\beta}\right), \sigma^2\right)\)</span>
means that the distribution is a normal distribution with mean <span class="math notranslate nohighlight">\(f\left(\mathbf{x}; \boldsymbol{\beta}\right)\)</span>
and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. The <span class="math notranslate nohighlight">\(y\)</span> is the random variable that is of interest to us. Note because we are finding
the conditional of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, there is nothing random about <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and hence the whole PDF
is of <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>Then consider the expectation of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> below:</p>
<div class="math notranslate nohighlight" id="equation-eq-conditional-expectation-1">
<span class="eqno">(146)<a class="headerlink" href="#equation-eq-conditional-expectation-1" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathbb{E}[y \mid \mathbf{x}] &amp;= \mathbb{E}\left[\mathcal{N}\left(f\left(\mathbf{x}; \boldsymbol{\beta}\right), \sigma^2\right)\right] \\
&amp;= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_D x_D \\
\end{aligned}
\end{split}\]</div>
<p>since by definition the expectation of a normal distribution is the mean parameter.</p>
<p>Linear regression can be interpreted as the conditional expectation of the target variable <span class="math notranslate nohighlight">\(y\)</span> given the features <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.
In other words, it provides an estimate of the <strong>expected</strong> value of the target variable, given a set of input features.</p>
<p>Consequently, finding the optimal <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>
gives rise the hypothesis <span class="math notranslate nohighlight">\(\hat{h} = \boldsymbol{\beta}^T \mathbf{x}\)</span>,
which also turns out to be the estimate of the conditional expectation of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\hat{\boldsymbol{\beta}} \iff \hat{h} = \hat{\boldsymbol{\beta}}^T \mathbf{x} \iff \widehat{\mathbb{E}}[y \mid \mathbf{x}]
\end{aligned}
\]</div>
<div class="proof example admonition" id="prf-example-conditional-expectation">
<p class="admonition-title"><span class="caption-number">Example 21 </span> (Example: Conditional Expectation)</p>
<section class="example-content" id="proof-content">
<p>Say <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> represent the height and weight of a person, respectively.</p>
<p>Let’s say we are given that <span class="math notranslate nohighlight">\(X = 170\)</span> cm, then the conditional expectation of <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(X = 170\)</span> cm,
denoted as <span class="math notranslate nohighlight">\(\mathbb{E}[Y \mid X = 170]\)</span> is the expected weight of a person with height <span class="math notranslate nohighlight">\(170\)</span> cm.</p>
<p>The inclusion of the concept of expectation is powerful! This is because it takes into account the
whole population of people with height <span class="math notranslate nohighlight">\(170\)</span> cm, not just a single person.</p>
</section>
</div></section>
<section id="mean-squared-error-estimates-the-conditional-expectation">
<h3><a class="toc-backref" href="#id23" role="doc-backlink">Mean Squared Error estimates the Conditional Expectation</a><a class="headerlink" href="#mean-squared-error-estimates-the-conditional-expectation" title="Link to this heading">#</a></h3>
<p>We will prove this later, but for now, the mean squared error is an unbiased estimator of the conditional expectation of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<p>See section 3.1.1. of <span id="id7">[]</span> for more details.</p>
</section>
</section>
<section id="linear-in-parameters">
<span id="id8"></span><h2><a class="toc-backref" href="#id24" role="doc-backlink">Linear in Parameters</a><a class="headerlink" href="#linear-in-parameters" title="Link to this heading">#</a></h2>
<p>One confusion that often arises is that linear regression defined in <a class="reference internal" href="#equation-eq-linear-regression-5">(143)</a> is not
only linear in the input features <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, but also linear in the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
<p>However, the more important focus is that the linearity is in the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, not the input features <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<p>This will be made apparent in the section detailing on fitting non-linear inputs <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.
For example, consider a non-linear transformation map defined by <span class="math notranslate nohighlight">\(\phi(\cdot)\)</span>, then <span class="math notranslate nohighlight">\(\phi(\mathbf{x})\)</span> is a non-linear function of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.
But if we write the linear regression model as follows:</p>
<div class="math notranslate nohighlight" id="equation-eq-non-linear-linear-regression-1">
<span class="eqno">(147)<a class="headerlink" href="#equation-eq-non-linear-linear-regression-1" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
y &amp;= \phi\left(\mathbf{x}; \boldsymbol{\beta}\right) \\
&amp;= \boldsymbol{\beta}^T \phi(\mathbf{x}) \\
&amp;= \beta_0 + \beta_1 \phi_1(\mathbf{x}) + \beta_2 \phi_2(\mathbf{x}) + \cdots + \beta_D \phi_D(\mathbf{x}) \\
\end{aligned}
\end{split}\]</div>
<p>Then we can see that the linearity is in the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>, not the input features <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.
Therefore, this expression is still a valid form of “linear regression”.</p>
</section>
<section id="basis-function-and-non-linearity">
<span id="id9"></span><h2><a class="toc-backref" href="#id25" role="doc-backlink">Basis Function and Non-Linearity</a><a class="headerlink" href="#basis-function-and-non-linearity" title="Link to this heading">#</a></h2>
<p>What immediately follows from the previous section on <a class="reference internal" href="#linear-in-parameters"><span class="std std-ref">linearity in parameters</span></a> is that we can
model non-linear functions by using a non-linear transformation map <span class="math notranslate nohighlight">\(\phi(\cdot)\)</span>.</p>
<p>Consider the basis function <span class="math notranslate nohighlight">\(\boldsymbol{\phi(\cdot)}\)</span> defined as follows:</p>
<div class="math notranslate nohighlight" id="equation-eq-basis-function-1">
<span class="eqno">(148)<a class="headerlink" href="#equation-eq-basis-function-1" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\boldsymbol{\phi}: \mathbb{R}^D &amp;\rightarrow \mathbb{R}^K \\
\mathbf{x} &amp;\mapsto \boldsymbol{\phi}(\mathbf{x}) \\
\end{aligned}
\end{split}\]</div>
<p>Notice that <span class="math notranslate nohighlight">\(\boldsymbol{\phi(\cdot)}\)</span> is a bolded, and is a function that maps a vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> of dimension <span class="math notranslate nohighlight">\(D\)</span> to a vector <span class="math notranslate nohighlight">\(\boldsymbol{\phi}(\mathbf{x})\)</span> of dimension <span class="math notranslate nohighlight">\(K\)</span>.</p>
<p>We can decompose the basis function <span class="math notranslate nohighlight">\(\boldsymbol{\phi(\cdot)}\)</span> into a set of basis functions <span class="math notranslate nohighlight">\(\phi_1(\cdot), \phi_2(\cdot), \ldots, \phi_K(\cdot)\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\phi_k: \mathbb{R}^D &amp;\rightarrow \mathbb{R} \\
\mathbf{x} &amp;\mapsto \phi_k(\mathbf{x}) \\
\end{aligned}
\end{split}\]</div>
<p>for the <span class="math notranslate nohighlight">\(k\)</span>-th component of the feature vector <span class="math notranslate nohighlight">\(\boldsymbol{\phi}\)</span>. Now we can write the linear regression model as follows:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-basis-function-1">
<span class="eqno">(149)<a class="headerlink" href="#equation-eq-linear-regression-basis-function-1" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
y &amp;= \boldsymbol{\beta}^T \boldsymbol{\phi}(\mathbf{x}) + \varepsilon \\
&amp;= \beta_0 + \beta_1 \phi_1(\mathbf{x}) + \beta_2 \phi_2(\mathbf{x}) + \cdots + \beta_K \phi_K(\mathbf{x}) + \varepsilon \\
&amp;= \sum_{k=0}^K \beta_k \phi_k(\mathbf{x}) + \varepsilon \\
\end{aligned}
\end{split}\]</div>
<p>Then if you denote <span class="math notranslate nohighlight">\(\mathbf{z} = \boldsymbol{\phi}(\mathbf{x})\)</span>, then the form is still exactly the same as <a class="reference internal" href="#equation-eq-linear-regression-5">(143)</a>.</p>
<p>The takeaway is that we can model non-linear functions by using a non-linear transformation map <span class="math notranslate nohighlight">\(\phi(\cdot)\)</span>.
So we are not restricted to modelling lines and hyperplanes, but can model any function that can be expressed as a linear combination of basis functions.</p>
<p>We will not dive too deep into this topic, but related content can be found in the references section.</p>
</section>
<section id="matrix-representation">
<h2><a class="toc-backref" href="#id26" role="doc-backlink">Matrix Representation</a><a class="headerlink" href="#matrix-representation" title="Link to this heading">#</a></h2>
<div class="proof remark admonition" id="prf-remark-notation-convention-linear-regression">
<p class="admonition-title"><span class="caption-number">Remark 38 </span> (Notation Convention)</p>
<section class="remark-content" id="proof-content">
<p>We have loosely used <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> earlier to denote a random variable, from now on, we will use <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> to denote
the design matrix (explained below).</p>
</section>
</div><p>We can represent the linear regression model in matrix form by collecting
all the input features <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> into a matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, and all the target variables <span class="math notranslate nohighlight">\(y\)</span> into a vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>,
as shown below:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-matrix-1">
<span class="eqno">(150)<a class="headerlink" href="#equation-eq-linear-regression-matrix-1" title="Link to this equation">#</a></span>\[\begin{split}
\left|
\begin{array}{l}
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon} \\
 \boldsymbol{\varepsilon} \sim \mathcal{N}(\boldsymbol{0}, \sigma^2 \mathbf{I})
\end{array}
\right.
\end{split}\]</div>
<p>or equivalently,</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-matrix-2">
<span class="eqno">(151)<a class="headerlink" href="#equation-eq-linear-regression-matrix-2" title="Link to this equation">#</a></span>\[\begin{split}
\underbrace{\left[\begin{array}{c}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(N)}
\end{array}\right]}_{=\mathbf{y}}=\underbrace{\left[\begin{array}{cccc}
1 &amp; x_1^{(1)} &amp; x_2^{(1)} &amp; \cdots &amp; x_D^{(1)} \\
1 &amp; x_1^{(2)} &amp; x_2^{(2)} &amp; \cdots &amp; x_D^{(2)} \\
\vdots &amp; \cdots &amp; \vdots &amp; \vdots \\
1 &amp; x_1^{(N)} &amp; x_2^{(N)} &amp; \cdots &amp; x_D^{(N)}
\end{array}\right]}_{=\mathbf{X}} \underbrace{\left[\begin{array}{c}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_D
\end{array}\right]}_{=\boldsymbol{\beta}}+\underbrace{\left[\begin{array}{c}
\varepsilon^{(1)} \\
\varepsilon^{(2)} \\
\vdots \\
\varepsilon^{(N)}
\end{array}\right]}_{=\boldsymbol{\varepsilon}},
\end{split}\]</div>
<p>where</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}\)</span> <strong>is the Design Matrix</strong> of shape <span class="math notranslate nohighlight">\(N \times D\)</span>
where <span class="math notranslate nohighlight">\(N\)</span> is the number of observations (training samples) and <span class="math notranslate nohighlight">\(D\)</span> is the number of independent feature/input variables.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \mathbf{X} = \begin{bmatrix} 1 &amp;  x_1^{(1)} &amp; x_2^{(1)} &amp; \cdots &amp; x_D^{(1)} \\
                    1 &amp;  x_1^{(2)} &amp; x_2^{(2)} &amp; \cdots &amp; x_D^{(2)} \\
                    \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
                    1 &amp;  x_1^{(N)} &amp; x_2^{(N)} &amp; \cdots &amp; x_D^{(N)} \end{bmatrix}_{N \times (D+1)} = \begin{bmatrix} \left(\mathbf{x^{(1)}}\right)^{T} \\ \left(\mathbf{x^{(2)}}\right)^{T} \\ \vdots \\ \left(\mathbf{x^{(N)}}\right)^{T}\end{bmatrix}
    \end{split}\]</div>
</li>
<li><p>The <span class="math notranslate nohighlight">\(n\)</span>-th row of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is defined as the transpose of <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span>, which is also known as the <span class="math notranslate nohighlight">\(n\)</span>-th training sample, represented as a <span class="math notranslate nohighlight">\(D+1\)</span>-dimensional vector.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \mathbf{x}^{(n)} = \begin{bmatrix} 1 \\ x_1^{(n)} \\ x_2^{(n)} \\ \vdots \\ x_D^{(n)} \end{bmatrix}_{(D+1) \times 1}
    \end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(x_d^{(n)}\)</span> is the <span class="math notranslate nohighlight">\(d\)</span>-th feature of the <span class="math notranslate nohighlight">\(n\)</span>-th training sample.</p>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{y}\)</span> <strong>the output</strong> is a column vector that contains the output for the <span class="math notranslate nohighlight">\(N\)</span> observations.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \mathbf{y} = \begin{bmatrix} y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(N)} \end{bmatrix}_{N \times 1}
    \end{split}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> <strong>the vector of coefficients/parameters:</strong> The column vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> contains all the coefficients of the linear model.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{\beta} = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_D \end{bmatrix}_{(D+1) \times 1}\end{split}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}\)</span> <strong>the random vector of the error terms:</strong> The column vector <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}\)</span> contains <span class="math notranslate nohighlight">\(N\)</span> error terms corresponding to the <span class="math notranslate nohighlight">\(N\)</span> observations.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \boldsymbol{\varepsilon} = \begin{bmatrix} \varepsilon^{(1)} \\ \varepsilon^{(2)} \\ \vdots \\ \varepsilon^{(N)} \end{bmatrix}_{N \times 1}
    \end{split}\]</div>
</li>
</ul>
<p>As we move along, we will make slight modification to the variables above, to accommodate the intercept term as seen in the Design Matrix.</p>
<section id="break-down-of-the-matrix-representation">
<h3><a class="toc-backref" href="#id27" role="doc-backlink">Break down of the Matrix Representation</a><a class="headerlink" href="#break-down-of-the-matrix-representation" title="Link to this heading">#</a></h3>
<p>A <strong>multivariate linear regression</strong> problem between an input variable <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span>
and output variable <span class="math notranslate nohighlight">\(y^{(n)}\)</span> can be represented as such:</p>
<div class="math notranslate nohighlight">
\[
y^{(n)} = \beta_0 + \beta_1 x_1^{(n)} + \beta_2 x_2^{(n)} + \cdots + \beta_D x_D^{(n)} + \varepsilon^{(n)}
\]</div>
<p>Since there exists <span class="math notranslate nohighlight">\(N\)</span> observations, we can write an equation for each observation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
y^{(1)} &amp;= \beta_0 x_0^{(1)} + \beta_1 x_1^{(1)} + \beta_2 x_2^{(1)} + \cdots + \beta_D x_D^{(1)} + \varepsilon^{(1)} \\
y^{(2)} &amp;= \beta_0 x_0^{(2)} + \beta_1 x_1^{(2)} + \beta_2 x_2^{(2)} + \cdots + \beta_D x_D^{(2)} + \varepsilon^{(2)} \\
\vdots \\
y^{(N)} &amp;= \beta_0 x_0^{(N)} + \beta_1 x_1^{(N)} + \beta_2 x_2^{(N)} + \cdots + \beta_D x_D^{(N)} + \varepsilon^{(N)}
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(x_0^{(n)} = 1\)</span> for all <span class="math notranslate nohighlight">\(n\)</span> is the intercept term.</p>
<p>We transform the above system of linear equations into matrix form as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} y^{(1)}  \\ y^{(2)} \\ y^{(3)} \\ \vdots \\ \mathbf{y}^{(m)} \end{bmatrix}_{m \times 1} = \begin{bmatrix} 1 &amp;  x_1^{(1)} &amp; x_2^{(1)} &amp; \cdots &amp; x_n^{(1)} \\
                1 &amp;  x_1^{(2)} &amp; x_2^{(2)} &amp; \cdots &amp; x_n^{(2)} \\
                \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
                1 &amp;  x_1^{(m)} &amp; x_2^{(m)} &amp; \cdots &amp; x_n^{(m)} \end{bmatrix}_{m \times (n+1)} \begin{bmatrix} \beta_0 \\ \beta_ 1 \\ \beta_2 \\ \vdots \\ \beta_n\end{bmatrix}_{(n+1) \times 1} + \begin{bmatrix} \varepsilon^{(1)} \\ \varepsilon^{(2)} \\ \varepsilon^{(3)} \\ \vdots \\ \varepsilon^{(m)} \end{bmatrix}_{m \times 1}\end{split}\]</div>
<p>We then write the above system of linear equations more compactly as <strong>y</strong> = <strong>Xβ</strong> + <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}\)</span>   where <span class="math notranslate nohighlight">\(ε\sim^{\text{i.i.d}}N(0, σ^2)\)</span> recovering back the equation at the start.</p>
<p>In what follows, we will restate some definitions again, and use the matrix representation
in <a class="reference internal" href="#equation-eq-linear-regression-matrix-2">(151)</a> to derive the closed-form solution for the coefficients <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
</section>
</section>
<section id="hypothesis-space">
<h2><a class="toc-backref" href="#id28" role="doc-backlink">Hypothesis Space</a><a class="headerlink" href="#hypothesis-space" title="Link to this heading">#</a></h2>
<p>Given the sample dataset <span class="math notranslate nohighlight">\(\mathcal{S} = \left\{\mathbf{x}^{(n)}, y^{(n)}\right\}_{n=1}^N\)</span>, the hypothesis space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is the set of all functions <span class="math notranslate nohighlight">\(h: \mathbb{R}^D \rightarrow \mathbb{R}\)</span> that can be learned from the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>.</p>
<p>We mentioned in the first section that the hypothesis space is given by:</p>
<div class="math notranslate nohighlight" id="equation-eq-hypothesis-space-linear-regression-1">
<span class="eqno">(152)<a class="headerlink" href="#equation-eq-hypothesis-space-linear-regression-1" title="Link to this equation">#</a></span>\[
\begin{aligned}
\mathcal{H} &amp;= \left\{ h \in \mathbb{R}^{D} \rightarrow \mathbb{R}, h\left(\mathbf{x}; \boldsymbol{\beta}\right) = \boldsymbol{\beta}^T \mathbf{x} + \beta_0 : \boldsymbol{\beta} \in \mathbb{R}^D, \beta_0 \in \mathbb{R} \right\}
\end{aligned}
\]</div>
<p>Now, we have also defined in <a class="reference internal" href="#basis-function-and-non-linearity"><span class="std std-ref">basis function and non-linearity</span></a>
that we can model non-linearity. Therefore, the hypothesis space can be expanded to include non-linear functions.</p>
<div class="math notranslate nohighlight" id="equation-eq-hypothesis-space-linear-regression-2">
<span class="eqno">(153)<a class="headerlink" href="#equation-eq-hypothesis-space-linear-regression-2" title="Link to this equation">#</a></span>\[
\begin{aligned}
\mathcal{H} &amp;= \left\{ h \in \mathbb{R}^{D} \rightarrow \mathbb{R}, h\left(\mathbf{x}; \boldsymbol{\beta}\right) = \boldsymbol{\beta}^T \mathbf{\phi}(\mathbf{x}) + \beta_0 : \boldsymbol{\beta} \in \mathbb{R}^D, \beta_0 \in \mathbb{R} \right\}
\end{aligned}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{\phi}(\mathbf{x})\)</span> is a non-linear function that maps <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to a higher dimensional space <span class="math notranslate nohighlight">\(\mathbb{R}^D\)</span>.</p>
</section>
<section id="loss-function">
<h2><a class="toc-backref" href="#id29" role="doc-backlink">Loss Function</a><a class="headerlink" href="#loss-function" title="Link to this heading">#</a></h2>
<p>As mentioned in the first section, we will be using the mean squared error (MSE) as our loss function.</p>
<p>We define a loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> that measures the error between the true label
<span class="math notranslate nohighlight">\(y\)</span> and the predicted label <span class="math notranslate nohighlight">\(\hat{h}\left(\mathbf{x}; \boldsymbol{\beta}\right)\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-loss-function-4">
<span class="eqno">(154)<a class="headerlink" href="#equation-eq-linear-regression-loss-function-4" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathcal{L}: \mathcal{X} \times \mathcal{Y} \times \mathcal{H} &amp;\to \mathbb{R}_{+} \\
\left((\mathbf{x}, y), h\right) &amp;\mapsto \mathcal{L}\left(\left(\mathbf{x}, y\right), h\right)
\end{aligned}
\end{split}\]</div>
<p>We present a few equivalent notations for the loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-loss-function-5">
<span class="eqno">(155)<a class="headerlink" href="#equation-eq-linear-regression-loss-function-5" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathcal{L}\left(\left(\mathbf{x}, y\right), h\right) &amp;= \mathcal{L}(y, \hat{y}) \\
&amp;= \mathcal{L}\left(y, \hat{h}\left(\mathbf{x}; \boldsymbol{\beta}\right)\right) \\
&amp;= \mathcal{L}\left(\boldsymbol{\beta}\right)
\end{aligned}
\end{split}\]</div>
<p>where the last equation is to emphasize that the loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is a function of the weight vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>
and not of the input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> for the context of machine learning.</p>
<p>The choice of loss function in regression is often the <a class="reference external" href="https://en.wikipedia.org/wiki/Mean_squared_error"><strong>mean squared error loss</strong></a> (MSE)
defined by:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-loss-function-6">
<span class="eqno">(156)<a class="headerlink" href="#equation-eq-linear-regression-loss-function-6" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathcal{L}\left(\left(\mathbf{x}, y\right), h\right) &amp;= \left(y - \hat{y}\right)^2 \\
&amp;= \left(y - \hat{h}\left(\mathbf{x}; \boldsymbol{\beta}\right)\right)^2 \\
&amp;= \left(y - \boldsymbol{\beta}^T \mathbf{x} \right)^2 \\
\end{aligned}
\end{split}\]</div>
<p>where we used the fact that <span class="math notranslate nohighlight">\(\hat{y} = \boldsymbol{\beta}^T \mathbf{x}\)</span>.</p>
<p>Then for our empirical loss, we can write:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-loss-function-7">
<span class="eqno">(157)<a class="headerlink" href="#equation-eq-linear-regression-loss-function-7" title="Link to this equation">#</a></span>\[
\begin{aligned}
\widehat{\mathcal{L}}\left(h \mid \mathcal{S}\right)
\end{aligned}
\]</div>
<p>to indicate that the empirical loss is a function of the hypothesis <span class="math notranslate nohighlight">\(h\)</span> and the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>.</p>
<p>We will detail in the next section on why the mean squared error loss is a sensible choice, amongst others.</p>
</section>
<section id="cost-function">
<h2><a class="toc-backref" href="#id30" role="doc-backlink">Cost Function</a><a class="headerlink" href="#cost-function" title="Link to this heading">#</a></h2>
<p>The cost function <span class="math notranslate nohighlight">\(\mathcal{J}\)</span> is the expected loss over the entire input-output space <span class="math notranslate nohighlight">\(\mathcal{X} \times \mathcal{Y}\)</span>.</p>
<p>Of course, we are not able to compute the expected loss over the entire input-output space <span class="math notranslate nohighlight">\(\mathcal{X} \times \mathcal{Y}\)</span>,
so we look at the expected loss over the training dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, denoted by <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\)</span>. This is the cost function
and is just the empirical risk function defined in <a class="reference internal" href="#equation-eq-empirical-risk-1">(139)</a>.</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-cost-function-1">
<span class="eqno">(158)<a class="headerlink" href="#equation-eq-linear-regression-cost-function-1" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\widehat{\mathcal{J}}\left(\boldsymbol{\beta} \mid \mathcal{S}\right) := \widehat{\mathcal{J}}\left(h \mid \mathcal{S}\right) &amp;= \frac{1}{N} \sum_{n=1}^{N} \mathcal{L}\left(\left(\mathbf{x}^{(n)}, y^{(n)}\right), h\right) \\
&amp;= \frac{1}{N} \sum_{n=1}^{N} \left(y^{(n)} - h\left(\mathbf{x}^{(n)}; \boldsymbol{\beta}\right)\right)^2 \\
&amp;= \frac{1}{N} \left\| \mathbf{y} - \mathbf{\hat{y}} \right\|_2^2 \\
&amp;= \frac{1}{N} \left\| \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \right\|_2^2
\end{aligned}
\end{split}\]</div>
<p>where the last equation is due to our matrix representation, a more concise notation.</p>
<p>Note that we also expressed the cost function as <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\left(\boldsymbol{\beta} \mid \mathcal{S}\right)\)</span>
to emphasize that the cost function is a function of the weight vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> and not of the input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.
This is because in optimization, we are trying to find the weight vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> that minimizes the cost function,
so we will be taking the derivative of the cost function with respect to the weight vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> later.</p>
<section id="convexity-and-differentiability">
<h3><a class="toc-backref" href="#id31" role="doc-backlink">Convexity and Differentiability</a><a class="headerlink" href="#convexity-and-differentiability" title="Link to this heading">#</a></h3>
<p>The cost function <span class="math notranslate nohighlight">\(\widehat{\mathcal{J}}\left(\boldsymbol{\beta} \mid \mathcal{S}\right)\)</span> is a convex function of the weight vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
<p>This is relatively easy to see as the cost function is a quadratic function of the weight vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>
and hence the convexity of the cost function follows from the convexity of the quadratic function.</p>
<p>Moreover, it is also differentiable with respect to the weight vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
<p>Why are these important? Well, maybe not that much in Linear Regression, since there exists a closed-form solution to the optimization problem.</p>
<p>But in gradient-based optimization, we will need to take the derivative of the cost function with respect to the weight vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.
Consequently, <a class="reference external" href="https://en.wikipedia.org/wiki/Convex_function"><strong>convexity</strong></a> guarantees that the optimization problem has a
global optima and <a class="reference external" href="https://en.wikipedia.org/wiki/Differentiable_function"><strong>differentiability</strong></a> guarantees that we can find the
optimal weight vector <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> by taking the derivative of the cost function with respect to the weight vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
</section>
</section>
<section id="objective-function">
<h2><a class="toc-backref" href="#id32" role="doc-backlink">Objective Function</a><a class="headerlink" href="#objective-function" title="Link to this heading">#</a></h2>
<p>We now formulate the optimization problem in terms of the objective function.</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-objective-function-1">
<span class="eqno">(159)<a class="headerlink" href="#equation-eq-linear-regression-objective-function-1" title="Link to this equation">#</a></span>\[\begin{split}
\begin{alignat}{1}
\underset{\boldsymbol{\beta} \in \boldsymbol{\Theta}}{\operatorname{argmin}} &amp;\quad&amp;\widehat{\mathcal{J}}\left(\boldsymbol{\beta} \mid \mathcal{S}\right) &amp;= \frac{1}{N} \left\| \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \right\|_2^2 \\
\end{alignat}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\Theta}\)</span> is the set of all possible weight vectors <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
<p>Equivalently, we are finding the below:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-objective-function-2">
<span class="eqno">(160)<a class="headerlink" href="#equation-eq-linear-regression-objective-function-2" title="Link to this equation">#</a></span>\[\begin{split}
\begin{alignat}{1}
\hat{\boldsymbol{\beta}} &amp;= \underset{\boldsymbol{\beta} \in \boldsymbol{\Theta}}{\operatorname{argmin}} &amp;\quad&amp;\widehat{\mathcal{J}}\left(\boldsymbol{\beta} \mid \mathcal{S}\right) = \underset{\boldsymbol{\beta} \in \boldsymbol{\Theta}}{\operatorname{argmin}} \quad \frac{1}{N} \left\| \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \right\|_2^2 \\
\end{alignat}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is the optimal estimate of the weight vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
</section>
<section id="model-fitting-via-least-squares">
<h2><a class="toc-backref" href="#id33" role="doc-backlink">Model Fitting via Least Squares</a><a class="headerlink" href="#model-fitting-via-least-squares" title="Link to this heading">#</a></h2>
<p>This method need no assumptions on the distribution of the data and just involves minimizing the
objective function <a class="reference internal" href="#equation-eq-linear-regression-objective-function-1">(159)</a> with respect to the weight vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.
In other words, we are finding the weight vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> that minimizes the
mean squared error (MSE) between the predicted values <span class="math notranslate nohighlight">\(\mathbf{\hat{y}}\)</span> and the actual values <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>.</p>
<section id="solving-for-1-dimensional-case">
<h3><a class="toc-backref" href="#id34" role="doc-backlink">Solving for 1-Dimensional Case</a><a class="headerlink" href="#solving-for-1-dimensional-case" title="Link to this heading">#</a></h3>
<p>Let’s consider the base case where each sample is a scalar, i.e. <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)} \in \mathbb{R}\)</span>.
That is to say, we seek to find:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-objective-function-3">
<span class="eqno">(161)<a class="headerlink" href="#equation-eq-linear-regression-objective-function-3" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\hat{\boldsymbol{\beta}} := \left(\hat{\beta}_0, \hat{\beta}_1\right) &amp;= \underset{\boldsymbol{\beta} \in \boldsymbol{\Theta}}{\operatorname{argmin}} \quad \widehat{\mathcal{J}}\left(\boldsymbol{\beta} \mid \mathcal{S}\right) \\
&amp;= \underset{\boldsymbol{\beta} \in \boldsymbol{\Theta}}{\operatorname{argmin}} \quad \frac{1}{N} \left\| \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \right\|_2^2 \\
&amp;= \underset{\beta_0, \beta_1 \in \boldsymbol{\Theta}}{\operatorname{argmin}} \quad \frac{1}{N} \sum_{n=1}^N \left( y^{(n)} - \left(\beta_0 + \beta_1 x^{(n)}\right) \right)^2 \\
\end{aligned}
\end{split}\]</div>
<p>where the last equality expands the matrix-vector product <span class="math notranslate nohighlight">\(\mathbf{X} \boldsymbol{\beta}\)</span> into
its constituent elements. This is a step back from the matrix representation of the data, but it is
useful for understanding the optimization problem and how to take derivatives at 2-Dimensional.</p>
<p>The following proof adapts from <span id="id10">[<a class="reference internal" href="../../bibliography.html#id15" title="Stanley H. Chan. Introduction to probability for Data Science. Michigan Publishing, 2021.">Chan, 2021</a>]</span>.</p>
<div class="proof admonition" id="proof">
<p>Proof. As with any two-dimensional optimization problem, the optimal point <span class="math notranslate nohighlight">\((\hat{\beta}_1, \hat{\beta}_0)\)</span> should have a zero gradient, meaning that</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial \hat{\beta}_1} \widehat{\mathcal{J}}\left(\hat{\beta}_0, \hat{\beta}_1 \mid \mathcal{S}\right)=0 \quad \text { and } \quad \frac{\partial}{\partial \hat{\beta}_0} \widehat{\mathcal{J}}\left(\hat{\beta}_0, \hat{\beta}_1 \mid \mathcal{S}\right)=0 .
\]</div>
<p>This should be familiar to you, even if you have only learned basic calculus. This pair of equations says that, at <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> minimum point, the directional slopes should be zero no matter which direction you are looking at.
The derivative with respect to <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial}{\partial \hat{\beta}_1} \widehat{\mathcal{J}}\left(\hat{\beta}_0, \hat{\beta}_1 \mid \mathcal{S}\right) &amp; =\frac{\partial}{\partial \hat{\beta}_1}\left\{\sum_{n=1}^N\left(y^{(n)}-\left(\hat{\beta}_1 x^{(n)}+\hat{\beta}_0\right)\right)^2\right\} \\
&amp;=\frac{\partial}{\partial \hat{\beta}_1}\left\{\left(y^{(1)}-\left(\hat{\beta}_1 x^{(1)}+\hat{\beta}_0\right)\right)^2+\left(y^{(2)}-\left(\hat{\beta}_1 x^{(2)}+\hat{\beta}_0\right)\right)^2+\cdots+\left(y^{(N)}-\left(\hat{\beta}_1 x^{(N)}+\hat{\beta}_0\right)\right)^2\right\} \\
&amp;=2\left(y^{(1)}-\left(\hat{\beta}_1 x^{(1)}+\hat{\beta}_0\right)\right)\left(-x^{(1)}\right)+\cdots+2\left(y^{(N)}-\left(\hat{\beta}_1 x^{(N)}+\hat{\beta}_0\right)\right)\left(-x^{(N)}\right) \\
&amp;=2\left(-\sum_{n=1}^N x^{(n)} y^{(n)}+\hat{\beta}_1 \sum_{n=1}^N \left(x^{(n)}\right)^2+\hat{\beta}_0 \sum_{n=1}^N x^{(n)}\right)
\end{aligned}
\end{split}\]</div>
<p>Similarly, the derivative with respect to <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial}{\partial \hat{\beta}_0} \widehat{\mathcal{J}}\left(\hat{\beta}_0, \hat{\beta}_1 \mid \mathcal{S}\right) &amp; =\frac{\partial}{\partial \hat{\beta}_0}\left\{\sum_{n=1}^N\left(y^{(n)}-\left(\hat{\beta}_1 x^{(n)}+\hat{\beta}_0\right)\right)^2\right\} \\
&amp; =2\left(y^{(1)}-\left(\hat{\beta}_1 x^{(1)}+\hat{\beta}_0\right)\right)(-1)+\cdots+2\left(y^{(N)}-\left(\hat{\beta}_1 x^{(n)}+\hat{\beta}_0\right)\right)(-1) \\
&amp; =2\left(-\sum_{n=1}^N y^{(n)}+\hat{\beta}_1 \sum_{n=1}^N x^{(n)}+\hat{\beta}_0 \sum_{n=1}^N 1\right)
\end{aligned}
\end{split}\]</div>
<p>Setting these two equations to zero, we have that</p>
<div class="math notranslate nohighlight" id="equation-eq-linreg-beta0">
<span class="eqno">(162)<a class="headerlink" href="#equation-eq-linreg-beta0" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\frac{\partial}{\partial \hat{\beta}_0} \widehat{\mathcal{J}}\left(\hat{\beta}_0, \hat{\beta}_1 \mid \mathcal{S}\right) = 0 &amp;\iff 2\left(-\sum_{n=1}^N y^{(n)}+\hat{\beta}_1 \sum_{n=1}^N x^{(n)}+\hat{\beta}_0 \sum_{n=1}^N 1\right) = 0 \\
&amp;\iff \hat{\beta}_0 \sum_{n=1}^N 1 = \sum_{n=1}^N y^{(n)} - \hat{\beta}_1 \sum_{n=1}^N x^{(n)} \\
&amp;\iff \hat{\beta}_0 N = \sum_{n=1}^N y^{(n)} - \hat{\beta}_1 \sum_{n=1}^N x^{(n)} \\
&amp;\iff \hat{\beta}_0 = \frac{1}{N} \sum_{n=1}^N y^{(n)} - \hat{\beta}_1 \frac{1}{N} \sum_{n=1}^N x^{(n)} \\
&amp;\iff \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x} \\
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight" id="equation-eq-linreg-beta1">
<span class="eqno">(163)<a class="headerlink" href="#equation-eq-linreg-beta1" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\frac{\partial}{\partial \hat{\beta}_1} \widehat{\mathcal{J}}\left(\hat{\beta}_0, \hat{\beta}_1 \mid \mathcal{S}\right) = 0 &amp;\iff 2\left(-\sum_{n=1}^N x^{(n)} y^{(n)}+\hat{\beta}_1 \sum_{n=1}^N \left(x^{(n)}\right)^2+\hat{\beta}_0 \sum_{n=1}^N x^{(n)}\right) = 0 \\
&amp;\iff \hat{\beta}_1 \sum_{n=1}^N \left(x^{(n)}\right)^2 = \sum_{n=1}^N x^{(n)} y^{(n)} - \hat{\beta}_0 \sum_{n=1}^N x^{(n)} \\
&amp;\iff \hat{\beta}_1 = \frac{\left(\sum_{n=1}^N x^{(n)} y^{(n)} - \hat{\beta}_0 \sum_{n=1}^N x^{(n)}\right)}{\sum_{n=1}^N \left(x^{(n)}\right)^2}  \\
&amp;\iff \hat{\beta}_1 = \frac{\left(\sum_{n=1}^N x^{(n)} y^{(n)} - \left(\bar{y} - \hat{\beta}_1 \bar{x}\right) \sum_{n=1}^N x^{(n)}\right)}{\sum_{n=1}^N \left(x^{(n)}\right)^2} \\
&amp;\iff \hat{\beta}_1 = \frac{\left(\sum_{n=1}^N x^{(n)} y^{(n)} - \bar{y} \sum_{n=1}^N x^{(n)} - \hat{\beta}_1 \bar{x} \sum_{n=1}^N x^{(n)} + \hat{\beta}_1 \bar{x} \sum_{n=1}^N x^{(n)}\right)}{\sum_{n=1}^N \left(x^{(n)}\right)^2} \\
&amp;\iff \hat{\beta}_1 = \frac{\left(\sum_{n=1}^N x^{(n)} y^{(n)} - \bar{y} \sum_{n=1}^N x^{(n)}\right)}{\sum_{n=1}^N \left(x^{(n)}\right)^2} \\
&amp;\iff \hat{\beta}_1 = \frac{\left(\sum_{n=1}^N x^{(n)} y^{(n)} - \bar{y} \sum_{n=1}^N x^{(n)}\right)}{\sum_{n=1}^N \left(x^{(n)} - \bar{x}\right)^2} \\
&amp;\iff \hat{\beta}_1 = \frac{\sum_{n=1}^N \left(x^{(n)} - \bar{x}\right) \left(y^{(n)} - \bar{y}\right)}{\sum_{n=1}^N \left(x^{(n)} - \bar{x}\right)^2} \\
&amp;\iff \hat{\beta}_1 = r_{XY} \frac{s_Y}{s_X} \\
\end{aligned}
\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\bar{y} = \frac{1}{N} \sum_{n=1}^N y^{(n)}\)</span> is the sample mean of the response variable <span class="math notranslate nohighlight">\(Y\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\bar{x} = \frac{1}{N} \sum_{n=1}^N x^{(n)}\)</span> is the sample mean of the predictor variable <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(s_Y = \sqrt{\frac{1}{N-1} \sum_{n=1}^N \left(y^{(n)} - \bar{y}\right)^2}\)</span> is the sample standard deviation of the response variable <span class="math notranslate nohighlight">\(Y\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(s_X = \sqrt{\frac{1}{N-1} \sum_{n=1}^N \left(x^{(n)} - \bar{x}\right)^2}\)</span> is the sample standard deviation of the predictor variable <span class="math notranslate nohighlight">\(X\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(r_{XY} = \frac{\sum_{n=1}^N \left(x^{(n)} - \bar{x}\right) \left(y^{(n)} - \bar{y}\right)}{\sqrt{\sum_{n=1}^N \left(x^{(n)} - \bar{x}\right)^2} \sqrt{\sum_{n=1}^N \left(y^{(n)} - \bar{y}\right)^2}}\)</span> is the sample correlation coefficient between the response and predictor variables.</p></li>
</ul>
<hr class="docutils" />
<p>Thus, we conclude that the optimal <strong>estimates</strong> of <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span> are given by:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-solution-1">
<span class="eqno">(164)<a class="headerlink" href="#equation-eq-linear-regression-solution-1" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\hat{\beta}_0 &amp;= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta}_1 &amp;= \frac{\sum_{n=1}^N \left(x^{(n)} - \bar{x}\right) \left(y^{(n)} - \bar{y}\right)}{\sum_{n=1}^N \left(x^{(n)} - \bar{x}\right)^2}
\end{aligned}
\end{split}\]</div>
<p>We can also see that the second derivative of both the equations gives us more than <span class="math notranslate nohighlight">\(0\)</span>, indicating
that the solution found is a <strong>global minimum</strong> (convexity).</p>
</div>
</section>
<section id="solving-for-d-dimensional-case">
<h3><a class="toc-backref" href="#id35" role="doc-backlink">Solving for <span class="math notranslate nohighlight">\(D\)</span>-Dimensional Case</a><a class="headerlink" href="#solving-for-d-dimensional-case" title="Link to this heading">#</a></h3>
<p>We can generalize the above to the <span class="math notranslate nohighlight">\(D\)</span>-dimensional case. We have in equation
<a class="reference internal" href="#equation-eq-linear-regression-matrix-1">(150)</a> that we can write the linear regression formulation
more compactly as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left|
\begin{array}{l}
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\varepsilon} \\
 \boldsymbol{\varepsilon} \overset{\small{\text{iid}}}{\sim} \mathcal{N}\left(\mathbf{0}, \sigma^2 \mathbf{I}\right)
\end{array}
\right.
\end{split}\]</div>
<p>We will minimize <a class="reference internal" href="#equation-eq-linear-regression-objective-function-1">(159)</a> with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{alignat}{1}
\underset{\boldsymbol{\beta} \in \boldsymbol{\Theta}}{\operatorname{argmin}} &amp;\quad&amp;\widehat{\mathcal{J}}\left(\boldsymbol{\beta} \mid \mathcal{S}\right) &amp;= \frac{1}{N} \left\| \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \right\|_2^2 \\
\end{alignat}
\end{split}\]</div>
<p>With some matrix calculus, we can derive that:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\nabla_{\boldsymbol{\beta}} \widehat{\mathcal{J}}\left(\hat{\boldsymbol{\beta}} \mid \mathcal{S}\right) &amp;= \nabla_{\boldsymbol{\beta}} \frac{1}{N} \left\| \mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}} \right\|_2^2 \\
&amp;= \frac{2}{N} \left(\mathbf{X}^T \mathbf{X} \hat{\boldsymbol{\beta}} - \mathbf{X}^T \mathbf{y}\right) \\
\end{aligned}
\end{split}\]</div>
<p>Equating it to <span class="math notranslate nohighlight">\(\boldsymbol{0}\)</span>, we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{X}^T \mathbf{X} \hat{\boldsymbol{\beta}} &amp;= \mathbf{X}^T \mathbf{y} \\
\end{aligned}
\end{split}\]</div>
<p>and we call this the <a class="reference external" href="https://en.wikipedia.org/wiki/Normal_equation"><strong>normal equations</strong></a>. We can solve
for <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> to get the estimates of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
<p>To solve for <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>, we can use the <a class="reference external" href="https://en.wikipedia.org/wiki/Invertible_matrix"><strong>matrix inverse</strong></a> of <span class="math notranslate nohighlight">\(\mathbf{X}^T \mathbf{X}\)</span> to get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\hat{\boldsymbol{\beta}} &amp;= \left(\mathbf{X}^T \mathbf{X}\right)^{-1} \mathbf{X}^T \mathbf{y} \\
\end{aligned}
\end{split}\]</div>
</section>
</section>
<section id="model-fitting-via-maximum-likelihood-estimation">
<h2><a class="toc-backref" href="#id36" role="doc-backlink">Model Fitting via Maximum Likelihood Estimation</a><a class="headerlink" href="#model-fitting-via-maximum-likelihood-estimation" title="Link to this heading">#</a></h2>
<p>We now have the objective function, so we can use it to find the optimal weight vector <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>.
We could directly set the derivative of the cost function in <a class="reference internal" href="#equation-eq-linear-regression-cost-function-1">(158)</a>
to <span class="math notranslate nohighlight">\(0\)</span> and solve for <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>. This is solvable and has a closed-form solution. However,
we will take the probabilistic approach where we will be invoking <a class="reference external" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum likelihood estimation</a> (MLE)
to find the optimal weight vector <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>. We will show that the maximizing the log-likelihood
is equivalent to minimizing the cost function.</p>
<hr class="docutils" />
<p>We split into two steps, finding
the <a class="reference external" href="https://en.wikipedia.org/wiki/Likelihood_function"><strong>likelihood function</strong></a> and then finding the
<a class="reference external" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation"><strong>maximum likelihood estimate</strong></a> (MLE) separately.</p>
<p>With slight abuse of notation, we denote the following:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{S} &amp;= \left\{ \left(\mathbf{x}^{(n)}, y^{(n)}\right) \right\}_{n=1}^{N} \\
\mathcal{X} &amp;= \left\{ \mathbf{x}^{(n)} \right\}_{n=1}^{N} \\
\mathcal{Y} &amp;= \left\{ y^{(n)} \right\}_{n=1}^{N} \\
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)} \in \mathbb{R}^{D}\)</span> is the <span class="math notranslate nohighlight">\(n\)</span>-th input and <span class="math notranslate nohighlight">\(y^{(n)} \in \mathbb{R}\)</span> is the <span class="math notranslate nohighlight">\(n\)</span>-th output.</p>
<section id="iid-assumption">
<h3><a class="toc-backref" href="#id37" role="doc-backlink">IID Assumption</a><a class="headerlink" href="#iid-assumption" title="Link to this heading">#</a></h3>
<p>When applying MLE, it is often assumed that the data is <a class="reference external" href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables"><strong>independent and identically distributed</strong></a> (IID). However, in the case of linear regression, we are finding the conditional likelihood function
and thus the <strong>identical distribution</strong> assumption is not fulfilled, and is also not required.</p>
<p>More concretely, we have the following (adapted from <a class="reference external" href="https://stats.stackexchange.com/questions/554821/what-the-i-i-d-assumption-of-the-errors-in-linear-regression-implies-for-the-re">here</a>):</p>
<ol class="arabic">
<li><p><span class="math notranslate nohighlight">\(y \mid \mathbf{x} \sim \mathcal{N}(\boldsymbol{\beta}^T \mathbf{x}, \sigma^2)\)</span> and here
we see that for a different <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, the distribution of <span class="math notranslate nohighlight">\(y\)</span> is different. Hence the identically
distributed assumption is not fulfilled.</p>
<p>However, <span class="math notranslate nohighlight">\(Y^{(n)} \mid X^{(n)} = \mathbf{x}^{(n)}\)</span> is however identically distributed because now
the <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> is fixed and we are only varying <span class="math notranslate nohighlight">\(y^{(n)}\)</span>.</p>
</li>
<li><p>The response variables are conditionally independent conditional on the explanatory variable. As a shorthand we may say that the values
<span class="math notranslate nohighlight">\(y^{(n)} \mid \mathbf{x}^{(n)}\)</span> are independent (though not identically distributed).</p></li>
<li><p>This is a much stronger assumption than in regression analysis. It is equivalent to making the standard regression assumptions, but also assuming that the underlying explanatory variables are IID. Once you assume that <span class="math notranslate nohighlight">\(X^{(1)}, X^{(2)}, \ldots, X^{(N)}\)</span> are IID
, the regression assumptions imply that the response variable is also marginally IID, which gives the joint IID result.
So there is nothing wrong in our problem formulation to say that the joint tuple <span class="math notranslate nohighlight">\(\{\mathbf{x}^{(n)}, y^{(n)}\}_{n=1}^{N}\)</span> is IID.</p></li>
</ol>
</section>
<section id="conditional-likelihood-function">
<h3><a class="toc-backref" href="#id38" role="doc-backlink">Conditional Likelihood Function</a><a class="headerlink" href="#conditional-likelihood-function" title="Link to this heading">#</a></h3>
<div class="proof definition admonition" id="def-conditional-likelihood-function-linear-regression">
<p class="admonition-title"><span class="caption-number">Definition 55 </span> (Conditional Likelihood Function)</p>
<section class="definition-content" id="proof-content">
<p>The conditional likelihood function is the probability of observing the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> given the model parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-likelihood-function">
<span class="eqno">(165)<a class="headerlink" href="#equation-eq-linear-regression-likelihood-function" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathcal{L}(\boldsymbol{\beta} \mid \mathcal{S}) &amp;\stackrel{\text { def }}{=} \mathbb{P}_{\mathcal{D}}\left(\mathcal{Y}\mid \mathcal{X} ; \boldsymbol{\beta}\right) \\
&amp;= \mathbb{P}_{\mathcal{D}}\left(y^{(1)}, y^{(2)}, \ldots, y^{(N)} \mid \mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(N)} ; \boldsymbol{\beta}\right) \\
&amp;= \prod_{n=1}^{N} \mathbb{P}_{\mathcal{D}}\left(y^{(n)} \mid \mathbf{x}^{(n)} ; \boldsymbol{\beta}\right) \\
&amp;= \prod_{n=1}^{N} \mathcal{N}\left(\boldsymbol{\beta}^{\top} \mathbf{x}^{(n)}, \sigma^2\right) &amp;&amp; (*) \\
&amp;= \prod_{n=1}^{N} \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left(-\frac{\left(y^{(n)} - \boldsymbol{\beta}^{\top} \mathbf{x}^{(n)}\right)^{2}}{2 \sigma^2}\right) \\
&amp;= \frac{1}{\left(2 \pi \sigma^2\right)^{N / 2}} \exp \left(-\frac{1}{2 \sigma^2} \sum_{n=1}^{N}\left(y^{(n)} - \boldsymbol{\beta}^{\top} \mathbf{x}^{(n)}\right)^{2}\right) \\
&amp;= \frac{1}{\left(2 \pi \sigma^2\right)^{N / 2}} \exp \left(-\frac{1}{2 \sigma^2} \left\|\mathbf{y} - \mathbf{X} \boldsymbol{\beta}\right\|_2^2\right) &amp;&amp; (**) \\
\end{aligned}
\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((*)\)</span> is due to the fact that we are assuming that the
conditional distribution of the output given the input is a normal distribution with mean <span class="math notranslate nohighlight">\(\boldsymbol{\beta}^{\top} \mathbf{x}^{(n)}\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>,
as detailed earlier, a consequence of the noise term <span class="math notranslate nohighlight">\(\varphi\)</span> being normally distributed;</p></li>
<li><p><span class="math notranslate nohighlight">\((**)\)</span> is vectorizing the summation.</p></li>
</ul>
</section>
</div><div class="proof remark admonition" id="remark-likelihood-function-notation-clash">
<p class="admonition-title"><span class="caption-number">Remark 39 </span> (Notation Clash)</p>
<section class="remark-content" id="proof-content">
<p>There is a notation clash here since <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is overloaded with the likelihood function and the loss function.
We will continue using it as it is when the context is clear. Moreover, we will soon realize
that both the likelihood function and the loss function are related in a way that minimizing the loss function
is equivalent to maximizing the likelihood function. There is however still notation abuse since the likelihood
function is more similar to the cost function as it takes all the data points into account.</p>
</section>
</div><p>For more details, see <a class="reference internal" href="../../probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html#def:conditional-likelihood-machine-learning">Definition 171</a> in <a class="reference internal" href="#../../../probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.md"><span class="xref myst">the section on conditional likelihood</span></a>.</p>
</section>
<section id="conditional-log-likelihood-function">
<h3><a class="toc-backref" href="#id39" role="doc-backlink">Conditional Log-Likelihood Function</a><a class="headerlink" href="#conditional-log-likelihood-function" title="Link to this heading">#</a></h3>
<p>It is typical to work with the log-likelihood function instead of the likelihood function since
it can cause <a class="reference external" href="https://en.wikipedia.org/wiki/Arithmetic_underflow"><strong>numerical underflow</strong></a> when the likelihood function is very small.
That means computers cannot multiply very small numbers together. Imagine multiplying <span class="math notranslate nohighlight">\(1\)</span> billion
of <span class="math notranslate nohighlight">\(10^{-100}\)</span> together, the result is <span class="math notranslate nohighlight">\(0\)</span> in floating point arithmetic. But adding the log of
<span class="math notranslate nohighlight">\(10^{-100}\)</span> billion times is no longer causing numerical underflow.</p>
<p>Furthermore, all we need to know is that we are maximizing the likelihood function,
and since log is a monotonic function, maximizing the log-likelihood function is equivalent to maximizing the likelihood function,
meaning they return the same optimal parameters.</p>
<div class="proof definition admonition" id="def-conditional-log-likelihood-function-linear-regression">
<p class="admonition-title"><span class="caption-number">Definition 56 </span> (Conditional Log-Likelihood Function)</p>
<section class="definition-content" id="proof-content">
<p>The conditional log-likelihood function is the log of the probability of observing the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> given the model parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>
is defined as the log of the conditional likelihood function in <a class="reference internal" href="#def-conditional-likelihood-function-linear-regression">Definition 55</a>.</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-log-likelihood-function">
<span class="eqno">(166)<a class="headerlink" href="#equation-eq-linear-regression-log-likelihood-function" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\log \mathcal{L}(\boldsymbol{\beta} \mid \mathcal{S}) &amp;= \log \frac{1}{\left(2 \pi \sigma^2\right)^{N / 2}} \exp \left(-\frac{1}{2 \sigma^2} \left\|\mathbf{y} - \mathbf{X} \boldsymbol{\beta}\right\|_2^2\right) \\
&amp;= -\frac{N}{2} \log \left(2 \pi \sigma^2\right) -\frac{1}{2 \sigma^2} \left\|\mathbf{y} - \mathbf{X} \boldsymbol{\beta}\right\|_2^2 \\
\end{aligned}
\end{split}\]</div>
</section>
</div><p>We should already see where this is going. The right hand side of equation
<a class="reference internal" href="#equation-eq-linear-regression-log-likelihood-function">(166)</a> has a term <span class="math notranslate nohighlight">\(\left\|\mathbf{y} - \mathbf{X} \boldsymbol{\beta}\right\|_2^2\)</span>
which looks very similar to the loss function in <a class="reference internal" href="#equation-eq-linear-regression-cost-function-1">(158)</a>.</p>
</section>
<section id="maximum-likelihood-estimation">
<h3><a class="toc-backref" href="#id40" role="doc-backlink">Maximum Likelihood Estimation</a><a class="headerlink" href="#maximum-likelihood-estimation" title="Link to this heading">#</a></h3>
<p>With the likelihood function defined, we can start our discussion on Maximum Likelihood Estimation (MLE).</p>
<p>Since likelihood function, we can maximize it to find the optimal parameters <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>.</p>
<p>Recall in the objective function in <a class="reference internal" href="#equation-eq-linear-regression-objective-function-2">(160)</a> below, we are minimizing the cost function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{alignat}{1}
\hat{\boldsymbol{\beta}} &amp;= \underset{\boldsymbol{\beta} \in \boldsymbol{\Theta}}{\operatorname{argmin}} &amp;\quad&amp;\widehat{\mathcal{J}}\left(\boldsymbol{\beta} \mid \mathcal{S}\right) = \underset{\boldsymbol{\beta} \in \boldsymbol{\Theta}}{\operatorname{argmin}} \quad \frac{1}{N} \left\| \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \right\|_2^2 \\
\end{alignat}
\end{split}\]</div>
<p>To do the same, instead of maximizing the log-likelihood function, we can minimize the negative log-likelihood function,
which yields the same optimal parameters.</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-maximum-likelihood-estimation-1">
<span class="eqno">(167)<a class="headerlink" href="#equation-eq-linear-regression-maximum-likelihood-estimation-1" title="Link to this equation">#</a></span>\[\begin{split}
\begin{alignat}{1}
\hat{\boldsymbol{\beta}} &amp;= \underset{\boldsymbol{\beta} \in \boldsymbol{\Theta}}{\operatorname{argmax}} \quad \log \mathcal{L}(\boldsymbol{\beta} \mid \mathcal{S}) \\
&amp;= \underset{\boldsymbol{\beta} \in \boldsymbol{\Theta}}{\operatorname{argmin}} \quad -\log \mathcal{L}(\boldsymbol{\beta} \mid \mathcal{S}) \\
&amp;= \underset{\boldsymbol{\beta} \in \boldsymbol{\Theta}}{\operatorname{argmin}} \quad \frac{N}{2} \log \left(2 \pi \sigma^2\right) + \frac{1}{2 \sigma^2} \left\|\mathbf{y} - \mathbf{X} \boldsymbol{\beta}\right\|_2^2 \\
\end{alignat}
\end{split}\]</div>
<p>We seek to find the optimal parameters <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> that minimize the negative log-likelihood function.
We compute the gradient of the negative log-likelihood function with respect to the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-linear-regression-maximum-likelihood-estimation-2">
<span class="eqno">(168)<a class="headerlink" href="#equation-eq-linear-regression-maximum-likelihood-estimation-2" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\frac{\partial}{\partial \hat{\boldsymbol{\beta}}} \left(-\log \mathcal{L}(\hat{\boldsymbol{\beta}} \mid \mathcal{S})\right) &amp;= \frac{\partial}{\partial \hat{\boldsymbol{\beta}}} \left(\frac{N}{2} \log \left(2 \pi \sigma^2\right) + \frac{1}{2 \sigma^2} \left\|\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}}\right\|_2^2\right) \\
&amp;= \frac{\partial}{\partial \hat{\boldsymbol{\beta}}}\left(\frac{1}{2 \sigma^2} \left\|\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}}\right\|_2^2\right) \\
&amp;= \frac{\partial}{\partial \hat{\boldsymbol{\beta}}}\left(\frac{1}{2 \sigma^2} \left(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}}\right)^T \left(\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}}\right)\right) &amp;&amp; (*) \\
&amp;= \frac{\partial}{\partial \hat{\boldsymbol{\beta}}}\left(\frac{1}{2 \sigma^2} \left(\mathbf{y}^T \mathbf{y} - 2 \mathbf{y}^T \mathbf{X} \hat{\boldsymbol{\beta}} + \hat{\boldsymbol{\beta}}^T \mathbf{X}^T \mathbf{X} \hat{\boldsymbol{\beta}}\right)\right) \\
&amp;= \frac{1}{\sigma^2} \left(-\mathbf{y}^T \mathbf{X} + \hat{\boldsymbol{\beta}}^T \mathbf{X}^T \mathbf{X}\right) \in \mathbb{R}^{1 \times D} &amp;&amp; (**) \\
\end{aligned}
\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((*)\)</span> is the expansion of the norm <span class="math notranslate nohighlight">\(\left\|\mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}}\right\|_2^2\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\((**)\)</span> is the chain rule, where <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \boldsymbol{\beta}} \boldsymbol{\beta}^T \mathbf{A} \boldsymbol{\beta} = \mathbf{A} + \mathbf{A}^T\)</span>.</p></li>
</ul>
<p>Next, we will set the gradient to zero and solve for the optimal parameters <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial}{\partial \hat{\boldsymbol{\beta}}} \left(-\log \mathcal{L}(\hat{\boldsymbol{\beta}} \mid \mathcal{S})\right) = \boldsymbol{0}^{\top} &amp;\overset{(**)}{\iff} \frac{1}{\sigma^2} \left(-\mathbf{y}^T \mathbf{X} + \hat{\boldsymbol{\beta}}^T \mathbf{X}^T \mathbf{X}\right) = \boldsymbol{0}^{\top} \\
&amp;\iff \hat{\boldsymbol{\beta}}^T \mathbf{X}^T \mathbf{X} = \mathbf{y}^T \mathbf{X} &amp;&amp; (\star) \\
&amp;\iff \hat{\boldsymbol{\beta}}^T = \mathbf{y}^T \mathbf{X} \left(\mathbf{X}^T \mathbf{X}\right)^{-1} &amp;&amp; (\star \star) \\
&amp;\iff \hat{\boldsymbol{\beta}} = \left(\mathbf{X}^T \mathbf{X}\right)^{-1} \mathbf{X}^T \mathbf{y} &amp;&amp; (\star \star \star) \\
\end{aligned}
\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\star)\)</span> is just rearranging the terms.</p></li>
<li><p><span class="math notranslate nohighlight">\((\star \star)\)</span> is the definition of the matrix inverse where we right-multiply the by <span class="math notranslate nohighlight">\(\left(\mathbf{X}^T \mathbf{X}\right)^{-1}\)</span>. This is
allowed because <span class="math notranslate nohighlight">\(\left(\mathbf{X}^T \mathbf{X}\right)\)</span> is <a class="reference external" href="https://en.wikipedia.org/wiki/Positive-definite_matrix"><strong>positive definite</strong></a> (invertible)
and rank of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is <span class="math notranslate nohighlight">\(D\)</span>.</p></li>
</ul>
<p>We now see that the solution is the same as the solution to the normal equations.</p>
</section>
<section id="mle-estimates-the-conditional-expectation-of-the-target-variable-given-the-input">
<h3><a class="toc-backref" href="#id41" role="doc-backlink">MLE estimates the conditional expectation of the target variable given the input</a><a class="headerlink" href="#mle-estimates-the-conditional-expectation-of-the-target-variable-given-the-input" title="Link to this heading">#</a></h3>
<p>In <a class="reference internal" href="#equation-eq-conditional-expectation-1">(146)</a>, we saw that linear regression can be expressed
as a conditional expectation of the target variable given the input under the probabilistic model.</p>
<p>Therefore, the MLE estimate <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>, when plugged into the equation</p>
<div class="math notranslate nohighlight">
\[
\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_D x_D
\]</div>
<p>gives us the estimate of the conditional expectation of the target variable given the input.</p>
<p>See <a class="reference external" href="https://www.sophieheloisebennett.com/posts/linear-regression-conditional-mean/#:~:text=%E2%80%9CLinear%20regression%20estimates%20the%20conditional,of%20the%20response%20variable%20Y%20.">here</a>
for more info.</p>
</section>
</section>
<section id="performance-metrics">
<h2><a class="toc-backref" href="#id42" role="doc-backlink">Performance Metrics</a><a class="headerlink" href="#performance-metrics" title="Link to this heading">#</a></h2>
<p>While not the focus of this post, we will briefly discuss some of the most common performance metrics for linear regression.</p>
<section id="mean-squared-error-mse">
<h3><a class="toc-backref" href="#id43" role="doc-backlink">Mean Squared Error (MSE)</a><a class="headerlink" href="#mean-squared-error-mse" title="Link to this heading">#</a></h3>
<p>As we have seen earlier in the loss function section, the Mean Squared Error (MSE)
can also be used to evaluate the performance of a linear regression model.</p>
<p>Mean Squared Error (MSE) is a widely used metric for evaluating the performance of regression models. It represents the average of the squared differences between the actual values and the predicted values of the dependent variable. Lower MSE values indicate a better fit of the model, as the squared differences between the predicted and actual values are smaller on average.</p>
<p>The MSE is calculated as follows:</p>
<div class="math notranslate nohighlight">
\[
\text{MSE} = \frac{1}{N} \sum_{n=1}^{N}(y^{(n)} - \hat{y}^{(n)})^2
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{MSE}\)</span> is the Mean Squared Error,</p></li>
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the number of observations,</p></li>
<li><p><span class="math notranslate nohighlight">\(y^{(n)}\)</span> is the actual value of the dependent variable for observation <span class="math notranslate nohighlight">\(n\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{y}^{(n)}\)</span> is the predicted value of the dependent variable for observation <span class="math notranslate nohighlight">\(n\)</span>.</p></li>
</ul>
<p>MSE has several desirable properties as a loss function for regression tasks:</p>
<ol class="arabic simple">
<li><p>It is non-negative, meaning that it always returns a positive value, or zero when the predictions are perfect.</p></li>
<li><p>It penalizes larger errors more heavily than smaller errors, as the differences between the predicted and actual values are squared. This makes the model more sensitive to outliers and encourages it to fit the data more closely.</p></li>
<li><p>It is differentiable, which allows for the application of optimization techniques like gradient descent to find the best model parameters.</p></li>
</ol>
<p>However, MSE has some limitations:</p>
<ol class="arabic simple">
<li><p>The squared differences can lead to overemphasis on outliers, which might not be desirable in some applications.</p></li>
<li><p>The scale of MSE depends on the scale of the dependent variable, making it challenging to compare the performance of models with different output scales.</p></li>
</ol>
<p>In practice, other metrics like Root Mean Squared Error (RMSE) or Mean Absolute Error (MAE) can be used alongside MSE to better understand the model’s performance and address its limitations.</p>
</section>
<section id="r-squared">
<h3><a class="toc-backref" href="#id44" role="doc-backlink">R-Squared</a><a class="headerlink" href="#r-squared" title="Link to this heading">#</a></h3>
<p>R-squared (<span class="math notranslate nohighlight">\(R^2\)</span>) is a statistical measure that represents the proportion of the variance in the dependent variable (response) that can be explained by the independent variables (predictors) in a linear regression model. It is a value between 0 and 1, and a higher R-squared value indicates a better fit of the model. The R-squared metric can be understood as the proportion of variation in the data that is accounted for by the fitted model.</p>
<div class="proof definition admonition" id="def-r-squared">
<p class="admonition-title"><span class="caption-number">Definition 57 </span> (R-squared)</p>
<section class="definition-content" id="proof-content">
<p>The R-squared value is defined as:</p>
<div class="math notranslate nohighlight">
\[
R^2 = \dfrac{SS_{tot} - SS_{res}}{SS_{tot}} = 1 - \frac{SS_{res}}{SS_{tot}}
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(R^2\)</span> is the R-squared value,</p></li>
<li><p><span class="math notranslate nohighlight">\(SS_{res}\)</span> is the residual sum of squares (sum of the squared differences between the actual and predicted values),</p></li>
<li><p><span class="math notranslate nohighlight">\(SS_{tot}\)</span> is the total sum of squares (sum of the squared differences between the actual values and the mean of the actual values).</p></li>
</ul>
<p>Mathematically, the residual sum of squares is defined as:</p>
<div class="math notranslate nohighlight">
\[
SS_{res} = \sum_{n=1}^{N}(y^{(n)} - \hat{y}^{(n)})^2
\]</div>
<p>And the total sum of squares is defined as:</p>
<div class="math notranslate nohighlight">
\[
SS_{tot} = \sum_{n=1}^{n}(y^{(n)} - \bar{y})^2
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the number of observations,</p></li>
<li><p><span class="math notranslate nohighlight">\(y^{(n)}\)</span> is the actual value of the dependent variable for observation <span class="math notranslate nohighlight">\(n\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{y}^{(n)}\)</span> is the predicted value of the dependent variable for observation <span class="math notranslate nohighlight">\(n\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\bar{y}\)</span> is the mean of the actual values of the dependent variable.</p></li>
</ul>
</section>
</div><p>One should realize that <span class="math notranslate nohighlight">\(SS_{tot}\)</span> refers to the total variation in the response variable, which can be
thought of as the amount of variation inherently in the response before we fit the regression model.
In constrast, <span class="math notranslate nohighlight">\(SS_{res}\)</span> refers to the variation in the response variable that is left unexplained
after fitting the regression model.</p>
<p>Consequently, the definition of R-squared is fairly straight-forward; it is the percentage of the response variable variation that is explained by a linear model.
In other words, R-squared is Explained variation / Total variation.</p>
<p>R-squared is always between 0 and 100% where</p>
<ul class="simple">
<li><p>0% indicates that the model explains none of the variability of the response data around its mean.</p></li>
<li><p>100% indicates that the model explains all the variability of the response data around its mean.</p></li>
</ul>
<p>As popular as it is, R-squared is not without its flaws.</p>
<ol class="arabic simple">
<li><p>Every time you add a predictor to a model, the R-squared increases, even if due to chance alone. It never decreases. Consequently, a model with more terms may appear to have a better fit simply because it has more terms.</p></li>
<li><p>If a model has too many predictors and higher order polynomials, it begins to model the random noise in the data. This condition is known as <a class="reference external" href="https://blog.minitab.com/blog/adventures-in-statistics/the-danger-of-overfitting-regression-models">overfitting the model</a> and it produces misleadingly high R-squared values and a lessened ability to make predictions.</p></li>
</ol>
<p>A limitation of R-squared is that it can only increase, or stay the same, as you add more predictors to the model, even if those predictors are not truly improving the model’s fit. This is why it’s important to consider other metrics like adjusted R-squared, which penalizes the model for the inclusion of unnecessary predictors, or use other methods like cross-validation to evaluate the model’s performance.</p>
</section>
<section id="adjusted-r-squared">
<h3><a class="toc-backref" href="#id45" role="doc-backlink">Adjusted R-Squared</a><a class="headerlink" href="#adjusted-r-squared" title="Link to this heading">#</a></h3>
<p>To address the limitations of R-squared, we can use the adjusted R-squared metric, where
it is a modification of the R-squared (<span class="math notranslate nohighlight">\(R^2\)</span>) metric that takes into account the number of predictors in a linear regression model. It adjusts the R-squared value to account for the addition of predictors, preventing the artificial inflation of the R-squared value when adding more predictors that may not necessarily improve the model’s fit.</p>
<p>See <a class="reference external" href="https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2">here</a> for more information.</p>
</section>
</section>
<section id="assumptions-of-linear-regression">
<h2><a class="toc-backref" href="#id46" role="doc-backlink">Assumptions of Linear Regression</a><a class="headerlink" href="#assumptions-of-linear-regression" title="Link to this heading">#</a></h2>
<p>This section is adapted from the blog post,
<a class="reference external" href="https://jeffmacaluso.github.io/post/LinearRegressionAssumptions/">Linear Regression Assumptions by Jeff Macaluso</a>.</p>
<p>While it is important for one to check the assumptions of linear regression, it is also
important to know that we do not require them to <em>fit</em> the model. We only require them to <em>interpret</em> the model.
The difference is subtle, but important. If you view the linear regression from the Ordinary Least Squares (OLS) perspective,
then the only assumption you need is for the matrix <span class="math notranslate nohighlight">\(\mathbf{X}^T\mathbf{X}\)</span> to be invertible.</p>
<p>For a more rigorous treatment of the assumptions of linear regression, see <a class="reference external" href="https://stats.stackexchange.com/questions/16381/what-is-a-complete-list-of-the-usual-assumptions-for-linear-regression">here</a>.</p>
<section id="linearity">
<h3><a class="toc-backref" href="#id47" role="doc-backlink">Linearity</a><a class="headerlink" href="#linearity" title="Link to this heading">#</a></h3>
<p>This assumes that there is a linear relationship between the predictors (e.g. independent variables or features) and the response variable (e.g. dependent variable or label). This also assumes that the predictors are additive.</p>
<p><strong>Why it can happen:</strong> There may not just be a linear relationship among the data. Modeling is about trying to estimate a function that explains a process, and linear regression would not be a fitting estimator (pun intended) if there is no linear relationship.</p>
<p><strong>What it will affect:</strong> The predictions will be extremely inaccurate because our model is <a class="reference external" href="https://cdn-images-1.medium.com/max/1125/1*_7OPgojau8hkiPUiHoGK_w.png">underfitting</a>. This is a serious violation that should not be ignored. However, do not forget we have learnt that we can model non-linear relationships using polynomial regression.</p>
<p><strong>How to detect it:</strong> If there is only one predictor, this is pretty easy to test with a scatter plot. Most cases aren’t so simple, so we’ll have to modify this by using a scatter plot to see our predicted values versus the actual values (in other words, view the residuals). Ideally, the points should lie on or around a diagonal line on the scatter plot.</p>
<p><strong>How to fix it:</strong> Either adding polynomial terms to some of the predictors or applying nonlinear transformations . If those do not work, try adding additional variables to help capture the relationship between the predictors and the label.</p>
<hr class="docutils" />
<p>We can see that the data is linearly related to the fitted values. This is a good sign that the linear regression model is appropriate for this dataset.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1"># Load your dataset</span>
<span class="linenos"> 2</span><span class="c1"># df = pd.read_csv(&#39;your_dataset.csv&#39;)</span>
<span class="linenos"> 3</span>
<span class="linenos"> 4</span><span class="c1"># For illustration purposes, we create a sample dataset</span>
<span class="linenos"> 5</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="linenos"> 6</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="linenos"> 7</span><span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="linenos"> 8</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;X&#39;</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y</span><span class="p">})</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="c1"># Fit the linear regression model</span>
<span class="linenos">11</span><span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">])</span>
<span class="linenos">12</span><span class="n">model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="linenos">13</span>
<span class="linenos">14</span><span class="c1"># Assumption 1: Linearity</span>
<span class="linenos">15</span><span class="c1"># Use a scatter plot and the fitted values to visually inspect linearity</span>
<span class="linenos">16</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="linenos">17</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;X&#39;</span><span class="p">],</span> <span class="n">model</span><span class="o">.</span><span class="n">fittedvalues</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Fitted Line&#39;</span><span class="p">)</span>
<span class="linenos">18</span><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="linenos">19</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="linenos">20</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="linenos">21</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Linearity Check&#39;</span><span class="p">)</span>
<span class="linenos">22</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/c4ea16e393bde8dc408966d34f6b8d48fe661b36aa2821b4def3d521f4ee42a8.svg" src="../../_images/c4ea16e393bde8dc408966d34f6b8d48fe661b36aa2821b4def3d521f4ee42a8.svg" />
</div>
</div>
</section>
<hr class="docutils" />
<section id="homoscedasticity">
<h3><a class="toc-backref" href="#id48" role="doc-backlink">Homoscedasticity</a><a class="headerlink" href="#homoscedasticity" title="Link to this heading">#</a></h3>
<p>This assumes homoscedasticity, which is the same <strong>variance</strong> within our error terms. Heteroscedasticity, the violation of homoscedasticity, occurs when we don’t have an even <strong>variance</strong> across the error terms.</p>
<p><strong>Why it can happen:</strong> Our model may be giving too much weight to a subset of the data, particularly where the error variance was the largest.</p>
<p><strong>What it will affect:</strong> Significance tests for coefficients due to the standard errors being biased. Additionally, the confidence intervals will be either too wide or too narrow.</p>
<p><strong>How to detect it:</strong> Plot the residuals and see if the variance appears to be uniform.</p>
<p><strong>How to fix it:</strong> Heteroscedasticity (can you tell I like the <em>scedasticity</em> words?) can be solved either by using <a class="reference external" href="https://en.wikipedia.org/wiki/Least_squares#Weighted_least_squares">weighted least squares regression</a> instead of the standard OLS or transforming either the dependent or highly skewed variables. Performing a log transformation on the dependent variable is not a bad place to start.</p>
<hr class="docutils" />
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="c1"># Assumption 2: Homoscedasticity</span>
<span class="linenos">2</span><span class="c1"># Use a scatter plot of residuals vs. fitted values to visually inspect homoscedasticity</span>
<span class="linenos">3</span><span class="n">residuals</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">resid</span>
<span class="linenos">4</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fittedvalues</span><span class="p">,</span> <span class="n">residuals</span><span class="p">)</span>
<span class="linenos">5</span><span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="linenos">6</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Fitted Values&#39;</span><span class="p">)</span>
<span class="linenos">7</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Residuals&#39;</span><span class="p">)</span>
<span class="linenos">8</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Homoscedasticity Check&#39;</span><span class="p">)</span>
<span class="linenos">9</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/373a50fa74e35b322703ed4ef151a920dfe0c2b39594564325067f8a9a0ee8e8.svg" src="../../_images/373a50fa74e35b322703ed4ef151a920dfe0c2b39594564325067f8a9a0ee8e8.svg" />
</div>
</div>
</section>
<section id="normality-of-the-error-terms">
<h3><a class="toc-backref" href="#id49" role="doc-backlink">Normality of the Error Terms</a><a class="headerlink" href="#normality-of-the-error-terms" title="Link to this heading">#</a></h3>
<p>More specifically, this assumes that the error terms of the model are <strong>normally distributed</strong>. Linear regressions other than <a class="reference external" href="https://en.wikipedia.org/wiki/Ordinary_least_squares">Ordinary Least Squares (OLS)</a> may also assume normality of the predictors or the label, but that is not the case here.</p>
<p><strong>Why it can happen:</strong> This can actually happen if either the predictors or the label are significantly non-normal. Other potential reasons could include the linearity assumption being violated or outliers affecting our model.</p>
<p><strong>What it will affect:</strong> A violation of this assumption could cause issues with either shrinking or inflating our confidence intervals.</p>
<p><strong>How to detect it:</strong> There are a variety of ways to do so, but we’ll look at both a histogram and the p-value from the Anderson-Darling test for normality.</p>
<p><strong>How to fix it:</strong> It depends on the root cause, but there are a few options. Nonlinear transformations of the variables, excluding specific variables (such as long-tailed variables), or removing outliers may solve this problem.</p>
<hr class="docutils" />
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1"># Assumption 3: Normality of error terms</span>
<span class="linenos"> 2</span><span class="c1"># Use a Q-Q plot to visually inspect normality</span>
<span class="linenos"> 3</span><span class="n">sm</span><span class="o">.</span><span class="n">qqplot</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">line</span><span class="o">=</span><span class="s1">&#39;45&#39;</span><span class="p">,</span> <span class="n">fit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos"> 4</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Normality Check - Q-Q Plot&#39;</span><span class="p">)</span>
<span class="linenos"> 5</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
<span class="linenos"> 6</span>
<span class="linenos"> 7</span><span class="c1"># Alternatively, use the Shapiro-Wilk test for normality</span>
<span class="linenos"> 8</span><span class="c1"># (Null hypothesis: The residuals are normally distributed)</span>
<span class="linenos"> 9</span><span class="n">shapiro_test</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">shapiro</span><span class="p">(</span><span class="n">residuals</span><span class="p">)</span>
<span class="linenos">10</span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shapiro-Wilk Test:&#39;</span><span class="p">,</span> <span class="n">shapiro_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/ac205e65b116d84368cf99b79c2969ad404670eb1699951506a250a3ecf28792.svg" src="../../_images/ac205e65b116d84368cf99b79c2969ad404670eb1699951506a250a3ecf28792.svg" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shapiro-Wilk Test: ShapiroResult(statistic=0.9846267871943117, pvalue=0.29841614058046384)
</pre></div>
</div>
</div>
</div>
</section>
<section id="no-autocorrelation-between-error-terms">
<h3><a class="toc-backref" href="#id50" role="doc-backlink">No Autocorrelation between Error Terms</a><a class="headerlink" href="#no-autocorrelation-between-error-terms" title="Link to this heading">#</a></h3>
<p>This assumes no autocorrelation of the error terms. Autocorrelation being present typically indicates that we are missing some information that should be captured by the model.</p>
<p><strong>Why it can happen:</strong> In a time series scenario, there could be information about the past that we aren’t capturing. In a non-time series scenario, our model could be systematically biased by either under or over predicting in certain conditions. Lastly, this could be a result of a violation of the linearity assumption.</p>
<p><strong>What it will affect:</strong> This will impact our model estimates.</p>
<p><strong>How to detect it:</strong> We will perform a <a class="reference external" href="https://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic">Durbin-Watson test</a> to determine if either positive or negative correlation is present. Alternatively, you could create plots of residual autocorrelations.</p>
<p><strong>How to fix it:</strong> A simple fix of adding lag variables can fix this problem. Alternatively, interaction terms, additional variables, or additional transformations may fix this.</p>
<hr class="docutils" />
<p>We can use the some tests such as Durbin-Watson test to check for autocorrelation. The null hypothesis of the Durbin-Watson test is that there is no autocorrelation. The test statistic is a number between 0 and 4. The closer the test statistic is to 0, the more positive autocorrelation there is. The closer the test statistic is to 4, the more negative autocorrelation there is. A value of 2 indicates no autocorrelation.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1"># Assumption: No autocorrelation in residuals</span>
<span class="linenos"> 2</span><span class="c1"># Use the Durbin-Watson test to check for autocorrelation</span>
<span class="linenos"> 3</span><span class="c1"># (Null hypothesis: No autocorrelation in the residuals)</span>
<span class="linenos"> 4</span><span class="n">durbin_watson_stat</span> <span class="o">=</span> <span class="n">durbin_watson</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">resid</span><span class="p">)</span>
<span class="linenos"> 5</span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Durbin-Watson statistic:&#39;</span><span class="p">,</span> <span class="n">durbin_watson_stat</span><span class="p">)</span>
<span class="linenos"> 6</span>
<span class="linenos"> 7</span><span class="c1"># A value close to 2 indicates no autocorrelation,</span>
<span class="linenos"> 8</span><span class="c1"># while values &lt; 2 or &gt; 2 indicate positive or negative autocorrelation, respectively.</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="c1"># Alternatively, you can use a plot of residuals vs. time</span>
<span class="linenos">11</span><span class="n">residuals</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">resid</span>
<span class="linenos">12</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">residuals</span><span class="p">)</span>
<span class="linenos">13</span><span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="linenos">14</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time&#39;</span><span class="p">)</span>
<span class="linenos">15</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Residuals&#39;</span><span class="p">)</span>
<span class="linenos">16</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Autocorrelation Check - Residuals vs. Time&#39;</span><span class="p">)</span>
<span class="linenos">17</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="linenos">18</span>
<span class="linenos">19</span><span class="c1"># You can also use the ACF (Autocorrelation Function) plot from the statsmodels library</span>
<span class="linenos">20</span><span class="kn">from</span> <span class="nn">statsmodels.graphics.tsaplots</span> <span class="kn">import</span> <span class="n">plot_acf</span>
<span class="linenos">21</span>
<span class="linenos">22</span><span class="n">plot_acf</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">lags</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Autocorrelation Check - ACF plot&#39;</span><span class="p">)</span>
<span class="linenos">23</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Durbin-Watson statistic: 2.2849976037296673
</pre></div>
</div>
<img alt="../../_images/0ed98502464f1c565ebec8a0ff22e662beac179088390c2ce0e08f51f571bc4c.svg" src="../../_images/0ed98502464f1c565ebec8a0ff22e662beac179088390c2ce0e08f51f571bc4c.svg" />
<img alt="../../_images/f652c4d1accad172d80118760f8b0aa09c3d4d2de38740943d1aac235dbf25bc.svg" src="../../_images/f652c4d1accad172d80118760f8b0aa09c3d4d2de38740943d1aac235dbf25bc.svg" />
</div>
</div>
</section>
<section id="multicollinearity-among-predictors">
<h3><a class="toc-backref" href="#id51" role="doc-backlink">Multicollinearity among Predictors</a><a class="headerlink" href="#multicollinearity-among-predictors" title="Link to this heading">#</a></h3>
<p>This assumes that the predictors used in the regression are not correlated with each other. This won’t render our model unusable if violated, but it will cause issues with the interpretability of the model. This is why in the previous section, we need to make sure that the 3 variables, <strong>square feet, number of bedrooms, age of house</strong> are not highly correlated with each other, else additive effects may happen.</p>
<p><strong>Why it can happen:</strong> A lot of data is just naturally correlated. For example, if trying to predict a house price with square footage, the number of bedrooms, and the number of bathrooms, we can expect to see correlation between those three variables because bedrooms and bathrooms make up a portion of square footage.</p>
<p><strong>What it will affect:</strong> Multicollinearity causes issues with the interpretation of the coefficients. Specifically, you can interpret a coefficient as “an increase of 1 in this predictor results in a change of (coefficient) in the response variable, holding all other predictors constant.” This becomes problematic when multicollinearity is present because we can’t hold correlated predictors constant. Additionally, it increases the standard error of the coefficients, which results in them potentially showing as statistically insignificant when they might actually be significant.</p>
<p><strong>How to detect it:</strong> There are a few ways, but we will use a heatmap of the correlation as a visual aid and examine the <a class="reference external" href="https://en.wikipedia.org/wiki/Variance_inflation_factor">variance inflation factor (VIF)</a>.</p>
<p><strong>How to fix it:</strong> This can be fixed by other removing predictors with a high variance inflation factor (VIF) or performing dimensionality reduction.</p>
<hr class="docutils" />
<p>Let’s create another dataset to see how to detect multicollinearity.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1"># Assumption 5: No perfect multicollinearity</span>
<span class="linenos"> 2</span><span class="c1"># For a simple linear regression, multicollinearity is not an issue.</span>
<span class="linenos"> 3</span><span class="c1"># For multiple linear regression, check the variance inflation factor (VIF)</span>
<span class="linenos"> 4</span><span class="kn">from</span> <span class="nn">statsmodels.stats.outliers_influence</span> <span class="kn">import</span> <span class="n">variance_inflation_factor</span>
<span class="linenos"> 5</span>
<span class="linenos"> 6</span><span class="c1"># Load your multiple linear regression dataset</span>
<span class="linenos"> 7</span><span class="c1"># df_multi = pd.read_csv(&#39;your_multi_dataset.csv&#39;)</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="c1"># For illustration purposes, we create a sample dataset with three variables</span>
<span class="linenos">10</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="linenos">11</span><span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="linenos">12</span><span class="n">X2</span> <span class="o">=</span> <span class="n">X1</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="linenos">13</span><span class="n">X3</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">X1</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">X2</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="linenos">14</span><span class="n">y_multi</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">X1</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X2</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">X3</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="linenos">15</span><span class="n">df_multi</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;X1&#39;</span><span class="p">:</span> <span class="n">X1</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">:</span> <span class="n">X2</span><span class="p">,</span> <span class="s1">&#39;X3&#39;</span><span class="p">:</span> <span class="n">X3</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y_multi</span><span class="p">})</span>
<span class="linenos">16</span>
<span class="linenos">17</span><span class="c1"># Fit the multiple linear regression model</span>
<span class="linenos">18</span><span class="n">X_multi</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">df_multi</span><span class="p">[[</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">,</span> <span class="s1">&#39;X3&#39;</span><span class="p">]])</span>
<span class="linenos">19</span><span class="n">model_multi</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">df_multi</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="n">X_multi</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="linenos">20</span>
<span class="linenos">21</span><span class="c1"># Assumption: No perfect multicollinearity</span>
<span class="linenos">22</span><span class="c1"># Check the variance inflation factor (VIF) for each independent variable</span>
<span class="linenos">23</span><span class="n">vif</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="linenos">24</span><span class="n">vif</span><span class="p">[</span><span class="s1">&#39;Features&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">,</span> <span class="s1">&#39;X3&#39;</span><span class="p">]</span>
<span class="linenos">25</span><span class="n">vif</span><span class="p">[</span><span class="s1">&#39;VIF&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">variance_inflation_factor</span><span class="p">(</span><span class="n">X_multi</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vif</span><span class="p">[</span><span class="s1">&#39;Features&#39;</span><span class="p">]))]</span>
<span class="linenos">26</span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;VIF:&#39;</span><span class="p">)</span>
<span class="linenos">27</span><span class="n">display</span><span class="p">(</span><span class="n">vif</span><span class="p">)</span>
<span class="linenos">28</span>
<span class="linenos">29</span><span class="c1"># Use a heatmap to visualize the correlation between variables</span>
<span class="linenos">30</span><span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">df_multi</span><span class="p">[[</span><span class="s1">&#39;X1&#39;</span><span class="p">,</span> <span class="s1">&#39;X2&#39;</span><span class="p">,</span> <span class="s1">&#39;X3&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="linenos">31</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="linenos">32</span><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;.2f&#39;</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mf">.5</span><span class="p">)</span>
<span class="linenos">33</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Heatmap Correlation Plot&#39;</span><span class="p">)</span>
<span class="linenos">34</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>VIF:
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Features</th>
      <th>VIF</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>X1</td>
      <td>12.928109</td>
    </tr>
    <tr>
      <th>1</th>
      <td>X2</td>
      <td>12.534023</td>
    </tr>
    <tr>
      <th>2</th>
      <td>X3</td>
      <td>8.184352</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../../_images/0756747acce724e9de4e39f344bda93bf11c89cbe01e7a2ff54727f84f65d20d.svg" src="../../_images/0756747acce724e9de4e39f344bda93bf11c89cbe01e7a2ff54727f84f65d20d.svg" />
</div>
</div>
</section>
</section>
<section id="feature-scaling">
<h2><a class="toc-backref" href="#id52" role="doc-backlink">Feature Scaling</a><a class="headerlink" href="#feature-scaling" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Do we need to scale the features before fitting a linear regression model?</p>
</div></blockquote>
<p>This question is often asked in the context of multiple linear regression, but it is actually a more general question about the use of <a class="reference external" href="https://en.wikipedia.org/wiki/Feature_scaling#Standardization">standardization</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Feature_scaling#Normalization">normalization</a> in machine learning.</p>
<p><a class="reference external" href="https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia">Reference</a></p>
<p>The scale and location of the explanatory variables does not affect the <em>validity</em> of the regression model in any way.</p>
<p>Consider the model <span class="math notranslate nohighlight">\(y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \varepsilon\)</span>. If you scale <span class="math notranslate nohighlight">\(x_1\)</span> by a factor <span class="math notranslate nohighlight">\(a\)</span>, the model becomes <span class="math notranslate nohighlight">\(y = \beta_0 + \beta_1 a x_1 + \beta_2 x_2 + \beta_3 x_3 + \epsilon\)</span>. The coefficients <span class="math notranslate nohighlight">\(\beta_1, \beta_2, \beta_3\)</span> are not affected by this scaling. The reason is that these are the slopes of the fitting surface - how much the surface changes if you change <span class="math notranslate nohighlight">\(x_1, x_2, x_3\)</span> one unit. This does not depend on location. (The intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> does depend on location, but this is not a problem.)</p>
<p>Let’s see for ourselves.</p>
<p>Let’s look at the feature <span class="math notranslate nohighlight">\(x_1\)</span> and its corresponding coefficient <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>.</p>
<p>We claim that scaling <span class="math notranslate nohighlight">\(x_1\)</span> with a factor <span class="math notranslate nohighlight">\(a\)</span> scales <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>by a factor <span class="math notranslate nohighlight">\(\frac{1}{a}\)</span>. To see this, note that</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_1(x_1)=\dfrac{\sum_{n=1}^{N}\left(x_{1}^{(n)}−\bar{x}_1\right)\left(y^{(n)}−\bar{y}\right)}{\sum_{n=1}^{N}\left(x_{1}^{(n)}−\bar{x}_1\right)^2}
\]</div>
<p>Thus</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_1(ax_1)=\dfrac{\sum_{n=1}^{N}\left(ax_{1}^{(n)}−a\bar{x}_1\right)\left(y^{(n)}−\bar{y}\right)}{\sum_{n=1}^{N}\left(ax_{1}^{(n)}−a\bar{x}_1\right)^2}=\dfrac{\sum_{n=1}^{N}\left(x_{1}^{(n)}−\bar{x}_1\right)\left(y^{(n)}−\bar{y}\right)}{\sum_{n=1}^{N}\left(x_{1}^{(n)}−\bar{x}_1\right)^2}=\dfrac{1}{a}\hat{\beta}_1(x_1)
\]</div>
<p>Thus, scaling simply corresponds to scaling the corresponding slopes. Because if we scale <strong>square feet (</strong><span class="math notranslate nohighlight">\(x_1\)</span>) by a factor of <span class="math notranslate nohighlight">\(\frac{1}{10}\)</span>, then if the original <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>
was <strong>square feet</strong> is 100, then the above shows that the new <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> will be multiplied by 10, becoming 1000, therefore, the interpretation of the coefficients did not change.</p>
<p>However, if you are using Gradient Descent (an optimization algorithm) in Regression, then centering, or scaling the variables, may prove to be faster for convergence.</p>
</section>
<hr class="docutils" />
<section id="model-inference">
<h2><a class="toc-backref" href="#id53" role="doc-backlink">Model Inference</a><a class="headerlink" href="#model-inference" title="Link to this heading">#</a></h2>
<p>Say we have obtained the following estimates for the coefficients of the multiple linear regression model:</p>
<div class="math notranslate nohighlight">
\[
\hat{\beta}_0 = 92451, \hat{\beta}_1 = 139, \hat{\beta}_2 = -8621, \hat{\beta}_3 = -81
\]</div>
<p>and so our estimated model <span class="math notranslate nohighlight">\(\hat{y}: = \hat{h}(\mathbf{x})\)</span> of the ground truth <span class="math notranslate nohighlight">\(y:= f(\mathbf{x})\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\hat{y} = 92451+139x_1-8621x_2-81x_3
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_1 = \text{square feet}\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(x_2 = \text{number of bed rooms}\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(x_3 = \text{age of house}\)</span></p></li>
</ul>
<p>The coefficient value signifies how much the mean of the dependent variable <span class="math notranslate nohighlight">\(y\)</span> changes given a one-unit shift in the independent variable <span class="math notranslate nohighlight">\(x\)</span> while <strong>holding other variables in the model constant.</strong> This property of holding the other variables constant is crucial because it allows you to assess the effect of each variable in isolation from the others.</p>
<p>For example, if we hold the number of bedrooms and age of house constant, then the coefficient of square feet is 139, which means that for every additional square foot, the price of the house increases by 139 dollars.</p>
</section>
<section id="mean-squared-error-is-an-unbiased-estimator-of-the-variance-of-the-error">
<h2><a class="toc-backref" href="#id54" role="doc-backlink">Mean Squared Error is an Unbiased Estimator of the Variance of the Error</a><a class="headerlink" href="#mean-squared-error-is-an-unbiased-estimator-of-the-variance-of-the-error" title="Link to this heading">#</a></h2>
<p>Mean Squared Error (MSE) is a commonly used metric for evaluating the performance of a regression model. It measures the average squared difference between the predicted values and the actual values. In the context of linear regression, the MSE is an unbiased estimator of the variance of the error term. This means that, on average, the MSE is equal to the true variance of the error term in the population.</p>
<p>The MSE is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
MSE(\boldsymbol{\beta}) := \hat{mathcal{J}}(\boldsymbol{\beta}) &amp;= \frac{1}{N} \sum_{n=1}^{N} \left(y^{(n)} - \hat{y}^{(n)}\right)^2 \\
&amp;= \frac{1}{N} \sum_{n=1}^{N} \left(y^{(n)} - \left(\beta_0 + \beta_1 x_1^{(n)} + \ldots \beta_D x_D^{(n)}\right)\right)^2
\end{aligned}
\end{split}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the number of observations.</p></li>
<li><p><span class="math notranslate nohighlight">\(y^{(n)}\)</span> is the actual value of the dependent variable for observation <span class="math notranslate nohighlight">\(n\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\hat{y}^{(n)}\)</span> is the predicted value of the dependent variable for observation <span class="math notranslate nohighlight">\(n\)</span>.</p></li>
</ul>
<p>In the context of linear regression, the error term is defined as the difference between the actual value and the predicted value:</p>
<div class="math notranslate nohighlight">
\[
\varepsilon^{(n)} = y^{(n)} - \hat{y}^{(n)} = y^{(n)} - \left(\beta_0 + \beta_1 x_1^{(n)} + \ldots \beta_D x_D^{(n)}\right)
\]</div>
<p>The variance of the error term is:</p>
<div class="math notranslate nohighlight">
\[
\sigma^2 = \frac{1}{N} \sum_{n=1}^{N} \left(\varepsilon^{(n)}\right)^2
\]</div>
<p>However, in practice, we do not know the true values of the parameters <span class="math notranslate nohighlight">\(\beta_0, \beta_1, \ldots, \beta_D\)</span>.
We estimate them using the sample data <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, which gives us the predicted values <span class="math notranslate nohighlight">\(\hat{y}^{(n)} = \beta_0 + \beta_1 x_1^{(n)} + \ldots \beta_D x_D^{(n)}\)</span>.
Therefore, the estimated variance of the error term is:</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^2 = \frac{1}{N} \sum_{n=1}^{N} \left(y^{(n)} - \hat{y}^{(n)}\right)^2 = \hat{\mathcal{J}}(\hat{\boldsymbol{\beta}}) = MSE(\hat{\boldsymbol{\beta}})
\]</div>
<p>The MSE is an unbiased estimator of the variance of the error term because the expected value of the MSE is equal to the true variance of the error term:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[MSE(\hat{\boldsymbol{\beta}})] = \mathbb{E}\left[\frac{1}{N} \sum_{n=1}^{N} \left(y^{(n)} - \hat{y}^{(n)}\right)^2\right] = \frac{1}{N} \sum_{n=1}^{N} \mathbb{E}\left[\left(y^{(n)} - \hat{y}^{(n)}\right)^2\right] = \frac{1}{N} \sum_{n=1}^{N} \sigma^2 = \sigma^2
\]</div>
<p>This property is important because it ensures that the MSE provides an accurate measure of the model’s performance. If the MSE were a biased estimator, it would consistently overestimate or underestimate the true variance of the error term, leading to incorrect inferences about the model’s performance. Since the MSE is an unbiased estimator, it provides a reliable estimate of the model’s error variance, making it a useful metric for model evaluation and comparison.</p>
</section>
<section id="estimated-coefficients-are-multivariate-normally-distributed">
<h2><a class="toc-backref" href="#id55" role="doc-backlink">Estimated Coefficients are Multivariate Normally Distributed</a><a class="headerlink" href="#estimated-coefficients-are-multivariate-normally-distributed" title="Link to this heading">#</a></h2>
<p>With the assumption that the error term <span class="math notranslate nohighlight">\(\varepsilon \mid X \sim N[0, \sigma^2\mathbf{I}]\)</span>, we derive the weaker condition<a class="footnote-reference brackets" href="#weaker-condition" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>  of <span class="math notranslate nohighlight">\(\varepsilon \sim N[0, \sigma^2\mathbf{I}]\)</span>. It is necessary to make this assumption for us to <em><strong>know the distribution of</strong></em> <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>, because without this assumption, we will be at a loss of what distribution <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> can take on. We claim first that since <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}|\mathbf{X} \sim N[0, \sigma^2\mathbf{I}]\)</span> follows a multivariate normal distribution of mean <span class="math notranslate nohighlight">\(0\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\mathbf{I}\)</span>, then it implies that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> also follows a multivariate normal distribution of mean <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}\)</span>.</p>
<p>To justify this, we need to recall the variance-covariance matrix for <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> to have variance <span class="math notranslate nohighlight">\(\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}\)</span>\footnote{it is necessary to know the mean and variance of <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}\)</span> in order to find the variance-covariance of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>.}, and earlier on we also established the proof of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> being an unbiased estimator, which simply states that <span class="math notranslate nohighlight">\(\mathrm{E}[\hat{\boldsymbol{\beta}}] = \boldsymbol{\beta}\)</span>. So we know  the mean and variance of our OLS estimators <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>, but one last question, why does <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}\)</span> taking on a multivariate normal distribution imply that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> must also follow a multivariate normal distribution?</p>
<p>This is a consequence of that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is a linear estimator, recall that we are treating <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> as constant and non-stochastic, and consequently <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is linear function of the error vector <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}} = \boldsymbol{\beta} + (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\boldsymbol{\varepsilon} = g(\boldsymbol{\varepsilon})
\]</div>
<p>And using the property that <em><strong>a linear transform of a normal random variable is normal</strong></em>, we can easily see that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> must be normal as well since <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is a linear transformation of <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}\)</span>.</p>
<p>Thus so far we established the least squares parameters <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is normally distributed with mean <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}\)</span>. More formally, we say that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> follows a <strong>multi-variate normal distribution</strong> because <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is a random vector, the distinction is that the variance of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is now a variance-covariance matrix instead of just a variance vector. But recall we do not know the real value of <span class="math notranslate nohighlight">\(\sigma^2\)</span> and therefore we replace <span class="math notranslate nohighlight">\(\sigma^2\)</span> with the <strong>unbiased variance estimator of the error variance, which we define and name it to be the Mean Squared Error (MSE)</strong> It is useful to remind yourself that we are learning ML, although MSE has a slightly different formula from the MSE in sklearn, the end goal is the same, refer to <a class="reference external" href="https://stats.stackexchange.com/questions/448005/confusion-on-mean-squared-error-for-regression">Confusion on MSE</a>.</p>
</section>
<section id="assessing-the-accuracy-of-the-coefficient-estimates">
<h2><a class="toc-backref" href="#id56" role="doc-backlink">Assessing the Accuracy of the Coefficient Estimates</a><a class="headerlink" href="#assessing-the-accuracy-of-the-coefficient-estimates" title="Link to this heading">#</a></h2>
<p>This section is adapted from An Introduction to Statistical Learning <span id="id12">[]</span>.</p>
<p>Recall that we are ultimately always interested in drawing conclusions about the population, not the particular sample we observed. This is an important sentence to understand, the reason we are testing our hypothesis on the population parameter instead of the estimated parameter is because we are interested in knowing our real population parameter, and we are using the estimated parameter to provide some statistical gauge. In the Simple Linear Regression setting, we are often interested in learning about the population intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> and the population slope <span class="math notranslate nohighlight">\(\beta_1\)</span>. As you know, <strong>confidence intervals and hypothesis tests</strong> are two related, but different, ways of learning about the values of population parameters. Here, we will learn how to calculate confidence intervals and conduct different hypothesis tests for both <span class="math notranslate nohighlight">\(\beta_0\)</span> and <span class="math notranslate nohighlight">\(\beta_1\)</span>.We turn our heads back to the SLR section, because when we ingest and digest concepts, it is important to start from baby steps first and generalize.</p>
<p>As a continuation of our running example in the univariate setting, we have 1 predictor which is the
area of the house, in square feet, and we are interested in learning about the population intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> and the population slope <span class="math notranslate nohighlight">\(\beta_1\)</span>.</p>
<p>As an example, assumed we have already learned that the estimated intercept <span class="math notranslate nohighlight">\(\hat{\beta}_0\)</span> and the estimated slope <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> are given by:</p>
<div class="math notranslate nohighlight" id="equation-eq-estimated-intercept-slope-linear-regression-univariate-1">
<span class="eqno">(169)<a class="headerlink" href="#equation-eq-estimated-intercept-slope-linear-regression-univariate-1" title="Link to this equation">#</a></span>\[
\beta_0 = 114418, \beta_1 = 962
\]</div>
<p>where we round the numbers to the nearest integer for simplicity.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;House Size (sqft)&#39;</span><span class="p">]</span>
<span class="linenos"> 2</span><span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;Price ($)&#39;</span><span class="p">]</span>
<span class="linenos"> 3</span><span class="n">coefficients</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="linenos"> 4</span><span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span> <span class="o">=</span> <span class="n">coefficients</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">coefficients</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="linenos"> 5</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Estimated intercept: </span><span class="si">{</span><span class="n">intercept</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="linenos"> 6</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Estimated slope: </span><span class="si">{</span><span class="n">slope</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="linenos"> 7</span>
<span class="linenos"> 8</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="linenos"> 9</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">coefficients</span><span class="p">)(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">X</span><span class="p">)),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="linenos">10</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;House Size (sqft)&#39;</span><span class="p">)</span>
<span class="linenos">11</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Price ($)&#39;</span><span class="p">)</span>
<span class="linenos">12</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Estimated intercept: 114418.53
Estimated slope: 961.70
</pre></div>
</div>
<img alt="../../_images/c791606b6274e8caf85cbbf6ac8e21ef7693609515e3947683ffe4579f371d51.svg" src="../../_images/c791606b6274e8caf85cbbf6ac8e21ef7693609515e3947683ffe4579f371d51.svg" />
</div>
</div>
<p>As we can see above from both the fitted plot and the OLS coefficients, there does seem to be a linear relationship between the two. Furthermore, the OLS regression line’s equation can be easily calculated and given by:</p>
<div class="math notranslate nohighlight" id="equation-eq-estimated-intercept-slope-linear-regression-univariate-2">
<span class="eqno">(170)<a class="headerlink" href="#equation-eq-estimated-intercept-slope-linear-regression-univariate-2" title="Link to this equation">#</a></span>\[
\hat{y} = \beta_0 + \beta_1x_1 = 114418 + 962x_1
\]</div>
<p>where <span class="math notranslate nohighlight">\(x_1\)</span> is the area of the house in square feet.</p>
<p>And so we know the <strong>estimated</strong> slope parameter <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> is <span class="math notranslate nohighlight">\(962\)</span>, and apparently there exhibits a “relationship” between <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. Remember, if there is no relationship, then our optimal <strong>estimated</strong> parameter <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> should be 0, as a coefficient of <span class="math notranslate nohighlight">\(0\)</span> means that <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(y\)</span> has no relationship (or at least in the linear form, the same however, cannot be said for non-linear models!). But be careful, although we can be certain that there is a relationship between <code class="docutils literal notranslate"><span class="pre">house</span> <span class="pre">area</span></code> and the <code class="docutils literal notranslate"><span class="pre">sale</span> <span class="pre">price</span></code>, but it is only <strong>limited</strong> to the <span class="math notranslate nohighlight">\(100\)</span> training samples that we have!</p>
<p>In fact, we want to know if there is a relationship between the <em><strong>population parameter</strong></em> of all of the <code class="docutils literal notranslate"><span class="pre">house</span> <span class="pre">area</span></code> and its corresponding <code class="docutils literal notranslate"><span class="pre">sale</span> <span class="pre">price</span></code> in the whole <strong>population (country)</strong>. More concretely, we want to know if the <em><strong>true population slope</strong></em> is close to the estimated slope parameter <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> that we have obtained from our sample.</p>
<p>It follows that we also want to ascertain that the <strong>true population slope</strong> <span class="math notranslate nohighlight">\(\beta_1\)</span>is <strong>unlikely</strong> to be 0 as well. Note that <span class="math notranslate nohighlight">\(0\)</span> is a common benchmark we use in linear regression, but it, in fact can be any number. This is why we have to draw <strong>inferences</strong> from <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> to make substantiate conclusion on the true population slope <span class="math notranslate nohighlight">\(\beta_1\)</span>.</p>
<p>Let us formulate our question/hypothesis by asking the question: <strong>Do our <code class="docutils literal notranslate"><span class="pre">house</span> <span class="pre">area</span></code> and <code class="docutils literal notranslate"><span class="pre">sale</span> <span class="pre">price</span></code> exhibit a true linear relationship in our population? Can we make inferences of our true population parameters based on the estimated parameters (OLS estimates)?</strong></p>
<p>Thus, we can use the infamous scientific method <strong>Hypothesis Testing</strong> by defining our null hypothesis and alternate hypothesis as follows:</p>
<ul class="simple">
<li><p>Null Hypothesis <span class="math notranslate nohighlight">\(H_0\)</span>: <span class="math notranslate nohighlight">\(\beta_1=0\)</span></p></li>
<li><p>Alternative Hypothesis <span class="math notranslate nohighlight">\(H_1\)</span>: <span class="math notranslate nohighlight">\(\beta_1\neq 0\)</span></p></li>
</ul>
<p>Basically, the null hypothesis says that <span class="math notranslate nohighlight">\(\beta_1=0\)</span>, indicating that there is no relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(y\)</span>. Indeed, if <span class="math notranslate nohighlight">\(\beta_1=0\)</span>, our original model reduces to <span class="math notranslate nohighlight">\(y=\beta_0+\varepsilon\)</span>, and this shows <span class="math notranslate nohighlight">\(X\)</span> does not depend on <span class="math notranslate nohighlight">\(y\)</span> at all. To test the <strong>null hypothesis</strong>, we <strong>instead</strong> need to determine whether <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>, our OLS estimate for <span class="math notranslate nohighlight">\(\beta_1\)</span>, is <em><strong>sufficiently far from 0</strong></em> so that we are <strong>confident</strong> that the real parameter <span class="math notranslate nohighlight">\(\beta_1\)</span> is non-zero. Note the distinction here that we emphasized that we are performing a hypothesis testing on the <strong>true population parameter</strong> but we depend on the value of the <strong>estimate of the true population parameter since we have no way to know the underlying true population parameter.</strong></p>
<hr class="docutils" />
<section id="standard-error">
<h3><a class="toc-backref" href="#id57" role="doc-backlink">Standard Error</a><a class="headerlink" href="#standard-error" title="Link to this heading">#</a></h3>
<p>Now here is the intuition, yet again, we need to gain an understanding first before we go into further mathematical details. We already established subtly that even if we can easily calculate <span class="math notranslate nohighlight">\(\hat\beta_1\)</span>, and it turns out to be non-zero, how can we feel <strong>“safe”</strong> that our true parameter <span class="math notranslate nohighlight">\(\beta_1\)</span> is also non-zero? Well if you think deeper, it all boils down to whether our <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> is a <strong>good estimate</strong> of <span class="math notranslate nohighlight">\(\beta_1\)</span>; and this can be quantified by checking the standard error of <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>. By definition, we should remember the standard error of <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> means the amount of dispersion of the various <span class="math notranslate nohighlight">\(\hat\beta_1\)</span> around its mean (recall the mean of <span class="math notranslate nohighlight">\(\hat\beta_1\)</span> is <span class="math notranslate nohighlight">\(\beta_1\)</span>). So if <span class="math notranslate nohighlight">\(\text{SE}(\hat{\beta_1})\)</span> is small, this means <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> does not vary much around its mean and recall that the mean of <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> is none other than <span class="math notranslate nohighlight">\(\beta_1\)</span> due to the unbiasedness property of OLS estimator; consequently, it is quite “safe” to say that which ever sample you take from the population, the <span class="math notranslate nohighlight">\(\hat\beta_1\)</span> you calculated should not fluctuate much from the mean <span class="math notranslate nohighlight">\(\beta_1\)</span>, indicating that <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\hat\beta_1\)</span> are quite “close to each other”.</p>
<p>But here is the catch yet again, how small is small? If for example, <span class="math notranslate nohighlight">\(\hat{\beta_1} = 0.5\)</span> but if <span class="math notranslate nohighlight">\(\text{SE}(\hat{\beta_1}) = 0.499\)</span>, then this standard error is not small relative to the value of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>. <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>. One should understand in a rough sense that the standard error of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is the <strong>average amount that this particular <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> differs from the actual value of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>.</strong> So if for example, the above example says that the average amount that this <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> differs from the real <span class="math notranslate nohighlight">\(\beta_1\)</span> is 0.499, and this is not very promising because it means that our real <span class="math notranslate nohighlight">\(\beta_1\)</span> could be <span class="math notranslate nohighlight">\(0.5-0.499 = 0.001\)</span>, which is very close to <span class="math notranslate nohighlight">\(0\)</span>, we do not really feel safe about our <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> since it can still suggest that our real <span class="math notranslate nohighlight">\(\beta_1\)</span> is near 0 and we cannot reject the null hypothesis that <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span>.</p>
<p>Conversely, if our standard error of <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> is very large, say equals to 10000, this does not necessarily mean our <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> is an inaccurate estimate of <span class="math notranslate nohighlight">\(\beta_1\)</span>, because if our <span class="math notranslate nohighlight">\(\beta_1\)</span> is so large relative to 10000, say our <span class="math notranslate nohighlight">\(\beta_1 = 1000000000000\)</span>, then this standard error is small relatively and we still have strong evidence to reject the null hypothesis. In a similar vein as the previous example, on average, we should expect our <span class="math notranslate nohighlight">\(\beta_1\)</span> to be around <span class="math notranslate nohighlight">\(1000000000000 \pm 10000\)</span>, which is nowhere close to 0.</p>
<p>So we also must find a metric to calculate how small is small, or conversely, how big is big. Therefore, to understand that if <span class="math notranslate nohighlight">\(\text{SE}(\hat{\beta_1}) &lt;&lt;&lt; \hat{\beta_1}\)</span>, that is, standard error of the estimator <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> is way smaller than <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>, then it is safe to say that our <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> provides strong evidence that the real parameter is non-zero as well.</p>
<p>For example, if <span class="math notranslate nohighlight">\(\hat{\beta_1} = 0.5\)</span> and <span class="math notranslate nohighlight">\(\text{SE}(\hat{\beta_1}) = 0.0001\)</span>, then even though <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> is quite near <span class="math notranslate nohighlight">\(0\)</span>, we can still feel confident to say that the real parameter <span class="math notranslate nohighlight">\(\beta_1\)</span> is not 0, simply because our estimate <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> has such a small standard error/variation that it is very unlikely for our <span class="math notranslate nohighlight">\(\beta_1\)</span> to be 0.</p>
<p>What I have described above is merely an intuition, talk is cheap and we need to use scientific methods to quantify it. To quantify what I have said above, we use t-statistics to calculate the number of standard deviations that <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> is away from <span class="math notranslate nohighlight">\(0\)</span>. In fact the formula allows any number, not limited to 0.</p>
</section>
<section id="t-statistics">
<h3><a class="toc-backref" href="#id58" role="doc-backlink">T-Statistics</a><a class="headerlink" href="#t-statistics" title="Link to this heading">#</a></h3>
<p>In statistics, the <strong>t-statistic</strong> is the ratio of the difference of the <strong>estimated</strong> value of a true population parameter from its <strong>hypothesized</strong> value to its <strong>standard error</strong>. A good intuitive of <a class="reference external" href="https://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-is-a-t-test-and-why-is-it-like-telling-a-kid-to-clean-up-that-mess-in-the-kitchen">explanation of t-statistics can be read here</a>.</p>
<p>Let <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> be an estimator of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> in some statistical model. Then a <strong>t-statistic</strong> for this parameter <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> is any quantity of the form</p>
<div class="math notranslate nohighlight">
\[t_{\hat{\boldsymbol{\beta}}} = \dfrac{\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}_H}{\text{SE}(\hat{\boldsymbol{\beta}})}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_H\)</span> is the value we want to test in the hypothesis. By default, statistical software sets <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_H = 0\)</span>.</p>
<p>In the regression setting, we further take note that the <strong>t-statistic</strong> for each individual coefficient <span class="math notranslate nohighlight">\(\hat{\beta}_i\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[t_{\hat{\boldsymbol{\beta}}i} = [t_{\hat{\boldsymbol{\beta}}}]_{(i+1) \times (i+1)}\]</div>
<p><em><strong>If our null hypothesis is really true, that</strong></em> <span class="math notranslate nohighlight">\(\beta_1=0\)</span><em><strong>, then if we calculate our t-value to be 0, then we can understand it as the number of standard deviations that</strong></em> <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> <em><strong>is 0, which means that</strong></em> <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span> <em><strong>is 0. This might be hard to reconcile at first, but if we see the formula of the t-statistics, and that by definition we set</strong></em> <span class="math notranslate nohighlight">\(\beta_H=0\)</span>, then it is apparent that if <span class="math notranslate nohighlight">\(t_{\hat{\beta}}=0\)</span>, it forces the formula to become <span class="math notranslate nohighlight">\(t_{\hat{\beta}}=0=\dfrac{\hat{\beta}-0}{\text{SE}(\hat{\beta})} \Longrightarrow \hat{\beta}=0\)</span> ; even more concretely with an example, we replace <span class="math notranslate nohighlight">\(\beta_{H}\)</span> with our favorite <strong>true population parameter</strong> <span class="math notranslate nohighlight">\(\beta_1\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> with <span class="math notranslate nohighlight">\(\hat{\beta}_1\)</span>, then it just means that if <span class="math notranslate nohighlight">\(\beta_1\)</span> were really <span class="math notranslate nohighlight">\(0\)</span>, i.e. no relationship of <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(x_1\)</span>, and if we also get <span class="math notranslate nohighlight">\(t_{\hat{\beta}_1}\)</span>to be 0 as well (To re-explain this part as a bit cumbersome). In which case we accept the null hypothesis; on the other hand, if our t-value is none-zero, it means that <span class="math notranslate nohighlight">\(\hat{\beta}_1 \neq 0\)</span>.</p>
<p>Consequently, we can conclude that greater the magnitude of <span class="math notranslate nohighlight">\(|t|\)</span> (<span class="math notranslate nohighlight">\(t\)</span> can be either positive or negative), the greater the evidence to reject the null hypothesis. The closer <span class="math notranslate nohighlight">\(t\)</span> is to 0, the more likely there isn’t a significant evidence to reject the null hypothesis.</p>
<section id="implementing-t-statistics">
<h4><a class="toc-backref" href="#id59" role="doc-backlink">Implementing T-Statistics</a><a class="headerlink" href="#implementing-t-statistics" title="Link to this heading">#</a></h4>
<p>To find the t-value of <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>, we need to first know what is the standard error of <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>, which is <span class="math notranslate nohighlight">\(\text{SE}(\hat{\mathbf{\beta_1}})\)</span>. The formula is given by</p>
<div class="math notranslate nohighlight">
\[
\text{SE}(\hat{\beta_1}) = \sqrt{\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}}
\]</div>
<p>But since we do not know the value of the population variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> of the error term, we estimate it with</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^2 = \dfrac{\text{Residual Sum of Squares (RSS)}}{N - D - 1}
\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the number of samples, <span class="math notranslate nohighlight">\(D\)</span> is the number of features.</p>
<p>The python code is implemented as follows, we will use sklearn’s LinearRegression to find the parameters <span class="math notranslate nohighlight">\(\hat{\beta_0}\)</span> and <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>, and then we will use the formula above to find the standard error of <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>, and then we will use the formula of t-statistics to find the t-value of <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="linenos"> 2</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="linenos"> 3</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="linenos"> 6</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of samples: </span><span class="si">{</span><span class="n">N</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos"> 7</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of features: </span><span class="si">{</span><span class="n">D</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="n">X_with_intercept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">X</span><span class="p">))</span>
<span class="linenos">10</span><span class="nb">print</span><span class="p">(</span><span class="n">X_with_intercept</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="linenos">11</span>
<span class="linenos">12</span><span class="c1"># Instantiate and fit the LinearRegression model</span>
<span class="linenos">13</span><span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="linenos">14</span><span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="linenos">15</span>
<span class="linenos">16</span><span class="c1"># Predict using the fitted model</span>
<span class="linenos">17</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="linenos">18</span>
<span class="linenos">19</span><span class="c1"># Compute the residual sum of squares (RSS)</span>
<span class="linenos">20</span><span class="n">rss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
<span class="linenos">21</span><span class="c1"># Compute the sigma square estimate</span>
<span class="linenos">22</span><span class="n">sigma_square_hat</span> <span class="o">=</span> <span class="n">rss</span> <span class="o">/</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="p">(</span><span class="n">D</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="linenos">23</span><span class="c1"># Compute the variance-covariance matrix</span>
<span class="linenos">24</span><span class="n">var_beta_hat</span> <span class="o">=</span> <span class="n">sigma_square_hat</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_with_intercept</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_with_intercept</span><span class="p">))</span>
<span class="linenos">25</span>
<span class="linenos">26</span><span class="c1"># Print the standard errors and t-values for each coefficient</span>
<span class="linenos">27</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">D</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
<span class="linenos">28</span>    <span class="n">standard_error</span> <span class="o">=</span> <span class="n">var_beta_hat</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">**</span> <span class="mf">0.5</span>   <span class="c1"># standard error for beta_0 and beta_1</span>
<span class="linenos">29</span>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Standard Error of (beta_hat[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]): </span><span class="si">{</span><span class="n">standard_error</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">30</span>
<span class="linenos">31</span>    <span class="c1"># Get the coefficient value (beta_hat)</span>
<span class="linenos">32</span>    <span class="n">coef_value</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="linenos">33</span>    <span class="n">t_values</span> <span class="o">=</span> <span class="n">coef_value</span> <span class="o">/</span> <span class="n">standard_error</span>
<span class="linenos">34</span>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t_value of (beta_hat[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]): </span><span class="si">{</span><span class="n">t_values</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">35</span>
<span class="linenos">36</span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;━&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="linenos">37</span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;━&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of samples: 100
Number of features: 1
(100, 2)
Standard Error of (beta_hat[0]): 2442.4808292051985
t_value of (beta_hat[0]): 46.845211452439884
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Standard Error of (beta_hat[1]): 41.81741692793702
t_value of (beta_hat[1]): 22.997689705070346
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
</pre></div>
</div>
</div>
</div>
<p>Well, we are only interested in the t-value of <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>, which shows a value of 22. Quoting from <a class="reference external" href="https://statisticsbyjim.com/hypothesis-testing/t-tests-t-values-t-distributions-probabilities/">the author Jim Frost’s book, he mentioned that</a> the tricky thing about t-values is that they are a unitless statistic, which makes them difficult to interpret on their own. We performed a t-test and it produced a t-value of 38.36. What does this t-value mean exactly? We know that the sample mean doesn’t equal the null hypothesis value because this t-value doesn’t equal zero. However, we don’t know how exceptional our value is if the null hypothesis is correct.</p>
<p>This is why we need to look at the P-values of the t-test. But before that, we look at t-distributions first.</p>
</section>
</section>
<section id="t-distributions">
<h3><a class="toc-backref" href="#id60" role="doc-backlink">T-Distributions</a><a class="headerlink" href="#t-distributions" title="Link to this heading">#</a></h3>
<p>How T-distribution is formed:</p>
<ol class="arabic">
<li><p>Define our null and alternative hypothesis. In Simple Linear Regression, we simply set:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{aligned}
    &amp; H_0: &amp;\beta_1 &amp;= 0 \\
    &amp; H_{A}: &amp;\beta_1 &amp;\neq 0
    \end{aligned}
    \end{split}\]</div>
</li>
<li><p>Determine the test statistic that is most suitable for this parameter <span class="math notranslate nohighlight">\(\beta_1\)</span>. Note that <span class="math notranslate nohighlight">\(\beta_1\)</span> is actually a constant and not a random variable, and we cannot know the distribution of <span class="math notranslate nohighlight">\(\beta_1\)</span> anyways since population is unknown. However, we can make use of the OLS estimator of <span class="math notranslate nohighlight">\(\beta_1\)</span> to draw inferences about it. So although we do not know <span class="math notranslate nohighlight">\(\beta_1\)</span> we are able to derive the sampling distribution of <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> which we have shown in the first section to be:</p>
<div class="math notranslate nohighlight">
\[
    \hat{\boldsymbol{\beta}} \sim N\left(\boldsymbol{\beta},  \hat{\sigma}^2(\mathbf{X}^T\mathbf{X})^{-1}\right)
    \]</div>
</li>
</ol>
<p>We could jolly well define <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> to be our test statistic and calculate <span class="math notranslate nohighlight">\(P(\hat{\beta_1} \geq B_H)\)</span> where <span class="math notranslate nohighlight">\(B_H\)</span> is the hypothesis we want to test, if not for the fact that we do not even know the value of <span class="math notranslate nohighlight">\(\sigma^2\)</span> - which we desperately need because it appeared in the variance of <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span>. So this is the reason why we need to use an equivalent test statistic:</p>
<div class="math notranslate nohighlight">
\[
t_{\hat{\mathbf{\beta_1}}} = \dfrac{\hat{\mathbf{\beta_1}} - \mathbf{\beta_1}_H}{\text{SE}(\hat{\mathbf{\beta_1}})} \sim t(df)
\]</div>
<p>This means that if our <span class="math notranslate nohighlight">\(\hat{\beta_1}\)</span> follows a normal distribution of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} \sim N\left(\boldsymbol{\beta},  \hat{\sigma}^2(\mathbf{X}^T\mathbf{X})^{-1}\right)\)</span>, then an equivalent distribution is <span class="math notranslate nohighlight">\(t_{\hat{\mathbf{\beta_1}}} \sim t(df)\)</span>.</p>
<p>So it is clear that for any <span class="math notranslate nohighlight">\(a, b \in \mathbb{R}\)</span>, finding <span class="math notranslate nohighlight">\(P(a \leq \hat{\beta_1} \leq b)\)</span> is the same as finding</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}\left(\dfrac{a}{\text{SE}(\hat{\beta_1})} \leq t_{\hat{\mathbf{\beta_1}}} \leq \dfrac{b}{\text{SE}(\hat{\beta_1})} \right)
\]</div>
<p>So finally, we have formed a sampling distribution of t-statistics, hinged on the assumption that <em><strong>our null hypothesis is true, <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span> (this is important).</strong></em></p>
<p><a class="reference external" href="https://statisticsbyjim.com/hypothesis-testing/t-tests-t-values-t-distributions-probabilities/">Jim Statistics</a> provides an intuitive idea.</p>
</section>
<section id="p-values">
<h3><a class="toc-backref" href="#id61" role="doc-backlink">P-Values</a><a class="headerlink" href="#p-values" title="Link to this heading">#</a></h3>
<p>We use the resulting test statistic to calculate the p-value. As always, the p-value is the answer to the question “how likely is it that we’d get a test statistic <span class="math notranslate nohighlight">\(t_{\hat{\boldsymbol{\beta}}}\)</span> as extreme as we did if the null hypothesis were true?” The p-value is determined by referring to a t-distribution with <span class="math notranslate nohighlight">\(m-(n+1) = m-2\)</span> degrees of freedom where we recall that <span class="math notranslate nohighlight">\(m\)</span> is the number of data, and <span class="math notranslate nohighlight">\(n\)</span> is the number of variables. The <span class="math notranslate nohighlight">\(n+1\)</span> is because we are actually fitting 2 parameters, so we add on 1 intercept term which has a dummy variable <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p>Finally, we make a decision:</p>
<ol class="arabic simple">
<li><p>If the p-value is smaller than the significance level <span class="math notranslate nohighlight">\(\alpha\)</span>, we reject the null hypothesis in favor of the alternative. We conclude “there is sufficient evidence at the <span class="math notranslate nohighlight">\(\alpha\)</span> level to conclude that there is a linear relationship in the population between the predictor x and response y.”</p></li>
<li><p>If the p-value is larger than the significance level <span class="math notranslate nohighlight">\(\alpha\)</span>, we fail to reject the null hypothesis. We conclude “there is not enough evidence at the <span class="math notranslate nohighlight">\(\alpha\)</span> level to conclude that there is a linear relationship in the population between the predictor x and response y.”</p></li>
</ol>
<p>Let us implement our p-values.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="linenos"> 2</span><span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="linenos"> 3</span>
<span class="linenos"> 4</span><span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="linenos"> 5</span><span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="linenos"> 6</span>
<span class="linenos"> 7</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="n">X_with_intercept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">X</span><span class="p">))</span>
<span class="linenos">10</span>
<span class="linenos">11</span><span class="c1"># RSE = sqrt(RSS/(m-n))</span>
<span class="linenos">12</span><span class="c1"># thus, sigma square estimate is RSS/(m-n)</span>
<span class="linenos">13</span><span class="n">sigma_square_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="p">(</span><span class="n">D</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="linenos">14</span><span class="n">var_beta_hat</span> <span class="o">=</span> <span class="n">sigma_square_hat</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_with_intercept</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_with_intercept</span><span class="p">))</span>
<span class="linenos">15</span>
<span class="linenos">16</span><span class="n">ols_parameters</span> <span class="o">=</span> <span class="p">[</span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="linenos">17</span>
<span class="linenos">18</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">D</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
<span class="linenos">19</span>    <span class="n">standard_error</span> <span class="o">=</span> <span class="n">var_beta_hat</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">**</span> <span class="mf">0.5</span>   <span class="c1"># standard error for beta_0 and beta_1</span>
<span class="linenos">20</span>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Standard Error of (beta_hat[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]): </span><span class="si">{</span><span class="n">standard_error</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">21</span>
<span class="linenos">22</span>    <span class="n">t_values</span> <span class="o">=</span> <span class="n">ols_parameters</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">/</span> <span class="n">standard_error</span>
<span class="linenos">23</span>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;t_value of (beta_hat[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]): </span><span class="si">{</span><span class="n">t_values</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">24</span>
<span class="linenos">25</span>    <span class="n">p_values</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">stats</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">t_values</span><span class="p">),</span> <span class="n">df</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="linenos">26</span>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;p_value of (beta_hat[</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">]): </span><span class="si">{</span><span class="n">p_values</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">27</span>
<span class="linenos">28</span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;━&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="linenos">29</span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;━&quot;</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Standard Error of (beta_hat[0]): 2442.4808292051985
t_value of (beta_hat[0]): 46.845211452439884
p_value of (beta_hat[0]): 0.0
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Standard Error of (beta_hat[1]): 41.81741692793702
t_value of (beta_hat[1]): 22.997689705070346
p_value of (beta_hat[1]): 0.0
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
</pre></div>
</div>
</div>
</div>
<p>We easily use python to calculate the statistics for us, as shown above.</p>
<p>Because the p-value is so small (usually less than <span class="math notranslate nohighlight">\(0.05\)</span>), this means that if our null hypothesis was actually true that <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span>, then there is only a <span class="math notranslate nohighlight">\(0.0\%\)</span> chance to get this <span class="math notranslate nohighlight">\(t = 38.348\)</span> value. In other words, it is unlikely to observe such a substantial association between the predictor and the response due to chance, in the absence of any real association between the predictor and the response. We can reject the null hypothesis and conclude that <span class="math notranslate nohighlight">\(\beta_1\)</span> does not equal 0. There is sufficient evidence, at the <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span> level, to conclude that there is a linear relationship in the population between house area and the sale price.</p>
<p>Similarly, we can see the p-value for <span class="math notranslate nohighlight">\(\hat{\beta_0}\)</span> to be less than <span class="math notranslate nohighlight">\(0.05\)</span> as well. Consequently, we conclude that <span class="math notranslate nohighlight">\(\beta_0 \neq 0\)</span> and <span class="math notranslate nohighlight">\(\beta_1 \neq 0\)</span>.</p>
<p>Since the above is a self implemented function, it is always good to check with the statsmodels.api from python to check if I am correct.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">X_with_intercept</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_html"><table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.844</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.842</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   528.9</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 22 Sep 2024</td> <th>  Prob (F-statistic):</th> <td>2.81e-41</td>
</tr>
<tr>
  <th>Time:</th>                 <td>06:23:27</td>     <th>  Log-Likelihood:    </th> <td> -1082.2</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   2168.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   2174.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>const</th> <td> 1.144e+05</td> <td> 2442.481</td> <td>   46.845</td> <td> 0.000</td> <td>  1.1e+05</td> <td> 1.19e+05</td>
</tr>
<tr>
  <th>x1</th>    <td>  961.7040</td> <td>   41.817</td> <td>   22.998</td> <td> 0.000</td> <td>  878.719</td> <td> 1044.689</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.135</td> <th>  Durbin-Watson:     </th> <td>   2.166</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.935</td> <th>  Jarque-Bera (JB):  </th> <td>   0.038</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.047</td> <th>  Prob(JB):          </th> <td>   0.981</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.992</td> <th>  Cond. No.          </th> <td>    117.</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</div></div>
</div>
<p>We see the t and p values coincide.</p>
</section>
<section id="the-pandora-box-of-confidence-intervals-p-values-and-hypothesis-testing">
<h3><a class="toc-backref" href="#id62" role="doc-backlink">The Pandora Box of Confidence Intervals, P-values and Hypothesis Testing</a><a class="headerlink" href="#the-pandora-box-of-confidence-intervals-p-values-and-hypothesis-testing" title="Link to this heading">#</a></h3>
<p>The focus of this post is not to go into details on how to interpret the
coefficients using statistical methods. As I’ve mentioned, regression analysis in itself
can be one whole undergraduate module spanning more than 1 term/semester. To have a more
thorough understanding, I would recommend reading the regression chapter in An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.</p>
</section>
</section>
<hr class="docutils" />
<section id="time-complexity">
<h2><a class="toc-backref" href="#id63" role="doc-backlink">Time Complexity</a><a class="headerlink" href="#time-complexity" title="Link to this heading">#</a></h2>
<p>Time Complexity is an important topic, you do not want your code to run for 1 billion years, and therefore, an efficient code will be important to businesses. That is also why Time Complexity questions are becoming increasingly popular in Machine Learning and Data Science interviews!</p>
<p>The Linear Algorithm that we used here simply uses matrix multiplication. We will also ignore the codes that are of constant time O(1). For example, <code class="docutils literal notranslate"><span class="pre">self.coef_=None</span></code> in the constructor is O(1) and we do not really wish to consider this in the <em>grand scheme of things.</em></p>
<p>What is the really important ones are in code lines 37–40. Given X to be a m by n matrix/array, where m is the number of samples and n the number of features. In addition, y is a m by 1 vector. Refer to this <a class="reference external" href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations">Wikipedia Page</a> for a handy helpsheet on the various time complexity for mathematical operations.</p>
<p>Line 37: <code class="docutils literal notranslate"><span class="pre">np.dot(X.T,X)</span></code> In the dot product, we transpose the m × n matrix to become n × m, this operation takes O(m × n) time because we are effectively performing two for loops. Next up is performing matrix multiplication, note carefully that <code class="docutils literal notranslate"><span class="pre">np.dot</span></code> between two 2-d arrays does not mean <a class="reference external" href="https://stackoverflow.com/questions/3744094/time-and-space-complexity-of-vector-dot-product-computation#:~:text=Assuming%20that%20multiplication%20and%20addition,computed%2C%20i.e.%20ai%20*%20bi%20.">dot product</a>, instead they are matrix multiplication, which takes O(m × n²) time. The output matrix of this step is n× n.</p>
<p>Line 38: Inverting a n × n matrix takes n³ time. The output matrix is n × n.</p>
<p>Line 39: Now we perform matrix multiplication of n × n and n × m, which gives O(m × n²), the output matrix is n × m.</p>
<p>Line 40: Lastly, the time complexity is O(m × n).</p>
<p>Adding them all up gives you O(2mn+2mn²+n³) whereby simple triangle inequality of mn&lt;mn² implies we can remove the less dominant 2mn term. In the end, the run time complexity of this Linear Regression Algorithm using Normal Equation is O(n²(m+n)). However, you noticed that there are two variables in the bigO notation, and you wonder if we can further reduce the bigO notation to a single variable? Well, if the number of variables is small, which means n is kept small and maybe constant, we can reduce the time complexity to O(m), however, if your variables are increasing, then your time complexity will explode if n → ∞.</p>
</section>
<section id="references-and-further-readings">
<h2><a class="toc-backref" href="#id64" role="doc-backlink">References and Further Readings</a><a class="headerlink" href="#references-and-further-readings" title="Link to this heading">#</a></h2>
<ul>
<li><p>Mohri, Mehryar, Rostamizadeh Afshi and Talwalkar Ameet. Foundations of Machine Learning. The MIT Press, 2018.</p></li>
<li><p>Murphy, Kevin P. “Chapter 21.3. K-Means Clustering.” In Probabilistic Machine Learning: An Introduction. MIT Press, 2022.</p></li>
<li><p>Hal Daumé III. “Chapter 3.4. K-Means Clustering.” In A Course in Machine Learning, January 2017.</p></li>
<li><p>Hal Daumé III. “Chapter 15.1. K-Means Clustering.” In A Course in Machine Learning, January 2017.</p></li>
<li><p>Bishop, Christopher M. “Chapter 9.1. K-Means Clustering.” In Pattern Recognition and Machine Learning. New York: Springer-Verlag, 2016.</p></li>
<li><p>James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. “Chapter 12.4.1. K-Means Clustering.” In An Introduction to Statistical Learning: With Applications in R. Boston: Springer, 2022.</p></li>
<li><p>Hastie, Trevor, Tibshirani, Robert and Friedman, Jerome. “Chapter 14.3. Cluster Analysis.” In The Elements of Statistical Learning. New York, NY, USA: Springer New York Inc., 2001.</p></li>
<li><p>Raschka, Sebastian. “Chapter 10.1. Grouping objects by similarity using k-means.” In Machine Learning with PyTorch and Scikit-Learn.</p></li>
<li><p>Jung, Alexander. “Chapter 8.1. Hard Clustering with K-Means.” In Machine Learning: The Basics. Singapore: Springer Nature Singapore, 2023.</p></li>
<li><p>Vincent, Tan. “Lecture 17a.” In MA4270 Data Modelling and Computation.</p></li>
<li><p><a class="reference external" href="https://online.stat.psu.edu/stat462/">https://online.stat.psu.edu/stat462/</a></p></li>
<li><p><a class="reference external" href="https://statisticsbyjim.com/regression/">Statistics by Jim - Regression</a></p>
<p><a class="reference external" href="https://statisticsbyjim.com/regression/interpret-coefficients-p-values-regression/">Interpreting MLR Coefficients and P-values</a></p>
<p><a class="reference external" href="https://blog.minitab.com/en/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit">Goodness of Fit and R-Squared</a></p>
<p><a class="reference external" href="https://blog.minitab.com/en/statistics-and-quality-data-analysis/what-is-a-t-test-and-why-is-it-like-telling-a-kid-to-clean-up-that-mess-in-the-kitchen">T-Test</a></p>
</li>
</ul>
<p><a class="reference external" href="http://mlwiki.org/index.php/Normal_Equation">Normal Equation (ML Wiki) Wholesome and Mathematically Rigorous</a> (This is a must read)</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Ordinary_least_squares">Ordinary Least Squares Wikipedia</a></p>
<p><a class="reference external" href="https://jeffmacaluso.github.io/post/LinearRegressionAssumptions/">Linear Regression Assumptions by Jeff Macaluso from Microsoft</a></p>
<p><a class="reference external" href="http://statweb.stanford.edu/~jtaylo/courses/stats203/">Stanford’s STATS203 class - Consider downloading them before it’s gone</a></p>
<p><a class="reference external" href="https://www.kaggle.com/shrutimechlearn/step-by-step-assumptions-linear-regression">Kaggle Linear Regression Assumptions</a></p>
<p><a class="reference external" href="https://online.stat.psu.edu/stat462/node/164/">Linear Regression Additive Effects (PSU STATS462)</a></p>
<p><a class="reference external" href="https://mylearningsinaiml.wordpress.com/concepts/regression/hands-on-linear-regression/">Hands on Linear Regression</a></p>
<p><a class="reference external" href="https://realpython.com/linear-regression-in-python/#multiple-linear-regression">Real Python Linear Regression</a></p>
<p><a class="reference external" href="https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/">Normal Equation Derivation II</a></p>
<p><a class="reference external" href="https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia">Feature Scaling does not affect Linear Regressions’ validity of Coefficients</a></p>
<p><a class="reference external" href="https://online.stat.psu.edu/stat462/">Hypothesis Testing on Optimal Coefficients</a></p>
<p><a class="reference external" href="https://stats.stackexchange.com/questions/220507/linear-regression-conditional-expectations-and-expected-values/220509#220509">Conditional Mean and Expectation of Linear Regression</a></p>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="dirac-delta" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">1</a><span class="fn-bracket">]</span></span>
<p>A Dirac delta (function) is a function that is zero everywhere except at zero, where it is infinite. It is named after the physicist Paul Dirac.</p>
</aside>
<aside class="footnote brackets" id="weaker-condition" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">2</a><span class="fn-bracket">]</span></span>
<p>Note that we have proved <span class="math notranslate nohighlight">\(\mathbb{E}[\boldsymbol{\varepsilon}] = \boldsymbol{0}\)</span> and <span class="math notranslate nohighlight">\(\mathrm{Var}[\boldsymbol{\varepsilon}] = \sigma^2\mathbf{I}\)</span> earlier by the Law of Iterated Expectation on <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon} \mid \mathbf{X}\)</span>, but it is necessary to further assume that <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}\)</span> itself has a normal distribution, or else we will not know the distribution of <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}\)</span>.</p>
</aside>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./influential/linear_regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="01_intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Linear Regression</p>
      </div>
    </a>
    <a class="right-next"
       href="03_implementation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Implementation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">Intuition</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#univariate-linear-model">Univariate Linear Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-linear-model">Multivariate Linear Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-hypothesis-space">The Hypothesis Space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-loss-function">The Loss Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-formulation">Problem Formulation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-normal-distribution-gaussian-noise-and-the-mean-squared-error">The Normal Distribution, Gaussian Noise and the Mean Squared Error</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#target-variable-is-normally-distributed">Target Variable is Normally Distributed</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-conditional-distribution-of-y-given-mathbf-x-is-normally-distributed">The Conditional Distribution of <span class="math notranslate nohighlight">\(y\)</span> Given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is Normally Distributed</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-estimates-the-conditional-expectation">Mean Squared Error estimates the Conditional Expectation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-in-parameters">Linear in Parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basis-function-and-non-linearity">Basis Function and Non-Linearity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-representation">Matrix Representation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#break-down-of-the-matrix-representation">Break down of the Matrix Representation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-space">Hypothesis Space</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">Loss Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-function">Cost Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convexity-and-differentiability">Convexity and Differentiability</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#objective-function">Objective Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-fitting-via-least-squares">Model Fitting via Least Squares</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-for-1-dimensional-case">Solving for 1-Dimensional Case</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-for-d-dimensional-case">Solving for <span class="math notranslate nohighlight">\(D\)</span>-Dimensional Case</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-fitting-via-maximum-likelihood-estimation">Model Fitting via Maximum Likelihood Estimation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iid-assumption">IID Assumption</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-likelihood-function">Conditional Likelihood Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-log-likelihood-function">Conditional Log-Likelihood Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation">Maximum Likelihood Estimation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mle-estimates-the-conditional-expectation-of-the-target-variable-given-the-input">MLE estimates the conditional expectation of the target variable given the input</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-metrics">Performance Metrics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-mse">Mean Squared Error (MSE)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#r-squared">R-Squared</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adjusted-r-squared">Adjusted R-Squared</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions-of-linear-regression">Assumptions of Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linearity">Linearity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#homoscedasticity">Homoscedasticity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normality-of-the-error-terms">Normality of the Error Terms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#no-autocorrelation-between-error-terms">No Autocorrelation between Error Terms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multicollinearity-among-predictors">Multicollinearity among Predictors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-scaling">Feature Scaling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-inference">Model Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-is-an-unbiased-estimator-of-the-variance-of-the-error">Mean Squared Error is an Unbiased Estimator of the Variance of the Error</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimated-coefficients-are-multivariate-normally-distributed">Estimated Coefficients are Multivariate Normally Distributed</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assessing-the-accuracy-of-the-coefficient-estimates">Assessing the Accuracy of the Coefficient Estimates</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standard-error">Standard Error</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#t-statistics">T-Statistics</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-t-statistics">Implementing T-Statistics</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#t-distributions">T-Distributions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#p-values">P-Values</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-pandora-box-of-confidence-intervals-p-values-and-hypothesis-testing">The Pandora Box of Confidence Intervals, P-values and Hypothesis Testing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#time-complexity">Time Complexity</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>