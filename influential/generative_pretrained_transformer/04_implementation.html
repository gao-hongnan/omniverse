<!-- _templates/html.html -->

<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>The Implementation of Generative Pre-trained Transformers (GPT) &#8212; Omniverse</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=bb35926c" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MYW5YKC2WF"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"defeq": "\\overset{\\text{def}}{=}", "defa": "\\overset{\\text{(a)}}{=}", "defb": "\\overset{\\text{(b)}}{=}", "defc": "\\overset{\\text{(c)}}{=}", "defd": "\\overset{\\text{(d)}}{=}", "st": "\\mid", "mod": "\\mid", "S": "\\Omega", "s": "\\omega", "e": "\\exp", "P": "\\mathbb{P}", "R": "\\mathbb{R}", "expectation": "\\mathbb{E}", "v": "\\mathbf{v}", "a": "\\mathbf{a}", "b": "\\mathbf{b}", "c": "\\mathbf{c}", "u": "\\mathbf{u}", "w": "\\mathbf{w}", "x": "\\mathbf{x}", "y": "\\mathbf{y}", "z": "\\mathbf{z}", "0": "\\mathbf{0}", "1": "\\mathbf{1}", "A": "\\mathbf{A}", "B": "\\mathbf{B}", "C": "\\mathbf{C}", "E": "\\mathcal{F}", "eventA": "\\mathcal{A}", "lset": "\\left\\{", "rset": "\\right\\}", "lsq": "\\left[", "rsq": "\\right]", "lpar": "\\left(", "rpar": "\\right)", "lcurl": "\\left\\{", "rcurl": "\\right\\}", "pmf": "p_X", "pdf": "f_X", "pdftwo": "f_{X,Y}", "pdfjoint": "f_{\\mathbf{X}}", "pmfjointxy": "p_{X, Y}", "pdfjointxy": "f_{X, Y}", "cdf": "F_X", "pspace": "(\\Omega, \\mathcal{F}, \\mathbb{P})", "var": "\\operatorname{Var}", "std": "\\operatorname{Std}", "bern": "\\operatorname{Bernoulli}", "binomial": "\\operatorname{Binomial}", "geometric": "\\operatorname{Geometric}", "poisson": "\\operatorname{Poisson}", "uniform": "\\operatorname{Uniform}", "normal": "\\operatorname{Normal}", "gaussian": "\\operatorname{Gaussian}", "gaussiansymbol": "\\mathcal{N}", "exponential": "\\operatorname{Exponential}", "iid": "\\textbf{i.i.d.}", "and": "\\text{and}", "O": "\\mathcal{O}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="application/vnd.jupyter.widget-state+json">{"state": {"f6f7f9552e2b4d2eb45578854373105f": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a68398afd56b4f0ba76798b1c572e99d": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "78ebac039a9942cf92cc564a503c0364": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_f6f7f9552e2b4d2eb45578854373105f", "max": 665.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_a68398afd56b4f0ba76798b1c572e99d", "value": 665.0}}, "ddd4ce63e9484eb2ad6db998eb5fd3cd": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7d432b44ebda4c989da78061ea3089ed": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "0b977d9805864d6d9eda9eba85af8e29": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_ddd4ce63e9484eb2ad6db998eb5fd3cd", "placeholder": "\u200b", "style": "IPY_MODEL_7d432b44ebda4c989da78061ea3089ed", "value": "config.json:\u2007100%"}}, "67c55fab45354f5098b44415901871eb": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "21d146be4c1547f891fdf0d12217e2eb": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "954d4212ac564fa78be4f8899c66468d": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_67c55fab45354f5098b44415901871eb", "placeholder": "\u200b", "style": "IPY_MODEL_21d146be4c1547f891fdf0d12217e2eb", "value": "\u2007665/665\u2007[00:00&lt;00:00,\u200752.3kB/s]"}}, "e92d9ef6200648eba4b7a2edc31279cd": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "236d1c2df8b84ed59a2b3464790b05e8": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_0b977d9805864d6d9eda9eba85af8e29", "IPY_MODEL_78ebac039a9942cf92cc564a503c0364", "IPY_MODEL_954d4212ac564fa78be4f8899c66468d"], "layout": "IPY_MODEL_e92d9ef6200648eba4b7a2edc31279cd"}}, "bdd45a40a45745c1878e2a50b475b53d": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "e5f887e01e3e4c89a1651945963cec00": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "08c7d39f861d4609ae075e9f26646e3e": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_bdd45a40a45745c1878e2a50b475b53d", "max": 548105171.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_e5f887e01e3e4c89a1651945963cec00", "value": 548105171.0}}, "47807274c5294578a5227b8e66eb0a44": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "5e63bb2f16004751a7a5a69c9a7e473b": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "32507fe012554b61a5088e5539b2e7ba": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_47807274c5294578a5227b8e66eb0a44", "placeholder": "\u200b", "style": "IPY_MODEL_5e63bb2f16004751a7a5a69c9a7e473b", "value": "model.safetensors:\u2007100%"}}, "d8a26e387673456c88a2f269eeed42d6": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "91bc285deb244c8692178b5d04a84ee1": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "fce08925502d4adba265b7d64aa62825": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_d8a26e387673456c88a2f269eeed42d6", "placeholder": "\u200b", "style": "IPY_MODEL_91bc285deb244c8692178b5d04a84ee1", "value": "\u2007548M/548M\u2007[00:04&lt;00:00,\u2007116MB/s]"}}, "94d4d418d9dd4097bc5bdfa6d087ca77": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "ed726384b6ba493881813bdbf04790fe": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_32507fe012554b61a5088e5539b2e7ba", "IPY_MODEL_08c7d39f861d4609ae075e9f26646e3e", "IPY_MODEL_fce08925502d4adba265b7d64aa62825"], "layout": "IPY_MODEL_94d4d418d9dd4097bc5bdfa6d087ca77"}}}, "version_major": 2, "version_minor": 0}</script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script crossorigin="anonymous" data-jupyter-widgets-cdn="https://cdn.jsdelivr.net/npm/" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@1.0.6/dist/embed-amd.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'influential/generative_pretrained_transformer/04_implementation';</script>
    <link rel="canonical" href="https://www.gaohongnan.com/influential/generative_pretrained_transformer/04_implementation.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Training a Mini-GPT to Learn Two-Digit Addition" href="05_adder.html" />
    <link rel="prev" title="The Concept of Generative Pre-trained Transformers (GPT)" href="03_concept.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Omniverse - Home"/>
    <img src="../../_static/logo.png" class="logo__image only-dark pst-js-only" alt="Omniverse - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Omniverse
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notations/machine_learning.html">Machine Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Influential Ideas and Papers</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="01_intro.html">Generative Pre-trained Transformers</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="02_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_concept.html">The Concept of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">The Implementation of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_adder.html">Training a Mini-GPT to Learn Two-Digit Addition</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../low_rank_adaptation/01_intro.html">Low-Rank Adaptation Of Large Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../low_rank_adaptation/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../low_rank_adaptation/03_implementation.html">Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../empirical_risk_minimization/01_intro.html">Empirical Risk Minimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../empirical_risk_minimization/02_concept.html">Concept: Empirical Risk Minimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../empirical_risk_minimization/03_bayes_optimal_classifier.html">Bayes Optimal Classifier</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../learning_theory/01_intro.html">Is The Learning Problem Solvable?</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../learning_theory/02_concept.html">Concept: Learning Theory</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../kmeans_clustering/01_intro.html">Lloyd’s K-Means Clustering Algorithm</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/02_concept.html">Concept: K-Means Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/03_implementation.html">Implementation: K-Means (Lloyd)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/04_image_segmentation.html">Application: Image Compression and Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/05_conceptual_questions.html">Conceptual Questions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../naive_bayes/01_intro.html">Naive Bayes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/03_implementation.html">Naives Bayes Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/04_example_penguins.html">Naive Bayes Application: Penguins</a></li>
<li class="toctree-l2"><a class="reference internal" href="../naive_bayes/05_application_mnist.html">Naive Bayes Application (MNIST)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../gaussian_mixture_models/01_intro.html">Mixture Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../gaussian_mixture_models/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gaussian_mixture_models/03_implementation.html">Gaussian Mixture Models Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../linear_regression/01_intro.html">Linear Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../linear_regression/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_regression/03_implementation.html">Implementation</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Playbook</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../playbook/training/intro.html">Training Dynamics And Tricks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../playbook/training/how_to_calculate_flops_in_transformer_based_models.html">How to Calculate the Number of FLOPs in Transformer Based Models?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../playbook/training/why_cosine_annealing_warmup_stabilize_training.html">Why Does Cosine Annealing With Warmup Stabilize Training?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../playbook/training/how_to_finetune_decoder_with_last_token_pooling.html">How To Fine-Tune Decoder-Only Models For Sequence Classification Using Last Token Pooling?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../playbook/training/how_to_finetune_decoder_with_cross_attention.html">How To Fine-Tune Decoder-Only Models For Sequence Classification With Cross-Attention?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../playbook/training/how_to_teacher_student_knowledge_distillation.html">How To Do Teacher-Student Knowledge Distillation?</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html">Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling.</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_inspect_function_and_class_signatures.html">How to Inspect Function and Class Signatures in Python?</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Probability Theory</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/intro.html">Chapter 1. Mathematical Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/01_combinatorics.html">Permutations and Combinations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/02_calculus.html">Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/03_contours.html">Contour Maps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/02_probability/intro.html">Chapter 2. Probability</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0202_probability_space.html">Probability Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0203_probability_axioms.html">Probability Axioms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0204_conditional_probability.html">Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0205_independence.html">Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0206_bayes_theorem.html">Baye’s Theorem and the Law of Total Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/summary.html">Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/intro.html">Chapter 3. Discrete Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0301_random_variables.html">Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0302_discrete_random_variables.html">Discrete Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0303_probability_mass_function.html">Probability Mass Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0304_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0305_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0306_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/intro.html">Discrete Uniform Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/intro.html">Bernoulli Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/iid.html">Independent and Identically Distributed (IID)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/intro.html">Binomial Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_implementation.html">Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_application.html">Real World Examples</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/geometric/intro.html">Geometric Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/geometric/0310_geometric_distribution_concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/intro.html">Poisson Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_implementation.html">Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/summary.html">Important</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/intro.html">Chapter 4. Continuous Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/from_discrete_to_continuous.html">From Discrete to Continuous</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0401_continuous_random_variables.html">Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0402_probability_density_function.html">Probability Density Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0403_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0404_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0405_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0406_mean_median_mode.html">Mean, Median and Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0407_continuous_uniform_distribution.html">Continuous Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0408_exponential_distribution.html">Exponential Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0409_gaussian_distribution.html">Gaussian Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0410_skewness_and_kurtosis.html">Skewness and Kurtosis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0411_convolve_and_sum_of_random_variables.html">Convolution and Sum of Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0412_functions_of_random_variables.html">Functions of Random Variables</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/intro.html">Chapter 5. Joint Distributions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/05_joint_distributions/from_single_variable_to_joint_distributions.html">From Single Variable to Joint Distributions</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/intro.html">Joint PMF and PDF</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/intro.html">Joint Expectation and Correlation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/intro.html">Conditional PMF and PDF</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/intro.html">Conditional Expectation and Variance</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0505_sum_of_random_variables/intro.html">Sum of Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0505_sum_of_random_variables/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0506_random_vectors/intro.html">Random Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0506_random_vectors/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/intro.html">Multivariate Gaussian Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/application_transformation.html">Application: Plots and Transformations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/psd.html">Covariance Matrix is Positive Semi-Definite</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/eigendecomposition.html">Eigendecomposition and Covariance Matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/geometry_of_multivariate_gaussian.html">The Geometry of Multivariate Gaussians</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/intro.html">Chapter 6. Sample Statistics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/intro.html">Moment Generating and Characteristic Functions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function.html">Moment Generating Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function_application_sum_of_rv.html">Application: Moment Generating Function and the Sum of Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/characteristic_function.html">Characteristic Function</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/intro.html">Probability Inequalities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/concept.html">Probability Inequalities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/application.html">Application: Learning Theory</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/intro.html">Law of Large Numbers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/convergence.html">Convergence of Sample Average</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/application.html">Application: Learning Theory</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/08_estimation_theory/intro.html">Chapter 8. Estimation Theory</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/08_estimation_theory/maximum_likelihood_estimation/intro.html">Maximum Likelihood Estimation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html">Concept</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Operations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../operations/distributed/intro.html">Distributed Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/01_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/02_basics.html">Basics Of Distributed Data Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/03_how_to_setup_slurm_in_aws.html">How to Setup SLURM and ParallelCluster in AWS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/04_ablation.html">Ablations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../operations/profiling/intro.html">Profiling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/01_synchronize.html">Synchronize CUDA To Time CUDA Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/02_timeit.html">Profiling Code With Timeit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/03_time_profiler.html">PyTorch’s Event And Profiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/04_small_gpt_profile.html">Profile GPT Small Time And Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/05_memory_leak.html">CUDA Memory Allocations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../operations/machine_learning_lifecycle/00_intro.html">The Lifecycle of an AIOps System</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/01_problem_formulation.html">Stage 1. Problem Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/02_project_scoping.html">Stage 2. Project Scoping And Framing The Problem</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/03_dataops_pipeline.html">Stage 3. Data Pipeline (Data Engineering and DataOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/031_data_source_and_format.html">Stage 3.1. Data Source and Formats</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/032_data_model_and_storage.html">Stage 3.2. Data Model and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/033_etl.html">Stage 3.3. Extract, Transform, Load (ETL)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/04_mlops_data_pipeline.html">Stage 4. Data Extraction (MLOps), Data Analysis (Data Science), Data Preparation (Data Science)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/05_ml_training_pipeline.html">Stage 5. Model Development and Training (MLOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/051_model_selection.html">Stage 5.1. Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/052_metric_selection.html">Stage 5.2. Metric Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/053_experiment_tracking.html">Stage 5.3. Experiment Tracking And Versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/054_model_testing.html">Stage 5.4. Model Testing</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/06_model_evaluation.html">Stage 6. Model Evaluation (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/07_model_validation_registry_and_pushing_model_to_production.html">Stage 7. Model Validation, Registry and Pushing Model to Production (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/08_model_deployment_and_serving.html">Stage 8. Model Serving (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/09_model_monitoring.html">Stage 9. Model Monitoring (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/010_continuous_integration_deployment_learning_and_training.html">Stage 10. Continuous Integration, Deployment, Learning and Training (DevOps, DataOps, MLOps)</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/config_management/intro.html">Config, State, Metadata Management</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/config_management/concept.html">Configuration Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/config_management/01-pydra.html">Pydantic And Hydra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/config_management/02-state.html">State And Metadata Management</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/design_patterns/intro.html">Design Patterns</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/dependency_inversion_principle.html">Dependency Inversion Principle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/named_constructor.html">Named Constructor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/strategy.html">Strategy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/registry.html">Registry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/god_object_pattern.html">Context Object Pattern (God Object)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/factory_method.html">Factory Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/singleton.html">Singleton</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/python/intro.html">Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/new_vs_init.html">Init vs New</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/gil.html">Global Interpreter Lock (GIL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/iterator_protocol.html">The Iterator Protocol</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/decorator.html">Decorator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/generators_over_lists.html">Generators Over Lists For Memory Efficiency</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/pydantic.html">Pydantic Is All You Need - Jason Liu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/mutable_default.html">Do Not Use Mutable Default Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/set_vs_list.html">Set Over List For Frequent Membership Tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/late_binding_closures.html">Late Binding Closures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/python/is_vs_equality.html">Is vs Equality</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/intro.html">Concurrency, Parallelism and Asynchronous Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/overview.html">Overview Of Concurrency, Parallelism, and Asynchronous Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/insights/locks_for_thread_safety.html">Thread Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/generator_yield.html">A Rudimentary Introduction to Generator and Yield in Python</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../computer_science/type_theory/intro.html">Type Theory, A Very Rudimentary Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/01-subtypes.html">Subtypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/02-type-safety.html">Type Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/03-subsumption.html">Subsumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/04-generics.html">Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/05-typevar-bound-constraints.html">Bound and Constraint in Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/06-invariance-covariance-contravariance.html">Invariance, Covariance and Contravariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/07-pep-3124-overloading.html">Function Overloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/08-pep-661-sentinel-values.html">Sentinel Types</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Structures and Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/complexity_analysis/intro.html">Complexity Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/complexity_analysis/master_theorem.html">Master Theorem </a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/array/intro.html">List/Array</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/array/concept.html">Concept</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsa/array/questions/intro.html">Questions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsa/array/questions/01-two-sum.html">Two Sum</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/hash_map/intro.html">Hash Map</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/hash_map/concept.html">Concept</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsa/hash_map/questions/intro.html">Questions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsa/hash_map/questions/01-two-sum.html">Two Sum</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsa/hash_map/questions/49-group-anagrams.html">Group Anagrams</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/two_pointers/intro.html">Two Pointers And Sliding Window</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/two_pointers/two_pointers.html">Two Pointers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/two_pointers/sliding_window.html">Sliding Window</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsa/two_pointers/questions/intro.html">Questions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../dsa/two_pointers/questions/two_pointers/intro.html">Two Pointers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../dsa/two_pointers/questions/two_pointers/26-remove-duplicates-from-sorted-array.html">Remove Duplicates from Sorted Array</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../dsa/two_pointers/questions/two_pointers/167-two-sum-ii-input-array-is-sorted.html">Two Sum II - Input Array Is Sorted</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../dsa/two_pointers/questions/sliding_window/intro.html">Sliding Window</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../dsa/two_pointers/questions/sliding_window/438-find-all-anagrams-in-a-string.html">Find All Anagrams in a String</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/stack/intro.html">Stack</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/stack/concept.html">Concept</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsa/stack/questions/intro.html">Questions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsa/stack/questions/20-valid-parentheses.html">Valid Parentheses</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsa/stack/questions/155-min-stack.html">Min Stack</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsa/stack/questions/232-implement-queue-using-stacks.html">Implement Queue using Stacks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsa/stack/questions/344-reverse-string.html">Reverse String</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/queue/intro.html">Queue</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/queue/concept.html">Concept</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../dsa/queue/dequeue.html">Double Ended Queue</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../dsa/queue/questions/hot-potatoes.html">Easy - Hot Potatoes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../dsa/queue/questions/125-valid-palindrome.html">Palindrome Checker</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/intro.html">Linear Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/intro.html">Binary Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/problems/875-koko-eating-bananas.html">Koko Eating Bananas</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/01_preliminaries/intro.html">Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/01-fields.html">Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/02-systems-of-linear-equations.html">Systems of Linear Equations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/02_vectors/intro.html">Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/01-vector-definition.html">Vector and Its Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/02-vector-operation.html">Vector and Its Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/03-vector-norm.html">Vector Norm and Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/04-vector-products.html">A First Look at Vector Products</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../citations.html">IEEE (Style) Citations</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/gao-hongnan/omniverse/blob/main/omniverse/influential/generative_pretrained_transformer/04_implementation.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse/issues/new?title=Issue%20on%20page%20%2Finfluential/generative_pretrained_transformer/04_implementation.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/influential/generative_pretrained_transformer/04_implementation.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>The Implementation of Generative Pre-trained Transformers (GPT)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dependencies">Dependencies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#composing-the-configurations">Composing the Configurations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducibility">Reproducibility</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#utilities">Utilities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization-and-vocabulary">Tokenization and Vocabulary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-and-dataloading-poor-man-s-dataloader">Dataset and Dataloading (Poor Man’s Dataloader)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-mapping">Memory Mapping</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-sequence">Input Sequence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#context-length-context-window-block-size">Context Length / Context Window / Block Size</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shuffling-and-discrete-uniform-sampling">Shuffling and Discrete Uniform Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#construction-input-sequences">Construction Input Sequences</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#construction-target-sequences">Construction Target Sequences</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#asynchronous-data-loading-and-prefetching">Asynchronous Data Loading and Prefetching</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#collating-everything-together">Collating Everything Together</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-pytorch-s-dataset-and-dataloader">Using PyTorch’s Dataset and Dataloader</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-pre-trained-transformer-gpt">Generative Pre-trained Transformer (GPT)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modifications-from-gpt-1-and-model-stability">Modifications from GPT-1 and Model Stability</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-2-variants">GPT-2 Variants</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-2-model-architecture-huggingface">GPT-2 Model Architecture (HuggingFace)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#token-embeddings">Token Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#output-of-the-embedding-layer">Output Of The Embedding Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-representation-of-input-sequence-mathbf-x">One-Hot Representation of Input Sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding-process">One-Hot Encoding Process</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-layer-is-matmul-of-one-hot-encoded-input-matrix-and-embedding-matrix-weights">Embedding Layer Is Matmul Of One-Hot Encoded Input Matrix And Embedding Matrix Weights</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Definition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#lookup">Lookup</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#semantic-representation">Semantic Representation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-embeddings">Positional Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualising-positional-encodings">Visualising Positional Encodings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-of-positional-encoding">An Example of Positional Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encodings-via-embeddings">Positional Encodings Via Embeddings</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">Layer Normalization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learnable-affine-transformation">Learnable Affine Transformation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-norm-stabilises-activation-distributions">Layer Norm Stabilises Activation Distributions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-connection">Residual Connection</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layernorm-and-residual-connection">LayerNorm and Residual Connection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-of-residual-block-and-addnorm">Implementation of Residual Block and AddNorm</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention">Self-Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-of-attention-mechanism">Intuition of Attention Mechanism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-embedding-and-vector-representation-process">Token Embedding and Vector Representation Process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#queries-keys-and-values">Queries, Keys, and Values</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#database-analogy">Database Analogy</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#queries-keys-and-values-in-attention-mechanism">Queries, Keys, and Values in Attention Mechanism</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-projections">Linear Projections</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaled-dot-product-attention">Scaled Dot-Product Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">Definition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-scoring-function">Attention Scoring Function</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-down-the-dot-product-of-query-and-key-vectors">Scaling Down the Dot Product of Query and Key Vectors</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">Softmax</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#context-vector-matrix">Context Vector/Matrix</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-stability-and-gradient-saturation">Numerical Stability and Gradient Saturation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-variance-of-dot-product">Visualizing Variance of Dot Product</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#projections-lead-to-dynamic-context-vectors">Projections Lead to Dynamic Context Vectors</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">Implementation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#heatmap">Heatmap</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#masked-causal-self-attention">Masked/Causal Self-Attention</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#last-token-has-full-context">Last Token has Full Context</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-complexity-of-self-attention">Computational Complexity of Self-Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-enables-parallelism">Self-Attention Enables Parallelism</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#complexity-per-layer">Complexity per Layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sequential-operations">Sequential Operations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-path-length">Maximum Path Length</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention">Multi-Head Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">Intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id34">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id35">Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#permutation-invariance">Permutation Invariance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applying-layernorm-and-residual-connections-to-multi-head-attention-output">Applying LayerNorm and Residual Connections to Multi-Head Attention Output</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positionwise-feed-forward-networks">Positionwise Feed-Forward Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#independent-processing">Independent Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#identical-application">Identical Application</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id37">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#projection-to-a-higher-dimension-space">Projection to a Higher Dimension Space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-error-linear-unit-gelu">Gaussian Error Linear Unit (GELU)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id39">Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applying-layernorm-and-residual-connections-to-positionwise-ffn-output">Applying LayerNorm and Residual Connections to Positionwise FFN Output</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-head">Softmax Head</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together-to-form-the-gpt">Putting it all Together to form the GPT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#patch-composer-configuration-with-model-config">Patch Composer Configuration with Model Config</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder-blocks">Decoder Blocks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder">Decoder</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="the-implementation-of-generative-pre-trained-transformers-gpt">
<h1>The Implementation of Generative Pre-trained Transformers (GPT)<a class="headerlink" href="#the-implementation-of-generative-pre-trained-transformers-gpt" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://twitter.com/gaohongnan"><img alt="Twitter Handle" src="https://img.shields.io/badge/Twitter-&#64;gaohongnan-blue?style=social&amp;logo=twitter" /></a>
<a class="reference external" href="https://linkedin.com/in/gao-hongnan"><img alt="LinkedIn Profile" src="https://img.shields.io/badge/&#64;gaohongnan-blue?style=social&amp;logo=linkedin" /></a>
<a class="reference external" href="https://github.com/gao-hongnan"><img alt="GitHub Profile" src="https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&amp;logo=github" /></a>
<img alt="Tag" src="https://img.shields.io/badge/Tag-Organized_Chaos-orange" />
<a class="reference external" href="https://github.com/gao-hongnan/omniverse/tree/5221d5d8b9bd845568b2e323d908be282c6e8434/omnivault/transformer"><img alt="Code" src="https://img.shields.io/badge/View-Code-blue?style=flat-square&amp;logo=github" /></a></p>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#dependencies" id="id42">Dependencies</a></p></li>
<li><p><a class="reference internal" href="#composing-the-configurations" id="id43">Composing the Configurations</a></p></li>
<li><p><a class="reference internal" href="#reproducibility" id="id44">Reproducibility</a></p></li>
<li><p><a class="reference internal" href="#utilities" id="id45">Utilities</a></p></li>
<li><p><a class="reference internal" href="#tokenization-and-vocabulary" id="id46">Tokenization and Vocabulary</a></p></li>
<li><p><a class="reference internal" href="#dataset-and-dataloading-poor-man-s-dataloader" id="id47">Dataset and Dataloading (Poor Man’s Dataloader)</a></p>
<ul>
<li><p><a class="reference internal" href="#memory-mapping" id="id48">Memory Mapping</a></p></li>
<li><p><a class="reference internal" href="#input-sequence" id="id49">Input Sequence</a></p></li>
<li><p><a class="reference internal" href="#context-length-context-window-block-size" id="id50">Context Length / Context Window / Block Size</a></p></li>
<li><p><a class="reference internal" href="#shuffling-and-discrete-uniform-sampling" id="id51">Shuffling and Discrete Uniform Sampling</a></p></li>
<li><p><a class="reference internal" href="#construction-input-sequences" id="id52">Construction Input Sequences</a></p></li>
<li><p><a class="reference internal" href="#construction-target-sequences" id="id53">Construction Target Sequences</a></p></li>
<li><p><a class="reference internal" href="#asynchronous-data-loading-and-prefetching" id="id54">Asynchronous Data Loading and Prefetching</a></p></li>
<li><p><a class="reference internal" href="#collating-everything-together" id="id55">Collating Everything Together</a></p></li>
<li><p><a class="reference internal" href="#using-pytorch-s-dataset-and-dataloader" id="id56">Using PyTorch’s Dataset and Dataloader</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#generative-pre-trained-transformer-gpt" id="id57">Generative Pre-trained Transformer (GPT)</a></p>
<ul>
<li><p><a class="reference internal" href="#modifications-from-gpt-1-and-model-stability" id="id58">Modifications from GPT-1 and Model Stability</a></p></li>
<li><p><a class="reference internal" href="#gpt-2-variants" id="id59">GPT-2 Variants</a></p></li>
<li><p><a class="reference internal" href="#gpt-2-model-architecture-huggingface" id="id60">GPT-2 Model Architecture (HuggingFace)</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#token-embeddings" id="id61">Token Embeddings</a></p>
<ul>
<li><p><a class="reference internal" href="#implementation" id="id62">Implementation</a></p></li>
<li><p><a class="reference internal" href="#output-of-the-embedding-layer" id="id63">Output Of The Embedding Layer</a></p></li>
<li><p><a class="reference internal" href="#one-hot-representation-of-input-sequence-mathbf-x" id="id64">One-Hot Representation of Input Sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span></a></p>
<ul>
<li><p><a class="reference internal" href="#definition" id="id65">Definition</a></p></li>
<li><p><a class="reference internal" href="#one-hot-encoding-process" id="id66">One-Hot Encoding Process</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#embedding-layer-is-matmul-of-one-hot-encoded-input-matrix-and-embedding-matrix-weights" id="id67">Embedding Layer Is Matmul Of One-Hot Encoded Input Matrix And Embedding Matrix Weights</a></p>
<ul>
<li><p><a class="reference internal" href="#id8" id="id68">Definition</a></p></li>
<li><p><a class="reference internal" href="#lookup" id="id69">Lookup</a></p></li>
<li><p><a class="reference internal" href="#semantic-representation" id="id70">Semantic Representation</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#positional-embeddings" id="id71">Positional Embeddings</a></p>
<ul>
<li><p><a class="reference internal" href="#id11" id="id72">Definition</a></p></li>
<li><p><a class="reference internal" href="#id13" id="id73">Implementation</a></p></li>
<li><p><a class="reference internal" href="#visualising-positional-encodings" id="id74">Visualising Positional Encodings</a></p></li>
<li><p><a class="reference internal" href="#an-example-of-positional-encoding" id="id75">An Example of Positional Encoding</a></p></li>
<li><p><a class="reference internal" href="#positional-encodings-via-embeddings" id="id76">Positional Encodings Via Embeddings</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#layer-normalization" id="id77">Layer Normalization</a></p>
<ul>
<li><p><a class="reference internal" href="#id17" id="id78">Definition</a></p></li>
<li><p><a class="reference internal" href="#learnable-affine-transformation" id="id79">Learnable Affine Transformation</a></p></li>
<li><p><a class="reference internal" href="#id18" id="id80">Implementation</a></p></li>
<li><p><a class="reference internal" href="#layer-norm-stabilises-activation-distributions" id="id81">Layer Norm Stabilises Activation Distributions</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#residual-connection" id="id82">Residual Connection</a></p>
<ul>
<li><p><a class="reference internal" href="#layernorm-and-residual-connection" id="id83">LayerNorm and Residual Connection</a></p></li>
<li><p><a class="reference internal" href="#implementation-of-residual-block-and-addnorm" id="id84">Implementation of Residual Block and AddNorm</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#self-attention" id="id85">Self-Attention</a></p>
<ul>
<li><p><a class="reference internal" href="#intuition-of-attention-mechanism" id="id86">Intuition of Attention Mechanism</a></p></li>
<li><p><a class="reference internal" href="#token-embedding-and-vector-representation-process" id="id87">Token Embedding and Vector Representation Process</a></p></li>
<li><p><a class="reference internal" href="#queries-keys-and-values" id="id88">Queries, Keys, and Values</a></p>
<ul>
<li><p><a class="reference internal" href="#database-analogy" id="id89">Database Analogy</a></p></li>
<li><p><a class="reference internal" href="#queries-keys-and-values-in-attention-mechanism" id="id90">Queries, Keys, and Values in Attention Mechanism</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#linear-projections" id="id91">Linear Projections</a></p></li>
<li><p><a class="reference internal" href="#scaled-dot-product-attention" id="id92">Scaled Dot-Product Attention</a></p>
<ul>
<li><p><a class="reference internal" href="#id26" id="id93">Definition</a></p></li>
<li><p><a class="reference internal" href="#attention-scoring-function" id="id94">Attention Scoring Function</a></p></li>
<li><p><a class="reference internal" href="#scaling-down-the-dot-product-of-query-and-key-vectors" id="id95">Scaling Down the Dot Product of Query and Key Vectors</a></p></li>
<li><p><a class="reference internal" href="#softmax" id="id96">Softmax</a></p></li>
<li><p><a class="reference internal" href="#context-vector-matrix" id="id97">Context Vector/Matrix</a></p></li>
<li><p><a class="reference internal" href="#numerical-stability-and-gradient-saturation" id="id98">Numerical Stability and Gradient Saturation</a></p></li>
<li><p><a class="reference internal" href="#visualizing-variance-of-dot-product" id="id99">Visualizing Variance of Dot Product</a></p></li>
<li><p><a class="reference internal" href="#projections-lead-to-dynamic-context-vectors" id="id100">Projections Lead to Dynamic Context Vectors</a></p></li>
<li><p><a class="reference internal" href="#id30" id="id101">Implementation</a></p></li>
<li><p><a class="reference internal" href="#heatmap" id="id102">Heatmap</a></p></li>
<li><p><a class="reference internal" href="#masked-causal-self-attention" id="id103">Masked/Causal Self-Attention</a></p></li>
<li><p><a class="reference internal" href="#last-token-has-full-context" id="id104">Last Token has Full Context</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#computational-complexity-of-self-attention" id="id105">Computational Complexity of Self-Attention</a></p></li>
<li><p><a class="reference internal" href="#self-attention-enables-parallelism" id="id106">Self-Attention Enables Parallelism</a></p>
<ul>
<li><p><a class="reference internal" href="#complexity-per-layer" id="id107">Complexity per Layer</a></p></li>
<li><p><a class="reference internal" href="#sequential-operations" id="id108">Sequential Operations</a></p></li>
<li><p><a class="reference internal" href="#maximum-path-length" id="id109">Maximum Path Length</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#multi-head-attention" id="id110">Multi-Head Attention</a></p>
<ul>
<li><p><a class="reference internal" href="#intuition" id="id111">Intuition</a></p></li>
<li><p><a class="reference internal" href="#id34" id="id112">Definition</a></p></li>
<li><p><a class="reference internal" href="#id35" id="id113">Implementation</a></p></li>
<li><p><a class="reference internal" href="#permutation-invariance" id="id114">Permutation Invariance</a></p></li>
<li><p><a class="reference internal" href="#applying-layernorm-and-residual-connections-to-multi-head-attention-output" id="id115">Applying LayerNorm and Residual Connections to Multi-Head Attention Output</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#positionwise-feed-forward-networks" id="id116">Positionwise Feed-Forward Networks</a></p>
<ul>
<li><p><a class="reference internal" href="#independent-processing" id="id117">Independent Processing</a></p></li>
<li><p><a class="reference internal" href="#identical-application" id="id118">Identical Application</a></p></li>
<li><p><a class="reference internal" href="#id37" id="id119">Definition</a></p></li>
<li><p><a class="reference internal" href="#projection-to-a-higher-dimension-space" id="id120">Projection to a Higher Dimension Space</a></p></li>
<li><p><a class="reference internal" href="#gaussian-error-linear-unit-gelu" id="id121">Gaussian Error Linear Unit (GELU)</a></p></li>
<li><p><a class="reference internal" href="#id39" id="id122">Implementation</a></p></li>
<li><p><a class="reference internal" href="#applying-layernorm-and-residual-connections-to-positionwise-ffn-output" id="id123">Applying LayerNorm and Residual Connections to Positionwise FFN Output</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#softmax-head" id="id124">Softmax Head</a></p></li>
<li><p><a class="reference internal" href="#putting-it-all-together-to-form-the-gpt" id="id125">Putting it all Together to form the GPT</a></p>
<ul>
<li><p><a class="reference internal" href="#patch-composer-configuration-with-model-config" id="id126">Patch Composer Configuration with Model Config</a></p></li>
<li><p><a class="reference internal" href="#decoder-blocks" id="id127">Decoder Blocks</a></p></li>
<li><p><a class="reference internal" href="#decoder" id="id128">Decoder</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#references-and-further-readings" id="id129">References and Further Readings</a></p></li>
</ul>
</nav>
<section id="dependencies">
<h2><a class="toc-backref" href="#id42" role="doc-backlink">Dependencies</a><a class="headerlink" href="#dependencies" title="Link to this heading">#</a></h2>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">tiktoken</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.backends.cudnn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">numpy.typing</span> <span class="kn">import</span> <span class="n">ArrayLike</span><span class="p">,</span> <span class="n">NDArray</span>
<span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span><span class="p">,</span> <span class="n">computed_field</span><span class="p">,</span> <span class="n">model_validator</span>
<span class="kn">from</span> <span class="nn">rich.pretty</span> <span class="kn">import</span> <span class="n">pprint</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="composing-the-configurations">
<h2><a class="toc-backref" href="#id43" role="doc-backlink">Composing the Configurations</a><a class="headerlink" href="#composing-the-configurations" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Composer</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2024</span>
    <span class="n">debug</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="n">url</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt&quot;</span>
    <span class="n">dataset_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;tinyshakespeare&quot;</span>
    <span class="n">data_folder</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s2">&quot;./data/tinyshakespeare&quot;</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Path to the data folder&quot;</span><span class="p">)</span>

    <span class="n">train_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Path to the train file&quot;</span><span class="p">)</span>
    <span class="n">valid_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Path to the valid file&quot;</span><span class="p">)</span>

    <span class="n">encoding_name</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;gpt2&quot;</span><span class="p">,</span> <span class="s2">&quot;r50k_base&quot;</span><span class="p">,</span> <span class="s2">&quot;p50k_base&quot;</span><span class="p">,</span> <span class="s2">&quot;p50k_edit&quot;</span><span class="p">,</span> <span class="s2">&quot;cl100k_base&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;gpt2&quot;</span>

    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Batch size&quot;</span><span class="p">)</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Block size, an alias for max length/context window size.&quot;</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="s2">&quot;context_length&quot;</span>
    <span class="p">)</span>
    <span class="n">device_type</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="s2">&quot;cuda&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Device to use&quot;</span><span class="p">)</span>

    <span class="c1"># model parameters</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Dimension of the model&quot;</span><span class="p">)</span>
    <span class="n">d_ff</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Dimension of the feed forward layer&quot;</span><span class="p">)</span>
    <span class="n">H</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Number of heads&quot;</span><span class="p">,</span> <span class="n">alias</span><span class="o">=</span><span class="s2">&quot;num_heads&quot;</span><span class="p">)</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mi">50257</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Vocabulary size&quot;</span><span class="p">)</span>
    <span class="n">num_decoder_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Number of decoder blocks&quot;</span><span class="p">)</span>

    <span class="nd">@model_validator</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;after&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">set_train_valid_paths</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Composer</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_folder</span><span class="p">)</span> <span class="o">/</span> <span class="s2">&quot;train.bin&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">valid_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_folder</span><span class="p">)</span> <span class="o">/</span> <span class="s2">&quot;valid.bin&quot;</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@model_validator</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;after&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">set_device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Composer</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_type</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@model_validator</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;after&quot;</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">set_debug_fields</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Composer</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="mi">8</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="mi">4</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">H</span> <span class="o">=</span> <span class="mi">2</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">d_ff</span> <span class="o">=</span> <span class="mi">4</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">class</span> <span class="nc">Config</span><span class="p">:</span>
        <span class="n">extra</span> <span class="o">=</span> <span class="s2">&quot;allow&quot;</span>
        <span class="n">arbitrary_types_allowed</span> <span class="o">=</span> <span class="kc">True</span>


<span class="n">composer</span> <span class="o">=</span> <span class="n">Composer</span><span class="p">(</span><span class="n">debug</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">composer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Composer</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">seed</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2024</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">debug</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">url</span>=<span style="color: #008000; text-decoration-color: #008000">'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">dataset_name</span>=<span style="color: #008000; text-decoration-color: #008000">'tinyshakespeare'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">data_folder</span>=<span style="color: #008000; text-decoration-color: #008000">'./data/tinyshakespeare'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">train_path</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">PosixPath</span><span style="font-weight: bold">(</span><span style="color: #008000; text-decoration-color: #008000">'data/tinyshakespeare/train.bin'</span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">valid_path</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">PosixPath</span><span style="font-weight: bold">(</span><span style="color: #008000; text-decoration-color: #008000">'data/tinyshakespeare/valid.bin'</span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">encoding_name</span>=<span style="color: #008000; text-decoration-color: #008000">'gpt2'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">batch_size</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">block_size</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">device_type</span>=<span style="color: #008000; text-decoration-color: #008000">'cpu'</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">device</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">device</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">type</span>=<span style="color: #008000; text-decoration-color: #008000">'cpu'</span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">d_model</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">d_ff</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">H</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">vocab_size</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">50257</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">num_decoder_blocks</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">6</span>
<span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
</section>
<section id="reproducibility">
<h2><a class="toc-backref" href="#id44" role="doc-backlink">Reproducibility</a><a class="headerlink" href="#reproducibility" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">configure_deterministic_mode</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># fmt: off</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">use_deterministic_algorithms</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">warn_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span>        <span class="o">=</span> <span class="kc">False</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span>    <span class="o">=</span> <span class="kc">True</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span>          <span class="o">=</span> <span class="kc">False</span>

    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;CUBLAS_WORKSPACE_CONFIG&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;:4096:8&#39;</span>
    <span class="c1"># fmt: on</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;Deterministic mode is activated. This will negatively impact performance and may cause increase in CUDA memory footprint.&quot;</span><span class="p">,</span>
        <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">,</span>
        <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span> <span class="nf">seed_all</span><span class="p">(</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1992</span><span class="p">,</span>
    <span class="n">seed_torch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">set_torch_deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="c1"># fmt: off</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PYTHONHASHSEED&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>       <span class="c1"># set PYTHONHASHSEED env var at fixed value</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>                    <span class="c1"># numpy pseudo-random generator</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>                              <span class="c1"># python&#39;s built-in pseudo-random generator</span>

    <span class="k">if</span> <span class="n">seed_torch</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>           <span class="c1"># pytorch (both CPU and CUDA)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="n">set_torch_deterministic</span><span class="p">:</span>
            <span class="n">configure_deterministic_mode</span><span class="p">()</span>
    <span class="c1"># fmt: on</span>
    <span class="k">return</span> <span class="n">seed</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seed_all</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">set_torch_deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024
</pre></div>
</div>
</div>
</div>
</section>
<section id="utilities">
<h2><a class="toc-backref" href="#id45" role="doc-backlink">Utilities</a><a class="headerlink" href="#utilities" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">am_i_in_jupyter</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">get_ipython</span>
        <span class="k">if</span> <span class="s2">&quot;IPKernelApp&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">config</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
    <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="kc">True</span>

<span class="n">IN_JUPYTER</span> <span class="o">=</span> <span class="n">am_i_in_jupyter</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="tokenization-and-vocabulary">
<h2><a class="toc-backref" href="#id46" role="doc-backlink">Tokenization and Vocabulary</a><a class="headerlink" href="#tokenization-and-vocabulary" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Traditional tokenization methods often involve steps such as
<strong><em>lower-casing</em></strong>, <strong><em>punctuation stripping</em></strong>, and <strong><em>splitting on
whitespace</em></strong>. Additionally, these methods might encode out-of-vocabulary
words using a special token to enable the model to handle unseen words
during evaluation or testing phases. For instance, language models (LMs) may
struggle with interpreting emojis due to such constraints.</p></li>
<li><p>These conventional approaches can inadvertently restrict the natural
language input space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>, consequently limiting the model space
<span class="math notranslate nohighlight">\(\mathcal{H}\)</span>. This limitation stems from the fact that the scope of
<span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is inherently dependent on the comprehensiveness of
<span class="math notranslate nohighlight">\(\mathcal{X}\)</span> as we can see
<span class="math notranslate nohighlight">\(\mathcal{H} = \mathcal{H}(\mathcal{X} ; \boldsymbol{\Theta})\)</span>, which means
that the model space <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is a function of the input space
<span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and the parameter space <span class="math notranslate nohighlight">\(\boldsymbol{\Theta}\)</span>.</p></li>
<li><p>To resolve this, the idea of <strong><em>byte-level encoding</em></strong> can be used - since
you theoretically can encode any character in the world in <strong><em>UTF-8
encoding</em></strong>.</p></li>
<li><p>However, the limitation is current byte-level language models tend to
perform poorly on word level tasks.</p></li>
<li><p>The authors then introduced the BPE algorithm (is “byte-level” because it
operates on UTF-8 encoded strings) where they striked a balance between
character-level and word-level tokenization.</p></li>
<li><p>So in summary, BPE is the <strong>tokenizer</strong> used to encode the input text into a
sequence of tokens - which form the input representation to the model.</p></li>
</ul>
<p>Byte pair encoding (BPE) is a way of converting text into tokens and is used as
the tokenizer in the training of GPT-2. It has a couple desirable
properties<a class="footnote-reference brackets" href="#id40" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>:</p>
<ul class="simple">
<li><p>It’s reversible and lossless, so you can convert tokens back into the
original text</p></li>
<li><p>It works on arbitrary text, even text that is not in the tokeniser’s
training data</p></li>
<li><p>It compresses the text: the token sequence is shorter than the bytes
corresponding to the original text. On average, in practice, each token
corresponds to about 4 bytes.</p></li>
<li><p>It attempts to let the model see common subwords. For instance, “ing” is a
common subword in English, so BPE encodings will often split “encoding” into
tokens like “encod” and “ing” (instead of e.g. “enc” and “oding”). Because
the model will then see the “ing” token again and again in different
contexts, it helps models generalise and better understand grammar.</p></li>
</ul>
<div class="seealso admonition">
<p class="admonition-title">References</p>
<ul class="simple">
<li><p><a class="github reference external" href="https://github.com/karpathy/minbpe">karpathy/minbpe</a></p></li>
<li><p><a class="github reference external" href="https://github.com/openai/tiktoken">openai/tiktoken</a></p></li>
<li><p><a class="github reference external" href="https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb">openai/openai-cookbook</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt">Tokenizers - HuggingFace</a></p></li>
</ul>
</div>
<p>We first download the dataset from the Karpathy’s
<a class="reference external" href="https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt">repo</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">download</span><span class="p">(</span><span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">dataset_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">dest_folder</span><span class="p">:</span> <span class="n">Path</span> <span class="o">|</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
    <span class="n">dest_folder_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">dest_folder</span><span class="p">)</span>

    <span class="n">dest_folder_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">filepath</span> <span class="o">=</span> <span class="n">dest_folder_path</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dataset_name</span><span class="si">}</span><span class="s2">.txt&quot;</span>

    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">corpus</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">text</span>
    <span class="n">response</span><span class="o">.</span><span class="n">raise_for_status</span><span class="p">()</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">iter_content</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">8192</span><span class="p">):</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">filepath</span><span class="p">,</span> <span class="n">corpus</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">filepath</span><span class="p">,</span> <span class="n">corpus</span> <span class="o">=</span> <span class="n">download</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">url</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">dataset_name</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">data_folder</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">PosixPath</span><span style="font-weight: bold">(</span><span style="color: #008000; text-decoration-color: #008000">'data/tinyshakespeare/tinyshakespeare.txt'</span><span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
<p>We print the first <span class="math notranslate nohighlight">\(100\)</span> characters from the corpus below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">corpus</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>First Citizen:
Before we proceed any further, hear me speak.

All:
Speak, speak.

First Citizen:
You
</pre></div>
</div>
</div>
</div>
<p>We print out all Tiktoken encodings, and note to ourself that we will be using
<code class="docutils literal notranslate"><span class="pre">gpt2</span></code> - in which the vocabulary size <span class="math notranslate nohighlight">\(\lvert \mathcal{V} \rvert = 50257\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;All TikToken encodings: </span><span class="si">{</span><span class="n">tiktoken</span><span class="o">.</span><span class="n">list_encoding_names</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># encode with tiktoken gpt2 bpe</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">encoding_name</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vocabulary size: </span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">n_vocab</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>All TikToken encodings: [&#39;gpt2&#39;, &#39;r50k_base&#39;, &#39;p50k_base&#39;, &#39;p50k_edit&#39;, &#39;cl100k_base&#39;]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Vocabulary size: 50257
</pre></div>
</div>
</div>
</div>
<p>We then slice the <code class="docutils literal notranslate"><span class="pre">corpus</span></code> into <code class="docutils literal notranslate"><span class="pre">train-valid</span></code> subsets with a ratio of
<span class="math notranslate nohighlight">\(9:1\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">[:</span> <span class="nb">int</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="mf">0.9</span><span class="p">)]</span>
<span class="n">valid_data</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">N</span> <span class="o">*</span> <span class="mf">0.9</span><span class="p">)</span> <span class="p">:]</span>
</pre></div>
</div>
</div>
</div>
<p>We then encode <code class="docutils literal notranslate"><span class="pre">train_data</span></code> and <code class="docutils literal notranslate"><span class="pre">valid_data</span></code> using the tokenizer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_ordinary</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="n">valid_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_ordinary</span><span class="p">(</span><span class="n">valid_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;train has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_ids</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> tokens&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;val has </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">valid_ids</span><span class="p">)</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> tokens&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train has 301,966 tokens
val has 36,059 tokens
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">train_ids</span><span class="p">[:</span><span class="mi">2</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">80</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">train_ids</span><span class="p">[:</span><span class="mi">100</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>First Citizen
--------------------------------------------------------------------------------
First Citizen:
Before we proceed any further, hear me speak.

All:
Speak, speak.

First Citizen:
You are all resolved rather to die than to famish?

All:
Resolved. resolved.

First Citizen:
First, you know Caius Marcius is chief enemy to the people.

All:
We know&#39;t, we know&#39;t.

First Citizen:
Let us kill him, and we
</pre></div>
</div>
</div>
</div>
<p>Lastly, we save the tokenized corpus into <code class="docutils literal notranslate"><span class="pre">.bin</span></code> file for later usage.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># export to bin files</span>
<span class="n">train_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">)</span>
<span class="n">valid_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">valid_ids</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">)</span>

<span class="n">train_ids</span><span class="o">.</span><span class="n">tofile</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">train_path</span><span class="p">)</span>
<span class="n">valid_ids</span><span class="o">.</span><span class="n">tofile</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">valid_path</span><span class="p">)</span>

<span class="c1"># train.bin has 301,966 tokens</span>
<span class="c1"># val.bin has 36,059 tokens</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="dataset-and-dataloading-poor-man-s-dataloader">
<h2><a class="toc-backref" href="#id47" role="doc-backlink">Dataset and Dataloading (Poor Man’s Dataloader)</a><a class="headerlink" href="#dataset-and-dataloading-poor-man-s-dataloader" title="Link to this heading">#</a></h2>
<p>To batch the corpus into mini-batch of <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> for training using PyTorch
framework, we would need to create an efficient way of loading. The easy way out
is of course to use PyTorch’s <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class and work from there, but to keep
this post similar to what Karpathy used, we would try to understand how he
approached it.</p>
<p>As Karpathy puts it, he implemented a poor man’s
<a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">dataloader</a>.
We will start by dissecting the code and understanding how it works and finally,
show that everything can be done with PyTorch’s <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">Dataloader</span></code>.</p>
<section id="memory-mapping">
<h3><a class="toc-backref" href="#id48" role="doc-backlink">Memory Mapping</a><a class="headerlink" href="#memory-mapping" title="Link to this heading">#</a></h3>
<p>Firstly, Karpathy uses <code class="docutils literal notranslate"><span class="pre">numpy</span></code>’s
<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.memmap.html">memory mapping</a>
(<code class="docutils literal notranslate"><span class="pre">numpy.memmap</span></code>) to load the data. Memory mapping is used to create a
memory-mapped array from a binary file. This involves mapping the contents of a
file directly into the virtual memory space of the calling process. This allows
applications to access the file data as if it were loaded in memory, using
pointer operations or array indexing, without the need for explicit read or
write operations.</p>
<p>This essentially means that you can access small segments of a large file
without having to load the entire file into memory. The concept draws
similarities to the use of <a class="reference external" href="https://wiki.python.org/moin/Generators">generators</a>
in Python, where you can iterate over a large dataset without having to load the
entire dataset into memory.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">memmap</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">train_path</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">)</span>
<span class="n">train_data_dtype</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">dtype</span>
<span class="n">train_data_shape</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">shape</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;data_dtype: </span><span class="si">{</span><span class="n">train_data_dtype</span><span class="si">}</span><span class="s2">, data_shape: </span><span class="si">{</span><span class="n">train_data_shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>data_dtype: uint16, data_shape: (301966,)
</pre></div>
</div>
</div>
</div>
<p>We see that the shape of train data is <code class="docutils literal notranslate"><span class="pre">(301966,)</span></code>, which means that it is a 1D (flattened) array
with <span class="math notranslate nohighlight">\(301966\)</span> elements - this is basically the length of the entire train corpus, in terms of
tokens.</p>
</section>
<section id="input-sequence">
<h3><a class="toc-backref" href="#id49" role="doc-backlink">Input Sequence</a><a class="headerlink" href="#input-sequence" title="Link to this heading">#</a></h3>
<p>However, we are not going to pass the entire training corpus as is to the model.
Instead, we are going to pass a <strong>batch</strong> of sequences (each sequence of length
<code class="docutils literal notranslate"><span class="pre">context_length</span></code>) to the model at a time.</p>
<p>Let’s consider a sequence
<span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, x_2, \ldots, x_T) \in \mathbb{Z}^{1 \times T}\)</span>, where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_t \in \mathbf{x}\)</span> represents the <span class="math notranslate nohighlight">\(t\)</span>-th token in the sequence,</p></li>
<li><p>Each token <span class="math notranslate nohighlight">\(x_t\)</span> is an element of a predefined vocabulary
<span class="math notranslate nohighlight">\(\mathcal{V} := \mathcal{X}\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(T\)</span> denotes the total number of tokens in the sequence, i.e., the sequence
length.</p></li>
</ul>
<p>In practice, we handle multiple sequences at once by grouping them into a batch.
The batch size, denoted as <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>, is then presented to the model for
parallel processing.</p>
<p>A batch of sequences is represented as a matrix <span class="math notranslate nohighlight">\(\mathbf{x}^{\mathcal{B}}\)</span>,
where each row corresponds to a sequence in the batch. If the batch size is
<span class="math notranslate nohighlight">\(\mathcal{B}\)</span> and each sequence within the batch has a fixed length <span class="math notranslate nohighlight">\(T\)</span>, then
<span class="math notranslate nohighlight">\(\mathbf{x}^{\mathcal{B}}\)</span> can be expressed as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \mathbf{x}^{\mathcal{B}} = \begin{bmatrix}
    \mathbf{x}_1 \\
    \mathbf{x}_2 \\
    \vdots \\
    \mathbf{x}_\mathcal{B}
    \end{bmatrix} =
    \begin{bmatrix}
    x_{1,1} &amp; x_{1,2} &amp; \ldots &amp; x_{1,T} \\
    x_{2,1} &amp; x_{2,2} &amp; \ldots &amp; x_{2,T} \\
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    x_{\mathcal{B},1} &amp; x_{\mathcal{B},2} &amp; \ldots &amp; x_{\mathcal{B},T}
    \end{bmatrix} \in \mathbb{Z}^{\mathcal{B} \times T}
\end{split}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> represents the <span class="math notranslate nohighlight">\(i\)</span>-th sequence in the batch, and <span class="math notranslate nohighlight">\(x_{i,j}\)</span>
denotes the <span class="math notranslate nohighlight">\(j\)</span>-th token in the <span class="math notranslate nohighlight">\(i\)</span>-th sequence of the batch. It’s important to
note that while we represent the sequences in a real-valued space
<span class="math notranslate nohighlight">\(\mathbb{Z}^{\mathcal{B} \times T}\)</span> for mathematical convenience, in practice,
each <span class="math notranslate nohighlight">\(x_{i,j}\)</span> corresponds to a discrete token from the vocabulary <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>
so using <span class="math notranslate nohighlight">\(\mathbb{Z}^{+}\)</span> would be more appropriate.</p>
</section>
<section id="context-length-context-window-block-size">
<h3><a class="toc-backref" href="#id50" role="doc-backlink">Context Length / Context Window / Block Size</a><a class="headerlink" href="#context-length-context-window-block-size" title="Link to this heading">#</a></h3>
<p><span class="math notranslate nohighlight">\(T\)</span> is often referred to as the sequence length, or in the context of GPT, it is
the <code class="docutils literal notranslate"><span class="pre">block_size</span></code> or <code class="docutils literal notranslate"><span class="pre">context_length</span></code> or <code class="docutils literal notranslate"><span class="pre">max_seq_len</span></code>.</p>
<p>It is the length of the sequence that the model will be trained on and is also
the context length/context window that we often hear about.</p>
<p>For example,
<a class="reference external" href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024">Gemini 1.5</a>
was announced to have a standard <span class="math notranslate nohighlight">\(128,000\)</span> token context window, up to a maximum
of <span class="math notranslate nohighlight">\(1\)</span> million max length.</p>
<p>Let’s look at an example, if we define our <span class="math notranslate nohighlight">\(T\)</span> to be <span class="math notranslate nohighlight">\(32\)</span>, then we would expect
each sequence to be of length <span class="math notranslate nohighlight">\(32\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">first_sequence</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">0</span><span class="o">+</span><span class="mi">32</span><span class="p">]</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">first_sequence</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">first_sequence</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">first_sequence_decoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">first_sequence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">first_sequence_decoded</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">memmap</span><span style="font-weight: bold">([</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5962</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">22307</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">25</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">198</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8421</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">356</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5120</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">597</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2252</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │      </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3285</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">502</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2740</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">198</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">198</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3237</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">25</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │     </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">198</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5248</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">461</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2740</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">198</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">198</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5962</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">22307</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">25</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">198</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1639</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">389</span><span style="font-weight: bold">]</span>, <span style="color: #808000; text-decoration-color: #808000">dtype</span>=<span style="color: #800080; text-decoration-color: #800080">uint16</span><span style="font-weight: bold">)</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>,<span style="font-weight: bold">)</span>
</pre>
</div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>First Citizen:
Before we proceed any further, hear me speak.

All:
Speak, speak.

First Citizen:
You are
</pre></div>
</div>
</div>
</div>
<p>The example is just extracting <span class="math notranslate nohighlight">\(1\)</span> such sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> from the train
corpus. To leverage the prowess of linear algebra operations in CUDA, we would
typically pass a batch of sequences <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> to the model at a time.</p>
<p>Furthermore, we would require some level of randomness in the sequences that we
pass to the model to enable generalisation. <strike>You really do not want the
model to overfit to an ordered sequence of tokens in the training
corpus.</strike></p>
<p>To this end, let’s see how Karpathy implements batching and shuffling of the
sequences.</p>
</section>
<section id="shuffling-and-discrete-uniform-sampling">
<h3><a class="toc-backref" href="#id51" role="doc-backlink">Shuffling and Discrete Uniform Sampling</a><a class="headerlink" href="#shuffling-and-discrete-uniform-sampling" title="Link to this heading">#</a></h3>
<p>To enable shuffling, Karpathy generates a tensor of random integers (essentially a list of
random integers), which serve as indices. These indices are used to select
random sequences from the training (and validation) data.</p>
<p>For simplicity, let’s look at the case where batch size is reduced to <span class="math notranslate nohighlight">\(\mathcal{B} = 1\)</span>.
This means we only need to sample <span class="math notranslate nohighlight">\(1\)</span> sequence from the training data - and consequently
we only need <span class="math notranslate nohighlight">\(1\)</span> random index.</p>
<p>We can easily achieve this via <code class="docutils literal notranslate"><span class="pre">torch.randint</span></code> which generates random integers
from a discrete uniform distribution over the half-open interval <span class="math notranslate nohighlight">\([l, h)\)</span>,
and since we only want to sample <span class="math notranslate nohighlight">\(1\)</span> sequence, we set <code class="docutils literal notranslate"><span class="pre">size=(1,)</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="n">generator</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>

<span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span> <span class="o">-</span> <span class="n">composer</span><span class="o">.</span><span class="n">block_size</span>
<span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
<span class="n">indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">high</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">122484</span><span style="font-weight: bold">])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span><span style="font-weight: bold">])</span>
</pre>
</div></div>
</div>
<p>The mathematical operation performed by <code class="docutils literal notranslate"><span class="pre">torch.randint(low,</span> <span class="pre">high,</span> <span class="pre">size,</span> <span class="pre">generator)</span></code> can be described as drawing samples from a uniform discrete distribution. Each element of the resulting tensor is an independent and identically distributed <span id="id2">[<a class="reference internal" href="../../bibliography.html#id17" title="Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.">Radford <em>et al.</em>, 2019</a>]</span> (i.i.d.) random variable <span class="math notranslate nohighlight">\(X_i\)</span> with the following probability mass function (PMF):</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(X_i = k) = \frac{1}{h - l} \quad \text{for} \, k = l, \ldots, h-1 
\]</div>
<p>This PMF implies that each integer in the range <span class="math notranslate nohighlight">\([l, h-1]\)</span> has an equal probability of being selected.</p>
<p>In our demonstration, we selected a random index, specifically <span class="math notranslate nohighlight">\(136,016\)</span>, from
our training dataset. This serves as a starting point for constructing a
sequence, denoted as <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. This sequence consists of the token found at
the chosen index and extends to include the subsequent <span class="math notranslate nohighlight">\(T\)</span> tokens, where <span class="math notranslate nohighlight">\(T\)</span>
represents the block size. For the sake of simplicity, and to align with our
predefined settings, we have chosen <span class="math notranslate nohighlight">\(T = 8\)</span>. This block size is predetermined in
our <code class="docutils literal notranslate"><span class="pre">composer</span></code> configuration, activated specifically under a <code class="docutils literal notranslate"><span class="pre">debug</span></code> mode.</p>
<p>In code, we can achieve this by slicing the training data from the random index
to the random index plus the block size. This is done by <code class="docutils literal notranslate"><span class="pre">train_data[random_index:random_index+block_size]</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random_sequence</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="n">indices</span> <span class="p">:</span> <span class="n">indices</span> <span class="o">+</span> <span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">]</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">random_sequence</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">random_sequence</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">random_sequence_decoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">random_sequence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">random_sequence</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">memmap</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11503</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">290</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">21120</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">30</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">880</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">788</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">29448</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│      </span><span style="color: #808000; text-decoration-color: #808000">dtype</span>=<span style="color: #800080; text-decoration-color: #800080">uint16</span><span style="font-weight: bold">)</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">(</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>,<span style="font-weight: bold">)</span>
</pre>
</div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> priest and clerk? well then, amen
</pre></div>
</div>
</div>
</div>
<p>One might wonder why the highest value of the random integers is
<code class="docutils literal notranslate"><span class="pre">len(self.train_data)</span> <span class="pre">-</span> <span class="pre">self.block_size</span></code>. This is mostly to prevent index out of
range errors. As we shall soon see, we are using these <code class="docutils literal notranslate"><span class="pre">indices</span></code> to slice a
sequence of length <code class="docutils literal notranslate"><span class="pre">block_size</span></code> from the data where you start slicing from the
index <code class="docutils literal notranslate"><span class="pre">index</span></code> and end at <code class="docutils literal notranslate"><span class="pre">index</span> <span class="pre">+</span> <span class="pre">block_size</span></code>.</p>
</section>
<section id="construction-input-sequences">
<h3><a class="toc-backref" href="#id52" role="doc-backlink">Construction Input Sequences</a><a class="headerlink" href="#construction-input-sequences" title="Link to this heading">#</a></h3>
<p>Now that we understand how to sample a single sequence from the training data,
let’s look at how we can sample a batch of sequences.
PyTorch made it easy for you, as we can just simply change the <code class="docutils literal notranslate"><span class="pre">size</span></code> parameter
to <code class="docutils literal notranslate"><span class="pre">(batch_size,)</span></code> so we can sample <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> number of indices - and
consequently <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> number of sequences.</p>
<p>In our case, if we set <span class="math notranslate nohighlight">\(\mathcal{B} = 2\)</span>, we would expect to get <span class="math notranslate nohighlight">\(2\)</span> random
indices.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">generator</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>
<span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span> <span class="o">-</span> <span class="n">composer</span><span class="o">.</span><span class="n">block_size</span>
<span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,)</span>
<span class="n">indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">high</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">122484</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">196406</span><span style="font-weight: bold">])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span><span style="font-weight: bold">])</span>
</pre>
</div></div>
</div>
<p>We then construct a batch of input sequences <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> by selecting the
tokens at the indices <span class="math notranslate nohighlight">\(122,484\)</span> and <span class="math notranslate nohighlight">\(196,406\)</span> and the next <span class="math notranslate nohighlight">\(T\)</span> tokens via a for
loop - and using <code class="docutils literal notranslate"><span class="pre">torch.stack</span></code> to stack the sequences into a tensor of shape
<span class="math notranslate nohighlight">\(\mathbb{Z}^{\mathcal{B} \times T}\)</span>.</p>
<p>So the first row of the batch would be the sequence starting at index <span class="math notranslate nohighlight">\(122,484\)</span>
and the second row would be the sequence starting at index <span class="math notranslate nohighlight">\(196,406\)</span>, with each
sequence having a length of <span class="math notranslate nohighlight">\(T=8\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">((</span><span class="n">train_data</span><span class="p">[</span><span class="n">index</span> <span class="p">:</span> <span class="n">index</span> <span class="o">+</span> <span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">])</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11503</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">290</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">21120</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">30</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">880</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">788</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">29448</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span>  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">326</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8616</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">373</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14855</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">198</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">198</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">37286</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span><span style="font-weight: bold">])</span>
</pre>
</div></div>
</div>
<p>It is worth reconciling the fact that the slicing uses <code class="docutils literal notranslate"><span class="pre">[index:index</span> <span class="pre">+</span> <span class="pre">block_size]</span></code> and
therefore completes the reasoning behind the <code class="docutils literal notranslate"><span class="pre">len(self.train_data)</span> <span class="pre">-</span> <span class="pre">self.block_size</span></code> in
the <code class="docutils literal notranslate"><span class="pre">torch.randint</span></code> function call - to prevent index out of range errors. Consider
that if we do not subtract <code class="docutils literal notranslate"><span class="pre">block_size</span></code> from the length of the training data, we might
end up with an index that is the last index of the training data, and when we add
<code class="docutils literal notranslate"><span class="pre">block_size</span></code> to it, we would end up with an index that is out of range.</p>
</section>
<section id="construction-target-sequences">
<h3><a class="toc-backref" href="#id53" role="doc-backlink">Construction Target Sequences</a><a class="headerlink" href="#construction-target-sequences" title="Link to this heading">#</a></h3>
<p>As we will define more formally later, GPT model is an autoregressive
self-supervised learning model <span id="id3">[<a class="reference internal" href="../../bibliography.html#id18" title="Minhyeok Lee. A mathematical interpretation of autoregressive generative pre-trained transformer and self-supervised learning. Mathematics, 2023. URL: https://www.mdpi.com/2227-7390/11/11/2451, doi:10.3390/math11112451.">Lee, 2023</a>]</span> that directly learns a
conditional probability distribution <span class="math notranslate nohighlight">\(\mathbb{P}(x_t | x_{&lt;t} ; \Theta)\)</span> over
the vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span> of tokens, which is conditioned on the entire
history of tokens <span class="math notranslate nohighlight">\(x_{&lt;t} = (x_1, x_2, \ldots, x_{t-1})\)</span>.</p>
<p>We have seen earlier how to construct an input sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> from the
training data. To put things into perspective, we consider again the first
sequence that we constructed from the training data:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x} = \begin{bmatrix} 11503 &amp; 290 &amp; 21120 &amp; 30 &amp; 880 &amp; 788 &amp; 11 &amp; 29448 \end{bmatrix}
\]</div>
<p>representing the sentence <code class="docutils literal notranslate"><span class="pre">'priest</span> <span class="pre">and</span> <span class="pre">clerk?</span> <span class="pre">well</span> <span class="pre">then,</span> <span class="pre">amen'</span></code>.</p>
<p>Given the autoregressive and self-supervised nature, in order to construct the
target sequence <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>, we simply shift the input sequence by one token to
the left. This means that the target sequence <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{y} = \begin{bmatrix} 290 &amp; 21120 &amp; 30 &amp; 880 &amp; 788 &amp; 11 &amp; 29448 &amp; 13 \end{bmatrix}
\]</div>
<p>representing the sentence <code class="docutils literal notranslate"><span class="pre">'and</span> <span class="pre">clerk?</span> <span class="pre">well</span> <span class="pre">then,</span> <span class="pre">amen.'</span></code>. Note here <span class="math notranslate nohighlight">\(13\)</span> is the
index of the next token after the last token in the input sequence.</p>
<p>This behaviour is autoregressive because we are using the context tokens
<span class="math notranslate nohighlight">\(x_{&lt;t}\)</span> to predict the next token <span class="math notranslate nohighlight">\(x_t\)</span>, and self-supervised because we are
using the input sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to construct the target sequence
<span class="math notranslate nohighlight">\(\mathbf{y}\)</span> without any external labels.</p>
<p>To illustrate further, the prediction process during training is cumulative:</p>
<ul class="simple">
<li><p>For predicting <span class="math notranslate nohighlight">\(x_2\)</span>, the model uses <span class="math notranslate nohighlight">\(x_1\)</span> as context:
<span class="math notranslate nohighlight">\(\mathbb{P}\left(x_2 \mid x_1\right)\)</span>.</p></li>
<li><p>For predicting <span class="math notranslate nohighlight">\(x_3\)</span>, the model uses both <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> as context:
<span class="math notranslate nohighlight">\(\mathbb{P}\left(x_3 \mid x_1, x_2\right)\)</span>.</p></li>
<li><p>This pattern continues, such that for predicting <span class="math notranslate nohighlight">\(x_t\)</span>, the model uses
<span class="math notranslate nohighlight">\(x_1, x_2, \ldots, x_{t-1}\)</span> as context:
<span class="math notranslate nohighlight">\(\mathbb{P}\left(x_t \mid x_1, x_2, \ldots, x_{t-1}\right)\)</span></p></li>
</ul>
<p>In code, we can achieve this by simply slicing the adding a <code class="docutils literal notranslate"><span class="pre">1</span></code> to the <code class="docutils literal notranslate"><span class="pre">index</span></code>
in the <code class="docutils literal notranslate"><span class="pre">train_data</span></code> slicing operation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">((</span><span class="n">train_data</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">])</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span>  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">290</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">21120</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">30</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">880</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">788</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">29448</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8616</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">373</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14855</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">198</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">198</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">37286</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">406</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span><span style="font-weight: bold">])</span>
</pre>
</div><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39; and clerk? well then, amen.&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="asynchronous-data-loading-and-prefetching">
<h3><a class="toc-backref" href="#id54" role="doc-backlink">Asynchronous Data Loading and Prefetching</a><a class="headerlink" href="#asynchronous-data-loading-and-prefetching" title="Link to this heading">#</a></h3>
<p>As we approach the last part of the code, Karpathy moves <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> to the
device and returns them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">composer</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
    <span class="c1"># pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This is a common operation in PyTorch, where we move the data to the underlying
device (CPU or GPU or MPS) to leverage the processing capabilities of the
device. It goes without saying that modern deep learning models are trained on
GPUs - and CUDA is the de facto standard for GPU-accelerated computing.</p>
<p>CUDA allows a <code class="docutils literal notranslate"><span class="pre">pin_memory</span></code> and <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> parameter to be set when transferring
tensor data from CPU to GPU. The <code class="docutils literal notranslate"><span class="pre">pin_memory</span></code> parameter is used to allow <code class="docutils literal notranslate"><span class="pre">.to(&quot;cuda&quot;)</span></code>
to be more <a class="reference external" href="https://devblogs.nvidia.com/how-optimize-data-transfers-cuda-cc/">performant</a>
as it avoids some implicit CPU-to-CPU copies. Tensors which are pinned in memory
also allow the transfer from CPU to GPU to be done asynchronously via <code class="docutils literal notranslate"><span class="pre">non_blocking</span></code> with respect to
the host<a class="footnote-reference brackets" href="#id41" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>.</p>
<p>It can be useful because we can do some other work in CPU while the data is being
transferred to GPU. Consider the below scenario:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">tensor.pin_memory().to(&quot;cuda&quot;,</span> <span class="pre">non_blocking=True)</span></code> will transfer the tensor
to the GPU asynchronously, and the CPU can continue doing some other work.</p></li>
<li><p>While waiting, CPU can do some other operations without waiting for the
transfer to complete,</p></li>
<li><p>Once <code class="docutils literal notranslate"><span class="pre">tensor</span></code> is transferred to the GPU, then we can do some other operations
on the GPU.</p></li>
</ul>
<p>What is worth noting is that CUDA manages the synchronization such that
operations on the GPU will not start until the transfer is complete. However, CUDA
programming is complex and is out of the scope of this post. Interested readers
can see the reference section.</p>
</section>
<section id="collating-everything-together">
<h3><a class="toc-backref" href="#id55" role="doc-backlink">Collating Everything Together</a><a class="headerlink" href="#collating-everything-together" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">Tuple</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;get_batch&quot;</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span>
    <span class="n">composer</span><span class="p">:</span> <span class="n">Composer</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">split</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">],</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">generator</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="n">device_type</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="s2">&quot;cuda&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="c1"># We recreate np.memmap every batch to avoid a memory leak, as per</span>
    <span class="c1"># https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122</span>
    <span class="k">if</span> <span class="n">split</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">memmap</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">train_path</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">memmap</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">valid_path</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">)</span>

    <span class="n">low</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="n">block_size</span>
    <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,)</span>

    <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">high</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="n">index</span> <span class="p">:</span> <span class="n">index</span> <span class="o">+</span> <span class="n">block_size</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
        <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">index</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">block_size</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span> <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">indices</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
        <span class="c1"># pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">generator</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>
<span class="n">train_batch</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span>
    <span class="n">composer</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">block_size</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">train_batch</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11503</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">290</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">21120</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">30</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">880</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">788</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">29448</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span>  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">326</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8616</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">373</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14855</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">198</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">198</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">37286</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span>  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">290</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">21120</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">30</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">880</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">788</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">29448</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8616</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">373</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">14855</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">198</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">198</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">37286</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">406</span><span style="font-weight: bold">]])</span>
</pre>
</div></div>
</div>
</section>
<section id="using-pytorch-s-dataset-and-dataloader">
<h3><a class="toc-backref" href="#id56" role="doc-backlink">Using PyTorch’s Dataset and Dataloader</a><a class="headerlink" href="#using-pytorch-s-dataset-and-dataloader" title="Link to this heading">#</a></h3>
<p>It is relatively simple to understand - and since there is not a need to
<a class="reference external" href="https://pytorch.org/docs/stable/data.html#dataloader-collate-fn">collate</a> the
data, which makes things a bit easier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>


<span class="k">class</span> <span class="nc">ShakespeareDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">memmap</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span> <span class="p">:</span> <span class="n">idx</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>


<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">ShakespeareDataset</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">train_path</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">)</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">ShakespeareDataset</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">valid_path</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">)</span>

<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="n">composer</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">valid_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">valid_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># Adjust based on your system</span>
    <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span> <span class="k">if</span> <span class="n">composer</span><span class="o">.</span><span class="n">device_type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">else</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>
    <span class="n">pprint</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="k">break</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">[</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span>   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">198</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2061</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8169</span>,     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2471</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">20388</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3792</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">42602</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2636</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">30</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">262</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1502</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">373</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">17687</span><span style="font-weight: bold">]])</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span>  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">198</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2061</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8169</span>,     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2471</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">20388</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">66</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">42602</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2636</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">30</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">262</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1502</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">373</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">17687</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">13</span><span style="font-weight: bold">]])</span>
<span style="font-weight: bold">]</span>
</pre>
</div></div>
</div>
<p>So in the <code class="docutils literal notranslate"><span class="pre">__len__</span></code> method, we need to return the length of the dataset. Let’s
say there are 100 tokens in the dataset with a context window of 10. Then, we
need to return 90 (100 - 10) for the <code class="docutils literal notranslate"><span class="pre">__len__</span></code> method. This means we can have a
possible 90 sequences of 10 tokens each. Again, this is because if any token
index after 90 would result in a sequence that cannot be formed (out of bounds).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_dataset</span><span class="o">.</span><span class="fm">__len__</span><span class="p">(),</span> <span class="n">valid_dataset</span><span class="o">.</span><span class="fm">__len__</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(301958, 36051)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="generative-pre-trained-transformer-gpt">
<h2><a class="toc-backref" href="#id57" role="doc-backlink">Generative Pre-trained Transformer (GPT)</a><a class="headerlink" href="#generative-pre-trained-transformer-gpt" title="Link to this heading">#</a></h2>
<p>The GPT-2 architecture is a <strong><em>transformer</em></strong>-based model, and as the name
suggests, it is a continuation of the GPT-1 model with some minor modifications.</p>
<p>GPT-2 utilizes a <strong>Transformer</strong> architecture <span id="id5">[<a class="reference internal" href="../../bibliography.html#id19" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017. arXiv:1706.03762.">Vaswani <em>et al.</em>, 2017</a>]</span> as
its backbone, which is distinguished by <strong><em>self-attention mechanisms</em></strong>. In
short, we switched from bi-directional cross attention to uni-directional
self-attention.</p>
<figure class="align-default" id="sebastian-decoder">
<img alt="../../_images/sebastian-decoder.jpeg" src="../../_images/sebastian-decoder.jpeg" />
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">GPT Architecture. Image Credit: Build a Large Language Model (From Scratch) by Sebastian Raschka</span><a class="headerlink" href="#sebastian-decoder" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="modifications-from-gpt-1-and-model-stability">
<h3><a class="toc-backref" href="#id58" role="doc-backlink">Modifications from GPT-1 and Model Stability</a><a class="headerlink" href="#modifications-from-gpt-1-and-model-stability" title="Link to this heading">#</a></h3>
<p>Modifications from GPT-1 include:</p>
<ul>
<li><p><strong>Layer normalization</strong> is repositioned to the <strong><em>input</em></strong> of each
sub-block, mirroring a <strong><em>pre-activation residual network</em></strong>. This
modification is believed to offer training stability and model performance.
By normalizing the inputs to each sub-block, it is conjectured to alleviate
issues tied to <strong><em>internal covariate shift</em></strong>, thus aiding in smoother and
potentially faster training.</p></li>
<li><p>GPT-2 introduces an <strong><em>additional layer normalization step</em></strong> that is
executed <strong><em>after the final self-attention block</em></strong> within the model. This
additional normalization step can help ensure that the outputs of the
transformer layers are normalized before being passed to subsequent layers
or used in further processing, further contributing to model stability.</p></li>
<li><p>The GPT-2 paper introduces a modification to the standard weight
initialization for the model’s residual layers. Specifically, the weights
are scaled by a factor of <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{N_{\text{decoder_blocks}}}}\)</span>,
where <span class="math notranslate nohighlight">\(N_{\text{decoder_blocks}}\)</span> represents the number of blocks (or
layers) in the Transformer’s decoder.</p>
<p>The rationale, as quoted from the paper: <em>“A modified initialization which
accounts for the accumulation on the residual path with model depth is
used”</em> <span id="id6">[<a class="reference internal" href="../../bibliography.html#id17" title="Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.">Radford <em>et al.</em>, 2019</a>]</span>, is to ensure that the variance of the
input to the block is the same as the variance of the block’s output. This
is to ensure that the signal is neither amplified nor diminished as it
passes through the block. As the model depth increases, the activations get
added/acculumated, and hence the scaling factor is
<span class="math notranslate nohighlight">\(\frac{1}{\sqrt{N_{\text{decoder_blocks}}}}\)</span>, to scale it down.</p>
</li>
<li><p>Clearly, we can see the empahsis on model stability. In training large
language models, <strong>numerical stability</strong> is paramount; the cost of training
is significantly high, with every loss and gradient spike that fails to
recover necessitating a return to a previous checkpoint, resulting in
substantial GPU hours and potentially tens of thousands of dollars wasted.</p></li>
<li><p>The model’s <strong>vocabulary</strong> is expanded to 50,257 tokens.</p></li>
<li><p>The context window size is increased from 512 to 1024 tokens, enhancing the
model’s ability to maintain coherence over longer text spans.</p></li>
<li><p>A larger batch size of 512, GPT-2 benefits from more stable and effective
gradient estimates during training, contributing to improved learning
outcomes.</p></li>
<li><p>The GPT-2 paper introduces a modification to the standard weight
initialization for the model’s residual layers. Specifically, the weights
are scaled by a factor of <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{N_{\text{decoder_blocks}}}}\)</span>,
where <span class="math notranslate nohighlight">\(N_{\text{decoder_blocks}}\)</span> represents the number of blocks (or
layers) in the Transformer’s decoder.</p>
<p>The rationale, as quoted from the paper: <em>“A modified initialization which
accounts for the accumulation on the residual path with model depth is
used”</em> <span id="id7">[<a class="reference internal" href="../../bibliography.html#id17" title="Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.">Radford <em>et al.</em>, 2019</a>]</span>, is to ensure that the variance of the
input to the block is the same as the variance of the block’s output. This
is to ensure that the signal is neither amplified nor diminished as it
passes through the block. As the model depth increases, the activations get
added/acculumated, and hence the scaling factor is
<span class="math notranslate nohighlight">\(\frac{1}{\sqrt{N_{\text{decoder_blocks}}}}\)</span>, to scale it down.</p>
<ul>
<li><p>In practice, seeing how Karpathy implemented it, it seems that the
scalings are implemented on the <em>projection</em> layers of the
MultiHeadAttention and PositionwiseFFN layers, as seen below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># apply special scaled init to the residual projections, per GPT-2 paper</span>
<span class="k">for</span> <span class="n">pn</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">pn</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;c_proj.weight&#39;</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">n_layer</span><span class="p">))</span>
</pre></div>
</div>
</li>
<li><p>My guess is that the projection layers in both MultiHeadAttention and
PositionwiseFFN are critical junctures where the model’s representations
are linearly transformed. These layers significantly influence the
model’s ability to learn and propagate signals effectively through its
depth. Scaling the weights of these projection layers helps to control
the rate at which information (and error gradients) is dispersed
throughout the network, directly affecting learning stability and
efficiency.</p></li>
</ul>
</li>
<li><p>I did not implement the custom scaling and just went ahead with default
weight scaling:</p>
<ul>
<li><p>Weights initialization for the decoder:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">normal_init_modules</span> <span class="o">=</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">normal_init_modules</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Weights initialization for the context projection and the context fully
connected layers are done using Xavier Uniform initialization.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Initialize parameters of the linear layers.&quot;&quot;&quot;</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">[</span><span class="s2">&quot;context_fc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">[</span><span class="s2">&quot;context_fc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">[</span><span class="s2">&quot;context_fc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">[</span><span class="s2">&quot;context_projection&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">[</span><span class="s2">&quot;context_projection&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">[</span><span class="s2">&quot;context_projection&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</section>
<section id="gpt-2-variants">
<h3><a class="toc-backref" href="#id59" role="doc-backlink">GPT-2 Variants</a><a class="headerlink" href="#gpt-2-variants" title="Link to this heading">#</a></h3>
<p>To this end, we encapsulate some key parameters in
<a class="reference internal" href="#decoder-concept-gpt-2-family-duplicate"><span class="std std-numref">Table 4</span></a> below, which provides
specifications for several GPT-2 variants, distinguished by their scale.</p>
<div class="pst-scrollable-table-container"><table class="table" id="decoder-concept-gpt-2-family-duplicate">
<caption><span class="caption-number">Table 4 </span><span class="caption-text">GPT-2 Family</span><a class="headerlink" href="#decoder-concept-gpt-2-family-duplicate" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Parameters</p></th>
<th class="head"><p>Layers</p></th>
<th class="head"><p>d_model</p></th>
<th class="head"><p>H</p></th>
<th class="head"><p>d_ff</p></th>
<th class="head"><p>Activation</p></th>
<th class="head"><p>Vocabulary Size</p></th>
<th class="head"><p>Context Window</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>117M</p></td>
<td><p>12</p></td>
<td><p>768</p></td>
<td><p>12</p></td>
<td><p>3072</p></td>
<td><p>GELU</p></td>
<td><p>50,257</p></td>
<td><p>1024</p></td>
</tr>
<tr class="row-odd"><td><p>345M</p></td>
<td><p>24</p></td>
<td><p>1024</p></td>
<td><p>16</p></td>
<td><p>4096</p></td>
<td><p>GELU</p></td>
<td><p>50,257</p></td>
<td><p>1024</p></td>
</tr>
<tr class="row-even"><td><p>762M</p></td>
<td><p>36</p></td>
<td><p>1280</p></td>
<td><p>20</p></td>
<td><p>5120</p></td>
<td><p>GELU</p></td>
<td><p>50,257</p></td>
<td><p>1024</p></td>
</tr>
<tr class="row-odd"><td><p>1542M</p></td>
<td><p>48</p></td>
<td><p>1600</p></td>
<td><p>25</p></td>
<td><p>6400</p></td>
<td><p>GELU</p></td>
<td><p>50,257</p></td>
<td><p>1024</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="gpt-2-model-architecture-huggingface">
<h3><a class="toc-backref" href="#id60" role="doc-backlink">GPT-2 Model Architecture (HuggingFace)</a><a class="headerlink" href="#gpt-2-model-architecture-huggingface" title="Link to this heading">#</a></h3>
<p>The below is without the head/softmax layer, from HuggingFace.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GPT2Model</span><span class="p">,</span> <span class="n">GPT2LMHeadModel</span> <span class="c1"># type: ignore[import-untyped]</span>
<span class="kn">from</span> <span class="nn">torchinfo</span> <span class="kn">import</span> <span class="n">summary</span>

<span class="n">gpt</span> <span class="o">=</span> <span class="n">GPT2Model</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">summary</span><span class="p">(</span><span class="n">gpt</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gpt</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.9.20/x64/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "236d1c2df8b84ed59a2b3464790b05e8"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "ed726384b6ba493881813bdbf04790fe"}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
GPT2Model                                     --
├─Embedding: 1-1                              38,597,376
├─Embedding: 1-2                              786,432
├─Dropout: 1-3                                --
├─ModuleList: 1-4                             --
│    └─GPT2Block: 2-1                         --
│    │    └─LayerNorm: 3-1                    1,536
│    │    └─GPT2Attention: 3-2                2,362,368
│    │    └─LayerNorm: 3-3                    1,536
│    │    └─GPT2MLP: 3-4                      4,722,432
│    └─GPT2Block: 2-2                         --
│    │    └─LayerNorm: 3-5                    1,536
│    │    └─GPT2Attention: 3-6                2,362,368
│    │    └─LayerNorm: 3-7                    1,536
│    │    └─GPT2MLP: 3-8                      4,722,432
│    └─GPT2Block: 2-3                         --
│    │    └─LayerNorm: 3-9                    1,536
│    │    └─GPT2Attention: 3-10               2,362,368
│    │    └─LayerNorm: 3-11                   1,536
│    │    └─GPT2MLP: 3-12                     4,722,432
│    └─GPT2Block: 2-4                         --
│    │    └─LayerNorm: 3-13                   1,536
│    │    └─GPT2Attention: 3-14               2,362,368
│    │    └─LayerNorm: 3-15                   1,536
│    │    └─GPT2MLP: 3-16                     4,722,432
│    └─GPT2Block: 2-5                         --
│    │    └─LayerNorm: 3-17                   1,536
│    │    └─GPT2Attention: 3-18               2,362,368
│    │    └─LayerNorm: 3-19                   1,536
│    │    └─GPT2MLP: 3-20                     4,722,432
│    └─GPT2Block: 2-6                         --
│    │    └─LayerNorm: 3-21                   1,536
│    │    └─GPT2Attention: 3-22               2,362,368
│    │    └─LayerNorm: 3-23                   1,536
│    │    └─GPT2MLP: 3-24                     4,722,432
│    └─GPT2Block: 2-7                         --
│    │    └─LayerNorm: 3-25                   1,536
│    │    └─GPT2Attention: 3-26               2,362,368
│    │    └─LayerNorm: 3-27                   1,536
│    │    └─GPT2MLP: 3-28                     4,722,432
│    └─GPT2Block: 2-8                         --
│    │    └─LayerNorm: 3-29                   1,536
│    │    └─GPT2Attention: 3-30               2,362,368
│    │    └─LayerNorm: 3-31                   1,536
│    │    └─GPT2MLP: 3-32                     4,722,432
│    └─GPT2Block: 2-9                         --
│    │    └─LayerNorm: 3-33                   1,536
│    │    └─GPT2Attention: 3-34               2,362,368
│    │    └─LayerNorm: 3-35                   1,536
│    │    └─GPT2MLP: 3-36                     4,722,432
│    └─GPT2Block: 2-10                        --
│    │    └─LayerNorm: 3-37                   1,536
│    │    └─GPT2Attention: 3-38               2,362,368
│    │    └─LayerNorm: 3-39                   1,536
│    │    └─GPT2MLP: 3-40                     4,722,432
│    └─GPT2Block: 2-11                        --
│    │    └─LayerNorm: 3-41                   1,536
│    │    └─GPT2Attention: 3-42               2,362,368
│    │    └─LayerNorm: 3-43                   1,536
│    │    └─GPT2MLP: 3-44                     4,722,432
│    └─GPT2Block: 2-12                        --
│    │    └─LayerNorm: 3-45                   1,536
│    │    └─GPT2Attention: 3-46               2,362,368
│    │    └─LayerNorm: 3-47                   1,536
│    │    └─GPT2MLP: 3-48                     4,722,432
├─LayerNorm: 1-5                              1,536
======================================================================
Total params: 124,439,808
Trainable params: 124,439,808
Non-trainable params: 0
======================================================================
GPT2Config {
  &quot;_name_or_path&quot;: &quot;gpt2&quot;,
  &quot;activation_function&quot;: &quot;gelu_new&quot;,
  &quot;architectures&quot;: [
    &quot;GPT2LMHeadModel&quot;
  ],
  &quot;attn_pdrop&quot;: 0.1,
  &quot;bos_token_id&quot;: 50256,
  &quot;embd_pdrop&quot;: 0.1,
  &quot;eos_token_id&quot;: 50256,
  &quot;initializer_range&quot;: 0.02,
  &quot;layer_norm_epsilon&quot;: 1e-05,
  &quot;model_type&quot;: &quot;gpt2&quot;,
  &quot;n_ctx&quot;: 1024,
  &quot;n_embd&quot;: 768,
  &quot;n_head&quot;: 12,
  &quot;n_inner&quot;: null,
  &quot;n_layer&quot;: 12,
  &quot;n_positions&quot;: 1024,
  &quot;reorder_and_upcast_attn&quot;: false,
  &quot;resid_pdrop&quot;: 0.1,
  &quot;scale_attn_by_inverse_layer_idx&quot;: false,
  &quot;scale_attn_weights&quot;: true,
  &quot;summary_activation&quot;: null,
  &quot;summary_first_dropout&quot;: 0.1,
  &quot;summary_proj_to_labels&quot;: true,
  &quot;summary_type&quot;: &quot;cls_index&quot;,
  &quot;summary_use_proj&quot;: true,
  &quot;task_specific_params&quot;: {
    &quot;text-generation&quot;: {
      &quot;do_sample&quot;: true,
      &quot;max_length&quot;: 50
    }
  },
  &quot;transformers_version&quot;: &quot;4.38.1&quot;,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 50257
}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># gpt_with_head = GPT2LMHeadModel.from_pretrained(&#39;gpt2&#39;)</span>
<span class="c1"># print(summary(gpt_with_head))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># gpt_medium = GPT2Model.from_pretrained(&#39;gpt2-medium&#39;)</span>
<span class="c1"># print(gpt_medium.config)</span>

<span class="c1"># gpt_large = GPT2Model.from_pretrained(&#39;gpt2-large&#39;)</span>
<span class="c1"># print(gpt_large.config)</span>

<span class="c1"># gpt_xl = GPT2Model.from_pretrained(&#39;gpt2-xl&#39;)</span>
<span class="c1"># print(gpt_xl.config)</span>
</pre></div>
</div>
</div>
</div>
<p>Notice that the config does not show the dimension of the feedforward network.
In GPT-2 source code, we can see what the dimension of the feedforward network
is. It is defined as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inner_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">n_inner</span> <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">n_inner</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">hidden_size</span>
</pre></div>
</div>
<p>This is why you do not see it in the <code class="docutils literal notranslate"><span class="pre">config</span></code> object as if not set, then it is
simply set to <code class="docutils literal notranslate"><span class="pre">4</span> <span class="pre">*</span> <span class="pre">hidden_size</span></code>.</p>
</section>
</section>
<section id="token-embeddings">
<h2><a class="toc-backref" href="#id61" role="doc-backlink">Token Embeddings</a><a class="headerlink" href="#token-embeddings" title="Link to this heading">#</a></h2>
<p>First, we will look at the first sequence, given by
<code class="docutils literal notranslate"><span class="pre">'</span> <span class="pre">priest</span> <span class="pre">and</span> <span class="pre">clerk?</span> <span class="pre">well</span> <span class="pre">then,</span> <span class="pre">amen'</span></code>, which we have already mapped to its
corresponding token IDs.</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x} = \begin{bmatrix} 11503 &amp; 290 &amp; 21120 &amp; 30 &amp; 880 &amp; 788 &amp; 11 &amp; 29448 \end{bmatrix} \in \mathbb{Z}^{1 \times 8}
\]</div>
<p>The shape is <span class="math notranslate nohighlight">\(1 \times 8\)</span>, which is a single sequence of <span class="math notranslate nohighlight">\(8\)</span> tokens. And in this
case, we have each word/punctuation mapped to a unique token ID, as seen below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Token ID: </span><span class="si">{</span><span class="n">token</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">, Token: </span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">token</span><span class="o">.</span><span class="n">item</span><span class="p">()])</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Token ID: 11503, Token:  priest
Token ID: 290, Token:  and
Token ID: 21120, Token:  clerk
Token ID: 30, Token: ?
Token ID: 880, Token:  well
Token ID: 788, Token:  then
Token ID: 11, Token: ,
Token ID: 29448, Token:  amen
</pre></div>
</div>
</div>
</div>
<p>Next, we need to map each token to a vector (embeddings) in a high-dimensional
space.</p>
<p>The integer tokens, by themselves, do not carry much information. For example,
the word <code class="docutils literal notranslate"><span class="pre">priest</span></code> is tokenized to be <code class="docutils literal notranslate"><span class="pre">11503</span></code>, which is an arbitrary integer. In
a one-dimensional Euclidean space, the word <code class="docutils literal notranslate"><span class="pre">priest</span></code> and the next word <code class="docutils literal notranslate"><span class="pre">and</span></code>,
indexed by <code class="docutils literal notranslate"><span class="pre">290</span></code>, <strong><em>would appear to be very far apart from each other</em></strong>.
However, if we were to change a tokenizer, and somehow the word <code class="docutils literal notranslate"><span class="pre">priest</span></code> is now
tokenized to be <code class="docutils literal notranslate"><span class="pre">291</span></code>, then the words <code class="docutils literal notranslate"><span class="pre">priest</span></code> and <code class="docutils literal notranslate"><span class="pre">and</span></code> <strong><em>would appear to be
very near to each other</em></strong>.</p>
<p>This means that the model could potentially learn the relationship of two tokens
based solely on their tokenized integers. To address this, we use <strong>embedding
vectors</strong>. While the initial mapping from words to vectors is dependent on the
tokenizer and may be <em>arbitrary</em>, during training, the model adjusts these
vectors so that words used in similar contexts come to have similar vectors.
This allows the model to capture <strong>semantic</strong> relationships between words - and
by extension, allows the model to capture relationships between tokens better.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x0</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">x0</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #008000; text-decoration-color: #008000">' priest and clerk? well then, amen'</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11503</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">290</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">21120</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">30</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">880</span>,   <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">788</span>,    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">11</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">29448</span><span style="font-weight: bold">])</span>
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

<span class="n">tok_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
<span class="n">x0_tok_embed</span> <span class="o">=</span> <span class="n">tok_embed</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">x0_tok_embed</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">x0_tok_embed</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.0213</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3146</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2616</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3730</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5715</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1229</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.8145</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4164</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4973</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.1740</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.6713</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1102</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.3167</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2943</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9573</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2935</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0623</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1054</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.8182</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.4184</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4016</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3422</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.9704</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2435</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0576</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0596</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2764</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2403</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.2707</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.5865</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4099</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.3797</span><span style="font-weight: bold">]]</span>, <span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">EmbeddingBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span><span style="font-weight: bold">])</span>
</pre>
</div></div>
</div>
<p>Now we see that for the sequence
<code class="docutils literal notranslate"><span class="pre">x0</span> <span class="pre">=</span> <span class="pre">'</span> <span class="pre">priest</span> <span class="pre">and</span> <span class="pre">clerk?</span> <span class="pre">well</span> <span class="pre">then,</span> <span class="pre">amen'</span> <span class="pre">=</span> <span class="pre">[</span> <span class="pre">11503,</span> <span class="pre">290,</span> <span class="pre">21120,</span> <span class="pre">30,</span> <span class="pre">880,</span> <span class="pre">788,</span> <span class="pre">11,</span> <span class="pre">29448]</span></code>,
we have the following token embeddings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="o">-</span><span class="mf">1.0213</span><span class="p">,</span>  <span class="mf">0.3146</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2616</span><span class="p">,</span>  <span class="mf">0.3730</span><span class="p">],</span>
 <span class="p">[</span> <span class="mf">0.5715</span><span class="p">,</span>  <span class="mf">0.1229</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8145</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4164</span><span class="p">],</span>
 <span class="p">[</span> <span class="mf">0.4973</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1740</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6713</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1102</span><span class="p">],</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">2.3167</span><span class="p">,</span>  <span class="mf">0.2943</span><span class="p">,</span>  <span class="mf">0.9573</span><span class="p">,</span>  <span class="mf">0.2935</span><span class="p">],</span>
 <span class="p">[</span> <span class="mf">0.0623</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1054</span><span class="p">,</span>  <span class="mf">0.8182</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.4184</span><span class="p">],</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">1.4016</span><span class="p">,</span>  <span class="mf">0.3422</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9704</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2435</span><span class="p">],</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.0576</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0596</span><span class="p">,</span>  <span class="mf">0.2764</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2403</span><span class="p">],</span>
 <span class="p">[</span> <span class="mf">1.2707</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5865</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4099</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3797</span><span class="p">]]</span>
</pre></div>
</div>
<p>Notice that for each token, the embedding vector is a <span class="math notranslate nohighlight">\(4\)</span>-dimensional vector.
This is because we have set the embedding dimension to be <span class="math notranslate nohighlight">\(4\)</span>, which is a
hyperparameter that we can set. In the case of GPT-2, the embedding dimension is
<span class="math notranslate nohighlight">\(768\)</span>.</p>
<section id="implementation">
<h3><a class="toc-backref" href="#id62" role="doc-backlink">Implementation</a><a class="headerlink" href="#implementation" title="Link to this heading">#</a></h3>
<p>Before we explain further, we will first implement the token embedding layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TokenEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="output-of-the-embedding-layer">
<h3><a class="toc-backref" href="#id63" role="doc-backlink">Output Of The Embedding Layer</a><a class="headerlink" href="#output-of-the-embedding-layer" title="Link to this heading">#</a></h3>
<p>Typically, the token embeddings are learned during training, and the learned
embeddings are used to represent the tokens in the input sequence.</p>
<ol class="arabic simple">
<li><p>We first unsqueeze the input tensor <code class="docutils literal notranslate"><span class="pre">x0</span></code> to add a batch dimension, resulting
in a shape of <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">8]</span></code> where <code class="docutils literal notranslate"><span class="pre">1</span></code> is the batch size <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> and <code class="docutils literal notranslate"><span class="pre">8</span></code> is
the sequence length <span class="math notranslate nohighlight">\(T\)</span>.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">tok_embed</span></code> layer is then applied to the input tensor, resulting in a
tensor of shape <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">8,</span> <span class="pre">4]</span></code>.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">z0_tok_embed</span></code> is the token embedding tensor, which is the transformed
input tensor <code class="docutils literal notranslate"><span class="pre">x0</span></code>. Here our <code class="docutils literal notranslate"><span class="pre">x0</span></code> was transformed from a sequence of
tokens to a sequence of token embeddings.</p></li>
<li><p>There is a weight matrix <code class="docutils literal notranslate"><span class="pre">W_e</span></code> that is <code class="docutils literal notranslate"><span class="pre">[V,</span> <span class="pre">D]</span></code> that transforms the input
tensor into the token embedding tensor via a matrix multiplication
<code class="docutils literal notranslate"><span class="pre">z0_tok_embed</span> <span class="pre">=</span> <span class="pre">x0_ohe</span> <span class="pre">&#64;</span> <span class="pre">W_e</span></code> (which we will see shortly).</p></li>
</ol>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">generator</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

<span class="n">tok_embed</span> <span class="o">=</span> <span class="n">TokenEmbedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">x0</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">if</span> <span class="n">x0</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">x0</span> <span class="c1"># [T] -&gt; [B, T]</span>
<span class="k">assert</span> <span class="n">x0</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">)</span> <span class="c1"># [B, T] = [1, 8]</span>

<span class="n">z0_tok_embed</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">tok_embed</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">z0_tok_embed</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span> <span class="c1"># [B, T, D] = [1, 8, 4]</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">z0_tok_embed</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">z0_tok_embed</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.0213</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3146</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2616</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3730</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5715</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1229</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.8145</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4164</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4973</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.1740</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.6713</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1102</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.3167</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2943</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9573</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2935</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0623</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1054</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.8182</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.4184</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4016</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3422</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.9704</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2435</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0576</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0596</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2764</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2403</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.2707</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.5865</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4099</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.3797</span><span style="font-weight: bold">]]]</span>, <span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">EmbeddingBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span><span style="font-weight: bold">])</span>
</pre>
</div></div>
</div>
<p>So this operation above is essentially a lookup operation, where we look up the
embedding vector for each token in the sequence. This is done by <code class="docutils literal notranslate"><span class="pre">tok_embed(x)</span></code>.
We run it against the first sequence for simplicity, and <code class="docutils literal notranslate"><span class="pre">z0_tok_embed</span></code> is the
resulting tensor, with a shape of <span class="math notranslate nohighlight">\(T \times D\)</span>. In our case, the sequence length
(block size) is <span class="math notranslate nohighlight">\(T = 8\)</span>, and the embedding dimension is <span class="math notranslate nohighlight">\(D = 4\)</span>. This means that
we have essentially mapped each of the <span class="math notranslate nohighlight">\(8\)</span> tokens representing
<code class="docutils literal notranslate"><span class="pre">priest</span> <span class="pre">and</span> <span class="pre">clerk?</span> <span class="pre">well</span> <span class="pre">then,</span> <span class="pre">amen</span></code> to a <span class="math notranslate nohighlight">\(4\)</span>-dimensional vector.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">priest</span></code> is mapped to <code class="docutils literal notranslate"><span class="pre">[-1.0213,</span> <span class="pre">0.3146,</span> <span class="pre">-0.2616,</span> <span class="pre">0.3730]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">and</span></code> is mapped to <code class="docutils literal notranslate"><span class="pre">[</span> <span class="pre">0.5715,</span> <span class="pre">0.1229,</span> <span class="pre">-0.8145,</span> <span class="pre">-1.4164]</span></code></p></li>
<li><p>…</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">amen</span></code> is mapped to <code class="docutils literal notranslate"><span class="pre">[</span> <span class="pre">1.2707,</span> <span class="pre">-0.5865,</span> <span class="pre">-1.4099,</span> <span class="pre">-1.3797]</span></code></p></li>
</ul>
<p>With each token being a vector, not only does the token carry more information,
it is also much easier to do linear algebra operations on the tokens. For
example, we can easily calculate the mean/sum of the embeddings for pooling, or
we can easily calculate the dot product between two tokens to measure their
similarity in a high-dimensional space (as compared to it being an integer with
only 1 dimension).</p>
<p>Furthermore, the embeddings are learned during training, and the model would try
to capture semantic relationships between tokens. For example, the model would
try to learn that <code class="docutils literal notranslate"><span class="pre">priest</span></code> and <code class="docutils literal notranslate"><span class="pre">clerk</span></code> are related in some way because they
refer to people, and <code class="docutils literal notranslate"><span class="pre">amen</span></code> is related to <code class="docutils literal notranslate"><span class="pre">priest</span></code> because it is often used in
religious contexts.</p>
<p>To this end, we denote the output of the token embedding layer as <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.
In what follows, we will see beneath how the token embedding layer is computed.</p>
</section>
<section id="one-hot-representation-of-input-sequence-mathbf-x">
<h3><a class="toc-backref" href="#id64" role="doc-backlink">One-Hot Representation of Input Sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span></a><a class="headerlink" href="#one-hot-representation-of-input-sequence-mathbf-x" title="Link to this heading">#</a></h3>
<p>First, we need to understand how the input sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> can be
represented as a one-hot encoded matrix.</p>
<p>The one-hot representation of the input sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is denoted as
<span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span>. This representation converts each token in the
sequence to a one-hot encoded vector, where each vector has a length equal to
the size of the vocabulary <span class="math notranslate nohighlight">\(V\)</span>.</p>
<section id="definition">
<h4><a class="toc-backref" href="#id65" role="doc-backlink">Definition</a><a class="headerlink" href="#definition" title="Link to this heading">#</a></h4>
<p>The one-hot encoded matrix <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X}^{\text{ohe}} = \begin{bmatrix}
o_{1,1} &amp; o_{1,2} &amp; \cdots &amp; o_{1,V} \\
o_{2,1} &amp; o_{2,2} &amp; \cdots &amp; o_{2,V} \\
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
o_{T,1} &amp; o_{T,2} &amp; \cdots &amp; o_{T,V}
\end{bmatrix} \in \{0, 1\}^{T \times V}
\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(T\)</span>: Total length of the sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(V\)</span>: Size of the vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(o_{t,j}\)</span>: Element of the one-hot encoded matrix <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span>
at row <span class="math notranslate nohighlight">\(t\)</span> and column <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
</ul>
<p>In addition, we have:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> is a <span class="math notranslate nohighlight">\(T \times V\)</span> matrix.</p></li>
<li><p>Elements of <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> are binary, i.e., they belong to
<span class="math notranslate nohighlight">\(\{0, 1\}\)</span>.</p></li>
<li><p>The row vector <span class="math notranslate nohighlight">\(\mathbf{o}_{t, :}\)</span> represents the one-hot encoded vector for
the token at position <span class="math notranslate nohighlight">\(t\)</span> in the sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p></li>
</ul>
</section>
<section id="one-hot-encoding-process">
<h4><a class="toc-backref" href="#id66" role="doc-backlink">One-Hot Encoding Process</a><a class="headerlink" href="#one-hot-encoding-process" title="Link to this heading">#</a></h4>
<p>For each token <span class="math notranslate nohighlight">\(x_t\)</span> at position <span class="math notranslate nohighlight">\(t\)</span> in the sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>
(<span class="math notranslate nohighlight">\(1 \leq t \leq T\)</span>), the corresponding row vector <span class="math notranslate nohighlight">\(\mathbf{o}_{t, :}\)</span> in
<span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{o}_{t, j} = \begin{cases}
1 &amp; \text{if } f_{\text{stoi}}(x_t) = j-1\\
0 &amp; \text{otherwise}
\end{cases}
\end{split}\]</div>
<p>for <span class="math notranslate nohighlight">\(j = 1, 2, \ldots, V\)</span>.</p>
<p>Here, <span class="math notranslate nohighlight">\(f_{\text{stoi}}(x_t)\)</span> maps the token <span class="math notranslate nohighlight">\(x_t\)</span> to its index <span class="math notranslate nohighlight">\(j-1\)</span> in the
vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>, the <span class="math notranslate nohighlight">\(j-1\)</span> is because zero-based indexing used in
python (where <span class="math notranslate nohighlight">\(0 \leq j-1 &lt; V\)</span>). Each row <span class="math notranslate nohighlight">\(\mathbf{o}_{t, :}\)</span> in
<span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> contains a single ‘1’ at the column <span class="math notranslate nohighlight">\(j\)</span> corresponding
to the vocabulary index of <span class="math notranslate nohighlight">\(x_t\)</span>, and ‘0’s elsewhere.</p>
<div class="proof example admonition" id="gpt-notations-one-hot-example">
<p class="admonition-title"><span class="caption-number">Example 2 </span> (Example)</p>
<section class="example-content" id="proof-content">
<p>For example, if the vocabulary
<span class="math notranslate nohighlight">\(\mathcal{V} = \{\text{cat}, \text{dog}, \text{mouse}\}\)</span> and the sequence
<span class="math notranslate nohighlight">\(\mathbf{x} = (\text{mouse}, \text{dog})\)</span>, then the one-hot encoded matrix
<span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> will be:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X}^{\text{ohe}} = \begin{bmatrix}
0 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{bmatrix} \in \{0, 1\}^{2 \times 3}
\end{split}\]</div>
<p>In this example:</p>
<ul class="simple">
<li><p>The sequence length <span class="math notranslate nohighlight">\(T = 2\)</span>.</p></li>
<li><p>The vocabulary size <span class="math notranslate nohighlight">\(V = 3\)</span>.</p></li>
<li><p>“mouse” corresponds to the third position in the vocabulary, and “dog” to
the second, which is seen in their respective one-hot vectors.</p></li>
</ul>
</section>
</div><p>We write the one hot encoding proces for the input sequence <code class="docutils literal notranslate"><span class="pre">x0</span></code> as follows
in python.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x0_ohe</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">x0_ohe</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>  <span class="c1"># [B, T, V] = [1, 8, 50257]</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">token_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x0</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()):</span>
    <span class="k">assert</span> <span class="n">x0_ohe</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">token_id</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="mf">1.0</span>  <span class="c1"># check if the one-hot encoding is correct</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="embedding-layer-is-matmul-of-one-hot-encoded-input-matrix-and-embedding-matrix-weights">
<h3><a class="toc-backref" href="#id67" role="doc-backlink">Embedding Layer Is Matmul Of One-Hot Encoded Input Matrix And Embedding Matrix Weights</a><a class="headerlink" href="#embedding-layer-is-matmul-of-one-hot-encoded-input-matrix-and-embedding-matrix-weights" title="Link to this heading">#</a></h3>
<p>Once the one hot encoding representation <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> is well
defined, we can then pass it as input through our GPT model, in which the first
layer is a embedding lookup table. In the GPT model architecture, the first
layer typically involves mapping the one-hot encoded input vectors into a
lower-dimensional, dense embedding space using the embedding matrix
<span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span>.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Matrix Description</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Dimensions</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>One-Hot Encoded Input Matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times V\)</span></p></td>
<td><p>Each row corresponds to a one-hot encoded vector representing a token in the sequence.</p></td>
</tr>
<tr class="row-odd"><td><p>Embedding Matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(V \times D\)</span></p></td>
<td><p>Each row is the embedding vector of the corresponding token in the vocabulary.</p></td>
</tr>
<tr class="row-even"><td><p>Embedded Input Matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times D\)</span></p></td>
<td><p>Each row is the embedding vector of the corresponding token in the input sequence.</p></td>
</tr>
<tr class="row-odd"><td><p>Embedding Vector for Token <span class="math notranslate nohighlight">\(t\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}_t\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1 \times D\)</span></p></td>
<td><p>The embedding vector for the token at position <span class="math notranslate nohighlight">\(t\)</span> in the input sequence.</p></td>
</tr>
<tr class="row-even"><td><p>Batched Input Tensor</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}^{\mathcal{B}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(B \times T \times D\)</span></p></td>
<td><p>A batched tensor containing <span class="math notranslate nohighlight">\(B\)</span> input sequences, each sequence is of shape <span class="math notranslate nohighlight">\(T \times D\)</span>.</p></td>
</tr>
</tbody>
</table>
</div>
<p>More concretely, we create an embedding matrix <span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span> of size
<span class="math notranslate nohighlight">\(V \times D\)</span>, where <span class="math notranslate nohighlight">\(V\)</span> is the vocabulary size, <span class="math notranslate nohighlight">\(D\)</span> is the dimensions of the
embeddings, we would then matrix multiply <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> with
<span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span> to get the output tensor <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X} = \mathbf{X}^{\text{ohe}} \cdot \mathbf{W}_{e}
\]</div>
<p>Indeed, we see that the result of <code class="docutils literal notranslate"><span class="pre">tok_embed(x)</span></code> is the same as the result of
<code class="docutils literal notranslate"><span class="pre">x0_ohe</span> <span class="pre">&#64;</span> <span class="pre">W_e</span></code>. In other words, you can one hot encoded the input
sequence <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, x_2, \ldots, x_T)\)</span> and then matrix multiply it with
the embedding matrix <span class="math notranslate nohighlight">\(\mathbf{W}^{e}\)</span> (via a linear layer) to get the same
result as <code class="docutils literal notranslate"><span class="pre">tok_embed(x)</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">W_e</span> <span class="o">=</span> <span class="n">tok_embed</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="c1"># [V, D]</span>
<span class="n">x0_ohe</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="c1"># [B, T, V]</span>
<span class="n">z0_tok_embed_matmul</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">x0_ohe</span> <span class="o">@</span> <span class="n">W_e</span> <span class="c1"># [B, T, D]</span>
<span class="k">assert</span> <span class="n">z0_tok_embed_matmul</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span><span class="n">z0_tok_embed</span><span class="p">,</span> <span class="n">z0_tok_embed_matmul</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">msg</span><span class="o">=</span><span class="s2">&quot;The matrix multiplication is not correct.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Recall our tokenized sequence is
<code class="docutils literal notranslate"><span class="pre">[11503,</span> <span class="pre">290,</span> <span class="pre">21120,</span> <span class="pre">30,</span> <span class="pre">880,</span> <span class="pre">788,</span> <span class="pre">11,</span> <span class="pre">29448]</span></code>.</p></li>
<li><p>Converting it to one-hot encoding, we would have a matrix of size
<code class="docutils literal notranslate"><span class="pre">[8,</span> <span class="pre">50257]</span></code> (or more generally <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">T,</span> <span class="pre">V]</span></code> in the presence of batch size
<code class="docutils literal notranslate"><span class="pre">B</span></code>).</p></li>
<li><p>Each row is a one-hot vector of the token <span class="math notranslate nohighlight">\(x_{t} \in \mathbb{R}^{V}\)</span> at
position <span class="math notranslate nohighlight">\(t\)</span>. For example, the first row would be a one-hot vector of the
token <code class="docutils literal notranslate"><span class="pre">11503</span></code>, so every element in the first row is <span class="math notranslate nohighlight">\(0\)</span> except for the
<span class="math notranslate nohighlight">\(11503\)</span>-th element, which is <span class="math notranslate nohighlight">\(1\)</span>.</p></li>
<li><p>A minute quirk here is that the token <span class="math notranslate nohighlight">\(x_{t}\)</span> exists in the <strong><em>continuous
space</em></strong> instead of the <strong><em>discrete space</em></strong>. This is because we have to
perform the dot product between the one-hot vector and the embedding vector,
which is a continuous vector. This is more of a data type coercion.
Therefore, in our code, we also converted the one-hot vector to <code class="docutils literal notranslate"><span class="pre">.float()</span></code>.</p></li>
</ul>
<section id="id8">
<h4><a class="toc-backref" href="#id68" role="doc-backlink">Definition</a><a class="headerlink" href="#id8" title="Link to this heading">#</a></h4>
<p>The embedding matrix <span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span> is structured as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{W}_e &amp;= \left[\begin{array}{cccc}
w_{1,1} &amp; w_{1,2} &amp; \cdots &amp; w_{1, D} \\
w_{2,1} &amp; w_{2,2} &amp; \cdots &amp; w_{2, D} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{V, 1} &amp; w_{V, 2} &amp; \cdots &amp; w_{V, D}
\end{array}\right] \in \mathbb{R}^{V \times D}
\end{aligned}
\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{w}_j = (w_{j,1}, w_{j,2}, \ldots, w_{j,D}) \in \mathbb{R}^{1 \times D}\)</span>:</p>
<ul>
<li><p>Each row vector <span class="math notranslate nohighlight">\(\mathbf{w}_j\)</span> of the matrix <span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span> represents
the <span class="math notranslate nohighlight">\(D\)</span>-dimensional embedding vector for the <span class="math notranslate nohighlight">\(j\)</span>-th token in the
vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p></li>
<li><p>The subscript <span class="math notranslate nohighlight">\(j\)</span> ranges from 1 to <span class="math notranslate nohighlight">\(V\)</span>, indexing the tokens.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(V\)</span> is the vocabulary size.</p></li>
<li><p><span class="math notranslate nohighlight">\(D\)</span> is the hidden embedding dimension.</p></li>
</ul>
<p>Here is a visual representation of how each embedding vector is selected through
matrix multiplication:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{X}^{\text{ohe}} \cdot \mathbf{W}_{e} &amp;=
\begin{bmatrix}
0 &amp; 1 &amp; \cdots &amp; 0 \\
1 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 1
\end{bmatrix}_{T \times V}
\cdot
\begin{bmatrix}
w_{1,1} &amp; w_{1,2} &amp; \cdots &amp; w_{1,D} \\
w_{2,1} &amp; w_{2,2} &amp; \cdots &amp; w_{2,D} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{V,1} &amp; w_{V,2} &amp; \cdots &amp; w_{V,D}
\end{bmatrix}_{V \times D} \\
&amp;=
\begin{bmatrix}
w_{2,1} &amp; w_{2,2} &amp; \cdots &amp; w_{2,D} \\
w_{1,1} &amp; w_{1,2} &amp; \cdots &amp; w_{1,D} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{T,1} &amp; w_{T,2} &amp; \cdots &amp; w_{T,D}
\end{bmatrix}_{T \times D}
\end{aligned}
\end{split}\]</div>
<p>Each row in the resulting matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is the embedding of the
corresponding token in the input sequence, picked directly from <span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span>
by the one-hot vectors. In other words, the matrix <span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span> can be
visualized as a table where each row corresponds to a token’s embedding vector:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{c|cccc}
\text{Token Index} &amp; \text{Dimension 1} &amp; \text{Dimension 2} &amp; \cdots &amp; \text{Dimension } D \\
\hline
1 &amp; w_{1,1} &amp; w_{1,2} &amp; \cdots &amp; w_{1,D} \\
2 &amp; w_{2,1} &amp; w_{2,2} &amp; \cdots &amp; w_{2,D} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
V &amp; w_{V,1} &amp; w_{V,2} &amp; \cdots &amp; w_{V,D} \\
\end{array}
\end{split}\]</div>
</section>
<section id="lookup">
<h4><a class="toc-backref" href="#id69" role="doc-backlink">Lookup</a><a class="headerlink" href="#lookup" title="Link to this heading">#</a></h4>
<p>When the one-hot encoded input matrix <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> multiplies with
the embedding matrix <span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span>, each row of <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span>
effectively selects a corresponding row from <span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span>. This operation
simplifies to row selection because each row of <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span>
contains exactly one ‘1’ and the rest are ‘0’s.</p>
</section>
<section id="semantic-representation">
<h4><a class="toc-backref" href="#id70" role="doc-backlink">Semantic Representation</a><a class="headerlink" href="#semantic-representation" title="Link to this heading">#</a></h4>
<p>Now each row of the output tensor, indexed by <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(\mathbf{X}_{t, :}\)</span>: is the
<span class="math notranslate nohighlight">\(D\)</span> dimensional embedding vector for the token <span class="math notranslate nohighlight">\(x_t\)</span> at the <span class="math notranslate nohighlight">\(t\)</span>-th position in
the sequence. In this context, each token in the sequence is represented by a
<span class="math notranslate nohighlight">\(D\)</span> dimensional vector. So, the output tensor <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> captures the dense
representation of the sequence. Each token in the sequence is replaced by its
corresponding embedding vector from the embedding matrix <span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span>. As
before, the output tensor <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> carries semantic information about the
tokens in the sequence. The closer two vectors are in this embedding space, the
more semantically similar they are.</p>
</section>
</section>
</section>
<section id="positional-embeddings">
<h2><a class="toc-backref" href="#id71" role="doc-backlink">Positional Embeddings</a><a class="headerlink" href="#positional-embeddings" title="Link to this heading">#</a></h2>
<p>For the lack of a better phrase, we say that self-attention, the core function
of GPTs, is permutation invariant. While it is obvious that the input sequence
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is ordered in the sense that <span class="math notranslate nohighlight">\(x_1\)</span> comes before <span class="math notranslate nohighlight">\(x_2\)</span>, and <span class="math notranslate nohighlight">\(x_2\)</span>
comes before <span class="math notranslate nohighlight">\(x_3\)</span>, and so on, this information gets lost in the self-attention
mechanism. This means that the model does not differentiate “the cat ate the
mouse” from “the mouse ate the cat” as long as the tokens are the same - and
this is not desirable.</p>
<p>The dominant approach for preserving information about the order of tokens is to
represent this to the model as an additional input associated with each token.
These inputs are called positional encodings, and they can either be learned or
fixed <em>a priori</em> <span id="id9">[<a class="reference internal" href="../../bibliography.html#id5" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into Deep Learning. Cambridge University Press, 2023. URL: https://D2L.ai.">Zhang <em>et al.</em>, 2023</a>]</span>. What this means is that we can either
construct a learnable parameter that is updated during training, or we can
construct a fixed parameter that is not updated during training. For the sake of
completeness, we will discuss briefly the scheme where the positional encodings
are fixed <em>a priori</em> based on sinusoidal functions - which is also the scheme
described in the paper “Attention is All You Need” <span id="id10">[<a class="reference internal" href="../../bibliography.html#id19" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017. arXiv:1706.03762.">Vaswani <em>et al.</em>, 2017</a>]</span>.</p>
<section id="id11">
<h3><a class="toc-backref" href="#id72" role="doc-backlink">Definition</a><a class="headerlink" href="#id11" title="Link to this heading">#</a></h3>
<div class="proof definition admonition" id="decoder-positional-encoding">
<p class="admonition-title"><span class="caption-number">Definition 3 </span> (Positional Encoding)</p>
<section class="definition-content" id="proof-content">
<p>The positional encoding function
<span class="math notranslate nohighlight">\(\mathrm{PE}: \mathbb{N} \times \mathbb{N} \rightarrow \mathbb{R}\)</span> computes the
position encoding for each position <span class="math notranslate nohighlight">\(p := t \in \mathbb{N}\)</span> and each dimension
<span class="math notranslate nohighlight">\(d = 1, 2, \ldots, D \in \mathbb{N}\)</span> in the input embedding space as follows <span id="id12">[<a class="reference internal" href="../../bibliography.html#id18" title="Minhyeok Lee. A mathematical interpretation of autoregressive generative pre-trained transformer and self-supervised learning. Mathematics, 2023. URL: https://www.mdpi.com/2227-7390/11/11/2451, doi:10.3390/math11112451.">Lee, 2023</a>]</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\operatorname{PE}(p, d)= \begin{cases}\sin \left(\frac{p}{10000^{\frac{2 d}{D}}}\right) &amp; \text { if } d \text { is even, } \\ \cos \left(\frac{p}{10000^{\frac{2 d-1}{D}}}\right) &amp; \text { if } d \text { is odd. }\end{cases}
\end{split}\]</div>
<p>It is worth noting that <span class="math notranslate nohighlight">\(10,000\)</span> is an parameter that can be changed.</p>
</section>
</div><p>Now to relate the positional encoding formula back to the implementation, we
would resume where we left off in the previous section. For a given input matrix
(token embedding matrix output) <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{T \times D}\)</span>, where
<span class="math notranslate nohighlight">\(T\)</span> is the sequence length and <span class="math notranslate nohighlight">\(D\)</span> is the embedding dimension (denoted as
<span class="math notranslate nohighlight">\(d_{\text{model}}\)</span> in typical Transformer literature), the positional encoding
<span class="math notranslate nohighlight">\(\operatorname{PE}\)</span> is applied to integrate sequence positional information into
the embeddings. As a shorthand, the resultant matrix <span class="math notranslate nohighlight">\(\mathbf{X}'\)</span> (sometimes
denoted as <span class="math notranslate nohighlight">\(\mathbf{P}\)</span>) after applying positional encoding <em>elementwise</em> can be
expressed as follows:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}' = \operatorname{PE}(\mathbf{X}),
\]</div>
<p>where each element of <span class="math notranslate nohighlight">\(\mathbf{X}'\)</span>, denoted as <span class="math notranslate nohighlight">\(x'_{i, j}\)</span>, is calculated based
on the sinusoidal function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
x'_{i, j} =
\begin{cases}
\sin\left(\frac{i}{10000^{j/D}}\right) &amp; \text{if } j \mod 2 = 0 \\
\cos\left(\frac{i}{10000^{(j-1)/D}}\right) &amp; \text{if } j \mod 2 \neq 0
\end{cases}
\end{split}\]</div>
<p>for <span class="math notranslate nohighlight">\(i = 1, \ldots, T\)</span> and <span class="math notranslate nohighlight">\(j = 1, \ldots, D\)</span>.</p>
<p>We can update our original embeddings tensor <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> (recall this is the
output of the token embeddings layer) to include positional information:</p>
<div class="math notranslate nohighlight">
\[
\tilde{\mathbf{X}} := \mathbf{X} + \operatorname{PE}(X)
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\mathbf{X}'= \operatorname{PE}(\mathbf{X})\)</span> is independent of
<span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>, and it’s computed based on the positional encoding formula used in
transformers, which uses sinusoidal functions of different frequencies.</p>
</section>
<section id="id13">
<h3><a class="toc-backref" href="#id73" role="doc-backlink">Implementation</a><a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>


<span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">ABC</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">context_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_length</span> <span class="o">=</span> <span class="n">context_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="o">...</span>


<span class="k">class</span> <span class="nc">Sinusoid</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">):</span>
    <span class="n">P</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">context_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>

        <span class="n">P</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_positional_encoding</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;P&quot;</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># with this no need requires_grad=False</span>

    <span class="k">def</span> <span class="nf">_init_positional_encoding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the positional encoding tensor.&quot;&quot;&quot;</span>
        <span class="n">P</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">context_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">))</span>
        <span class="n">position</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_position_vector</span><span class="p">()</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_div_term_vector</span><span class="p">()</span>
        <span class="n">P</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">/</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">P</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">/</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">P</span>

    <span class="k">def</span> <span class="nf">_get_position_vector</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a vector representing the position of each token in a sequence.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">context_length</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_div_term_vector</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a vector representing the divisor term for positional encoding.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span>
            <span class="mi">10000</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_add_positional_encoding</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span>

    <span class="k">def</span> <span class="nf">_add_positional_encoding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add the positional encoding tensor to the input tensor.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">z</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">P</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
</pre></div>
</div>
</div>
</div>
<p>We now do a sum operation between the output of the token embeddings <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>
and the positional encodings <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> to get the final input to the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">generator</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>

<span class="n">pos_embed</span> <span class="o">=</span> <span class="n">Sinusoid</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">context_length</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">P</span> <span class="o">=</span> <span class="n">pos_embed</span><span class="o">.</span><span class="n">P</span>

<span class="n">z0_tok_embed_with_pos_embed</span> <span class="o">=</span> <span class="n">pos_embed</span><span class="p">(</span><span class="n">z0_tok_embed</span><span class="p">)</span>
<span class="n">z0_tok_embed_add_pos_embed</span> <span class="o">=</span> <span class="n">z0_tok_embed</span> <span class="o">+</span> <span class="n">P</span>

<span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed</span><span class="p">,</span> <span class="n">z0_tok_embed_add_pos_embed</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span> <span class="c1"># just to show that adding P to the z0 is the same as pos_embed(z0)</span>
</pre></div>
</div>
</div>
</div>
<p>As we have seen earlier using manual calculations, the input sequence’s first token/position at <span class="math notranslate nohighlight">\(t=1\)</span>
has values of <span class="math notranslate nohighlight">\([0, 1, 0, 1]\)</span> for the positional encoding with <span class="math notranslate nohighlight">\(D=4\)</span>. We simply add this positional
encoding to the token embeddings to get the final input embeddings. We can verify it visually below (or can add programmatically).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pprint</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">z0_tok_embed</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0000</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0000</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.8415</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5403</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0100</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9999</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9093</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4161</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0200</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9998</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1411</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.9900</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0300</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9996</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.7568</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.6536</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0400</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9992</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.9589</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2837</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0500</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9988</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2794</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9602</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0600</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9982</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6570</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7539</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0699</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9976</span><span style="font-weight: bold">]]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.0213</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3146</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2616</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3730</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5715</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1229</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.8145</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4164</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4973</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.1740</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.6713</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1102</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.3167</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2943</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9573</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2935</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0623</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1054</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.8182</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.4184</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4016</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3422</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.9704</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2435</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0576</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0596</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2764</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2403</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.2707</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.5865</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4099</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.3797</span><span style="font-weight: bold">]]]</span>, <span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">EmbeddingBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.0213</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.3146</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2616</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.3730</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.4130</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6632</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.8045</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4164</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.4066</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.5902</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.6513</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.8896</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.1756</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.6957</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9873</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.2931</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.6945</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.7590</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.8582</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4192</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.3605</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6258</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.9204</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7553</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3370</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9006</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3363</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7579</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.9277</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1674</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.3400</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3821</span><span style="font-weight: bold">]]]</span>, <span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">AddBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div></div>
</div>
<p>In the <code class="docutils literal notranslate"><span class="pre">forward</span></code> method of the <code class="docutils literal notranslate"><span class="pre">PositionalEncoding</span></code> class, the positional
encoding is added to the input <code class="docutils literal notranslate"><span class="pre">X</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">P</span><span class="p">[:,</span> <span class="p">:</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<p>This method slices the precalculated positional encodings tensor <code class="docutils literal notranslate"><span class="pre">self.P</span></code> to
match the sequence length of <code class="docutils literal notranslate"><span class="pre">X</span></code>, adds it to <code class="docutils literal notranslate"><span class="pre">X</span></code>, and then applies dropout. The
result, which is the sum of the original embeddings and the positional
encodings, is returned. So there’s no need to add the positional encodings to
<code class="docutils literal notranslate"><span class="pre">X</span></code> outside of this class.</p>
<p>So when you call <code class="docutils literal notranslate"><span class="pre">pos_embed(Z_tok_embed)</span></code>, it adds the positional encodings to
<code class="docutils literal notranslate"><span class="pre">Z_tok_embed</span></code> and applies dropout, then returns the result. You could store this
result in <code class="docutils literal notranslate"><span class="pre">Z_tok_embed_with_pos_embed</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Z_tok_embed_with_pos_embed</span> <span class="o">=</span> <span class="n">pos_embed</span><span class="p">(</span><span class="n">Z_tok_embed</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, <code class="docutils literal notranslate"><span class="pre">Z_tok_embed_with_pos_embed</span></code> contains the original embeddings with the
positional encodings added and dropout applied.</p>
<p>In the context of our “hello bot” example, the original tensor <code class="docutils literal notranslate"><span class="pre">Z_tok_embed</span></code>
represented the word embeddings, where each token in the sequence (i.e., <code class="docutils literal notranslate"><span class="pre">SOS</span></code>,
<code class="docutils literal notranslate"><span class="pre">hello</span></code>, <code class="docutils literal notranslate"><span class="pre">bot</span></code>, <code class="docutils literal notranslate"><span class="pre">EOS</span></code>) was converted into a 2-dimensional vector capturing the
semantic meaning of each token. After adding positional encoding, the new tensor
represents both the semantic and positional information of each token in the
sequence.</p>
<ul class="simple">
<li><p>The first row (<code class="docutils literal notranslate"><span class="pre">[1.1103,</span> <span class="pre">-0.6898]</span></code>) now encapsulates both the meaning of the
<code class="docutils literal notranslate"><span class="pre">SOS</span></code> token and the information that it’s the first token in the sequence.</p></li>
<li><p>The second row (<code class="docutils literal notranslate"><span class="pre">[0.0756,</span> <span class="pre">-0.2103]</span></code>) is now a representation of the word
<code class="docutils literal notranslate"><span class="pre">hello</span></code> that carries not just its semantics (e.g., being a greeting), but
also the information that it’s the second word in the sentence.</p></li>
<li><p>The third row (<code class="docutils literal notranslate"><span class="pre">[2.2618,</span> <span class="pre">0.2702]</span></code>) likewise carries both the semantics of
<code class="docutils literal notranslate"><span class="pre">bot</span></code> (likely related to AI or technology), and its position as the third
word in the sentence.</p></li>
<li><p>The last row (<code class="docutils literal notranslate"><span class="pre">[-0.8478,</span> <span class="pre">-0.0320]</span></code>) encapsulates the semantics of <code class="docutils literal notranslate"><span class="pre">EOS</span></code>
token, signifying end of a sentence, and the fact that it’s the last token
in the sentence.</p></li>
</ul>
<p>The idea here is that in natural language, word order matters. The sentence
“hello bot” is not the same as “bot hello” (okay maybe it is the same in this
example, a better one is cat eat mouse isn’t the same as mouse eat cat).</p>
<p>So, in a language model, we want our representations to capture not just what
words mean, but also where they are in a sentence. Positional encoding is a
technique to achieve this goal.</p>
</section>
<section id="visualising-positional-encodings">
<h3><a class="toc-backref" href="#id74" role="doc-backlink">Visualising Positional Encodings</a><a class="headerlink" href="#visualising-positional-encodings" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pos_embed_visual</span> <span class="o">=</span> <span class="n">Sinusoid</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="mi">48</span><span class="p">,</span> <span class="n">context_length</span><span class="o">=</span><span class="mi">96</span><span class="p">)</span>

<span class="n">P_visual</span> <span class="o">=</span> <span class="n">pos_embed_visual</span><span class="o">.</span><span class="n">P</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">pos</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">P_visual</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;RdGy&quot;</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">P_visual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">P_visual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Position in sequence&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Hidden dimension&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Positional encoding over hidden dimensions&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="mi">10</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">P_visual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">10</span><span class="p">)])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="mi">10</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">P_visual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">10</span><span class="p">)])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ff71fba3084ad741b4dcd9ec27b03b7b92397eb3f077097add233fa7fbefff23.png" src="../../_images/ff71fba3084ad741b4dcd9ec27b03b7b92397eb3f077097add233fa7fbefff23.png" />
</div>
</div>
<p>The positional encodings are depicted through sine and cosine functions, each
varying in wavelength across the hidden dimensions, to uniquely represent each
position. By examining these functions within individual hidden dimensions, we
gain deeper insights into the encoding patterns. Here, we present a
visualization of the positional encodings across hidden dimensions <span class="math notranslate nohighlight">\(d = 0, 1,
2, 3\)</span> for the initial <span class="math notranslate nohighlight">\(16\)</span> sequence positions <span id="id14">[<a class="reference internal" href="../../bibliography.html#id21" title="Phillip Lippe. UvA Deep Learning Tutorials. https://uvadlc-notebooks.readthedocs.io/en/latest/, 2023.">Lippe, 2023</a>]</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>


<span class="k">def</span> <span class="nf">plot_positional_encoding</span><span class="p">(</span><span class="n">pe</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">figsize</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot positional encoding for each hidden dimension.</span>

<span class="sd">    Args:</span>
<span class="sd">        pe: Positional encoding array.</span>
<span class="sd">        composer_block_size: Block size of the composer.</span>
<span class="sd">        figsize: Figure size for the plot.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">()</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span> <span class="k">for</span> <span class="n">a_list</span> <span class="ow">in</span> <span class="n">ax</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">a_list</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ax</span><span class="p">):</span>
        <span class="n">a</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">block_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">pe</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span><span class="n">block_size</span><span class="p">],</span>
            <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;C</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span>
            <span class="n">markersize</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
            <span class="n">markeredgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">a</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Encoding in hidden dimension d=</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">a</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Position in sequence&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">a</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Positional encoding&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">a</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">17</span><span class="p">))</span>
        <span class="n">a</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;both&quot;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s2">&quot;major&quot;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">a</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;both&quot;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s2">&quot;minor&quot;</span><span class="p">,</span> <span class="n">labelsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
        <span class="n">a</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">)</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">reset_orig</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_positional_encoding</span><span class="p">(</span><span class="n">P_visual</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/fe28619b5c8f74b2c7adc656a58746c05cfebfcc0bfd1c5f9395ad5d0eaf98b9.png" src="../../_images/fe28619b5c8f74b2c7adc656a58746c05cfebfcc0bfd1c5f9395ad5d0eaf98b9.png" />
</div>
</div>
<p>As we can see, the patterns between the hidden dimension 1 and 2 only differ in
the starting angle. The wavelength is <span class="math notranslate nohighlight">\(2\pi\)</span> , hence the repetition after
position 6 . The hidden dimensions 2 and 3 have about twice the wavelength
<span id="id15">[<a class="reference internal" href="../../bibliography.html#id21" title="Phillip Lippe. UvA Deep Learning Tutorials. https://uvadlc-notebooks.readthedocs.io/en/latest/, 2023.">Lippe, 2023</a>]</span>.</p>
</section>
<section id="an-example-of-positional-encoding">
<h3><a class="toc-backref" href="#id75" role="doc-backlink">An Example of Positional Encoding</a><a class="headerlink" href="#an-example-of-positional-encoding" title="Link to this heading">#</a></h3>
<p>To demonstrate how positional encodings are calculated for an input sequence
using the given formula, let’s take the first three tokens from the example
sequence <code class="docutils literal notranslate"><span class="pre">'</span> <span class="pre">priest</span> <span class="pre">and</span> <span class="pre">clerk?</span> <span class="pre">well</span> <span class="pre">then,</span> <span class="pre">amen'</span></code>. We’ll assume these tokens are
‘priest’, ‘and’, ‘clerk’ and that we’re dealing with an embedding dimension
<span class="math notranslate nohighlight">\(D = 4\)</span> for simplicity. The positions <span class="math notranslate nohighlight">\(p\)</span> of these tokens are 1, 2, and 3,
respectively.</p>
<p>For <span class="math notranslate nohighlight">\(D = 4\)</span>, each token’s positional encoding will be a vector of 4 elements.
Let’s calculate the positional encodings for <span class="math notranslate nohighlight">\(p = 1, 2, 3\)</span> (corresponding to
‘priest’, ‘and’, ‘clerk?’) and for each dimension <span class="math notranslate nohighlight">\(d = 0, 1, 2, 3\)</span>:</p>
<ul>
<li><p>Positional Encoding for <span class="math notranslate nohighlight">\(p = 1\)</span> (‘priest’)</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(d = 0\)</span> (even): <span class="math notranslate nohighlight">\(\sin\left(\frac{1}{10000^{0/4}}\right) = \sin(1)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(d = 1\)</span> (odd):
<span class="math notranslate nohighlight">\(\cos\left(\frac{1}{10000^{1/4}}\right) =
\cos\left(\frac{1}{\sqrt[4]{10000}}\right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(d = 2\)</span> (even):
<span class="math notranslate nohighlight">\(\sin\left(\frac{1}{10000^{2/4}}\right) =
\sin\left(\frac{1}{\sqrt{10000}}\right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(d = 3\)</span> (odd):
<span class="math notranslate nohighlight">\(\cos\left(\frac{1}{10000^{3/4}}\right) =
\cos\left(\frac{1}{\sqrt[4]{10000^3}}\right)\)</span></p></li>
<li><p>Final positional encoding for <span class="math notranslate nohighlight">\(p = 1\)</span> (‘priest’):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
        \begin{bmatrix}
        \sin(1) \\
        \cos\left(\frac{1}{\sqrt[4]{10000}}\right) \\
        \sin\left(\frac{1}{\sqrt{10000}}\right) \\
        \cos\left(\frac{1}{\sqrt[4]{10000^3}}\right)
        \end{bmatrix}
        \end{split}\]</div>
</li>
</ul>
</li>
<li><p>Positional Encoding for <span class="math notranslate nohighlight">\(p = 2\)</span> (‘and’)</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(d = 0\)</span> (even): <span class="math notranslate nohighlight">\(\sin\left(\frac{2}{10000^{0/4}}\right) = \sin(2)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(d = 1\)</span> (odd):
<span class="math notranslate nohighlight">\(\cos\left(\frac{2}{10000^{1/4}}\right) =
\cos\left(\frac{2}{\sqrt[4]{10000}}\right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(d = 2\)</span> (even):
<span class="math notranslate nohighlight">\(\sin\left(\frac{2}{10000^{2/4}}\right) =
\sin\left(\frac{2}{\sqrt{10000}}\right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(d = 3\)</span> (odd):
<span class="math notranslate nohighlight">\(\cos\left(\frac{2}{10000^{3/4}}\right) =
\cos\left(\frac{2}{\sqrt[4]{10000^3}}\right)\)</span></p></li>
<li><p>Final positional encoding for <span class="math notranslate nohighlight">\(p = 2\)</span> (‘and’):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
        \begin{bmatrix}
        \sin(2) \\
        \cos\left(\frac{2}{\sqrt[4]{10000}}\right) \\
        \sin\left(\frac{2}{\sqrt{10000}}\right) \\
        \cos\left(\frac{2}{\sqrt[4]{10000^3}}\right)
        \end{bmatrix}
        \end{split}\]</div>
</li>
</ul>
</li>
<li><p>Positional Encoding for <span class="math notranslate nohighlight">\(p = 3\)</span> (‘clerk?’)</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(d = 0\)</span> (even): <span class="math notranslate nohighlight">\(\sin\left(\frac{3}{10000^{0/4}}\right) = \sin(3)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(d = 1\)</span> (odd):
<span class="math notranslate nohighlight">\(\cos\left(\frac{3}{10000^{1/4}}\right) =
\cos\left(\frac{3}{\sqrt[4]{10000}}\right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(d = 2\)</span> (even):
<span class="math notranslate nohighlight">\(\sin\left(\frac{3}{10000^{2/4}}\right) =
\sin\left(\frac{3}{\sqrt{10000}}\right)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(d = 3\)</span> (odd):
<span class="math notranslate nohighlight">\(\cos\left(\frac{3}{10000^{3/4}}\right) =
\cos\left(\frac{3}{\sqrt[4]{10000^3}}\right)\)</span></p></li>
<li><p>Final positional encoding for <span class="math notranslate nohighlight">\(p = 3\)</span> (‘clerk?’):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
        \begin{bmatrix}
        \sin(3) \\
        \cos\left(\frac{3}{\sqrt[4]{10000}}\right) \\
        \sin\left(\frac{3}{\sqrt{10000}}\right) \\
        \cos\left(\frac{3}{\sqrt[4]{10000^3}}\right)
        \end{bmatrix}
        \end{split}\]</div>
</li>
</ul>
</li>
</ul>
<p>The uniqueness across different positions in the sequence is what’s important</p>
<ol class="arabic simple">
<li><p>For each token position (p): The encoding creates a vector of length D (4 in
the example).</p></li>
<li><p>Within each position’s vector: The values are calculated using alternating
sine and cosine functions across the dimensions (d=0 to d=3 in the example).</p></li>
<li><p>Across different positions: The key is that the encoding for position p=1 is
different from p=2, which is different from p=3, and so on.</p></li>
</ol>
<p>The uniqueness comes from the combination of:</p>
<ul class="simple">
<li><p>The position (p) changing for each token in the sequence</p></li>
<li><p>The dimension (d) varying within each token’s encoding</p></li>
<li><p>The use of different frequencies (controlled by 10000^(2d/D))</p></li>
</ul>
<p>This creates a pattern where:</p>
<ul class="simple">
<li><p>Each position has a unique overall encoding vector</p></li>
<li><p>Each dimension within that vector captures different aspects of the position
information</p></li>
<li><p>The relationship between encodings at different positions follows a
structured pattern</p></li>
</ul>
</section>
<section id="positional-encodings-via-embeddings">
<h3><a class="toc-backref" href="#id76" role="doc-backlink">Positional Encodings Via Embeddings</a><a class="headerlink" href="#positional-encodings-via-embeddings" title="Link to this heading">#</a></h3>
<p>In practice, the positional encodings are learned as part of the GPT-2
<span id="id16">[<a class="reference internal" href="../../bibliography.html#id17" title="Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.">Radford <em>et al.</em>, 2019</a>]</span>. So we can replicate the same by using a
<code class="docutils literal notranslate"><span class="pre">nn.Embedding</span></code> layer in PyTorch as in the token embeddings.</p>
<p><span class="math notranslate nohighlight">\(\mathbf{W}_{p}\)</span> is the positional embedding matrix. Each row of this matrix
corresponds to the embedding of a position in a sequence. This matrix is usually
of size <span class="math notranslate nohighlight">\(T \times D\)</span>, where <span class="math notranslate nohighlight">\(T\)</span> is the maximum length of a sequence we allow in
the model, and <span class="math notranslate nohighlight">\(D\)</span> is the dimension of the embedding space.</p>
<p>In other words, the <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> positional matrix introduced earlier has the
same shape as <span class="math notranslate nohighlight">\(\mathbf{W}_{p}\)</span>, and while the former is fixed, the latter is
learned during the training process.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PositionalEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">context_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="o">=</span><span class="n">context_length</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">positions</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">positions</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">generator</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>
<span class="n">seed_all</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">set_torch_deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">pos_embed</span> <span class="o">=</span> <span class="n">PositionalEmbedding</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">context_length</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">)</span>
<span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span> <span class="c1"># shape (t)</span>
<span class="n">z0_pos_embed</span> <span class="o">=</span> <span class="n">pos_embed</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span>

<span class="n">z0_tok_embed_with_pos_embed</span> <span class="o">=</span> <span class="n">z0_tok_embed</span> <span class="o">+</span> <span class="n">z0_pos_embed</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.2476</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3053</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.2803</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0927</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.8510</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1381</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1915</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.5725</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6144</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-3.0605</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.5109</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3032</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.7809</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.5955</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6474</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0677</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1859</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.2861</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.1882</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-2.0040</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4551</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.3198</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4025</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.1232</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7856</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1017</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.9343</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.5487</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2.2669</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3526</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0048</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.7453</span><span style="font-weight: bold">]]]</span>, <span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">AddBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div></div>
</div>
<p>To this end, we would have wrapped up the first two layers, where we first pass
an input sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> through the token embedding layer to obtain the
token embeddings <span class="math notranslate nohighlight">\(\mathbf{X} = \mathbf{W}_{e} \mathbf{x}\)</span>, and then add the
positional embeddings to the token embeddings to obtain the final embeddings.</p>
<p>The process to encode position into the embeddings is:</p>
<p>Given an input sequence <span class="math notranslate nohighlight">\(\mathbf{x} = \left(x_1, x_2, ..., x_{T}\right)\)</span>, where
<span class="math notranslate nohighlight">\(x_t\)</span> is the token at position <span class="math notranslate nohighlight">\(t\)</span> in the sequence, we have transformed the
input sequence into a sequence of token embeddings <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, holding both
the static semantics and the positional information of the input sequence.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\tilde{\mathbf{X}} &amp;= \mathbf{W}_{e} \mathbf{x} + \mathbf{W}_{p} \in \mathbb{R}^{T \times D} \\
                    &amp;= \mathbf{X} \oplus \mathbf{X}'
\end{aligned}
\end{split}\]</div>
<p>And note this is only for <span class="math notranslate nohighlight">\(1\)</span> sequence, and we can extend this to <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>
sequences in a batch.</p>
<div class="seealso admonition">
<p class="admonition-title">References</p>
<ul class="simple">
<li><p><a class="reference external" href="https://aman.ai/primers/ai/transformers/#positional-encoding">Positional Encodings - Aman Chadha</a></p></li>
<li><p><a class="reference external" href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html">Self-Attention and Positional Encoding - Dive into Deep Learning</a></p></li>
<li><p><a class="reference external" href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">Transformer Architecture: The Positional Encoding</a></p></li>
<li><p><a class="reference external" href="https://e2eml.school/transformers.html#positional_encoding">Positional Encoding - Brandon Rohrer</a></p></li>
</ul>
</div>
</section>
</section>
<section id="layer-normalization">
<h2><a class="toc-backref" href="#id77" role="doc-backlink">Layer Normalization</a><a class="headerlink" href="#layer-normalization" title="Link to this heading">#</a></h2>
<section id="id17">
<h3><a class="toc-backref" href="#id78" role="doc-backlink">Definition</a><a class="headerlink" href="#id17" title="Link to this heading">#</a></h3>
<p>Layer normalization is a technique applied in the context of neural networks to
stabilize the learning process by normalizing the inputs across the features for
each token in a sequence. Given a data representation
<span class="math notranslate nohighlight">\(\mathbf{Z} \in \mathbb{R}^{T \times D}\)</span>, where <span class="math notranslate nohighlight">\(T\)</span> is the sequence length
(number of tokens) and <span class="math notranslate nohighlight">\(D\)</span> is the hidden dimension (feature space), layer
normalization is applied <strong>independently</strong> to each vector (or token) across the
<strong>feature</strong> dimension <span class="math notranslate nohighlight">\(D\)</span>. You can think of each token <span class="math notranslate nohighlight">\(t=1, \ldots, T\)</span> as a
separate example, and <span class="math notranslate nohighlight">\(\mathbf{Z}_{t}\)</span> represents each row/token. We then
compute the mean and variance for each row/token and then apply the
normalization to each row/token. This process is repeated for each row/token in
the input matrix <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>.</p>
<p>When considering a batch of such sequences, represented as
<span class="math notranslate nohighlight">\(\mathbf{Z}^{\mathcal{B}} \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span>, where
<span class="math notranslate nohighlight">\(\mathcal{B}\)</span> is the batch size, layer normalization still focuses on
normalizing each token vector within each sequence in the batch. The operation
does not aggregate or normalize across different tokens (<span class="math notranslate nohighlight">\(T\)</span> dimension) or
different sequences in the batch (<span class="math notranslate nohighlight">\(\mathcal{B}\)</span> dimension); instead, it
normalizes the values across the features (<span class="math notranslate nohighlight">\(D\)</span> dimension) for each token.</p>
<p>For a single token <span class="math notranslate nohighlight">\(\mathbf{Z}_t \in \mathbb{R}^{1 \times D}\)</span> in a sequence
<span class="math notranslate nohighlight">\(\mathbf{Z} \in \mathbb{R}^{T \times D}\)</span>, the the normalization process involves
subtracting the mean <span class="math notranslate nohighlight">\(\mu_t\)</span> and dividing by the standard deviation <span class="math notranslate nohighlight">\(\sigma_t\)</span>
(adjusted with a small constant <span class="math notranslate nohighlight">\(\epsilon\)</span> for numerical stability) of its
features. This process ensures that, for each token, the features are centered
around zero with a unit variance.</p>
<div class="proof definition admonition" id="decoder-layer-normalization">
<p class="admonition-title"><span class="caption-number">Definition 4 </span> (Layer Normalization)</p>
<section class="definition-content" id="proof-content">
<p>Given a token <span class="math notranslate nohighlight">\(\mathbf{Z}_t \in \mathbb{R}^{1 \times D}\)</span> from the sequence
<span class="math notranslate nohighlight">\(\mathbf{Z} \in \mathbb{R}^{T \times D}\)</span>, the normalized output
<span class="math notranslate nohighlight">\(\overline{\mathbf{Z}}_t \in \mathbb{R}^{1 \times D}\)</span> for this token can be
expressed as follows:</p>
<div class="math notranslate nohighlight">
\[
\overline{\mathbf{Z}}_{t} = \frac{\mathbf{Z}_{t} - \mu_{t}}{\sqrt{\sigma_{t}^2 + \epsilon}}
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{Z}_t\)</span> is the vector of features for the token at position <span class="math notranslate nohighlight">\(t\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mu_t \in \mathbb{R}\)</span> is the mean of the features for this token,</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma_t^2 \in \mathbb{R}\)</span> is the variance of the features for this token,</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon \in \mathbb{R}\)</span> is a small constant added for numerical stability,
ensuring that we never divide by zero or approach zero in the denominator.</p></li>
</ul>
<p>The mean <span class="math notranslate nohighlight">\(\mu_t\)</span> and variance <span class="math notranslate nohighlight">\(\sigma_t^2\)</span> are computed as follows:</p>
<div class="math notranslate nohighlight">
\[
\mu_t \stackrel{\text { def }}{=} \frac{1}{D} \sum_{d=1}^D Z_{t d}, \quad \sigma_t^2 \stackrel{\text { def }}{=} \frac{1}{D} \sum_{d=1}^D\left(Z_{t d}-\mu_t\right)^2
\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\mathbf{Z}_{td}\)</span> represents the <span class="math notranslate nohighlight">\(d\)</span>-th feature of the token at position
<span class="math notranslate nohighlight">\(t\)</span>. The division and subtraction are applied element-wise across the feature
dimension <span class="math notranslate nohighlight">\(D\)</span> for the token, normalizing each feature based on the statistics of
the features within the same token.</p>
</section>
</div><p>Since layer normalization is performed for each token vector across the feature
dimension <span class="math notranslate nohighlight">\(D\)</span>, the process can be vectorized and applied simultaneously to all
<span class="math notranslate nohighlight">\(T\)</span> token vectors in the sequence.</p>
<ul class="simple">
<li><p>For each token <span class="math notranslate nohighlight">\(t = 1, \ldots, T\)</span>, the mean <span class="math notranslate nohighlight">\(\mu_t\)</span> and variance
<span class="math notranslate nohighlight">\(\sigma_t^2\)</span> are computed.</p></li>
<li><p>Each token vector <span class="math notranslate nohighlight">\(\mathbf{Z}_t\)</span> is then normalized using its respective
<span class="math notranslate nohighlight">\(\mu_t\)</span> and <span class="math notranslate nohighlight">\(\sigma_t^2\)</span>.</p></li>
</ul>
<p>This results in a normalized sequence
<span class="math notranslate nohighlight">\(\overline{\mathbf{Z}} \in \mathbb{R}^{T \times D}\)</span>, where each token vector
<span class="math notranslate nohighlight">\(\overline{\mathbf{Z}}_t\)</span> has been normalized independently. The normalized
sequence retains its original shape <span class="math notranslate nohighlight">\((T \times D)\)</span>.</p>
<p>When considering a batch of sequences, represented as
<span class="math notranslate nohighlight">\(\mathbf{Z}^{\mathcal{B}} \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span>, the
layer normalization process extends naturally:</p>
<ul class="simple">
<li><p>The normalization process is applied independently to each token vector in
each sequence within the batch. This means for each sequence <span class="math notranslate nohighlight">\(b\)</span> in the
batch <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>, and for each token <span class="math notranslate nohighlight">\(t\)</span> in each sequence, the process
computes <span class="math notranslate nohighlight">\(\mu_{bt}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{bt}^2\)</span>, and normalizes each
<span class="math notranslate nohighlight">\(\mathbf{Z}_{bt}\)</span> accordingly.</p></li>
<li><p>Since the operation is independent across tokens and sequences, it can be
parallelized, allowing for efficient computation over the entire batch.</p></li>
</ul>
<p>The result is a batch of normalized sequences,
<span class="math notranslate nohighlight">\(\overline{\mathbf{Z}}^{\mathcal{B}} \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span>,
where each token vector <span class="math notranslate nohighlight">\(\overline{\mathbf{Z}}_{bt}\)</span> in each sequence of the
batch has been normalized based on its own mean and variance.</p>
<div class="tip admonition">
<p class="admonition-title">Broadcasting</p>
<p>It is worth noting that the notation above involves broadcasting, we are
essentially subtracting a scalar value (<span class="math notranslate nohighlight">\(\mu_t\)</span>) from a vector (<span class="math notranslate nohighlight">\(\mathbf{Z}_t\)</span>)
and dividing by another scalar value (<span class="math notranslate nohighlight">\(\sigma_t\)</span>). This is fine in practice, as
the scalar values are broadcasted to match the shape of the vector during the
element-wise operations.</p>
<p>We can however make the definition clearer by removing the implicit
broadcasting, and say that for each activation <span class="math notranslate nohighlight">\(Z_{td}\)</span> (feature <span class="math notranslate nohighlight">\(d\)</span> of a token
at position <span class="math notranslate nohighlight">\(t\)</span>), we compute the normalized activation <span class="math notranslate nohighlight">\(\overline{Z}_{td}\)</span></p>
<div class="math notranslate nohighlight">
\[
\overline{\mathbf{Z}}_{td} = \frac{\mathbf{Z}_{td} - \mu_{t}}{\sqrt{\sigma_{t}^2 + \epsilon}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu_t\)</span> and <span class="math notranslate nohighlight">\(\sigma_t^2\)</span> are computed as before.</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">generator</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>
<span class="n">seed_all</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">set_torch_deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>

<span class="n">first_sequence_</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">first_sequence_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">first_sequence_</span><span class="p">)</span>
<span class="n">first_sequence_var</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">first_sequence_</span><span class="p">)</span>

<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">first_sequence_</span><span class="p">):</span>

    <span class="n">first_sequence_mean</span><span class="p">[</span><span class="n">index</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">first_sequence_var</span><span class="p">[</span><span class="n">index</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">first_sequence_mean</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">first_sequence_var</span><span class="p">)</span>

<span class="n">first_sentence_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">first_sequence_</span> <span class="o">-</span> <span class="n">first_sequence_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">first_sequence_var</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">first_sentence_norm</span><span class="p">)</span>

<span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">normalized_embedding</span> <span class="o">=</span> <span class="n">layer_norm</span><span class="p">(</span><span class="n">embedding</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">normalized_embedding</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">normalized_embedding</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span><span class="n">first_sentence_norm</span><span class="p">,</span> <span class="n">normalized_embedding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0398</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0398</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0398</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0398</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1287</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1287</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1287</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1287</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3987</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3987</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3987</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3987</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0230</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0230</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0230</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0230</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4512</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4512</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4512</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4512</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7168</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7168</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7168</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7168</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.1730</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0302</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.5639</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4210</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6077</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1970</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.1190</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.5296</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.3132</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0139</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1715</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4986</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span><span style="font-weight: bold">])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.1730</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0302</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.5639</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4210</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6077</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1970</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.1190</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.5296</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.3132</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0139</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1715</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4986</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3854</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.6702</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.9863</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2985</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3328</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.2407</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.5304</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.4383</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.9720</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5154</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.3870</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.9305</span><span style="font-weight: bold">]]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│      </span><span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">NativeLayerNormBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div></div>
</div>
<p>We see that indeed the assertion passed, and our calculations are correct. Note we must
set <code class="docutils literal notranslate"><span class="pre">unbiased=False</span></code> in the <code class="docutils literal notranslate"><span class="pre">torch.var</span></code> function to get the same result as the
<code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code> function because we are using population variance formula.</p>
<p>We can further confirm below now the mean and variance close to 0 and 1 respectively.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">normalized_embedding</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">normalized_embedding</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Example of mean and std for a single sentence across embedding dimensions:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean:&quot;</span><span class="p">,</span> <span class="n">mean</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Standard deviation:&quot;</span><span class="p">,</span> <span class="n">std</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Example of mean and std for a single sentence across embedding dimensions:
Mean: tensor([3.7253e-08, 2.9802e-08, 0.0000e+00], grad_fn=&lt;SelectBackward0&gt;)
Standard deviation: tensor([1.1547, 1.1547, 1.1547], grad_fn=&lt;SelectBackward0&gt;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="learnable-affine-transformation">
<h3><a class="toc-backref" href="#id79" role="doc-backlink">Learnable Affine Transformation</a><a class="headerlink" href="#learnable-affine-transformation" title="Link to this heading">#</a></h3>
<p>Normalizing the activations to have zero mean and unit variance can limit the
representational power of the network, and thus after computing the normalized
features <span class="math notranslate nohighlight">\(\hat{\mathbf{Z}}_t\)</span> for each token, we introduce a <strong><em>learnable</em></strong>
affine transformation (scaling and shifting), in terms of parameters <span class="math notranslate nohighlight">\(\gamma\)</span>
and <span class="math notranslate nohighlight">\(\beta\)</span>, which are of the same dimensionality as the feature space <span class="math notranslate nohighlight">\(D\)</span>, to
scale and shift the normalized features, allowing the model to “undo” the
normalization if it is beneficial for the learning process.</p>
<div class="math notranslate nohighlight">
\[
\overline{\mathbf{Z}}_t = \dfrac{\mathbf{Z}_t - \mu_t}{\sqrt{\sigma_t^2 + \epsilon}} \odot \gamma + \beta
\]</div>
<p>where <span class="math notranslate nohighlight">\(\overline{\mathbf{Z}}_t\)</span> represents the output of the layer normalization
for the token at position <span class="math notranslate nohighlight">\(t\)</span>, and <span class="math notranslate nohighlight">\(\odot\)</span> denotes element-wise multiplication.
And for each activation <span class="math notranslate nohighlight">\(\mathbf{Z}_{td}\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
\overline{\mathbf{Z}}_{t d}=\frac{\mathbf{Z}_{t d}-\mu_t}{\sqrt{\sigma_t^2+\epsilon}} \cdot \gamma_d + \beta_d
\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma_d\)</span> and <span class="math notranslate nohighlight">\(\beta_d\)</span> are the scaling and shifting parameters for the
<span class="math notranslate nohighlight">\(d\)</span>-th feature of the token at position <span class="math notranslate nohighlight">\(t\)</span>. However notice that I did not
index <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> by <span class="math notranslate nohighlight">\(t\)</span> because they are shared across all tokens in
the sequence.</p>
<p>The notation <span class="math notranslate nohighlight">\(\gamma_{d}\)</span> without indexing <span class="math notranslate nohighlight">\(t\)</span> implies that the scaling
parameter <span class="math notranslate nohighlight">\(\gamma\)</span> is feature-specific but shared across all tokens in the
sequence. It means that each feature dimension <span class="math notranslate nohighlight">\(d\)</span> across all tokens <span class="math notranslate nohighlight">\(t\)</span> in the
sequence has its own unique scaling parameter, but this parameter does not
change with different tokens. This is the common setup in layer normalization,
where <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> parameters are learned for each feature dimension <span class="math notranslate nohighlight">\(D\)</span>
and are applied identically across all tokens <span class="math notranslate nohighlight">\(T\)</span>.</p>
<p>Overall, layer norm is just taking each row in <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>, sum all <span class="math notranslate nohighlight">\(D\)</span>
elements in the row, and then calculate the mean and variance. Then, we subtract
the mean from each element in the row, divide by the standard deviation, and
then scale and shift the result using <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
</section>
<section id="id18">
<h3><a class="toc-backref" href="#id80" role="doc-backlink">Implementation</a><a class="headerlink" href="#id18" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.types</span> <span class="kn">import</span> <span class="n">_device</span><span class="p">,</span> <span class="n">_dtype</span>


<span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;normalized_shape&quot;</span><span class="p">,</span> <span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="s2">&quot;elementwise_affine&quot;</span><span class="p">]</span>

    <span class="n">normalized_shape</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">elementwise_affine</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">normalized_shape</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]],</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="p">,</span>
        <span class="n">elementwise_affine</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">_device</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="kc">None</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">_dtype</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">factory_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="n">dtype</span><span class="p">}</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">normalized_shape</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">normalized_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">normalized_shape</span><span class="p">,)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">normalized_shape</span> <span class="o">=</span> <span class="n">normalized_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">elementwise_affine</span> <span class="o">=</span> <span class="n">elementwise_affine</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">elementwise_affine</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalized_shape</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>  <span class="c1"># type: ignore[arg-type]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">normalized_shape</span><span class="p">,</span> <span class="o">**</span><span class="n">factory_kwargs</span><span class="p">))</span>  <span class="c1"># type: ignore[arg-type]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s2">&quot;gamma&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">elementwise_affine</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">ones_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">elementwise_affine</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;</span><span class="si">{normalized_shape}</span><span class="s2">, eps=</span><span class="si">{eps}</span><span class="s2">, elementwise_affine=</span><span class="si">{elementwise_affine}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">generator</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>
<span class="n">seed_all</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">set_torch_deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">ln1</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">z0_tok_embed_with_pos_embed_ln1</span> <span class="o">=</span> <span class="n">ln1</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed_ln1</span><span class="p">)</span>

<span class="n">ln1_pytorch</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">z0_tok_embed_with_pos_embed_ln1_pytorch</span> <span class="o">=</span> <span class="n">ln1_pytorch</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed_ln1_pytorch</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed_ln1</span><span class="p">,</span> <span class="n">z0_tok_embed_with_pos_embed_ln1_pytorch</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.5968</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3831</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.1393</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0744</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0827</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2975</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2551</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.6353</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5394</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.6060</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0629</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0037</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4485</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3857</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7287</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.1055</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6253</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.0642</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.3103</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.8714</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1253</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.2588</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.5041</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3706</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4071</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2899</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.3095</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4267</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.6178</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1055</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4185</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.0938</span><span style="font-weight: bold">]]]</span>, <span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">AddBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.5968</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3831</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.1393</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0744</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0827</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2975</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2551</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.6353</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5394</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.6060</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0629</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0037</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4485</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3857</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7287</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.1055</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6253</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.0642</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.3103</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.8714</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1253</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.2588</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.5041</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3706</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4071</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2899</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.3095</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4267</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.6178</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1055</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4185</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.0938</span><span style="font-weight: bold">]]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│      </span><span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">NativeLayerNormBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div></div>
</div>
</section>
<section id="layer-norm-stabilises-activation-distributions">
<h3><a class="toc-backref" href="#id81" role="doc-backlink">Layer Norm Stabilises Activation Distributions</a><a class="headerlink" href="#layer-norm-stabilises-activation-distributions" title="Link to this heading">#</a></h3>
<p>Besides the known fact that layer normalization enables convergence and provdies
regularization <span id="id19">[<a class="reference internal" href="../../bibliography.html#id21" title="Phillip Lippe. UvA Deep Learning Tutorials. https://uvadlc-notebooks.readthedocs.io/en/latest/, 2023.">Lippe, 2023</a>]</span>, it also stabilizes the distributions of
activations <span id="id20">[<a class="reference internal" href="../../bibliography.html#id5" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into Deep Learning. Cambridge University Press, 2023. URL: https://D2L.ai.">Zhang <em>et al.</em>, 2023</a>]</span>. Training deep neural networks are
challenging, loss can easily be exploded or vanished, and the gradients can be
unstable. One simple way is to ensure each layer’s activation has a similar
distribution - the intuition is that if each layer’s activation has a similar
distribution, then the gradients will also have a similar distribution, and this
will stabilize the training process. Layer normalization is one of the
techniques that can help to achieve this.</p>
<div class="seealso admonition">
<p class="admonition-title">References</p>
<ul class="simple">
<li><p><a class="reference external" href="https://d2l.ai/chapter_convolutional-modern/batch-norm.html#layer-normalization">Layer Normalization - Dive into Deep Learning</a></p></li>
<li><p><a class="reference external" href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#residual-connection-and-layer-normalization">Residual Connection and Layer Normalization - Dive into Deep Learning</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1607.06450">Layer Normalization - arXiv</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html">PyTorch Documentation: torch.nn.LayerNorm</a></p></li>
</ul>
</div>
</section>
</section>
<section id="residual-connection">
<h2><a class="toc-backref" href="#id82" role="doc-backlink">Residual Connection</a><a class="headerlink" href="#residual-connection" title="Link to this heading">#</a></h2>
<p>I have written a more detailed post on the intuition of
<a class="reference external" href="https://gao-hongnan.github.io/gaohn-galaxy/deep_learning/computer_vision/modern_convolutional_neural_networks/resnets/concept.html">ResNet</a>
which is heavily adapted from the chapter
<a class="reference external" href="https://d2l.ai/chapter_convolutional-modern/resnet.html">Residual Networks (ResNet) and ResNeXt</a>
from the Dive into Deep Learning book.</p>
<p>For the sake of intuition, we can think of the residual connection as a way to
ensure that the original input to a layer is not lost as it passes through the
model layers.</p>
<ul class="simple">
<li><p>Deep neural networks are known to suffer from the vanishing gradient
problem, where gradients become increasingly small as they are
backpropagated through the layers during training. Since we are
backpropagating backwards, the earlier layers are therefore more susceptible
to this problem. The weak gradient signal could often be close to <span class="math notranslate nohighlight">\(0\)</span>, and
this could lead to the model not learning well. Consequently, we mitigate
this problem by adding the original input to the output of the layer, so
that the gradient signal has a direct path to flow through the network.</p>
<ul>
<li><p>Furthermore, Eugene Yan’s blog post
<a class="reference external" href="https://eugeneyan.com/writing/attention/">Some Intuition on Attention and the Transformer</a>
also highlighted that attention acting as a filtering mechanism may
block information from passing through, directly resulting flat
gradients as a small change to the inputs of the attention layer may not
change the outputs that much. Skip (residual) connections help resolve
this.</p></li>
</ul>
</li>
<li><p>We will see later that Multi-Head Attention mechanism operates on a set of
tokens, instead of over a sequence. We encode positional information into
the tokens, but there is a risk that the positional information is lost in
the multi-head attention layers. The residual connection helps to ensure
that the positional information is not lost <span id="id21">[<a class="reference internal" href="../../bibliography.html#id21" title="Phillip Lippe. UvA Deep Learning Tutorials. https://uvadlc-notebooks.readthedocs.io/en/latest/, 2023.">Lippe, 2023</a>]</span>.</p></li>
<li><p>The other well known property of the residual connection is that it helps to
learn the identity function. Perhaps the scenario is that the best thing a
layer or a series of layer can learn is itself - and we don’t actually want
an update.</p></li>
</ul>
<p>We quote the following from the Dive into Deep Learning book:</p>
<blockquote class="epigraph">
<div><p>Consider <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>, the class of functions that a specific network
architecture (together with learning rates and other hyperparameter settings)
can reach. That is, for all <span class="math notranslate nohighlight">\(f \in \mathcal{F}\)</span> there exists some set of
parameters (e.g., weights and biases) that can be obtained through training on a
suitable dataset. Let us assume that <span class="math notranslate nohighlight">\(f^*\)</span> is th “truth” function that we really
would like to find. If it is in <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>, we are in good shape but
typically we will not b quite so lucky. Instead, we will try to find some
<span class="math notranslate nohighlight">\(f_{\mathcal{F}}^*\)</span> which is our best bet within <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>. For instance,
given a dataset with features <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and labels <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>, we might try
finding it by solving the following optimization problem:</p>
<div class="math notranslate nohighlight">
\[
f_{\mathcal{F}}^* \stackrel{\text { def }}{=} \underset{f}{\operatorname{argmin}} L(\mathbf{X}, \mathbf{y}, f) \text { subject to } f \in \mathcal{F} .
\]</div>
<p>It is only reasonable to assume that if we design a different and more powerful
architecture <span class="math notranslate nohighlight">\(\mathcal{F}^{\prime}\)</span> we should arrive at a better outcome. In
other words, we would expect that <span class="math notranslate nohighlight">\(f_{\mathcal{F}}^*\)</span> is “better” than
<span class="math notranslate nohighlight">\(f_{\mathcal{F}}^*\)</span>. However, if <span class="math notranslate nohighlight">\(\mathcal{F} \nsubseteq \mathcal{F}^{\prime}\)</span>
there is no guarantee that this should even happen. In fact,
<span class="math notranslate nohighlight">\(f_{\mathcal{F}^{\prime}}^*\)</span> might well be worse.</p>
<p>As illustrated by <a class="reference internal" href="#d2l-resnet-functionclasses"><span class="std std-numref">Fig. 3</span></a>, for non-nested function
classes, a larger function class does not always move closer to the “truth”
function <span class="math notranslate nohighlight">\(f^*\)</span>. For instance, on the left of
<a class="reference internal" href="#d2l-resnet-functionclasses"><span class="std std-numref">Fig. 3</span></a>, though
<span class="math notranslate nohighlight">\(\mathcal{F}_3\)</span> is closer to <span class="math notranslate nohighlight">\(f^*\)</span> than <span class="math notranslate nohighlight">\(\mathcal{F}_1, \mathcal{F}_6\)</span> moves
away and there is no guarantee that further increasing the complexity can reduce
the distance from <span class="math notranslate nohighlight">\(f^*\)</span>. With nested function classes where
<span class="math notranslate nohighlight">\(\mathcal{F}_1 \subseteq \ldots \subseteq \mathcal{F}_6\)</span> on the right of
<a class="reference internal" href="#d2l-resnet-functionclasses"><span class="std std-numref">Fig. 3</span></a>, we can avoid the aforementioned issue from
the non-nested function classes.</p>
<figure class="align-default" id="d2l-resnet-functionclasses">
<img alt="../../_images/d2l-resnet-functionclasses.svg" src="../../_images/d2l-resnet-functionclasses.svg" />
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">For non-nested function classes, a larger (indicated by area) function class
does not guarantee we will get closer to the “truth” function <span class="math notranslate nohighlight">\(f^*\)</span>. This
does not happen for nested function classes.</span><a class="headerlink" href="#d2l-resnet-functionclasses" title="Link to this image">#</a></p>
<div class="legend">
<p><strong>Image Credit:</strong>
<a class="reference external" href="https://d2l.ai/chapter_convolutional-modern/resnet.html">8.6. Residual Networks (ResNet) and ResNeXt - Dive Into Deep Learing</a></p>
</div>
</figcaption>
</figure>
</div></blockquote>
<section id="layernorm-and-residual-connection">
<h3><a class="toc-backref" href="#id83" role="doc-backlink">LayerNorm and Residual Connection</a><a class="headerlink" href="#layernorm-and-residual-connection" title="Link to this heading">#</a></h3>
<p>In our context, we would have the following for GPT-1 and older Transformers:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\operatorname{AddNorm}(\mathbf{x}) &amp;= \operatorname{LayerNorm}(\mathbf{x} + \operatorname{Sublayer}(\mathbf{x})) \\
\end{aligned}
\end{split}\]</div>
<p>where the layernorm is applied after the residual connection. In other words, if
<code class="docutils literal notranslate"><span class="pre">Sublayer</span></code> is a function that represents a sublayer (e.g.,
<code class="docutils literal notranslate"><span class="pre">MultiHeadAttention</span></code>), then in each sub-block of the decoder, we would compute
the output and then add the original input to the output, and then apply layer
normalization to the sum.</p>
<p>However, in GPT-2, there is a modification, more concretely, to shift the layer
normalization to the input of the sub-block. This means now instead of applying
layer normalization after the residual connection, we apply it before the
residual connection. For example, if <code class="docutils literal notranslate"><span class="pre">Sublayer</span></code> is a function that represents a
sublayer (e.g., <code class="docutils literal notranslate"><span class="pre">MultiHeadAttention</span></code>), then in each sub-block of the decoder, we
would first apply layer normalization to the input, then pass the normalized
input to the sublayer, and then add the original input to the output of the
sublayer.</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\operatorname{AddNorm}(\mathbf{x}) &amp;= \mathbf{x} + \operatorname{Sublayer}(\operatorname{LayerNorm}(\mathbf{x}))
\end{aligned}
\]</div>
</section>
<section id="implementation-of-residual-block-and-addnorm">
<h3><a class="toc-backref" href="#id84" role="doc-backlink">Implementation of Residual Block and AddNorm</a><a class="headerlink" href="#implementation-of-residual-block-and-addnorm" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>


<span class="k">class</span> <span class="nc">ResidualBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">sublayer</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">sublayer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">AddNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># fmt: off</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span>    <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">feature_dim</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">elementwise_affine</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># fmt: on</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">sublayer</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;G(F(x) + x) where G = layer norm and F = sublayer&quot;&quot;&quot;</span>
        <span class="c1"># FIXME: GPT-2 should be x + self.dropout(sublayer(self.layer_norm(x)))</span>
        <span class="n">output</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">sublayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="self-attention">
<h2><a class="toc-backref" href="#id85" role="doc-backlink">Self-Attention</a><a class="headerlink" href="#self-attention" title="Link to this heading">#</a></h2>
<div class="warning admonition">
<p class="admonition-title">Notation Abuse</p>
<p>We may have used <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> to represent the output of the token and positional
embedding layer, but in what follows we probably will default to using <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>.</p>
</div>
<section id="intuition-of-attention-mechanism">
<h3><a class="toc-backref" href="#id86" role="doc-backlink">Intuition of Attention Mechanism</a><a class="headerlink" href="#intuition-of-attention-mechanism" title="Link to this heading">#</a></h3>
<p>Attention is not a new concept, and one of the most influencial papers came from
<em>Neural Machine Translation by Jointly Learning to Align and Translate</em>
<span id="id22">[<a class="reference internal" href="../../bibliography.html#id20" title="Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. 2014. arXiv:1409.0473.">Bahdanau <em>et al.</em>, 2014</a>]</span>, a paper published during 2014. In the context of our
post, we would stick to one intuitive interpretation, that <em>the attention
mechanism describes a <strong>weighted average</strong> of (sequence) elements with the
weights <strong>dynamically</strong> computed based on an input query and elements’ keys</em>
<span id="id23">[<a class="reference internal" href="../../bibliography.html#id21" title="Phillip Lippe. UvA Deep Learning Tutorials. https://uvadlc-notebooks.readthedocs.io/en/latest/, 2023.">Lippe, 2023</a>]</span>. In other words, we want contextually relevant
information to be weighted more heavily than less relevant information. For
example, the sentence <em>the cat walks by the river bank</em> would require the word
<em>bank</em> to be weighted more heavily than the word <em>the</em> when the word <em>cat</em> is
being processed. The dynamic portion is also important because this allows the
model to adjust the weights based on an input sequence (note that the learned
weights are static but the interaction with the input sequence is dynamic). When
attending to the token <em>cat</em> in the sequence, we would want the token
<em>cat</em> to be a <strong>weighted average</strong> of all the tokens in the sequence, including
itself. This is the essence of the self-attention mechanism. Note carefully that
at this point we do not assume that the self-attention is causal as we want to
discuss it generally first.</p>
</section>
<section id="token-embedding-and-vector-representation-process">
<h3><a class="toc-backref" href="#id87" role="doc-backlink">Token Embedding and Vector Representation Process</a><a class="headerlink" href="#token-embedding-and-vector-representation-process" title="Link to this heading">#</a></h3>
<p>Given an input sequence <span class="math notranslate nohighlight">\(\mathbf{x} = \left(x_1, x_2, \ldots, x_T\right)\)</span>, where
<span class="math notranslate nohighlight">\(T\)</span> is the length of the sequence, and each <span class="math notranslate nohighlight">\(x_t \in \mathcal{V}\)</span> is a token in
the sequence, we use a generic embedding function <span class="math notranslate nohighlight">\(h_{\text{emb}}\)</span> to map each
token to a vector representation in a continuous vector space:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned} h_{\text{emb}} : \mathcal{V} &amp;\rightarrow \mathbb{R}^{D} \\ x_t
&amp;\mapsto \mathbf{z}_t \end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{V}\)</span> is the vocabulary of tokens (discrete space <span class="math notranslate nohighlight">\(\mathbb{Z}\)</span>),
and <span class="math notranslate nohighlight">\(D\)</span> is the dimension of the embedding space (continuous space). The output
of the embedding function <span class="math notranslate nohighlight">\(h_{\text{emb}}\)</span> is a sequence of vectors
<span class="math notranslate nohighlight">\(\mathbf{Z} = \left(\mathbf{z}_1, \mathbf{z}_2, \ldots, \mathbf{z}_T\right)\)</span>,
where each <span class="math notranslate nohighlight">\(\mathbf{z}_t \in \mathbb{R}^{D}\)</span> is the vector representation of the
token <span class="math notranslate nohighlight">\(x_t\)</span> in the sequence. As seen earlier, we represent the sequence of
vectors <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> as a matrix <span class="math notranslate nohighlight">\(\mathbf{Z} \in \mathbb{R}^{T \times D}\)</span>, where
each row of the matrix represents the vector representation of each token in the
sequence.</p>
</section>
<section id="queries-keys-and-values">
<h3><a class="toc-backref" href="#id88" role="doc-backlink">Queries, Keys, and Values</a><a class="headerlink" href="#queries-keys-and-values" title="Link to this heading">#</a></h3>
<section id="database-analogy">
<h4><a class="toc-backref" href="#id89" role="doc-backlink">Database Analogy</a><a class="headerlink" href="#database-analogy" title="Link to this heading">#</a></h4>
<p>Let’s draw an analogy to understand the concept of queries, keys, and values in
the context of the attention mechanism. Consider a database <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>
consisting of tuples of keys and values. For instance, the database
<span class="math notranslate nohighlight">\(\mathcal{D}\)</span> might consist of tuples
<code class="docutils literal notranslate"><span class="pre">{(&quot;Zhang&quot;,</span> <span class="pre">&quot;Aston&quot;),</span> <span class="pre">(&quot;Lipton&quot;,</span> <span class="pre">&quot;Zachary&quot;),</span> <span class="pre">(&quot;Li&quot;,</span> <span class="pre">&quot;Mu&quot;),</span> <span class="pre">(&quot;Smola&quot;,</span> <span class="pre">&quot;Alex&quot;),</span> <span class="pre">(&quot;Hu&quot;,</span> <span class="pre">&quot;Rachel&quot;),</span> <span class="pre">(&quot;Werness&quot;,</span> <span class="pre">&quot;Brent&quot;)}</span></code>
with the last name being the key and the first name being the value
<span id="id24">[<a class="reference internal" href="../../bibliography.html#id5" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into Deep Learning. Cambridge University Press, 2023. URL: https://D2L.ai.">Zhang <em>et al.</em>, 2023</a>]</span>. Operations on the database <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> can be performed
using queries <span class="math notranslate nohighlight">\(q\)</span> that operate on the keys and values in the database. More
concretely, if our query is “Li”, or more verbosely, “What is the first name
associated with the last name Li?”, the answer would be “Mu” - the <strong>key</strong>
associated with the <strong>query</strong> “What is the first name associated with the last
name Li?” is “Li”, and the <strong>value</strong> associated with the key “Li” is “Mu”.
Furthermore, if we also allowed for approximate matches, we would retrieve
(“Lipton”, “Zachary”) instead.</p>
<p>More rigorously, we denote
<span class="math notranslate nohighlight">\(\mathcal{D} \stackrel{\text { def }}{=}\left\{\left(\mathbf{k}_1, \mathbf{v}_1\right), \ldots\left(\mathbf{k}_m, \mathbf{v}_m\right)\right\}\)</span>
a database of <span class="math notranslate nohighlight">\(m\)</span> tuples of <em>keys</em> and <em>values</em>, as well as a query
<span class="math notranslate nohighlight">\(\mathbf{q}\)</span>. Then we can define the attention over <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\operatorname{Attention}(\mathbf{q}, \mathcal{D})
\stackrel{\operatorname{def}}{=} \sum_{t=1}^T \alpha\left(\mathbf{q},
\mathbf{k}_t\right) \mathbf{v}_t
\]</div>
<p>where
<span class="math notranslate nohighlight">\(\alpha\left(\mathbf{q}, \mathbf{k}_t\right) \in \mathbb{R}(t=1, \ldots, T)\)</span> are
scalar attention weights <span id="id25">[<a class="reference internal" href="../../bibliography.html#id5" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into Deep Learning. Cambridge University Press, 2023. URL: https://D2L.ai.">Zhang <em>et al.</em>, 2023</a>]</span>. The operation itself is
typically referred to as
<a class="reference external" href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html"><em>attention pooling</em></a>.
The term “attention” is used because this operation focuses specifically on
those terms that have a substantial weight, denoted as <span class="math notranslate nohighlight">\(\alpha\)</span>, meaning it
gives more importance to these terms. Consequently, the attention over
<span class="math notranslate nohighlight">\(\mathcal{D}\)</span> generates a linear combination of values contained in the
database. In fact, this contains the above example as a special case where all
but one weight is zero. Why so? Because the query is an exact match for one of
the keys.</p>
<p>To illustrate why in the case of an exact match within a database the attention
weights (<span class="math notranslate nohighlight">\(\alpha\)</span>) are all zero except for one, let’s use the attention formula
provided and consider a simplified example with vectors.</p>
<div class="proof example admonition" id="decoder-concept-attention-exact-match-scenario">
<p class="admonition-title"><span class="caption-number">Example 3 </span> (Exact Match Scenario)</p>
<section class="example-content" id="proof-content">
<p>Imagine a simplified database <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> consisting of 3 key-value pairs,
where each key <span class="math notranslate nohighlight">\(\mathbf{k}_t\)</span> and the query <span class="math notranslate nohighlight">\(\mathbf{q}\)</span> are represented as
vectors in some high-dimensional space, and the values <span class="math notranslate nohighlight">\(\mathbf{v}_t\)</span> are also
vectors (or can be scalar for simplicity in this example). For simplicity, let’s
assume our vectors are in a 2-dimensional space and represent them as follows:</p>
<ul class="simple">
<li><p>Keys (representing <span class="math notranslate nohighlight">\(3\)</span> keys in the database):</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{k}_1 = [1, 0]\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{k}_2 = [0, 1]\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{k}_3 = [1, 1]\)</span></p></li>
</ul>
</li>
<li><p>Values (corresponding to the keys):</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{v}_1 = [0.1, 0.9]\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{v}_2 = [0.2, 0.8]\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{v}_3 = [0.3, 0.7]\)</span></p></li>
</ul>
</li>
<li><p>Query (looking for an item/concept similar to <span class="math notranslate nohighlight">\(\mathbf{k}_1\)</span>):</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{q} = [1, 0]\)</span></p></li>
</ul>
</li>
</ul>
<p>The attention weights <span class="math notranslate nohighlight">\(\alpha(\mathbf{q}, \mathbf{k}_t)\)</span> indicate how similar or
relevant each key is to the query. In an exact match scenario, the similarity
calculation will result in a high value (e.g., <span class="math notranslate nohighlight">\(1\)</span>) when the query matches a key
exactly, and low values (e.g., <span class="math notranslate nohighlight">\(0\)</span>) otherwise. For simplicity, let’s use a
simple matching criterion where the weight is <span class="math notranslate nohighlight">\(1\)</span> for an exact match and <span class="math notranslate nohighlight">\(0\)</span>
otherwise:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha(\mathbf{q}, \mathbf{k}_1) = 1\)</span> (since
<span class="math notranslate nohighlight">\(\mathbf{q} =
\mathbf{k}_1\)</span>, exact match)</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha(\mathbf{q}, \mathbf{k}_2) = 0\)</span> (since
<span class="math notranslate nohighlight">\(\mathbf{q} \neq
\mathbf{k}_2\)</span>, no match)</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha(\mathbf{q}, \mathbf{k}_3) = 0\)</span> (since
<span class="math notranslate nohighlight">\(\mathbf{q} \neq
\mathbf{k}_3\)</span>, no match)</p></li>
</ul>
<p>Using the attention formula:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned} \operatorname{Attention}(\mathbf{q}, \mathcal{D}) &amp;=
\sum_{t=1}^3 \alpha(\mathbf{q}, \mathbf{k}_t) \mathbf{v}_t \\ &amp;= (1 \cdot
[0.1, 0.9]) + (0 \cdot [0.4, 0.6]) + (0 \cdot [0.7, 0.3]) \\ &amp;= [0.1, 0.9]
\end{aligned}
\end{split}\]</div>
<p>This calculation shows that because the attention weights for <span class="math notranslate nohighlight">\(\mathbf{k}_2\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{k}_3\)</span> are zero (due to no exact match), they don’t contribute to the
final attention output. Only <span class="math notranslate nohighlight">\(\mathbf{k}_1\)</span>, which exactly matches the query,
has a non-zero weight (1), making it the sole contributor to the attention
result. This is a direct consequence of the query being an exact match for one
of the keys, leading to a scenario where “all but one weight is zero.”</p>
</section>
</div></section>
<section id="queries-keys-and-values-in-attention-mechanism">
<h4><a class="toc-backref" href="#id90" role="doc-backlink">Queries, Keys, and Values in Attention Mechanism</a><a class="headerlink" href="#queries-keys-and-values-in-attention-mechanism" title="Link to this heading">#</a></h4>
<p>The database example is a neat analogy to understand the concept of queries,
keys, and values in the context of the attention mechanism. To put things into
perspective, each token <span class="math notranslate nohighlight">\(x_t\)</span> in the input sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> emits three
vectors through projecting its corresponding token and positional embedding
output <span class="math notranslate nohighlight">\(\mathbf{z}_t\)</span>, a query vector <span class="math notranslate nohighlight">\(\mathbf{q}_t\)</span>, a key vector
<span class="math notranslate nohighlight">\(\mathbf{k}_t\)</span>, and a value vector <span class="math notranslate nohighlight">\(\mathbf{v}_t\)</span>. Consider the earlier example
<em>cat walks by the river bank</em>, where each word is a token in the sequence. When
we start to process the first token <span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span>, <em>cat</em>, we would consider a
query vector <span class="math notranslate nohighlight">\(\mathbf{q}_1\)</span>, projected from <span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span>, to be used to
interact with the key vectors <span class="math notranslate nohighlight">\(\mathbf{k}_t\)</span> for <span class="math notranslate nohighlight">\(t \in \{1, 2, \ldots, T\}\)</span>, in
the sequence - determining how much <em>attention</em> “cat” should pay to every other
token in the sequence (including itself). Consequently, it will also emit a key
vector <span class="math notranslate nohighlight">\(\mathbf{k}_1\)</span> so that other tokens can interact with it. Subsequently,
the attention pooling will form a linear combination of the query vector
<span class="math notranslate nohighlight">\(\mathbf{q}_1\)</span> with every other key vector <span class="math notranslate nohighlight">\(\mathbf{k}_t\)</span> in the sequence,</p>
<div class="math notranslate nohighlight">
\[
\alpha(\mathbf{q}_1, \mathbf{k}_t) \in \mathbb{R} = \mathbf{q}_1 \cdot
\mathbf{k}_t \quad \text{for } t \in \{1, 2, \ldots, T\}
\]</div>
<p>and each <span class="math notranslate nohighlight">\(\alpha(\mathbf{q}_1, \mathbf{k}_t)\)</span> will indicate how much attention
the token “cat” should pay to the token at position <span class="math notranslate nohighlight">\(t\)</span> in the sequence. We
would later see that we would add a softmax normalization to the attention
scores to obtain the final attention weights.</p>
<p>We would then use the attention scores <span class="math notranslate nohighlight">\(\alpha(\mathbf{q}_1, \mathbf{k}_t)\)</span> to
create a weighted sum of the value vectors <span class="math notranslate nohighlight">\(\mathbf{v}_t\)</span> to form the new
representation of the token “cat”.</p>
<div class="math notranslate nohighlight">
\[
\operatorname{Attention}(\mathbf{q}_1, \mathbf{k}_t, \mathbf{v}_t) =
\sum_{t=1}^T \alpha(\mathbf{q}_1, \mathbf{k}_t) \mathbf{v}_t
\]</div>
<p>Consequently, the first token must also emit a value vector <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span>. You
can think of the value vector as carrying the actual information or content that
will be aggregated based on the attention scores.</p>
<p>To reiterate, the output
<span class="math notranslate nohighlight">\(\operatorname{Attention}(\mathbf{q}_1, \mathbf{k}_t, \mathbf{v}_t)\)</span> will be the
new representation of the token “cat” in the sequence, which is a weighted sum
of the value vectors <span class="math notranslate nohighlight">\(\mathbf{v}_t\)</span> based on the attention scores
<span class="math notranslate nohighlight">\(\alpha(\mathbf{q}_1, \mathbf{k}_t)\)</span> and now not only holds semantic and
positional information about the token “cat” itself but also contextual
information about the other tokens in the sequence. This allows the token “cat”
to have a better understanding of itself in the context of the whole sentence.
In this whole input sequence, the most ambiguous token is the token “bank” as it
can refer to a financial institution or a river bank. The attention mechanism
will help the token “bank” to understand its context in the sentence - likely
focusing more on the token “river” than the token “cat” or “walks” to understand
its context.</p>
<p>The same process will be repeated for each token in the sequence, where each
token will emit a query vector, a key vector, and a value vector. The attention
scores will be calculated for each token in the sequence, and the weighted sum
of the value vectors will be used to form the new representation of each token
in the sequence.</p>
<p>To end this off, we can intuitively think of the query, key and value as
follows:</p>
<ul class="simple">
<li><p><strong>Query</strong>: What does the token want to know? Maybe to the token <em>bank</em>, it
is trying to figure out if it is a financial institution or a river bank.
But obviously, when considering the token “bank” within such an input
sequence, the query vector generated for “bank” would not actually ask “Am I
a financial institution or a river bank?” but rather would be an abstract
feature vector in a <span class="math notranslate nohighlight">\(D\)</span> dimensional subspace that somehow captures the
potential and context meanings of the token “bank” and once it is used to
interact with the key vectors, it will help to determine later on how much
attention the token “bank” should pay to the other tokens in the sequence.</p></li>
<li><p><strong>Key</strong>: Carrying on from the previous point, if the query vector for the
token “bank” is being matched with the key vectors of the other tokens in
the sequence, the key “river” will be a good match for the query “bank” as
it will help the token “bank” to understand its context in the sentence. In
this subspace, the key vector for “river” will be a good match for the query
because it is more of an “offering” service to the query vector, and it will
know when it is deemed to be important to the query vector. As such, the
vectors in this subspace are able to identify itself as important or not
based on the query vector.</p></li>
<li><p><strong>Value</strong>: The value vector is the actual information or content that will
be aggregated based on the attention scores. If the attention mechanism
determines that “river” is highly relevant to understanding the context of
“bank” within the sentence, the value vector associated with “river” will be
given more weight in the aggregation process. This means that the
characteristics or features encoded in the “river” value vector
significantly influence the representation of the sentence or the specific
context being analyzed.</p></li>
</ul>
</section>
</section>
<section id="linear-projections">
<h3><a class="toc-backref" href="#id91" role="doc-backlink">Linear Projections</a><a class="headerlink" href="#linear-projections" title="Link to this heading">#</a></h3>
<p>We have discussed the concept of queries, keys, and values but have not yet
discussed how these vectors are obtained. As we have continuously emphasized,
the query, key, and value vectors lie in a <span class="math notranslate nohighlight">\(D\)</span>-dimensional subspace, and they
encode various abstract information about the tokens in the sequence.
Consequently, it is no surprise that these vectors are obtained through linear
transformations/projections of the token embeddings <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> using learned
weight matrices <span class="math notranslate nohighlight">\(\mathbf{W}^{\mathbf{Q}}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{W}^{\mathbf{K}}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{W}^{\mathbf{V}}\)</span>.</p>
<div class="proof definition admonition" id="decoder-concept-linear-projections-queries-keys-values">
<p class="admonition-title"><span class="caption-number">Definition 5 </span> (Linear Projections for Queries, Keys, and Values)</p>
<section class="definition-content" id="proof-content">
<p>In the self-attention mechanism, each token embedding
<span class="math notranslate nohighlight">\(\mathbf{z}_t \in \mathbb{R}^{D}\)</span> is projected into a new context vector across
different <strong>subspaces</strong>. This projection is accomplished through three distinct
<strong>linear transformations</strong>, each defined by a unique weight matrix:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{W}^{\mathbf{Q}} \in \mathbb{R}^{D \times d_q}, \quad
\mathbf{W}^{\mathbf{K}} \in \mathbb{R}^{D \times d_k}, \quad
\mathbf{W}^{\mathbf{V}} \in \mathbb{R}^{D \times d_v}
\]</div>
<p>where <span class="math notranslate nohighlight">\(d_q, d_k, d_v \in \mathbb{Z}^+\)</span> are the hidden dimensions of the
subspaces for the query, key, and value vectors, respectively.</p>
<div class="proof remark admonition" id="decoder-concept-linear-projections-queries-keys-values-remark">
<p class="admonition-title"><span class="caption-number">Remark 3 </span> (Dimensionality of the Subspaces)</p>
<section class="remark-content" id="proof-content">
<p>It is worth noting that this post is written in the context of understand
GPT models, and the dimensionality of the query, key, and value vectors are
the same and usually equal to the dimensionality of the token embeddings.
Thus, we may use <span class="math notranslate nohighlight">\(D\)</span> interchangeably to indicate <span class="math notranslate nohighlight">\(d_k, d_v\)</span> and <span class="math notranslate nohighlight">\(d_q\)</span>. This
is not always the case, as encoder-decoder models might have different
dimensionalities for the query, key, and value vectors. However, query and key
must have the same dimensionality for the dot product to work.</p>
</section>
</div><p>Each token embedding <span class="math notranslate nohighlight">\(\mathbf{z}_t\)</span> is transformed into three vectors:</p>
<ul class="simple">
<li><p>The <strong>query vector</strong> <span class="math notranslate nohighlight">\(\mathbf{q}_t\)</span>, representing what the token is looking
for in other parts of the input,</p></li>
<li><p>The <strong>key vector</strong> <span class="math notranslate nohighlight">\(\mathbf{k}_t\)</span>, representing how other tokens can be
found or matched,</p></li>
<li><p>The <strong>value vector</strong> <span class="math notranslate nohighlight">\(\mathbf{v}_t\)</span>, containing the actual information to be
used in the output.</p></li>
</ul>
<p>These transformations are formally defined as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{q}_t = \mathbf{z}_t \mathbf{W}^{Q}, \quad \mathbf{k}_t =
\mathbf{z}_t \mathbf{W}^{K}, \quad \mathbf{v}_t = \mathbf{z}_t \mathbf{W}^{V}
\]</div>
<p>with each residing in <span class="math notranslate nohighlight">\(d_q, d_k, d_v\)</span>-dimensional subspaces, respectively.</p>
<p>Given an input sequence of <span class="math notranslate nohighlight">\(T\)</span> tokens, the individual vectors for each token can
be stacked into matrices:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{Q} = \begin{bmatrix} \mathbf{q}_1 \\ \mathbf{q}_2 \\ \vdots \\
\mathbf{q}_T \end{bmatrix} \in \mathbb{R}^{T \times d_q}, \quad \mathbf{K} =
\begin{bmatrix} \mathbf{k}_1 \\ \mathbf{k}_2 \\ \vdots \\ \mathbf{k}_T
\end{bmatrix} \in \mathbb{R}^{T \times d_k}, \quad \mathbf{V} = \begin{bmatrix}
\mathbf{v}_1 \\ \mathbf{v}_2 \\ \vdots \\ \mathbf{v}_T \end{bmatrix} \in
\mathbb{R}^{T \times d_v}
\end{split}\]</div>
<p>where each row of these matrices corresponds to the query, key, and value
vectors for each token, respectively.</p>
<p>These matrices are generated through simple matrix multiplication of the token
embedding matrix <span class="math notranslate nohighlight">\(\mathbf{Z} \in \mathbb{R}^{T \times D}\)</span> with the weight
matrices
<span class="math notranslate nohighlight">\(\mathbf{W}^{\mathbf{Q}}, \mathbf{W}^{\mathbf{K}}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{W}^{\mathbf{V}}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Q} = \mathbf{Z} \mathbf{W}^{\mathbf{Q}}, \quad \mathbf{K} = \mathbf{Z}
\mathbf{W}^{\mathbf{K}}, \quad \mathbf{V} = \mathbf{Z} \mathbf{W}^{\mathbf{V}}
\]</div>
</section>
</div></section>
<section id="scaled-dot-product-attention">
<h3><a class="toc-backref" href="#id92" role="doc-backlink">Scaled Dot-Product Attention</a><a class="headerlink" href="#scaled-dot-product-attention" title="Link to this heading">#</a></h3>
<section id="id26">
<h4><a class="toc-backref" href="#id93" role="doc-backlink">Definition</a><a class="headerlink" href="#id26" title="Link to this heading">#</a></h4>
<div class="proof definition admonition" id="decoder-concept-scaled-dot-product-attention">
<p class="admonition-title"><span class="caption-number">Definition 6 </span> (Scaled Dot-Product Attention)</p>
<section class="definition-content" id="proof-content">
<p>The attention mechanism is a function that maps a set of queries, keys, and
values to an output, all of which are represented as matrices in a
<span class="math notranslate nohighlight">\(D\)</span>-dimensional space. Specifically, the function is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned} \text{Attention}: \mathbb{R}^{T \times d_q} \times \mathbb{R}^{T
\times d_k} \times \mathbb{R}^{T \times d_v} &amp; \rightarrow \mathbb{R}^{T \times
d_v} \\ (\mathbf{Q}, \mathbf{K}, \mathbf{V}) &amp; \mapsto
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) \end{aligned}
\end{split}\]</div>
<p>where given a query matrix <span class="math notranslate nohighlight">\(\mathbf{Q} \in \mathbb{R}^{T \times d_q}\)</span>, a key
matrix <span class="math notranslate nohighlight">\(\mathbf{K} \in \mathbb{R}^{T \times d_k}\)</span>, and a value matrix
<span class="math notranslate nohighlight">\(\mathbf{V} \in \mathbb{R}^{T \times d_v}\)</span>, the attention mechanism computes the
the output matrix as follows:</p>
<div class="math notranslate nohighlight">
\[
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) =
\text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{\top}}{\sqrt{d_k}}\right)\mathbf{V}
\in \mathbb{R}^{T \times d_v}
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{Q}\mathbf{K}^{\top}\)</span> represents the dot product between the query
and key matrices, resulting in a matrix of scores that indicate the degree
of alignment or relevance between each query and all keys.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sqrt{d_k}\)</span> is a scaling factor used to normalize the scores, preventing them
from becoming too large and ensuring a stable gradient during training. This
scaling factor is particularly important as it helps maintain the softmax
output in a numerically stable range <span id="id27">[<a class="reference internal" href="../../bibliography.html#id19" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017. arXiv:1706.03762.">Vaswani <em>et al.</em>, 2017</a>]</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{softmax}(\cdot)\)</span> is applied row-wise to convert scores into attention
weights, ensuring that for each query, the weights across all keys sum up
to 1. This normalization step allows the mechanism to effectively distribute
focus across the keys according to their relevance to each query.</p></li>
<li><p>The resulting matrix of attention weights is then used to compute a weighted
sum of the values in <span class="math notranslate nohighlight">\(\mathbf{V}\)</span>, producing the output matrix. This output
represents a series of context vectors, each corresponding to a query and
containing aggregated information from the most relevant parts of the input
sequence as determined by the attention weights.</p></li>
</ul>
</section>
</div><p>In what follows, we will break down the components of the attention mechanism
and explain how it works in detail:</p>
<ul class="simple">
<li><p>What is Attention Scoring Function?</p></li>
<li><p>Why Softmax?</p></li>
<li><p>Why Scale by <span class="math notranslate nohighlight">\(\sqrt{d_k}\)</span>?</p></li>
<li><p>What is Context Vector?</p></li>
</ul>
</section>
<section id="attention-scoring-function">
<h4><a class="toc-backref" href="#id94" role="doc-backlink">Attention Scoring Function</a><a class="headerlink" href="#attention-scoring-function" title="Link to this heading">#</a></h4>
<p>In order to know which tokens in the sequence are most relevant to the current
token, we need to calculate the attention scores between the query and key
vectors. Consequently, we would need a scoring function that measures the
influence or contribution of the <span class="math notranslate nohighlight">\(j\)</span>-th position on the <span class="math notranslate nohighlight">\(i\)</span>-th position in the
sequence. This is achieved through the dot product between the query and key
vectors, the reasoning through a
<a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_filter">Gaussian kernel</a> is rigorous and
provides a good
<a class="reference external" href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html">intuition</a>
why we chose the dot product as the scoring function (other than the fact that
it is a measure of similarity).</p>
<div class="proof definition admonition" id="decoder-concept-attention-scoring-function">
<p class="admonition-title"><span class="caption-number">Definition 7 </span> (Attention Scoring Function)</p>
<section class="definition-content" id="proof-content">
<p>Define the attention scoring function <span class="math notranslate nohighlight">\(\alpha(\cdot)\)</span> as a function
that calculates the relevance or influence of each position <span class="math notranslate nohighlight">\(t\)</span> in the sequence
on position <span class="math notranslate nohighlight">\(i\)</span>, known as the attention scores. The attention scoring function
<span class="math notranslate nohighlight">\(\alpha(\cdot)\)</span> is defined using the dot product between query and key
vectors, leveraging its property as a similarity measure.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned} \alpha: \mathbb{R}^{d_q} \times \mathbb{R}^{d_k} &amp; \rightarrow
\mathbb{R} \\ (\mathbf{q}, \mathbf{k}_t) &amp; \mapsto \alpha(\mathbf{q},
\mathbf{k}_t) \end{aligned}
\end{split}\]</div>
<p>Specifically, the function is expressed as:</p>
<div class="math notranslate nohighlight">
\[
\alpha(\mathbf{q}, \mathbf{k}_t) = \langle \mathbf{q}, \mathbf{k}_t \rangle =
\mathbf{q} \cdot \mathbf{k}_t \in \mathbb{R}
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{q}\)</span> is a query vector representing in the sequence, seeking
information or context.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{k}_t\)</span> is the key vector representing the <span class="math notranslate nohighlight">\(t\)</span>-th position in the
sequence, offering context or information.</p></li>
<li><p><span class="math notranslate nohighlight">\(\langle \mathbf{q}, \mathbf{k}_t \rangle\)</span> denotes the dot product between
the query vector <span class="math notranslate nohighlight">\(\mathbf{q}\)</span> and the key vector <span class="math notranslate nohighlight">\(\mathbf{k}_t\)</span>, which
quantifies the level of similarity or alignment between the current position
that <span class="math notranslate nohighlight">\(\mathbf{q}\)</span> is at (say <span class="math notranslate nohighlight">\(i\)</span>-th) and <span class="math notranslate nohighlight">\(t\)</span>-th positions in the sequence.</p></li>
</ul>
<p>The expression <span class="math notranslate nohighlight">\(\mathbf{q} \cdot \mathbf{k}_t\)</span> is a scalar value that indicates
the degree of alignment or relevance between the query at <span class="math notranslate nohighlight">\(i\)</span>-th position and
the key at <span class="math notranslate nohighlight">\(t\)</span>-th position in the sequence. We would need to calculate the
attention scores for each token in the sequence with respect to the query vector
<span class="math notranslate nohighlight">\(\mathbf{q}\)</span>, and the key vectors <span class="math notranslate nohighlight">\(\mathbf{k}_t\)</span> for
<span class="math notranslate nohighlight">\(t \in \{1, 2, \ldots, T\}\)</span>.</p>
<p>So this leads us to:</p>
<div class="math notranslate nohighlight">
\[
\alpha(\mathbf{q}, \mathbf{K}) = \mathbf{q}\mathbf{K}^{\top} \in \mathbb{R}^{1
\times T}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{K} = \begin{bmatrix} \mathbf{k}_1 \\ \mathbf{k}_2 \\ \vdots \\
\mathbf{k}_T \end{bmatrix} \in \mathbb{R}^{T \times d_k}
\end{split}\]</div>
<p>is the matrix of key vectors for each token in the sequence, and the output
<span class="math notranslate nohighlight">\(\alpha(\mathbf{q}, \mathbf{K}) \in \mathbb{R}^{1 \times T}\)</span> is a row
vector of attention scores for the query vector <span class="math notranslate nohighlight">\(\mathbf{q}\)</span> with respect to
each key vector <span class="math notranslate nohighlight">\(\mathbf{k}_t\)</span> for <span class="math notranslate nohighlight">\(t \in \{1, 2, \ldots, T\}\)</span>.</p>
<p>Lastly, there are <span class="math notranslate nohighlight">\(T\)</span> such queries in the input sequence <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>, and we
can stack all the query vectors <span class="math notranslate nohighlight">\(\mathbf{q}_t\)</span> into a matrix
<span class="math notranslate nohighlight">\(\mathbf{Q} \in \mathbb{R}^{T \times d_q}\)</span> to calculate the attention scores for
all the queries in the sequence with respect to all the key vectors in the
sequence.</p>
<div class="math notranslate nohighlight">
\[
\alpha(\mathbf{Q}, \mathbf{K}) = \mathbf{Q}\mathbf{K}^{\top} \in \mathbb{R}^{T
\times T}
\]</div>
<p>To this end, each row of the matrix <span class="math notranslate nohighlight">\(\mathbf{Q}\mathbf{K}^{\top}\)</span> represents the
attention scores for each query vector at position <span class="math notranslate nohighlight">\(i\)</span> in the sequence with
respect to all the key vectors in the sequence.</p>
</section>
</div></section>
<section id="scaling-down-the-dot-product-of-query-and-key-vectors">
<h4><a class="toc-backref" href="#id95" role="doc-backlink">Scaling Down the Dot Product of Query and Key Vectors</a><a class="headerlink" href="#scaling-down-the-dot-product-of-query-and-key-vectors" title="Link to this heading">#</a></h4>
<div class="proof definition admonition" id="decoder-concept-query-key-iid">
<p class="admonition-title"><span class="caption-number">Definition 8 </span> (Query and Key are Independent and Identically Distributed (i.i.d.))</p>
<section class="definition-content" id="proof-content">
<p>Under the assumption of the query <span class="math notranslate nohighlight">\(\mathbf{q}\)</span> and key <span class="math notranslate nohighlight">\(\mathbf{k}_t\)</span> are
<em><strong>independent and identically distributed</strong></em> (i.i.d.) random variables with a
gaussian distribution of mean <span class="math notranslate nohighlight">\(0\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{q} \overset{\mathrm{iid}}{\sim} \mathcal{N}(0, \sigma^2), \quad
\mathbf{k}_t \overset{\mathrm{iid}}{\sim} \mathcal{N}(0, \sigma^2)
\]</div>
</section>
</div><div class="proof definition admonition" id="decoder-concept-variance-dot-product">
<p class="admonition-title"><span class="caption-number">Definition 9 </span> (Variance of Dot Product)</p>
<section class="definition-content" id="proof-content">
<p>Given that <span class="math notranslate nohighlight">\(\mathbf{q} \overset{\mathrm{iid}}{\sim} \mathcal{N}(0, \sigma^2), \quad \mathbf{k}_t \overset{\mathrm{iid}}{\sim} \mathcal{N}(0, \sigma^2)\)</span>,
the variance of the dot product between the query vector <span class="math notranslate nohighlight">\(\mathbf{q}\)</span> and the key
vector <span class="math notranslate nohighlight">\(\mathbf{k}_t\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{V}[\mathbf{q} \cdot \mathbf{k}_t] = \sum_{i=1}^{d_k} \mathbb{V}[q_i
k_{ti}] = d_k \cdot \sigma^4.
\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. The dot product between <span class="math notranslate nohighlight">\(\mathbf{q}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{k}_t\)</span> can be expressed as the
sum of the products of their components:</p>
<div class="math notranslate nohighlight">
\[\mathbf{q} \cdot \mathbf{k}_t = \sum_{i=1}^{d_k} q_i k_{ti},\]</div>
<p>where <span class="math notranslate nohighlight">\(q_i\)</span> and <span class="math notranslate nohighlight">\(k_{ti}\)</span> are the <span class="math notranslate nohighlight">\(i\)</span>-th components of <span class="math notranslate nohighlight">\(\mathbf{q}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{k}_t\)</span>, respectively.</p>
<p>The variance of the sum of random variables (when these variables are
independent, which is our case since components are iid) is the sum of their
variances. The product <span class="math notranslate nohighlight">\(q_i k_{ti}\)</span> is a new random variable, and its variance
can be calculated as follows for a single pair of components:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{V}[q_i k_{ti}] = \mathbb{E}[(q_i k_{ti})^2] - (\mathbb{E}[q_i
k_{ti}])^2.
\]</div>
<p>Given that <span class="math notranslate nohighlight">\(q_i\)</span> and <span class="math notranslate nohighlight">\(k_{ti}\)</span> are independent and both have mean 0:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[q_i k_{ti}] = \mathbb{E}[q_i] \cdot \mathbb{E}[k_{ti}] = 0.\]</div>
<p>The expectation of the square of the product is:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[(q_i k_{ti})^2] = \mathbb{E}[q_i^2] \cdot \mathbb{E}[k_{ti}^2] =
\sigma^2 \cdot \sigma^2 = \sigma^4.
\]</div>
<p>Since <span class="math notranslate nohighlight">\(\mathbb{E}[q_i k_{ti}] = 0\)</span>, the variance of the product <span class="math notranslate nohighlight">\(q_i k_{ti}\)</span> is
simply <span class="math notranslate nohighlight">\(\sigma^4\)</span>.</p>
<p>For the dot product, we sum across all <span class="math notranslate nohighlight">\(d_k\)</span> components, and since the variance
of the sum of independent random variables is the sum of their variances:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{V}[\mathbf{q} \cdot \mathbf{k}_t] = \sum_{i=1}^{d_k} \mathbb{V}[q_i
k_{ti}] = d_k \cdot \sigma^4.
\]</div>
</div>
<p>We want to ensure that the variance of the dot product still remains the same as
the variance of the query and key vectors at <span class="math notranslate nohighlight">\(\sigma^2\)</span> regardless of the vector
dimensions. To do so, we scale down the dot product by <span class="math notranslate nohighlight">\(\sqrt{d_k}\)</span>, which is
the square root of the dimensionality of the key vectors, this operation would
scale the variance of the dot product down by <span class="math notranslate nohighlight">\(\sqrt{d_k}^2 = d_k\)</span> (since
variance of a scaled random variable is the square of the scale factor times the
original variance).</p>
<p>Now our variance would be <span class="math notranslate nohighlight">\(\sigma^4\)</span> - but it is still not the same as the
variance of the query and key vectors. This is okay because the original paper
assume the variance <span class="math notranslate nohighlight">\(\sigma^2 = 1\)</span> <span id="id28">[<a class="reference internal" href="../../bibliography.html#id19" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017. arXiv:1706.03762.">Vaswani <em>et al.</em>, 2017</a>]</span>, and therefore
it does not matter since <span class="math notranslate nohighlight">\(\sigma^2 = \sigma^4\)</span> when <span class="math notranslate nohighlight">\(\sigma^2 = 1\)</span>.</p>
<div class="proof definition admonition" id="decoder-concept-attention-scoring-function-with-scaling">
<p class="admonition-title"><span class="caption-number">Definition 10 </span> (Attention Scoring Function with Scaling)</p>
<section class="definition-content" id="proof-content">
<p>To this end, the updated scoring function is:</p>
<div class="math notranslate nohighlight">
\[
\alpha(\mathbf{Q}, \mathbf{K}) = \frac{\mathbf{Q}\mathbf{K}^{\top}}{\sqrt{d_k}}
\in \mathbb{R}^{T \times T}
\]</div>
</section>
</div><p>Before we look into the reason why we scale down the dot product, let’s first
complete the final block of the attention mechanism, which is the softmax
normalization.</p>
</section>
<section id="softmax">
<h4><a class="toc-backref" href="#id96" role="doc-backlink">Softmax</a><a class="headerlink" href="#softmax" title="Link to this heading">#</a></h4>
<div class="proof definition admonition" id="decoder-concept-attention-scores">
<p class="admonition-title"><span class="caption-number">Definition 11 </span> (Attention Scores)</p>
<section class="definition-content" id="proof-content">
<p>Currently the attention scores <span class="math notranslate nohighlight">\(\alpha(\mathbf{Q}, \mathbf{K})\)</span> are raw scores
that indicate the degree of alignment or relevance between each query and all
keys. They can be negative or positive, and they can be large or small. We
denote them as the raw <strong>attention scores</strong> <span class="math notranslate nohighlight">\(\alpha(\mathbf{Q}, \mathbf{K}) \in
\mathbb{R}^{T \times T}\)</span>.</p>
</section>
</div><div class="proof definition admonition" id="decoder-concept-softmax-normalization-attention-weights">
<p class="admonition-title"><span class="caption-number">Definition 12 </span> (Softmax Normalization and Attention Weights)</p>
<section class="definition-content" id="proof-content">
<p>It is common in deep learning to form a convex combination <span id="id29">[<a class="reference internal" href="../../bibliography.html#id5" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into Deep Learning. Cambridge University Press, 2023. URL: https://D2L.ai.">Zhang <em>et al.</em>, 2023</a>]</span>
of the attention scores <span class="math notranslate nohighlight">\(\alpha(\mathbf{Q}, \mathbf{K})\)</span> to obtain the
<strong>attention weights</strong>, denoted as <span class="math notranslate nohighlight">\(\text{softmax}(\alpha(\mathbf{Q}, \mathbf{K}))\)</span>, which
are non-negative and sum to <span class="math notranslate nohighlight">\(1\)</span>. This is achieved through the softmax
normalization function, which is defined as:</p>
<div class="math notranslate nohighlight">
\[
\text{softmax}(\alpha(\mathbf{Q}, \mathbf{K})) = \frac{\exp(\alpha(\mathbf{Q},
\mathbf{K}))}{\sum_{t=1}^T \exp(\alpha(\mathbf{Q}, \mathbf{k}_t))} \in
\mathbb{R}^{T \times T}
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\exp(\cdot)\)</span> is the exponential function, which is applied element-wise to
the raw attention scores <span class="math notranslate nohighlight">\(\alpha(\mathbf{Q}, \mathbf{K})\)</span>.</p></li>
<li><p>The denominator is the sum of the exponentials of the raw attention scores
across the <span class="math notranslate nohighlight">\(T\)</span> keys, ensuring that the attention weights sum to <span class="math notranslate nohighlight">\(1\)</span> for
each query, allowing the mechanism to effectively distribute focus across
the keys according to their relevance to each query.</p></li>
</ul>
</section>
</div><p>The choice of softmax is a convenient choice, but not the only choice. However,
it is convenient because it is both <em>differentiable</em>, which is often a desirable
property for training deep learning models that are optimized using
gradient-based methods, and it is also <em>monotonic</em>, which means that the
<strong>attention weights</strong> are preserved exactly in the order as the raw <strong>attention
scores</strong>.</p>
<div class="proof definition admonition" id="decoder-concept-attention-scoring-function-with-scaling-softmax">
<p class="admonition-title"><span class="caption-number">Definition 13 </span> (Attention Scoring Function with Scaling and Softmax)</p>
<section class="definition-content" id="proof-content">
<p>To this end, our final attention scoring function is:</p>
<div class="math notranslate nohighlight">
\[
\alpha(\mathbf{Q}, \mathbf{K}) =
\text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{\top}}{\sqrt{d_k}}\right) \in
\mathbb{R}^{T \times T}
\]</div>
</section>
</div></section>
<section id="context-vector-matrix">
<h4><a class="toc-backref" href="#id97" role="doc-backlink">Context Vector/Matrix</a><a class="headerlink" href="#context-vector-matrix" title="Link to this heading">#</a></h4>
<p>Consequently, we complete the walkthrough of the scaled dot-product attention
mechanism by calculating the context vector, which is the weighted sum of the
value vectors based on the attention weights obtained from the softmax
normalization.</p>
<div class="proof definition admonition" id="decoder-concept-context-vector-matrix">
<p class="admonition-title"><span class="caption-number">Definition 14 </span> (Context Vector/Matrix)</p>
<section class="definition-content" id="proof-content">
<p>Given the attention weights <span class="math notranslate nohighlight">\(\alpha(\mathbf{Q}, \mathbf{K})\)</span> and the value
matrix <span class="math notranslate nohighlight">\(\mathbf{V}\)</span>, the context vector <span class="math notranslate nohighlight">\(\mathbf{C}\)</span> is defined as the output
of the scaled dot-product attention mechanism:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{C} :=
\text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{\top}}{\sqrt{d_k}}\right)\mathbf{V}
\in \mathbb{R}^{T \times d_v}
\]</div>
<p>where each row <span class="math notranslate nohighlight">\(\mathbf{c}_t\)</span> of the context matrix <span class="math notranslate nohighlight">\(\mathbf{C}\)</span> is the
new embedding of the token at position <span class="math notranslate nohighlight">\(t\)</span> in the sequence, containing
not only the semantic and positional information of the token itself, but also
contextual information from the other tokens in the sequence.</p>
</section>
</div></section>
<section id="numerical-stability-and-gradient-saturation">
<h4><a class="toc-backref" href="#id98" role="doc-backlink">Numerical Stability and Gradient Saturation</a><a class="headerlink" href="#numerical-stability-and-gradient-saturation" title="Link to this heading">#</a></h4>
<p>We can now revisit on the underlying reason why we scale down the dot product
<span class="math notranslate nohighlight">\(\mathbf{Q}\mathbf{K}^{\top}\)</span> by <span class="math notranslate nohighlight">\(\sqrt{d_k}\)</span>.</p>
<p>First, the softmax function has all the desirable properties we want,
<em>smoothness</em>, <em>monotonicity</em>, and <em>differentiability</em>, but it is <em>sensitive</em> to
large input values.</p>
<p>The softmax function is defined as follows for a given logit <span class="math notranslate nohighlight">\(z_i\)</span> among a set
of logits <span class="math notranslate nohighlight">\(Z\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
\]</div>
<p>If the variance of the logits before applying softmax is too large (not scaled
down to a more manageable range), the exponential function <span class="math notranslate nohighlight">\(e^{z_i}\)</span> can lead to
extremely large output values for any <span class="math notranslate nohighlight">\(z_i\)</span> that is even slightly larger than
others in the set. This is due to the exponential function’s rapid growth with
respect to its input value.</p>
<div class="proof remark admonition" id="decoder-concept-gradient-saturation">
<p class="admonition-title"><span class="caption-number">Remark 4 </span> (Gradient Saturation)</p>
<section class="remark-content" id="proof-content">
<ul class="simple">
<li><p><strong>For one random element:</strong> If one of the logits <span class="math notranslate nohighlight">\(z_i\)</span> is significantly
larger than the others (which is more likely when the variance of the logits
is high), <span class="math notranslate nohighlight">\(e^{z_i}\)</span> will dominate the numerator and denominator of the
softmax function for this logit. This will cause the softmax output for this
logit to approach 1, as it essentially overshadows all other <span class="math notranslate nohighlight">\(e^{z_j}\)</span> terms
in the denominator.</p></li>
<li><p><strong>For all others:</strong> Simultaneously, the softmax outputs for all other logits
<span class="math notranslate nohighlight">\(z_j\)</span> (where <span class="math notranslate nohighlight">\(j \neq i\)</span>) will approach 0, because their <span class="math notranslate nohighlight">\(e^{z_j}\)</span>
contributions to the numerator will be negligible compared to <span class="math notranslate nohighlight">\(e^{z_i}\)</span> in
the denominator. Thus, the attention mechanism would almost exclusively
focus on the token corresponding to the dominant logit, ignoring valuable
information from other parts of the input sequence.</p></li>
<li><p>Furthermore, the gradients through the softmax function will be very small
(close to zero) for all logits except the dominant one, which can lead to
<em>gradient saturation</em> and even <em>vanishing gradients</em> during training.</p></li>
</ul>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Without scaling: large inputs</span>
<span class="n">logits_large</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">softmax_large</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">logits_large</span><span class="p">)</span>

<span class="n">d_k</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">scaling_factor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">d_k</span><span class="p">))</span>
<span class="n">scaled_logits</span> <span class="o">=</span> <span class="n">logits_large</span> <span class="o">/</span> <span class="n">scaling_factor</span>
<span class="n">softmax_scaled</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">scaled_logits</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Softmax without scaling:&quot;</span><span class="p">,</span> <span class="n">softmax_large</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Softmax with scaling:&quot;</span><span class="p">,</span> <span class="n">softmax_scaled</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Softmax without scaling: tensor([2.0611e-09, 4.5398e-05, 9.9995e-01])
Softmax with scaling: tensor([0.2010, 0.3126, 0.4864])
</pre></div>
</div>
</div>
</div>
<p>As we can see, a vector with large inputs can lead to a <em>sharpening</em> effect on
the output of the softmax function, essentially causing the output to be too
peaky, converging to 1 for the largest input and 0 for the rest (one-hot).</p>
<div class="proof remark admonition" id="decoder-concept-numerical-stability">
<p class="admonition-title"><span class="caption-number">Remark 5 </span> (Numerical Stability)</p>
<section class="remark-content" id="proof-content">
<p>We know the importance of weight initialization in deep learning models,
this is because it dictates the variance of the activations and gradients
throughout the network. Without going into the theory, it is intuitive
to think that having similar variance across all layer activations is
a desirable property for numerical stability.
By doing so, the model helps to ensure that the gradients are stable
during backpropagation, avoiding the vanishing or exploding gradients problem
and enabling effective learning.</p>
<p>In the specific context of the attention mechanism, the variance of the dot
products used to calculate attention scores is scaled down by the factor
<span class="math notranslate nohighlight">\(\frac{1}{\sqrt{d_k}}\)</span> to prevent softmax saturation. This allows each element
to have a chance to influence the model’s learning, rather than having a single
element dominate because of the variance scaling with <span class="math notranslate nohighlight">\(d_k\)</span>.</p>
</section>
</div></section>
<section id="visualizing-variance-of-dot-product">
<h4><a class="toc-backref" href="#id99" role="doc-backlink">Visualizing Variance of Dot Product</a><a class="headerlink" href="#visualizing-variance-of-dot-product" title="Link to this heading">#</a></h4>
<p>If we set <span class="math notranslate nohighlight">\(d_k = 512\)</span>, and mean <span class="math notranslate nohighlight">\(0\)</span> with unit variance, we will see in action
that indeed the scaled dot product has a variance of <span class="math notranslate nohighlight">\(1\)</span> while the unscaled dot
product has a variance of <span class="math notranslate nohighlight">\(512\)</span>, which coincides with our theoretical analysis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seed_all</span><span class="p">(</span><span class="mi">92</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

<span class="c1"># Set the dimensionality of the keys and queries</span>
<span class="n">d_k</span> <span class="o">=</span> <span class="mi">512</span>
<span class="c1"># Set the batch size, number of heads, and sequence length</span>
<span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">L</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">32</span>
<span class="c1"># Standard deviation for initialization</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mf">1.0</span>

<span class="c1"># Initialize Q and K with variance sigma^2</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">d_k</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigma</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">d_k</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigma</span>

<span class="c1"># Calculate dot products without scaling</span>
<span class="n">unscaled_dot_products</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Calculate the variance of the unscaled dot products</span>
<span class="n">unscaled_variance</span> <span class="o">=</span> <span class="n">unscaled_dot_products</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Apply the scaling factor 1 / sqrt(d_k)</span>
<span class="n">scaled_dot_products</span> <span class="o">=</span> <span class="n">unscaled_dot_products</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>

<span class="c1"># Calculate the variance of the scaled dot products</span>
<span class="n">scaled_variance</span> <span class="o">=</span> <span class="n">scaled_dot_products</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unscaled Variance: </span><span class="si">{</span><span class="n">unscaled_variance</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Scaled Variance: </span><span class="si">{</span><span class="n">scaled_variance</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Apply softmax to the scaled and unscaled dot products</span>
<span class="n">softmax_unscaled</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">unscaled_dot_products</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">softmax_scaled</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scaled_dot_products</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Unscaled Variance: 512.0117797851562
Scaled Variance: 1.0000230073928833
</pre></div>
</div>
</div>
</div>
</section>
<section id="projections-lead-to-dynamic-context-vectors">
<h4><a class="toc-backref" href="#id100" role="doc-backlink">Projections Lead to Dynamic Context Vectors</a><a class="headerlink" href="#projections-lead-to-dynamic-context-vectors" title="Link to this heading">#</a></h4>
<p>From the start, we mentioned <em>the attention mechanism describes a <strong>weighted
average</strong> of (sequence) elements with the weights <strong>dynamically</strong> computed based
on an input query and elements’ keys</em>. We can easily see the <strong>weighted
average</strong> part through self-attention. The <strong>dynamic</strong> part comes from the fact
that the context vectors are computed based on the input query and its
corresponding keys. There should be no confusion that all the learnable weights
in this self-attention mechanism are the weight matrices
<span class="math notranslate nohighlight">\(\mathbf{W}^{\mathbf{Q}}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{W}^{\mathbf{K}}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{W}^{\mathbf{V}}\)</span>, but the dynamic is really because the scoring
function uses a dot product <span class="math notranslate nohighlight">\(\mathbf{Q}\mathbf{K}^{\top}\)</span>, which is <strong>dynamic</strong>
because it is solely decided by the full input sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Unlike
static embeddings, where the word “cat” will always have the same embedding
vector, the context vector for the word “cat” will be different in different
sentences because it now depends on the full input sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<p>Consequently, the projection of the token embeddings into the query and key
space is needed.</p>
</section>
<section id="id30">
<h4><a class="toc-backref" href="#id101" role="doc-backlink">Implementation</a><a class="headerlink" href="#id30" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">ABC</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;The `forward` method must be implemented by the subclass.&quot;</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ScaledDotProductAttention</span><span class="p">(</span><span class="n">Attention</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">debug</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="c1"># fmt: off</span>
        <span class="n">d_q</span>               <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">attention_scores</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">dim0</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">d_q</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
        <span class="n">attention_scores</span>  <span class="o">=</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span> <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">attention_scores</span>

        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">)</span>

        <span class="n">context_vector</span>    <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="c1"># fmt: on</span>
        <span class="k">if</span> <span class="n">debug</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">context_vector</span><span class="p">,</span> <span class="n">attention_weights</span><span class="p">,</span> <span class="n">attention_scores</span>
        <span class="k">return</span> <span class="n">context_vector</span><span class="p">,</span> <span class="n">attention_weights</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">512</span>  <span class="c1"># batch size, head, context length, embedding dimension</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>  <span class="c1"># query</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>  <span class="c1"># key</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>  <span class="c1"># value</span>

<span class="c1"># Scaled Dot-Product Attention</span>
<span class="n">attention</span> <span class="o">=</span> <span class="n">ScaledDotProductAttention</span><span class="p">(</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">context_vector</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">context_vector</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">attention_weights</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">L</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">context_vector</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">attention_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># assert each row of attention_weights sums to 1</span>
<span class="c1"># assert each element of attention_weights is between 0 and 1</span>
<span class="n">attention_weights_summed_over_sequences</span> <span class="o">=</span> <span class="n">attention_weights</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span>
    <span class="n">attention_weights_summed_over_sequences</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">L</span><span class="p">)</span>
<span class="p">),</span> <span class="s2">&quot;The attention weights distribution induced by softmax should sum to 1.&quot;</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">attention_weights</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">attention_weights</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">),</span> <span class="s2">&quot;All attention weights should be between 0 and 1.&quot;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">512</span><span style="font-weight: bold">])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">32</span><span style="font-weight: bold">])</span>
</pre>
</div></div>
</div>
</section>
<section id="heatmap">
<h4><a class="toc-backref" href="#id102" role="doc-backlink">Heatmap</a><a class="headerlink" href="#heatmap" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">show_attention_heatmaps</span><span class="p">(</span>
    <span class="n">attention_weights</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">xlabel</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Keys&quot;</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;Queries&quot;</span><span class="p">,</span>
    <span class="n">show_title</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">figure_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">plot_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">attention_weights</span><span class="o">.</span><span class="n">shape</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">attention_weights</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">figure_kwargs</span> <span class="o">=</span> <span class="n">figure_kwargs</span> <span class="ow">or</span> <span class="p">{</span><span class="s2">&quot;figsize&quot;</span><span class="p">:</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">),</span> <span class="s2">&quot;sharex&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;sharey&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;squeeze&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">}</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="o">**</span><span class="n">figure_kwargs</span><span class="p">)</span>

    <span class="n">plot_kwargs</span> <span class="o">=</span> <span class="n">plot_kwargs</span> <span class="ow">or</span> <span class="p">{</span><span class="s2">&quot;cmap&quot;</span><span class="p">:</span> <span class="s2">&quot;viridis&quot;</span><span class="p">}</span>

    <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="p">(</span><span class="n">row_axes</span><span class="p">,</span> <span class="n">attention_weight</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">attention_weights</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">h</span><span class="p">,</span> <span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">head_attention</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">row_axes</span><span class="p">,</span> <span class="n">attention_weight</span><span class="p">)):</span>
            <span class="n">pcm</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">head_attention</span><span class="p">,</span> <span class="o">**</span><span class="n">plot_kwargs</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">b</span> <span class="o">==</span> <span class="n">B</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="p">)</span>  <span class="c1"># Only the last batch will have the xlabel</span>
            <span class="k">if</span> <span class="n">h</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">)</span>  <span class="c1"># Only the first head will have the ylabel</span>

            <span class="k">if</span> <span class="n">show_title</span><span class="p">:</span>
                <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Batch </span><span class="si">{</span><span class="n">b</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">, Head </span><span class="si">{</span><span class="n">h</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">pcm</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">,</span> <span class="n">shrink</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Connecting back to our earlier database query, the following identity matrix
would represent when the <em>query</em> and the <em>key</em> to be an exact match, indicated here
naively as if query is 0, then key is also 0, and so on. And thus
the weight matrix would have a diagonal of 1s and 0s elsewhere.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">show_attention_heatmaps</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Keys&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Queries&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/ead8eb659d8f788017c5be09f2c6368b7f35371ab9724d5412800cd4fe06ec0a.png" src="../../_images/ead8eb659d8f788017c5be09f2c6368b7f35371ab9724d5412800cd4fe06ec0a.png" />
</div>
</div>
</section>
<section id="masked-causal-self-attention">
<h4><a class="toc-backref" href="#id103" role="doc-backlink">Masked/Causal Self-Attention</a><a class="headerlink" href="#masked-causal-self-attention" title="Link to this heading">#</a></h4>
<p>In the context of GPT models, which is a decoder-only architecture, the
self-attention mechanism is often referred to as <strong>masked self-attention</strong> or
<strong>causal attention</strong>. The reason is that the attention mechanism is masked to
prevent information flow from future tokens to the current token. Given the
autoregressive and self-supervised nature of the GPT models, the prediction for
the current token should not be influenced by future tokens, as they are not
known during inference.</p>
<p>First, let’s connect to our earlier example of <code class="docutils literal notranslate"><span class="pre">z_0_tok_embed_with_pos_embed</span></code> to
see the non-masked self-attention mechanism in action.
We would create weights <span class="math notranslate nohighlight">\(\mathbf{W}^{\mathbf{Q}}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{W}^{\mathbf{K}}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{W}^{\mathbf{V}}\)</span> and project the token embeddings into the query, key
and value space. We then pass the query, key and value matrices into the scaled
dot-product attention mechanism to obtain the context matrix <span class="math notranslate nohighlight">\(\mathbf{C}\)</span>
as well as the attention weights <span class="math notranslate nohighlight">\(\alpha(\mathbf{Q}, \mathbf{K})\)</span>.
Note since it is for only one sample, we would use lower case
letters instead.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">seed_all</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">set_torch_deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">d_model</span>  <span class="c1"># batch size, head, context length, embedding dimension</span>
<span class="n">W_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">W_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">W_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">q0</span> <span class="o">=</span> <span class="n">W_q</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed</span><span class="p">)</span>
<span class="n">k0</span> <span class="o">=</span> <span class="n">W_k</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed</span><span class="p">)</span>
<span class="n">v0</span> <span class="o">=</span> <span class="n">W_v</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed</span><span class="p">)</span>

<span class="c1"># Scaled Dot-Product Attention</span>
<span class="n">attention</span> <span class="o">=</span> <span class="n">ScaledDotProductAttention</span><span class="p">(</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">context_vector</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">q0</span><span class="p">,</span> <span class="n">k0</span><span class="p">,</span> <span class="n">v0</span><span class="p">)</span>
<span class="n">attention_weights</span> <span class="o">=</span> <span class="n">attention_weights</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>

<span class="n">show_attention_heatmaps</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Keys&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Queries&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/48b7d441892610bad62156b85a2c7bb21c3652ed7e1478eb1ff16eef776b274b.png" src="../../_images/48b7d441892610bad62156b85a2c7bb21c3652ed7e1478eb1ff16eef776b274b.png" />
</div>
</div>
<p>However, there is just one problem. We are allowing the self-attention to attend
each token to all other tokens in the sequence, including itself. Since we have
<span class="math notranslate nohighlight">\(T\)</span> tokens in the sequence, each sub-sequence <span class="math notranslate nohighlight">\(\mathbf{x}_{1:t}\)</span> would look into
the future sub-sequence <span class="math notranslate nohighlight">\(\mathbf{x}_{t+1:T}\)</span>, which is trivial now for the model
to learn.</p>
<div class="note admonition">
<p class="admonition-title">Naive Way To Handle Future Mask</p>
<p>Naively, we can dissect the input sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> into <span class="math notranslate nohighlight">\(T\)</span> sub-sequences,
and run the self-attention <span class="math notranslate nohighlight">\(T\)</span> times independently. This means we are really
treating one sequence as <span class="math notranslate nohighlight">\(T\)</span> sub-samples. But this is inefficient.</p>
</div>
<p>To fix this, we introduce a <em>mask</em>, where the mask effectively zeros out the
attention scores for the future tokens in the sequence. This would result in the
usage of an upper-triangular matrix for the attention scores, and the softmax
would then zero out the future tokens in the sequence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">seed_all</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">set_torch_deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">d_model</span>  <span class="c1"># batch size, head, context length, embedding dimension</span>
<span class="n">W_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">W_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">W_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">q0</span> <span class="o">=</span> <span class="n">W_q</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed</span><span class="p">)</span>
<span class="n">k0</span> <span class="o">=</span> <span class="n">W_k</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed</span><span class="p">)</span>
<span class="n">v0</span> <span class="o">=</span> <span class="n">W_v</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed</span><span class="p">)</span>

<span class="n">tril_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">))</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">tril_mask</span><span class="p">)</span>

<span class="c1"># Scaled Dot-Product Attention</span>
<span class="n">attention</span> <span class="o">=</span> <span class="n">ScaledDotProductAttention</span><span class="p">(</span><span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">context_vector</span><span class="p">,</span> <span class="n">attention_weights</span><span class="p">,</span> <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">q0</span><span class="p">,</span> <span class="n">k0</span><span class="p">,</span> <span class="n">v0</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">tril_mask</span><span class="p">,</span> <span class="n">debug</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">attention_scores</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">)</span>

<span class="n">attention_weights</span> <span class="o">=</span> <span class="n">attention_weights</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
<span class="n">show_attention_heatmaps</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Keys&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Queries&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>, <span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>,  <span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5720</span>,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf<span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4732</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1816</span>,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf<span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2072</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6271</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5613</span>,    -inf,    -inf,    -inf,    -inf,    -inf<span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1750</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4334</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2993</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2260</span>,    -inf,    -inf,    -inf,    -inf<span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2643</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4695</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2959</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1012</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5819</span>,    -inf,    -inf,    -inf<span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1332</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.6132</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4208</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.7334</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5362</span>,    -inf,    -inf<span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0580</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3659</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2399</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5044</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7807</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.8140</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4476</span>,    -inf<span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2860</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2907</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1349</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2604</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2047</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0672</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1039</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2621</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│      </span><span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">SqueezeBackward1</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6581</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3419</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1832</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4218</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3950</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1854</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3406</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2979</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1762</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1877</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2305</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1937</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1302</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2579</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1596</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0987</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1197</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2227</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0876</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3117</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2518</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1260</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1111</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1448</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1908</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0387</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1368</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0000</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1705</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0958</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1119</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1662</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1044</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1370</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1155</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0986</span><span style="font-weight: bold">]]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│      </span><span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">SoftmaxBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div><img alt="../../_images/b71cd4c260b07e16c9f9e613e988daf5eedfe64305732e7a3d7987b38750d3c6.png" src="../../_images/b71cd4c260b07e16c9f9e613e988daf5eedfe64305732e7a3d7987b38750d3c6.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x0_decoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">x0</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">x0_decoded</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #008000; text-decoration-color: #008000">' priest and clerk? well then, amen'</span>
</pre>
</div></div>
</div>
<p>From the print logs and plots above, it is pretty clear how for one sequence,
the future mask is working. Consider our decoded first sample
<code class="docutils literal notranslate"><span class="pre">priest</span> <span class="pre">and</span> <span class="pre">clerk?</span> <span class="pre">well</span> <span class="pre">then,</span> <span class="pre">amen</span></code>. The result of <span class="math notranslate nohighlight">\(\mathbf{Q}\mathbf{K}^{\top}\)</span>
would be a matrix of attention weights of size <span class="math notranslate nohighlight">\(T \times T\)</span>. As we mentioned
earlier, if we do not mask, then for instance, the second row being the token
<code class="docutils literal notranslate"><span class="pre">and</span></code> would have information on every token, including <code class="docutils literal notranslate"><span class="pre">clerk</span></code>, <code class="docutils literal notranslate"><span class="pre">amen</span></code> etc. This
is considered <em>leakage</em> of information.</p>
<p>So when we provide such a triangular mask:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\text{mask} = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; \dots &amp; 0 \\
1 &amp; 1 &amp; 0 &amp; \dots &amp; 0 \\
1 &amp; 1 &amp; 1 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; 1 &amp; 1 &amp; \dots &amp; 1 \\
\end{bmatrix}
\end{split}\]</div>
<p>and apply it to the attention scores, we would see that the attention scores
would have <span class="math notranslate nohighlight">\(-\infty\)</span> for the future tokens,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{masked_attention_scores} &amp;= \text{attention_scores} \odot \text{mask} + (1 - \text{mask}) \cdot (-\infty) \\
&amp;= \begin{bmatrix}
q_1 \cdot k_1 &amp; -\infty &amp; -\infty &amp; \dots &amp; -\infty \\
q_2 \cdot k_1  &amp; q_2 \cdot k_2 &amp; -\infty &amp; \dots &amp; -\infty \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
q_T \cdot k_1 &amp; q_T \cdot k_2 &amp; q_T \cdot k_3 &amp; \dots &amp; q_T \cdot k_T \\
\end{bmatrix}
\end{aligned}
\end{split}\]</div>
<p>and lastly applying the softmax function, we would see that the future tokens
would have a softmax output of <span class="math notranslate nohighlight">\(0\)</span>, a neat trick to ask the loss function to not
consider the future tokens in the sequence. We can even use <code class="docutils literal notranslate"><span class="pre">ignore_index</span></code> in
PyTorch’s loss function like <code class="docutils literal notranslate"><span class="pre">nn.CrossEntropyLoss</span></code> to ignore the future tokens
via a given mask index.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{masked_attention_weights} &amp;= \text{softmax}(\text{masked_attention_scores}) \\
&amp;= \begin{bmatrix}
\text{softmax}(q_1 \cdot k_1) &amp; 0 &amp; 0 &amp; \dots &amp; 0 \\
\text{softmax}(q_2 \cdot k_1) &amp; \text{softmax}(q_2 \cdot k_2) &amp; 0 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\text{softmax}(q_T \cdot k_1) &amp; \text{softmax}(q_T \cdot k_2) &amp; \text{softmax}(q_T \cdot k_3) &amp; \dots &amp; \text{softmax}(q_T \cdot k_T) \\
\end{bmatrix}
\end{aligned}
\end{split}\]</div>
</section>
<section id="last-token-has-full-context">
<h4><a class="toc-backref" href="#id104" role="doc-backlink">Last Token has Full Context</a><a class="headerlink" href="#last-token-has-full-context" title="Link to this heading">#</a></h4>
<p>And finally, the <code class="docutils literal notranslate"><span class="pre">context_vector</span></code> is a matrix of size <span class="math notranslate nohighlight">\(T \times d_v\)</span> where each
row is the context vector for each token in the sequence, containing not only
the semantic and positional information of the token itself, but also contextual
information from the preceding tokens in the sequence.</p>
<p>For example:</p>
<ul class="simple">
<li><p>The first token <code class="docutils literal notranslate"><span class="pre">priest</span></code> would have a context vector that contains information
from the token <code class="docutils literal notranslate"><span class="pre">priest</span></code> itself.</p></li>
<li><p>The second token <code class="docutils literal notranslate"><span class="pre">and</span></code> would have a context vector that contains information
from the token <code class="docutils literal notranslate"><span class="pre">priest</span></code> and <code class="docutils literal notranslate"><span class="pre">and</span></code>.</p></li>
<li><p>The third token <code class="docutils literal notranslate"><span class="pre">clerk</span></code> would have a context vector that contains information
from the token <code class="docutils literal notranslate"><span class="pre">priest</span></code>, <code class="docutils literal notranslate"><span class="pre">and</span></code> and <code class="docutils literal notranslate"><span class="pre">clerk</span></code>.</p></li>
<li><p>…</p></li>
<li><p>The last token <code class="docutils literal notranslate"><span class="pre">amen</span></code> would have a context vector that contains information
from the token <code class="docutils literal notranslate"><span class="pre">priest</span></code>, <code class="docutils literal notranslate"><span class="pre">and</span></code>, <code class="docutils literal notranslate"><span class="pre">clerk</span></code>, …, <code class="docutils literal notranslate"><span class="pre">amen</span></code>.</p></li>
</ul>
<p>As a consequence, the last token would be the only token that has the full
context of the sequence, and this is why while generating text, we would only
need to use the context vector of the last token to generate the next token.</p>
<p>To this end, each sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is transformed to a <span class="math notranslate nohighlight">\(T \times d_v\)</span></p>
<p>It is worth noting that one can connect back to the theory earlier on how one sequence
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is decomposed to a <span class="math notranslate nohighlight">\(T \times d_v\)</span> matrix where <span class="math notranslate nohighlight">\(d_v = D\)</span> when only one head is involved.
The difference is huge because each row of the embedding <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> before passing through attention
would only hold that single token <span class="math notranslate nohighlight">\(x_t\)</span>’s semantic and positional info without notion
of any other tokens in the sequence. Now, each row of the context vector <span class="math notranslate nohighlight">\(\mathbf{c}_t\)</span>
would hold the semantic and positional info of the token <span class="math notranslate nohighlight">\(x_t\)</span> as well as the context
information from the preceding tokens in the sequence.</p>
<p>Lastly, each sequence can be thought of having <span class="math notranslate nohighlight">\(T\)</span> sub-samples, this is in line with our objective
to model the joint distribution of one sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> - which is decomposed into <span class="math notranslate nohighlight">\(T\)</span> conditional
probability.</p>
</section>
</section>
<section id="computational-complexity-of-self-attention">
<h3><a class="toc-backref" href="#id105" role="doc-backlink">Computational Complexity of Self-Attention</a><a class="headerlink" href="#computational-complexity-of-self-attention" title="Link to this heading">#</a></h3>
<p>It is easy to see that computing the dot product in self-attention quadratic
over its sequence length <span class="math notranslate nohighlight">\(T\)</span>. And naively, we would require
<span class="math notranslate nohighlight">\(\mathcal{O}(T^2 \cdot D)\)</span> time to compute the self-attention on a sequence of
length <span class="math notranslate nohighlight">\(T\)</span> and <span class="math notranslate nohighlight">\(D\)</span>-dimensional representations. We see a table listed below,
referenced from the paper <em>Attention is All You Need</em>
<span id="id31">[<a class="reference internal" href="../../bibliography.html#id19" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017. arXiv:1706.03762.">Vaswani <em>et al.</em>, 2017</a>]</span>, with complexity per layer, the number of
sequential operations, and maximum path length. The complexity is measured by
the upper bound of the number of operations to perform, while the maximum path
length represents the maximum number of steps a forward or backward signal has
to traverse to reach any other position. The lower this length, the better
gradient signals can backpropagate for long-range dependencies
<span id="id32">[<a class="reference internal" href="../../bibliography.html#id21" title="Phillip Lippe. UvA Deep Learning Tutorials. https://uvadlc-notebooks.readthedocs.io/en/latest/, 2023.">Lippe, 2023</a>]</span>.</p>
<div class="pst-scrollable-table-container"><table class="table" id="self-attention-complexity">
<caption><span class="caption-number">Table 5 </span><span class="caption-text">Master Theorem Cases</span><a class="headerlink" href="#self-attention-complexity" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Layer Type</p></th>
<th class="head"><p>Complexity per Layer</p></th>
<th class="head"><p>Sequential Operations</p></th>
<th class="head"><p>Maximum Path Length</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Self-Attention</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(T^2 \cdot D)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(1)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Recurrent</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(T \cdot D^2)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(T)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(T)\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Convolutional</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(K \cdot T \cdot D^2)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(\log_K(T))\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Self-Attention (restricted)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(R \cdot T \cdot D)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}\left(\frac{T}{R}\right)\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<p>In the table, we have:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(T\)</span> is the sequence length,</p></li>
<li><p><span class="math notranslate nohighlight">\(D\)</span> is the representation dimension,</p></li>
<li><p><span class="math notranslate nohighlight">\(K\)</span> is the kernel size of the convolution,</p></li>
<li><p><span class="math notranslate nohighlight">\(R\)</span> is the size of the neighborhood in restricted self-attention.</p></li>
</ul>
<p>It is not easy to see that the parallel computation of self-attention is
favourable, why so? Because unlike recurrent, where we need to strictly follow
the sequence, in self-attention, we <em>simultaneously</em> compute the attention
scores for all pairs of positions. This is why the maximum path length is
<span class="math notranslate nohighlight">\(\mathcal{O}(1)\)</span> for self-attention, since the minimum number of processing
steps required to propagate information from any input position to any other
position is a constant, regardless of the sequence length, within the same
layer, whereas for recurrent, it is <span class="math notranslate nohighlight">\(\mathcal{O}(T)\)</span> because information from
the first token in the sequence needs to pass through all the tokens to reach
the last token. It is worth noting that when <span class="math notranslate nohighlight">\(T &gt;&gt; D\)</span>, self-attention is more
computationally expensive than RNNs.</p>
</section>
<section id="self-attention-enables-parallelism">
<h3><a class="toc-backref" href="#id106" role="doc-backlink">Self-Attention Enables Parallelism</a><a class="headerlink" href="#self-attention-enables-parallelism" title="Link to this heading">#</a></h3>
<p>RNNs process data sequences in a serial manner, meaning they handle one element
of the sequence at a time in a step-wise fashion. This sequential processing is
due to the RNN’s architecture, which relies on the previous step’s hidden state
to compute the current step’s output. Because each step depends on the outcome
of the previous step, RNNs inherently have a difficult time leveraging parallel
computation within a sequence. The computational complexity of processing a
sequence is therefore <span class="math notranslate nohighlight">\(\mathcal{O}(T)\)</span>, as the model must iterate through each
element of the sequence one after the other.</p>
<p>GPT and other transformer-based models, on the other hand, use a different
approach that allows for much more parallelization. Transformers process entire
sequences of data at once, rather than step-by-step. This is possible due to the
attention mechanism, which computes relationships between all elements in a
sequence in parallel and hence the computational complexity of processing a
sequence is <span class="math notranslate nohighlight">\(\mathcal{O}(1)\)</span> - irrespective of the sequence length <span class="math notranslate nohighlight">\(T\)</span>.</p>
<section id="complexity-per-layer">
<h4><a class="toc-backref" href="#id107" role="doc-backlink">Complexity per Layer</a><a class="headerlink" href="#complexity-per-layer" title="Link to this heading">#</a></h4>
<p>Essentialy, this refers to the total computational complexity for processing a
layer given an input sequence of length <span class="math notranslate nohighlight">\(T\)</span> and model dimension <span class="math notranslate nohighlight">\(D\)</span>. For
self-attention, this is <span class="math notranslate nohighlight">\(\mathcal{O}(T^2 \cdot D)\)</span>, indicating the complexity
grows quadratically with the length of the input sequence and linearly with the
dimension of the model. This is due to the pairwise interactions between
elements in the sequence.</p>
</section>
<section id="sequential-operations">
<h4><a class="toc-backref" href="#id108" role="doc-backlink">Sequential Operations</a><a class="headerlink" href="#sequential-operations" title="Link to this heading">#</a></h4>
<p>This column refers to the depth of sequential operations that cannot be
parallelized. For self-attention, it’s marked as <span class="math notranslate nohighlight">\(\mathcal{O}(1)\)</span>, meaning that,
in terms of depth of operations that must be executed sequentially (one after
the other), self-attention does not increase with the sequence length. This is
because, despite the quadratic computational complexity in terms of total
operations (<span class="math notranslate nohighlight">\(\mathcal{O}(T^2 \cdot D)\)</span>), the attention computation is essentially
computed via matrix multiplication in one go.</p>
</section>
<section id="maximum-path-length">
<h4><a class="toc-backref" href="#id109" role="doc-backlink">Maximum Path Length</a><a class="headerlink" href="#maximum-path-length" title="Link to this heading">#</a></h4>
<p>This refers to the longest distance information must travel in the network from
input to output. For self-attention, this is also <span class="math notranslate nohighlight">\(\mathcal{O}(1)\)</span>, indicating
that any output unit can directly attend to any input unit without intermediate
steps. This contrasts with RNNs, where the path length can grow linearly with
the sequence length (<span class="math notranslate nohighlight">\(\mathcal{O}(T)\)</span>).</p>
<p>The <span class="math notranslate nohighlight">\(\mathcal{O}(1)\)</span> in the “Sequential Operations” and “Maximum Path Length”
columns for self-attention highlights its parallelization advantage and direct
information flow capability. It does not, however, denote the overall
computational complexity of processing a layer, which remains
<span class="math notranslate nohighlight">\(\mathcal{O}(T^2 \cdot D)\)</span> due to the pairwise attention computations.</p>
<div class="seealso admonition">
<p class="admonition-title">More Intuition in Andrej Karpathy’s Video</p>
<p>There’s much more intuition in Andrej Karpathy’s video on the
<a class="reference external" href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Let’s Build GPT: from scratch, in code, spelled out</a>.
He mentions things like:</p>
<ul class="simple">
<li><p>Averaging past context with for loops, the weakest for of aggregation.</p></li>
<li><p>Matrix multiply as weighted aggregation.</p></li>
<li><p>Adding softmax to make it a weighted average.</p></li>
<li><p>Attention as communication.</p></li>
<li><p>Attention has no notion of space and operates over sets.</p></li>
<li><p>There is no communication across batch dimension.</p></li>
</ul>
<p>This
<a class="reference external" href="https://blog.matdmiller.com/posts/2023-06-10_transformers/notebook.html#previous-token-averages---building-intuition-for-self-attention">blog post</a>
implements what Andrej mentioned in the video.</p>
</div>
</section>
</section>
</section>
<section id="multi-head-attention">
<h2><a class="toc-backref" href="#id110" role="doc-backlink">Multi-Head Attention</a><a class="headerlink" href="#multi-head-attention" title="Link to this heading">#</a></h2>
<p>In practice, given the same set of queries, keys, and values we may want our
model to combine knowledge from different behaviors of the same attention
mechanism, such as capturing dependencies of various ranges (e.g., shorter-range
vs. longer-range) within a sequence. Thus, it may be beneficial to allow our
attention mechanism to jointly use different representation subspaces of
queries, keys, and values <span id="id33">[<a class="reference internal" href="../../bibliography.html#id5" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into Deep Learning. Cambridge University Press, 2023. URL: https://D2L.ai.">Zhang <em>et al.</em>, 2023</a>]</span>.</p>
<p>What this implies is that the natural language is a complex space, and as
highlighted in the book <em>Speech and Language Processing</em> by Jurafsky and Martin,
there are distinct semantic, syntactic and discourse relationships that can hold
between words in a sentence. For example, the verb “ate” in the sentence “The
cat ate the mouse” has a semantic relationship with the noun “cat” and “mouse”,
and a syntactic relationship with the noun “the”. It would be difficult for a
single head to hold all such representations over just a singled weighted
average, and this is where multi-head attention comes in.</p>
<p>Multi-head attention is a mechanism that allows the model to jointly attend to
information from different representation subspaces at different positions.</p>
<section id="intuition">
<h3><a class="toc-backref" href="#id111" role="doc-backlink">Intuition</a><a class="headerlink" href="#intuition" title="Link to this heading">#</a></h3>
<p>Eugene Yan’s article
<a class="reference external" href="https://eugeneyan.com/writing/attention/">Some Intuition on Attention and the Transformer</a>
provides good intuition on multi-head attention. We use his sample sequence
<em>“The chicken crossed the road carelessly”</em> to illustrate the intuition.</p>
<ul class="simple">
<li><p>One head might specifically capture the action-subject relationship, linking
“crossed” with “chicken.”</p></li>
<li><p>Another head could focus on the action-object relationship, associating
“crossed” with “road.”</p></li>
<li><p>Yet another head might explore the manner in which the action is performed,
connecting “crossed” with “carelessly.”</p></li>
</ul>
</section>
<section id="id34">
<h3><a class="toc-backref" href="#id112" role="doc-backlink">Definition</a><a class="headerlink" href="#id34" title="Link to this heading">#</a></h3>
<p>The multi-head attention is a function that maps a query matrix
<span class="math notranslate nohighlight">\(\mathbf{Q} \in \mathbb{R}^{T \times d_q}\)</span>, a key matrix
<span class="math notranslate nohighlight">\(\mathbf{K} \in \mathbb{R}^{T \times d_k}\)</span>, and a value matrix
<span class="math notranslate nohighlight">\(\mathbf{V} \in \mathbb{R}^{T \times d_v}\)</span> to an output matrix defined as
<span class="math notranslate nohighlight">\(\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) \in \mathbb{R}^{T \times d_v}\)</span>.
The function is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{for } h = 1, 2, \ldots, H: \\
\mathbf{Q}_h = \mathbf{Z} \mathbf{W}_{h}^{\mathbf{Q}} \quad \mathbf{K}_h = \mathbf{Z} \mathbf{W}_{h}^{\mathbf{K}} \quad \mathbf{V}_h = \mathbf{Z} \mathbf{W}_{h}^{\mathbf{V}} \\
\text{head}_h = \text{Attention}(\mathbf{Q}_h, \mathbf{K}_h, \mathbf{V}_h) \\
\mathbf{A} := \text{MultiHead}(\mathbf{Z}) = \left(\text{head}_1 \oplus \text{head}_2 \oplus \ldots \oplus \text{head}_H \right) \mathbf{W}^O
\end{aligned}
\end{split}\]</div>
<p>where each <span class="math notranslate nohighlight">\(\text{head}_h\)</span> is the context vector <span class="math notranslate nohighlight">\(\mathbf{C}_h\)</span> obtained from
the <span class="math notranslate nohighlight">\(h\)</span>-th head of the self-attention mechanism. The <span class="math notranslate nohighlight">\(\oplus\)</span> operator denotes
the concatenation operation that concatenates the context vectors
<span class="math notranslate nohighlight">\(\mathbf{C}_1,
\mathbf{C}_2, \ldots, \mathbf{C}_H\)</span> along the feature dimension,
which essentially still result in the dimension <span class="math notranslate nohighlight">\(T \times D\)</span>. The <span class="math notranslate nohighlight">\(\mathbf{W}^O\)</span>
is a learnable weight matrix that projects the concatenated context vectors back
to the original dimensionality <span class="math notranslate nohighlight">\(D\)</span>.</p>
<p>Some other notations:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H\)</span> is the number of attention heads, which is a hyperparameter of the
multi-head attention mechanism.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}_{h}^{\mathbf{Q}} \in \mathbb{R}^{D \times d_q}\)</span>: The learnable
query weight matrix for the <span class="math notranslate nohighlight">\(h\)</span>-th head.</p>
<ul>
<li><p>Note that <span class="math notranslate nohighlight">\(d_q = \frac{D}{H}\)</span>, where <span class="math notranslate nohighlight">\(D\)</span> is the hidden dimension of the
token embeddings.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}_{h}^{\mathbf{K}} \in \mathbb{R}^{D \times d_k}\)</span>: The key weight
matrix for the <span class="math notranslate nohighlight">\(h\)</span>-th head.</p>
<ul>
<li><p>Note that <span class="math notranslate nohighlight">\(d_k = \frac{D}{H}\)</span>, where <span class="math notranslate nohighlight">\(D\)</span> is the hidden dimension of the
token embeddings.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}_{h}^{\mathbf{V}} \in \mathbb{R}^{D \times d_v}\)</span>: The value
weight matrix for the <span class="math notranslate nohighlight">\(h\)</span>-th head.</p>
<ul>
<li><p>Note that <span class="math notranslate nohighlight">\(d_v = \frac{D}{H}\)</span>, where <span class="math notranslate nohighlight">\(D\)</span> is the hidden dimension of the
token embeddings.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Q}_h\)</span>, <span class="math notranslate nohighlight">\(\mathbf{K}_h\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{V}_h\)</span> are the query, key, and
value matrices for the <span class="math notranslate nohighlight">\(h\)</span>-th head, respectively.</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{head}_h = \text{Attention}(\mathbf{Q}_h, \mathbf{K}_h, \mathbf{V}_h)\)</span>
is the context vector <span class="math notranslate nohighlight">\(\mathbf{C}_h\)</span> obtained from the <span class="math notranslate nohighlight">\(h\)</span>-th head of the
self-attention mechanism.</p></li>
<li><p><span class="math notranslate nohighlight">\(\oplus\)</span> is just
<span class="math notranslate nohighlight">\(\text{Concat}(\cdot)\)</span>, the concatenation operation that concatenates the head/context matrices <span class="math notranslate nohighlight">\(\mathbf{C}_1, \mathbf{C}_2, \ldots, \mathbf{C}_H\)</span>
along the feature dimension, resulting in a matrix of context vectors of
shape <span class="math notranslate nohighlight">\(\mathbb{R}^{T \times H \cdot d_v} = \mathbb{R}^{T \times D}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^O \in \mathbb{R}^{d_v \times H \cdot d_v}\)</span> is a learnable weight
matrix that projects the concatenated context vectors back to the original
dimensionality <span class="math notranslate nohighlight">\(D\)</span>.</p></li>
</ul>
<p>Without the batch dimension <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>, the output matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is
of shape <span class="math notranslate nohighlight">\(\mathbb{R}^{T \times D}\)</span>, where each row <span class="math notranslate nohighlight">\(\mathbf{a}_t\)</span> of the output
matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is the new embedding of the token at position <span class="math notranslate nohighlight">\(t\)</span> in the
sequence, containing not only the semantic and positional information of the
token itself, but also contextual information from the other tokens in the
sequence.</p>
</section>
<section id="id35">
<h3><a class="toc-backref" href="#id113" role="doc-backlink">Implementation</a><a class="headerlink" href="#id35" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MultiHeadedAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;d_model&quot;</span><span class="p">,</span>
        <span class="s2">&quot;d_k&quot;</span><span class="p">,</span>
        <span class="s2">&quot;d_q&quot;</span><span class="p">,</span>
        <span class="s2">&quot;d_v&quot;</span><span class="p">,</span>
        <span class="s2">&quot;H&quot;</span><span class="p">,</span>
        <span class="s2">&quot;W_Q&quot;</span><span class="p">,</span>
        <span class="s2">&quot;W_K&quot;</span><span class="p">,</span>
        <span class="s2">&quot;W_V&quot;</span><span class="p">,</span>
        <span class="s2">&quot;W_O&quot;</span><span class="p">,</span>
        <span class="s2">&quot;attention&quot;</span><span class="p">,</span>
        <span class="s2">&quot;dropout&quot;</span><span class="p">,</span>
        <span class="s2">&quot;context_vector&quot;</span><span class="p">,</span>
        <span class="s2">&quot;attention_weights&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">attention</span><span class="p">:</span> <span class="n">Attention</span><span class="p">,</span>
        <span class="n">H</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">H</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;The number of heads must divide the embedding dimension.&quot;</span>

        <span class="c1"># fmt: off</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span>   <span class="o">=</span> <span class="n">d_model</span>       <span class="c1"># D</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span>       <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">H</span>  <span class="c1"># stay true to notations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_q</span>       <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">H</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span>       <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">H</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">H</span>         <span class="o">=</span> <span class="n">H</span>             <span class="c1"># number of heads</span>

        <span class="c1"># shadow my notations, actually they are of shape D x D.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_Q</span>       <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_q</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>  <span class="c1"># D x D</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_K</span>       <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_V</span>       <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_O</span>       <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span>   <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">context_vector</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_weights</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>

        <span class="c1"># self._init_weights()</span>
        <span class="c1"># fmt: on</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">query</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">key</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">value</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Notations</span>
<span class="sd">        ---------</span>
<span class="sd">        B:      Batch size</span>
<span class="sd">        S or L: Source sequence length</span>
<span class="sd">        T or L: Target sequence length</span>
<span class="sd">        D:      Embedding dimension</span>
<span class="sd">        H:      Number of heads</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        query:  Although named as query, it is the embeddings `z` from the token_embedding + positional_embedding layer.</span>
<span class="sd">                type:  torch.Tensor</span>
<span class="sd">                shape: (B, S or T, D)</span>
<span class="sd">        key:    Although named as key, it is the embeddings `z` from the token_embedding + positional_embedding layer.</span>
<span class="sd">                type:  torch.Tensor</span>
<span class="sd">                shape: (B, S or T, D)</span>
<span class="sd">        value:  Although named as value, it is the embeddings `z` from the token_embedding + positional_embedding layer.</span>
<span class="sd">                type:  torch.Tensor</span>
<span class="sd">                shape: (B, S or T, D)</span>
<span class="sd">        mask:   Mask to be applied to the attention scores.</span>
<span class="sd">                type:  torch.BoolTensor</span>
<span class="sd">                shape: (B, 1, S or T, S or T)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        O:  The output of the multi-headed attention mechanism.</span>
<span class="sd">            type:  torch.Tensor</span>
<span class="sd">            shape: (B, S or T, D)</span>

<span class="sd">        Variables</span>
<span class="sd">        ---------</span>
<span class="sd">        W_Q.weight (D, D)</span>
<span class="sd">        W_K.weight (D, D)</span>
<span class="sd">        W_V.weight (D, D)</span>
<span class="sd">        W_O.weight (D, D)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># fmt: off</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">mask</span><span class="o">.</span><span class="n">ndim</span>     <span class="o">==</span> <span class="mi">4</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Mask should have 4 dimensions but got </span><span class="si">{</span><span class="n">mask</span><span class="o">.</span><span class="n">ndim</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="k">assert</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="s2">&quot;Batch size of mask and query must match.&quot;</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="s2">&quot;Mask should have shape (batch_size, 1, seq_len, seq_len).&quot;</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">==</span> <span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="s2">&quot;Mask should have shape (batch_size, 1, seq_len, seq_len).&quot;</span><span class="p">)</span>


        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_Q</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="c1"># Z @ W_Q -&gt; LxD @ DxD = LxD -&gt; [B, L, D]</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_K</span><span class="p">(</span><span class="n">key</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>   <span class="c1"># Z @ W_K</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_V</span><span class="p">(</span><span class="n">value</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span> <span class="c1"># Z @ W_V</span>

        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_qkv</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>        <span class="c1"># splitting happens -&gt; [B, H, L, D]</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_qkv</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose_qkv</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>

        <span class="c1"># Attention</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context_vector</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">context_vector_concat</span>                       <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reverse_transpose_qkv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">context_vector</span><span class="p">)</span>
        <span class="c1"># fmt: on</span>

        <span class="c1"># mypy complains because it infers `O` as `Any` but it is actually a tensor.</span>
        <span class="c1"># You can either cast it to tensor or use `self.W_O.forward(context_vector_concat)`.</span>
        <span class="n">O</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_O</span><span class="p">(</span><span class="n">context_vector_concat</span><span class="p">)</span>  <span class="c1"># context_vector_concat @ W_O -&gt; LxD @ DxD = LxD</span>
        <span class="k">return</span> <span class="n">O</span>  <span class="c1"># type: ignore[no-any-return]</span>

    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;See PyTorch&#39;s MultiHeadAttention code for reference.&quot;&quot;&quot;</span>
        <span class="c1"># we assume _qkv_same_embed_dim is True</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_Q</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_K</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_V</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W_O</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">transpose_qkv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q_or_k_or_v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Transposition for parallel computation of multiple attention heads.</span>
<span class="sd">        Why does transpose allow parallel computation? So originally the shape of</span>
<span class="sd">        the query, key, and value is (B, L, D), and we want to split the D into H</span>
<span class="sd">        heads to become (B, L, H, D / H). But this is not the shape we want (could</span>
<span class="sd">        be due to efficiency reasons), so we transpose the shape to (B, H, L, D / H)</span>
<span class="sd">        so all heads can be computed in parallel (efficiently).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        q_or_k_or_v: The query, key, or value tensor.</span>
<span class="sd">            type:  torch.Tensor</span>
<span class="sd">            shape: (B, L, D)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        q_or_k_or_v: The transposed query, key, or value tensor.</span>
<span class="sd">            type:  torch.Tensor</span>
<span class="sd">            shape: (B, H, L, D / H)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># fmt: off</span>
        <span class="c1"># 1. q_or_k_or_v is shape (B, L, D)</span>
        <span class="c1"># 2. aim to make it of shape (B, L, H, D / H = d_qkv)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">q_or_k_or_v</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">q_or_k_or_v</span>            <span class="o">=</span> <span class="n">q_or_k_or_v</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">)</span>

        <span class="c1"># 3. switch H from 3rd to 2nd dimension, or in python swap 2nd to 1st dimension and 1st to 2nd dimension</span>
        <span class="c1">#    shape (B, H, L, D / H = d_qkv)</span>
        <span class="n">q_or_k_or_v</span>            <span class="o">=</span> <span class="n">q_or_k_or_v</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="c1"># fmt: on</span>
        <span class="k">return</span> <span class="n">q_or_k_or_v</span>

    <span class="k">def</span> <span class="nf">reverse_transpose_qkv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q_or_k_or_v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reverse the transposition operation for concatenating multiple attention heads.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        q_or_k_or_v: The query, key, or value tensor.</span>
<span class="sd">            type:  torch.Tensor</span>
<span class="sd">            shape: (B, H, L, D / H)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        q_or_k_or_v: The transposed query, key, or value tensor.</span>
<span class="sd">            type:  torch.Tensor</span>
<span class="sd">            shape: (B, L, D)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># fmt: off</span>
        <span class="c1"># 1. q_or_k_or_v is shape (B, H, L, D / H = d_qkv)</span>
        <span class="c1"># 2. aim to make it of shape (B, L, H, D / H = d_qkv)</span>
        <span class="n">q_or_k_or_v</span> <span class="o">=</span> <span class="n">q_or_k_or_v</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

        <span class="c1"># 3. Merge H and d_qkv into D</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">q_or_k_or_v</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">q_or_k_or_v</span> <span class="o">=</span> <span class="n">q_or_k_or_v</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="c1"># fmt: on</span>
        <span class="k">return</span> <span class="n">q_or_k_or_v</span>

<span class="k">def</span> <span class="nf">construct_dummy_batch_future_masks</span><span class="p">(</span><span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Broadcast future mask from shape (L, L) to (B, L, L) then (B, 1, L, L).&quot;&quot;&quot;</span>
    <span class="c1"># Create a lower triangular mask for a single sequence</span>
    <span class="n">future_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
    <span class="n">future_mask</span> <span class="o">=</span> <span class="n">future_mask</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="c1"># broadcast future mask from shape (L, L) to (B, L, L)</span>
    <span class="n">future_masks</span> <span class="o">=</span> <span class="n">future_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># broadcast future mask from shape (B, L, L) to (B, 1, L, L)</span>
    <span class="n">future_masks</span> <span class="o">=</span> <span class="n">future_masks</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">(</span><span class="n">future_masks</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">generator</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>
<span class="n">seed_all</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">set_torch_deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">d_model</span>  <span class="c1"># batch size, head, context length, embedding dimension</span>
<span class="n">W_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">W_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">W_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">q0</span> <span class="o">=</span> <span class="n">W_q</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed</span><span class="p">)</span>
<span class="n">k0</span> <span class="o">=</span> <span class="n">W_k</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed</span><span class="p">)</span>
<span class="n">v0</span> <span class="o">=</span> <span class="n">W_v</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">q0</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">causal_attention</span> <span class="o">=</span> <span class="n">ScaledDotProductAttention</span><span class="p">()</span>
<span class="n">causal_mha</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="n">attention</span><span class="o">=</span><span class="n">causal_attention</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">H</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">tril_mask</span> <span class="o">=</span> <span class="n">construct_dummy_batch_future_masks</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>

<span class="n">A_BHL</span> <span class="o">=</span> <span class="n">causal_mha</span><span class="p">(</span><span class="n">q0</span><span class="p">,</span> <span class="n">k0</span><span class="p">,</span> <span class="n">v0</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">tril_mask</span><span class="p">)</span>

<span class="n">attention_weights</span> <span class="o">=</span> <span class="n">causal_mha</span><span class="o">.</span><span class="n">attention_weights</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">attention_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">show_attention_heatmaps</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Keys&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Queries&#39;</span><span class="p">,</span> <span class="n">show_title</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span><span style="font-weight: bold">])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span><span style="font-weight: bold">])</span>
</pre>
</div><img alt="../../_images/a240e887dd09dabc1dff70e721227b5a952ddb63d27745839bb51c5466f5925c.png" src="../../_images/a240e887dd09dabc1dff70e721227b5a952ddb63d27745839bb51c5466f5925c.png" />
</div>
</div>
</section>
<section id="permutation-invariance">
<h3><a class="toc-backref" href="#id114" role="doc-backlink">Permutation Invariance</a><a class="headerlink" href="#permutation-invariance" title="Link to this heading">#</a></h3>
<p>Neural networks without the notion of sequence will be permutation invariant
with respect to the inputs and the multi-head attention mechanism is no
exception. If we ignore the the batch <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> dimension for now, then if
we switch two tokens in the sequence via <span class="math notranslate nohighlight">\(\text{permute}(\mathbf{Z})\)</span>, or more
concretely, say we swap <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> in the sequence, then the output of the
multi-head attention mechanism would be the same up to a permutation of the rows
(<span class="math notranslate nohighlight">\(1 &lt;-&gt; 2\)</span>). This is why we would need the positional encoding to break the
permutation invariance.</p>
<p>Some extended ideas can be found in the chapter on
<a class="reference external" href="https://d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#breaking-the-symmetry">Numerical Stability and Initialization</a>,
from the book Dive into Deep Learning <span id="id36">[<a class="reference internal" href="../../bibliography.html#id5" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into Deep Learning. Cambridge University Press, 2023. URL: https://D2L.ai.">Zhang <em>et al.</em>, 2023</a>]</span>.</p>
</section>
<section id="applying-layernorm-and-residual-connections-to-multi-head-attention-output">
<h3><a class="toc-backref" href="#id115" role="doc-backlink">Applying LayerNorm and Residual Connections to Multi-Head Attention Output</a><a class="headerlink" href="#applying-layernorm-and-residual-connections-to-multi-head-attention-output" title="Link to this heading">#</a></h3>
<p>Recall earlier on the discussion of using Layer Normalization (LayerNorm) on the
output of the Multi-Head Attention mechanism with residual connections.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\operatorname{AddNorm}(\mathbf{x}) &amp;= \operatorname{LayerNorm}(\mathbf{x} + \operatorname{Sublayer}(\mathbf{x})) \\
\end{aligned}
\end{split}\]</div>
<ul class="simple">
<li><p>The Multi-Head Attention layer is the <span class="math notranslate nohighlight">\(\operatorname{Sublayer}(\cdot)\)</span>
function. So <span class="math notranslate nohighlight">\(\text{MultiHead}(\cdot) := \operatorname{Sublayer}(\cdot)\)</span>.</p></li>
<li><p>The output from the Multi-Head Attention to be
<span class="math notranslate nohighlight">\(\mathbf{A} \in \mathbb{R}^{T \times D}\)</span>, we would pass it to
<span class="math notranslate nohighlight">\(\operatorname{LayerNorm}(\mathbf{A} + \text{MultiHead}(\mathbf{A}))\)</span>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">generator</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>
<span class="n">seed_all</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">set_torch_deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">B</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">d_model</span>  <span class="c1"># batch size, head, context length, embedding dimension</span>
<span class="n">W_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">W_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">W_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">D</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">q0</span> <span class="o">=</span> <span class="n">W_q</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed</span><span class="p">)</span>
<span class="n">k0</span> <span class="o">=</span> <span class="n">W_k</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed</span><span class="p">)</span>
<span class="n">v0</span> <span class="o">=</span> <span class="n">W_v</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed</span><span class="p">)</span>

<span class="n">causal_attention</span> <span class="o">=</span> <span class="n">ScaledDotProductAttention</span><span class="p">()</span>
<span class="n">causal_mha</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="n">attention</span><span class="o">=</span><span class="n">causal_attention</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">H</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">tril_mask</span> <span class="o">=</span> <span class="n">construct_dummy_batch_future_masks</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>

<span class="c1">### AddNorm</span>
<span class="n">add_norm_1</span> <span class="o">=</span> <span class="n">AddNorm</span><span class="p">(</span><span class="n">feature_dim</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">z0_tok_embed_with_pos_embed_with_mha_and_addnorm1</span> <span class="o">=</span> <span class="n">add_norm_1</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">z</span><span class="p">:</span> <span class="n">causal_mha</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">tril_mask</span><span class="p">))</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed_with_mha_and_addnorm1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.6232</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5807</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0134</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0290</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.2443</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1824</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1235</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.5502</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5677</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.6455</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9800</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0978</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4259</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4103</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6807</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.1555</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6450</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.1543</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.2847</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.7754</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0385</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.1870</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.5566</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4082</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4143</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.3921</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.3443</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.3665</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.6815</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2323</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.5397</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.9095</span><span style="font-weight: bold">]]]</span>, <span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">AddBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div></div>
</div>
</section>
</section>
<section id="positionwise-feed-forward-networks">
<h2><a class="toc-backref" href="#id116" role="doc-backlink">Positionwise Feed-Forward Networks</a><a class="headerlink" href="#positionwise-feed-forward-networks" title="Link to this heading">#</a></h2>
<p>The term “positionwise feed-forward network” (FFN) in the context of Transformer
models refers to a dense neural network (otherwise known as multilayer
perceptron) that operates on the output of the Multi-Head Attention mechanism.
This component is called “positionwise” because it applies the <strong>same</strong>
feed-forward neural network (FFN) <strong>independently</strong> and <strong>identically</strong> to each
position <span class="math notranslate nohighlight">\(t\)</span> in the sequence of length <span class="math notranslate nohighlight">\(T\)</span>.</p>
<section id="independent-processing">
<h3><a class="toc-backref" href="#id117" role="doc-backlink">Independent Processing</a><a class="headerlink" href="#independent-processing" title="Link to this heading">#</a></h3>
<p>In the Transformer architecture, after the Multi-Head Attention mechanism
aggregates information from different positions in the sequence based on
attention scores, each element (or position) <span class="math notranslate nohighlight">\(t\)</span> in the sequence has an updated
representation. The positionwise FFN then processes each of these updated
representations. However, rather than considering the sequence as a whole or how
elements relate to each other at this stage, the FFN operates on each position
separately. This means that for a sequence of length <span class="math notranslate nohighlight">\(T\)</span>, the same FFN is
applied <span class="math notranslate nohighlight">\(T\)</span> times independently, and by extension, given a batch of sequences,
the FFN is applied <span class="math notranslate nohighlight">\(T \times \mathcal{B}\)</span> times, where <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> is the
batch size.</p>
</section>
<section id="identical-application">
<h3><a class="toc-backref" href="#id118" role="doc-backlink">Identical Application</a><a class="headerlink" href="#identical-application" title="Link to this heading">#</a></h3>
<p>The term “using the same FFN” signifies that the same set of parameters (weights
and biases) of the feed-forward neural network is used for each position in the
sequence. The rationale is that the transformation is consistent across all
sequence positions, so each element is transformed by the same learned function.
This means the weight matrices and bias vectors of the FFN are shared across all
positions in the sequence. In other words, if a sequence has <span class="math notranslate nohighlight">\(T=3\)</span>
positions/tokens, the weight matrices and bias vectors of the FFN are the same
for all three positions.</p>
</section>
<section id="id37">
<h3><a class="toc-backref" href="#id119" role="doc-backlink">Definition</a><a class="headerlink" href="#id37" title="Link to this heading">#</a></h3>
<p>Typically, a positionwise FFN consists of two linear transformations with a
non-linear activation function in between. The general form can be represented
as follows.</p>
<div class="proof definition admonition" id="def-positionwise-ffn">
<p class="admonition-title"><span class="caption-number">Definition 15 </span> (Position-wise Feedforward Networks)</p>
<section class="definition-content" id="proof-content">
<p>Given an input matrix <span class="math notranslate nohighlight">\(\mathbf{Z} \in \mathbb{R}^{T \times D}\)</span>, the
position-wise feedforward network computes the output matrix
<span class="math notranslate nohighlight">\(\mathbf{Z}^{\prime} \in \mathbb{R}^{T \times D}\)</span> via the following operations:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z}^{\prime}=\sigma_Z\left(\mathbf{Z} \mathbf{W}^{\text{FF}}_1 + \mathbf{b}^{\text{FF}}_1\right) \mathbf{W}^{\text{FF}}_2 + \mathbf{b}^{\text{FF}}_2
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{\text{FF}}_1 \in \mathbb{R}^{D \times d_{\text{ff}}}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{W}^{\text{FF}}_2 \in \mathbb{R}^{d_{\text{ff}} \times D}\)</span> are
learnable weight matrices.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{b}^{\text{FF}}_1 \in \mathbb{R}^{d_{\text{ff}}}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{b}^{\text{FF}}_2 \in \mathbb{R}^{D}\)</span> are learnable bias vectors.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma_Z\)</span> is a non-linear activation function, such as the Gaussian Error
Linear Unit (GELU) or the Rectified Linear Unit (ReLU).</p></li>
</ul>
</section>
</div></section>
<section id="projection-to-a-higher-dimension-space">
<h3><a class="toc-backref" href="#id120" role="doc-backlink">Projection to a Higher Dimension Space</a><a class="headerlink" href="#projection-to-a-higher-dimension-space" title="Link to this heading">#</a></h3>
<p>In the Transformer architecture, the dimensionality of the hidden layer in the
positionwise FFN, denoted as <span class="math notranslate nohighlight">\(d_{\text{ff}}\)</span>, is often chosen to be larger than
the dimensionality of the input and output embeddings, <span class="math notranslate nohighlight">\(D\)</span>. This means that the
FFN projects the input embeddings into a higher-dimensional space before
projecting them back to the original dimensionality.</p>
<p>The motivation behind this design choice is to allow the model to learn more
complex and expressive representations. By projecting the input embeddings into
a higher-dimensional space, the model capacity is increased, and the FFN can
capture more intricate patterns and relationships among the features. We then
project back (“unembedding”) the higher-dimensional representations to the
original dimensionality to maintain the consistency of the model.</p>
<p>In practice, a common choice for the dimensionality of the hidden layer is to
set <span class="math notranslate nohighlight">\(d_{\text{ff}}\)</span> to be a multiple of the input and output dimensionality <span class="math notranslate nohighlight">\(D\)</span>.
For example, in the original Transformer paper <span id="id38">[<a class="reference internal" href="../../bibliography.html#id19" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017. arXiv:1706.03762.">Vaswani <em>et al.</em>, 2017</a>]</span>, the
authors used <span class="math notranslate nohighlight">\(d_{\text{ff}} = 4 \times D\)</span>.</p>
</section>
<section id="gaussian-error-linear-unit-gelu">
<h3><a class="toc-backref" href="#id121" role="doc-backlink">Gaussian Error Linear Unit (GELU)</a><a class="headerlink" href="#gaussian-error-linear-unit-gelu" title="Link to this heading">#</a></h3>
<p>The Gaussian Error Linear Unit (GELU) is a non-linear activation function used
in the context of neural networks, which allows the model to capture more
complex patterns in the data compared to traditional activation functions like
ReLU. The GELU activation function is defined as:</p>
<div class="math notranslate nohighlight">
\[
\text{GELU}(x) = x \cdot \Phi(x)
\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the input to the activation function, and <span class="math notranslate nohighlight">\(\Phi(x)\)</span> represents the
cumulative distribution function (CDF) of the standard Gaussian distribution.
The GELU function, effectively, models inputs with a non-linear transformation
that weights inputs by their value, with a probabilistic gating mechanism
derived from the Gaussian distribution.</p>
<p>The cumulative distribution function <span class="math notranslate nohighlight">\(\Phi(x)\)</span> for a standard Gaussian
distribution is given by:</p>
<div class="math notranslate nohighlight">
\[
\Phi(x) = \frac{1}{2} \left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]
\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{erf}\)</span> denotes the error function, which is a special function
integral of the Gaussian distribution. Combining these, the GELU function can be
expressed as:</p>
<div class="math notranslate nohighlight">
\[
\text{GELU}(x) = x \cdot \frac{1}{2} \left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]
\]</div>
<p>I will not pretend I have went through the entire paper and motivation of GELU,
but usually, when new and “better” activation functions are proposed, they
usually serve as an alternative to the common activation functions such as ReLU
etc, where they solve some of the problems that the common activation functions
have. From the formulation, we can see that GELU obeys the following properties:</p>
<ul class="simple">
<li><p><strong>Non-linearity</strong>: GELU introduces non-linearity to the model, a given
requirement.</p></li>
<li><p><strong>Differentiability</strong>: GELU is smooth and differentiable everywhere, which
is beneficial for gradient-based optimization methods.</p></li>
<li><p><strong>Boundedness</strong>: GELU seems to be bounded below by <span class="math notranslate nohighlight">\(-0.17\)</span> and not upper
bounded, but practice we can show there is an upper bound if we normalize
the input.</p></li>
</ul>
<div class="proof remark admonition" id="remark-approx-gelu">
<p class="admonition-title"><span class="caption-number">Remark 6 </span> (Approximation of GELU)</p>
<section class="remark-content" id="proof-content">
<p>To further simplify the GELU function and enhance computational efficiency, an
approximation of the Gaussian CDF is commonly used in practice (extracted from
<a class="reference external" href="https://www.hindawi.com/journals/jmath/2023/4229924/">Mathematical Analysis and Performance Evaluation of the GELU Activation Function in Deep Learning</a>):</p>
<div class="math notranslate nohighlight">
\[
\Phi(\alpha x) \approx \frac{1}{2}\left(1+\tanh \left(\beta\left(\alpha x+\gamma(\alpha x)^3\right)\right)\right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta&gt;0\)</span> and <span class="math notranslate nohighlight">\(\gamma \in \mathbb{R}\)</span> are constants, selected to minimize
approximation error. Substituting this approximation into the GELU function, we
arrive at the final approximate form of the GELU activation function (Figure 1):</p>
<div class="math notranslate nohighlight">
\[
\operatorname{GELU}(x)=0.5 x\left(1+\tanh \left(\sqrt{\frac{2}{\pi}}\left(x+0.044715 x^3\right)\right)\right) .
\]</div>
</section>
</div><div class="proof definition admonition" id="def-gelu">
<p class="admonition-title"><span class="caption-number">Definition 16 </span> (GELU Activation Function)</p>
<section class="definition-content" id="proof-content">
<p>For a matrix <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> with elements <span class="math notranslate nohighlight">\(\mathbf{Z}_{t d}\)</span> where <span class="math notranslate nohighlight">\(t\)</span> indexes the
sequence (from 1 to <span class="math notranslate nohighlight">\(T\)</span> ) and <span class="math notranslate nohighlight">\(d\)</span> indexes the feature dimension (from 1 to <span class="math notranslate nohighlight">\(D\)</span>
), the GELU activation is applied <strong>element-wise</strong> to each element
<span class="math notranslate nohighlight">\(\mathbf{Z}_{t d}\)</span> independently:</p>
<div class="math notranslate nohighlight">
\[
\operatorname{GELU}\left(x_{t d}\right)=x_{t d} \cdot \frac{1}{2}\left[1+\operatorname{erf}\left(\frac{x_{t d}}{\sqrt{2}}\right)\right]
\]</div>
</section>
</div><div class="seealso admonition">
<p class="admonition-title">References</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.hindawi.com/journals/jmath/2023/4229924/">Mathematical Analysis and Performance Evaluation of the GELU Activation Function in Deep Learning</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1606.08415">Gaussian Error Linear Units (GELUs) </a></p></li>
</ul>
</div>
</section>
<section id="id39">
<h3><a class="toc-backref" href="#id122" role="doc-backlink">Implementation</a><a class="headerlink" href="#id39" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>


<span class="k">class</span> <span class="nc">PositionwiseFeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements a Position-wise FeedForward Network (FFN) used in Transformer models.</span>

<span class="sd">    This module applies two linear transformations with a non-linear activation</span>
<span class="sd">    in between. It is often used after the multi-head self-attention layer</span>
<span class="sd">    in Transformer models.</span>

<span class="sd">    The naming convention for the linear layers (&#39;context_fc&#39; and &#39;context_projection&#39;) is inspired by</span>
<span class="sd">    the functionality within the Transformer architecture:</span>

<span class="sd">    - &#39;context_fc&#39; (context fully connected): This layer expands the dimensionality</span>
<span class="sd">    of the input features, creating a richer representation. The expansion factor</span>
<span class="sd">    is often 4 in Transformer models, meaning the intermediate size is 4 times the</span>
<span class="sd">    size of the input/output dimensions.</span>

<span class="sd">    - &#39;context_projection&#39; (context projection): This layer projects the expanded</span>
<span class="sd">    features back down to the original dimension, synthesizing the information</span>
<span class="sd">    processed by the &#39;context_fc&#39; layer.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">d_ff</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># fmt: off</span>
        <span class="k">if</span> <span class="n">d_ff</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">d_ff</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">d_model</span> <span class="c1"># typical value for d_ff in Transformer models</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">({</span>
            <span class="s1">&#39;context_fc&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">),</span>
            <span class="s1">&#39;activation&#39;</span><span class="p">:</span> <span class="n">activation</span><span class="p">,</span>
            <span class="s1">&#39;context_projection&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">),</span>
            <span class="s1">&#39;dropout&#39;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>

        <span class="p">})</span>

        <span class="c1"># self._init_weights()</span>

    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize parameters of the linear layers.&quot;&quot;&quot;</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">[</span><span class="s2">&quot;context_fc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">[</span><span class="s2">&quot;context_fc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">[</span><span class="s2">&quot;context_fc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">[</span><span class="s2">&quot;context_projection&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">[</span><span class="s2">&quot;context_projection&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">[</span><span class="s2">&quot;context_projection&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">[</span><span class="s2">&quot;context_fc&quot;</span><span class="p">](</span><span class="n">z</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">[</span><span class="s2">&quot;activation&quot;</span><span class="p">](</span><span class="n">z</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">[</span><span class="s2">&quot;dropout&quot;</span><span class="p">](</span><span class="n">z</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">[</span><span class="s2">&quot;context_projection&quot;</span><span class="p">](</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="applying-layernorm-and-residual-connections-to-positionwise-ffn-output">
<h3><a class="toc-backref" href="#id123" role="doc-backlink">Applying LayerNorm and Residual Connections to Positionwise FFN Output</a><a class="headerlink" href="#applying-layernorm-and-residual-connections-to-positionwise-ffn-output" title="Link to this heading">#</a></h3>
<p>We also apply Layer Normalization (LayerNorm) and residual connections to the
output of the positionwise FFN in a similar manner to the Multi-Head Attention.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">generator</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>
<span class="n">seed_all</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">set_torch_deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">ffn</span> <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(</span><span class="n">approximate</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">),</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

<span class="c1">### AddNorm</span>
<span class="n">add_norm_2</span> <span class="o">=</span> <span class="n">AddNorm</span><span class="p">(</span><span class="n">feature_dim</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">z0_tok_embed_with_pos_embed_with_mha_and_addnorm1_and_ffn_addnorm2</span> <span class="o">=</span> <span class="n">add_norm_2</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed_with_mha_and_addnorm1</span><span class="p">,</span> <span class="n">ffn</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed_with_mha_and_addnorm1_and_ffn_addnorm2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.6878</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9070</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2814</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4995</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.5380</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2003</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0774</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.2603</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6362</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.6741</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.8753</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1626</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.1427</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.5486</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1518</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.5395</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.8401</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.3392</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0791</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.5799</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3811</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.6312</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.7194</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7070</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.9683</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.7928</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0119</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.1874</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.7260</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.5161</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.4984</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.7115</span><span style="font-weight: bold">]]]</span>, <span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">AddBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div></div>
</div>
</section>
</section>
<section id="softmax-head">
<h2><a class="toc-backref" href="#id124" role="doc-backlink">Softmax Head</a><a class="headerlink" href="#softmax-head" title="Link to this heading">#</a></h2>
<p>The softmax head is the final layer of the Transformer model that maps the
output of the positionwise FFN to the output vocabulary. The logits (without batch dimension) will have a shape
of <span class="math notranslate nohighlight">\(\mathbb{R}^{T \times V}\)</span>, where <span class="math notranslate nohighlight">\(V\)</span> is the size of the output vocabulary.</p>
<p>We also add another layer of Layer Normalization (LayerNorm) to the output of the
positionwise FFN before the softmax head.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Head</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ln_before_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">head</span> <span class="o">=</span> <span class="n">Head</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>
<span class="n">logits</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="o">=</span> <span class="n">head</span><span class="p">(</span><span class="n">ln_before_head</span><span class="p">(</span><span class="n">z0_tok_embed_with_pos_embed_with_mha_and_addnorm1_and_ffn_addnorm2</span><span class="p">))</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># [B, T, V]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">torch.Size</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">50257</span><span style="font-weight: bold">])</span>
</pre>
</div></div>
</div>
<p>Each sub-sequence will be able to predict the next token in the sequence with
a probability distribution over the output vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p>
</section>
<section id="putting-it-all-together-to-form-the-gpt">
<h2><a class="toc-backref" href="#id125" role="doc-backlink">Putting it all Together to form the GPT</a><a class="headerlink" href="#putting-it-all-together-to-form-the-gpt" title="Link to this heading">#</a></h2>
<section id="patch-composer-configuration-with-model-config">
<h3><a class="toc-backref" href="#id126" role="doc-backlink">Patch Composer Configuration with Model Config</a><a class="headerlink" href="#patch-composer-configuration-with-model-config" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;We use dataclass here for easy instantiating with hydra&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">MultiHeadedAttentionConfig</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">attention</span><span class="p">:</span> <span class="n">Attention</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">H</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>

    <span class="k">class</span> <span class="nc">Config</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Pydantic config.&quot;&quot;&quot;</span>

        <span class="n">arbitrary_types_allowed</span> <span class="o">=</span> <span class="kc">True</span>


<span class="c1"># TODO: add `field_validator` such that if `d_ff` is `None`, then `d_ff` is set to `4 * d_model`.</span>
<span class="k">class</span> <span class="nc">PositionwiseFeedForwardConfig</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">d_ff</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">activation</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(</span><span class="n">approximate</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">))</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">class</span> <span class="nc">Config</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Pydantic config.&quot;&quot;&quot;</span>

        <span class="n">arbitrary_types_allowed</span> <span class="o">=</span> <span class="kc">True</span>


<span class="k">class</span> <span class="nc">AddNormConfig</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">feature_dim</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span>


<span class="k">class</span> <span class="nc">DecoderBlockConfig</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">masked_self_attention_mha</span><span class="p">:</span> <span class="n">MultiHeadedAttentionConfig</span>
    <span class="n">feed_forward</span><span class="p">:</span> <span class="n">PositionwiseFeedForwardConfig</span>
    <span class="n">add_norm_1</span><span class="p">:</span> <span class="n">AddNormConfig</span>
    <span class="n">add_norm_2</span><span class="p">:</span> <span class="n">AddNormConfig</span>


<span class="k">class</span> <span class="nc">DecoderConfig</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">context_length</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># NOTE: alias=max_seq_len,block_size</span>
    <span class="n">num_decoder_blocks</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">decoder_block</span><span class="p">:</span> <span class="n">DecoderBlockConfig</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">masked_self_attention_mha_config</span> <span class="o">=</span> <span class="n">MultiHeadedAttentionConfig</span><span class="p">(</span><span class="n">attention</span><span class="o">=</span><span class="n">ScaledDotProductAttention</span><span class="p">(),</span> <span class="n">d_model</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">H</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">feed_forward_config</span> <span class="o">=</span> <span class="n">PositionwiseFeedForwardConfig</span><span class="p">(</span><span class="n">d_model</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(</span><span class="n">approximate</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">),</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">add_norm_config_1</span> <span class="o">=</span> <span class="n">AddNormConfig</span><span class="p">(</span><span class="n">feature_dim</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">add_norm_config_2</span> <span class="o">=</span> <span class="n">AddNormConfig</span><span class="p">(</span><span class="n">feature_dim</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Create DecoderBlockConfig</span>
<span class="n">decoder_block_config</span> <span class="o">=</span> <span class="n">DecoderBlockConfig</span><span class="p">(</span>
    <span class="n">masked_self_attention_mha</span><span class="o">=</span><span class="n">masked_self_attention_mha_config</span><span class="p">,</span>
    <span class="n">feed_forward</span><span class="o">=</span><span class="n">feed_forward_config</span><span class="p">,</span>
    <span class="n">add_norm_1</span><span class="o">=</span><span class="n">add_norm_config_1</span><span class="p">,</span>
    <span class="n">add_norm_2</span><span class="o">=</span><span class="n">add_norm_config_2</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Create the overall DecoderConfig</span>
<span class="n">model_config</span> <span class="o">=</span> <span class="n">DecoderConfig</span><span class="p">(</span>
    <span class="n">d_model</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">context_length</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span>
    <span class="n">num_decoder_blocks</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">decoder_block</span><span class="o">=</span><span class="n">decoder_block_config</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># patch Composer</span>

<span class="n">composer</span><span class="o">.</span><span class="n">model_config</span> <span class="o">=</span> <span class="n">model_config</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">model_config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">DecoderConfig</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">d_model</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">vocab_size</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">50257</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">context_length</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">8</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">num_decoder_blocks</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">dropout</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">decoder_block</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">DecoderBlockConfig</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">masked_self_attention_mha</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">MultiHeadedAttentionConfig</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #808000; text-decoration-color: #808000">attention</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">ScaledDotProductAttention</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">  </span><span style="font-weight: bold">(</span>dropout<span style="font-weight: bold">)</span>: <span style="color: #800080; text-decoration-color: #800080; font-weight: bold">Dropout</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">p</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0</span>, <span style="color: #808000; text-decoration-color: #808000">inplace</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span><span style="font-weight: bold">)</span>
<span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #808000; text-decoration-color: #808000">d_model</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #808000; text-decoration-color: #808000">H</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #808000; text-decoration-color: #808000">dropout</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">feed_forward</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">PositionwiseFeedForwardConfig</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #808000; text-decoration-color: #808000">d_model</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #808000; text-decoration-color: #808000">d_ff</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #808000; text-decoration-color: #808000">activation</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">GELU</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">approximate</span>=<span style="color: #008000; text-decoration-color: #008000">'tanh'</span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #808000; text-decoration-color: #808000">dropout</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   │   </span><span style="color: #808000; text-decoration-color: #808000">bias</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">add_norm_1</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">AddNormConfig</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">feature_dim</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #808000; text-decoration-color: #808000">dropout</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span><span style="font-weight: bold">)</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="color: #808000; text-decoration-color: #808000">add_norm_2</span>=<span style="color: #800080; text-decoration-color: #800080; font-weight: bold">AddNormConfig</span><span style="font-weight: bold">(</span><span style="color: #808000; text-decoration-color: #808000">feature_dim</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #808000; text-decoration-color: #808000">dropout</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1</span><span style="font-weight: bold">)</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="font-weight: bold">)</span>
<span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
</section>
<section id="decoder-blocks">
<h3><a class="toc-backref" href="#id127" role="doc-backlink">Decoder Blocks</a><a class="headerlink" href="#decoder-blocks" title="Link to this heading">#</a></h3>
<p>The decoder block consists of the following transformations in pseudo-code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ln_1</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ln_2</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
</pre></div>
</div>
<p>Essentially, we take the embeddings output from the token and positional
layers, and go through the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">-&gt;</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">z</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">z</span> <span class="o">+</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
</pre></div>
</div>
<p>then we pass the output through the positionwise FFN, abbreivate <code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">+</span> <span class="pre">MultiHeadAttention(LayerNorm(z))</span></code> as <code class="docutils literal notranslate"><span class="pre">z_mha</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">z_mha</span> <span class="o">-&gt;</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">z_mha</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PositionwiseFFN</span><span class="p">(</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">z_mha</span><span class="p">))</span> <span class="o">-&gt;</span> <span class="n">z_mha</span> <span class="o">+</span> <span class="n">PositionwiseFFN</span><span class="p">(</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">z_mha</span><span class="p">))</span>
</pre></div>
</div>
<p>It is worth noting that my implementation of <code class="docutils literal notranslate"><span class="pre">AddNorm</span></code> is different from the GPT-2 paper, with
<code class="docutils literal notranslate"><span class="pre">self.layer_norm(x</span> <span class="pre">+</span> <span class="pre">sublayer(self.dropout(x)))</span></code> instead of <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">+</span> <span class="pre">sublayer(self.layer_norm(x))</span></code>.
Both works, but the former is the one mentioned in the GPT-2 paper.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GPTDecoderBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;GPTDecoderBlock focuses on masked self-attention and feed-forward layers.</span>

<span class="sd">    The architecture follows the GPT-style decoder, which only has masked</span>
<span class="sd">    self-attention and position-wise feed-forward layers, omitting the</span>
<span class="sd">    encoder-decoder cross-attention.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">DecoderConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># fmt: off</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">masked_self_attention_mha</span> <span class="o">=</span> <span class="n">MultiHeadedAttention</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_block</span><span class="o">.</span><span class="n">masked_self_attention_mha</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;python&quot;</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span>              <span class="o">=</span> <span class="n">PositionwiseFeedForward</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_block</span><span class="o">.</span><span class="n">feed_forward</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;python&quot;</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_norm_1</span>                <span class="o">=</span> <span class="n">AddNorm</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_block</span><span class="o">.</span><span class="n">add_norm_1</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;python&quot;</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add_norm_2</span>                <span class="o">=</span> <span class="n">AddNorm</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="o">.</span><span class="n">decoder_block</span><span class="o">.</span><span class="n">add_norm_2</span><span class="o">.</span><span class="n">model_dump</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;python&quot;</span><span class="p">))</span>
        <span class="c1"># fmt: on</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>  <span class="c1"># that&#39;s tgt in torch code base</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">target_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">,</span>  <span class="c1"># that&#39;s tgt_mask in torch code base</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_norm_1</span><span class="p">(</span>
            <span class="n">z</span><span class="p">,</span>
            <span class="k">lambda</span> <span class="n">z</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">masked_self_attention_mha</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">z</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">z</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">z</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">target_masks</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_norm_2</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">z</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="decoder">
<h3><a class="toc-backref" href="#id128" role="doc-backlink">Decoder</a><a class="headerlink" href="#decoder" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">cast</span><span class="p">,</span> <span class="n">overload</span><span class="p">,</span> <span class="n">Type</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">override</span><span class="p">,</span> <span class="n">TypeAlias</span>

<span class="k">class</span> <span class="nc">_NotGiven</span><span class="p">:</span>

    <span class="n">_instance</span><span class="p">:</span> <span class="n">_NotGiven</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">_NotGiven</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">_NotGiven</span><span class="p">:</span>  <span class="c1"># noqa: PYI034</span>
        <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_instance</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_instance</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">_NotGiven</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>  <span class="c1"># noqa: UP008</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_instance</span>

    <span class="k">def</span> <span class="fm">__bool__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">False</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This method is used to define the boolean value of an instance of `_NotGiven`.</span>
<span class="sd">        By returning `False`, it allows `_NotGiven` to be used in boolean contexts (like</span>
<span class="sd">        `if` statements) to signify the absence of a value. This is especially useful</span>
<span class="sd">        for checking if an argument was provided or not in a function.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="nd">@override</span>
    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;NOT_GIVEN&quot;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="s2">&quot;NOT_GIVEN&quot;</span>

    <span class="k">def</span> <span class="fm">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> instances are immutable&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__delattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> instances are immutable&quot;</span><span class="p">)</span>


<span class="n">NOT_GIVEN</span> <span class="o">=</span> <span class="n">_NotGiven</span><span class="p">()</span>
<span class="n">NotGiven</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="n">_NotGiven</span>


<span class="k">def</span> <span class="nf">construct_dummy_batch_target_padding_masks</span><span class="p">(</span><span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a dummy batch of target padding masks of shape (B, 1, L, L) which</span>
<span class="sd">    assumes there is no padding token involved.&quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">BaseDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Abstract base class for a decoder in a transformer-like architecture.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">DecoderConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_tokens</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>  <span class="c1"># force keyword only arguments to prevent errors</span>
        <span class="n">target_padding_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span> <span class="o">|</span> <span class="n">NotGiven</span> <span class="o">=</span> <span class="n">NOT_GIVEN</span><span class="p">,</span>
        <span class="n">future_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span> <span class="o">|</span> <span class="n">NotGiven</span> <span class="o">=</span> <span class="n">NOT_GIVEN</span><span class="p">,</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">NotGiven</span> <span class="o">=</span> <span class="n">NOT_GIVEN</span><span class="p">,</span>  <span class="c1"># that&#39;s memory in torch code base</span>
        <span class="n">encoder_hidden_states_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span> <span class="o">|</span> <span class="n">NotGiven</span> <span class="o">=</span> <span class="n">NOT_GIVEN</span><span class="p">,</span>  <span class="c1"># that&#39;s memory_mask in torch code base</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">:</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes weights of the given module using Xavier uniform initialization.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">module</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

<span class="c1"># NOTE: seq_len &lt;= context_length == max_seq_len</span>
<span class="k">class</span> <span class="nc">GPTDecoder</span><span class="p">(</span><span class="n">BaseDecoder</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">DecoderConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="c1"># fmt: off</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span>       <span class="p">:</span> <span class="nb">int</span>           <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tok_embed</span>     <span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span>     <span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">context_length</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_blocks</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">GPTDecoderBlock</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_decoder_blocks</span><span class="p">)])</span> <span class="c1"># PyTorch did not make ModuleList a proper container, maybe open a PR to make it inherit Generic[T]???</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span>       <span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span>    <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span>    <span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span>          <span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span>     <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>  <span class="c1"># last layer</span>
        <span class="c1"># fmt: on</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">)</span>

        <span class="c1"># apply special scaled init to the residual projections, per GPT-2 paper</span>
        <span class="k">for</span> <span class="n">parameter_name</span><span class="p">,</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">parameter_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;context_projection.weight&quot;</span><span class="p">):</span>
                <span class="n">mean</span> <span class="o">=</span> <span class="mf">0.0</span>
                <span class="n">std_dev</span> <span class="o">=</span> <span class="mf">0.02</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">num_decoder_blocks</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">))</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">parameter</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std_dev</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">total_trainable_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the number of trainable parameters in the model.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">total_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the total number of parameters in the model, including non-trainable.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>

    <span class="nd">@override</span>
    <span class="k">def</span> <span class="nf">_init_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">normal_init_modules</span> <span class="o">=</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">normal_init_modules</span><span class="p">):</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">&quot;bias&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">module</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="nf">create_target_masks</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">target_padding_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">,</span> <span class="n">future_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">:</span>
        <span class="o">...</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="nf">create_target_masks</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">target_padding_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">,</span> <span class="n">future_masks</span><span class="p">:</span> <span class="n">NotGiven</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">:</span>
        <span class="o">...</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="nf">create_target_masks</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">target_padding_masks</span><span class="p">:</span> <span class="n">NotGiven</span><span class="p">,</span> <span class="n">future_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">:</span>
        <span class="o">...</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="nf">create_target_masks</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">target_padding_masks</span><span class="p">:</span> <span class="n">NotGiven</span><span class="p">,</span> <span class="n">future_masks</span><span class="p">:</span> <span class="n">NotGiven</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">:</span>
        <span class="o">...</span>

    <span class="k">def</span> <span class="nf">create_target_masks</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">target_padding_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span> <span class="o">|</span> <span class="n">NotGiven</span> <span class="o">=</span> <span class="n">NOT_GIVEN</span><span class="p">,</span>
        <span class="n">future_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span> <span class="o">|</span> <span class="n">NotGiven</span> <span class="o">=</span> <span class="n">NOT_GIVEN</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">:</span>
        <span class="n">target_masks_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">target_padding_masks</span> <span class="ow">is</span> <span class="n">NOT_GIVEN</span> <span class="ow">and</span> <span class="n">future_masks</span> <span class="ow">is</span> <span class="n">NOT_GIVEN</span><span class="p">:</span>
            <span class="n">target_padding_masks</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">,</span> <span class="n">construct_dummy_batch_target_padding_masks</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">future_masks</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">,</span> <span class="n">construct_dummy_batch_future_masks</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>

        <span class="c1"># FIXME: CAN SOMEONE PLEASE HELP ME WITH TYPING HERE?? I AM SO STUCK IN CASTING HELL.</span>
        <span class="k">if</span> <span class="n">target_padding_masks</span> <span class="ow">is</span> <span class="n">NOT_GIVEN</span><span class="p">:</span>
            <span class="n">target_padding_masks</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">,</span> <span class="n">construct_dummy_batch_target_padding_masks</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">future_masks</span> <span class="ow">is</span> <span class="n">NOT_GIVEN</span><span class="p">:</span>
            <span class="n">future_masks</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">,</span> <span class="n">construct_dummy_batch_future_masks</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>

        <span class="k">assert</span> <span class="n">target_padding_masks</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">future_masks</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">target_masks_shape</span>  <span class="c1"># type: ignore[union-attr]</span>

        <span class="k">return</span> <span class="n">cast</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">logical_and</span><span class="p">(</span><span class="n">cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">target_padding_masks</span><span class="p">),</span> <span class="n">cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">future_masks</span><span class="p">))</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_tokens</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>  <span class="c1"># force keyword only arguments to prevent errors</span>
        <span class="n">target_padding_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span> <span class="o">|</span> <span class="n">NotGiven</span> <span class="o">=</span> <span class="n">NOT_GIVEN</span><span class="p">,</span>
        <span class="n">future_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span> <span class="o">|</span> <span class="n">NotGiven</span> <span class="o">=</span> <span class="n">NOT_GIVEN</span><span class="p">,</span>
        <span class="c1"># fmt: off</span>
        <span class="n">encoder_hidden_states</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">|</span> <span class="n">NotGiven</span> <span class="o">=</span> <span class="n">NOT_GIVEN</span><span class="p">,</span>  <span class="c1"># that&#39;s memory in torch code base and is ensured not used here</span>
        <span class="n">encoder_hidden_states_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span> <span class="o">|</span> <span class="n">NotGiven</span> <span class="o">=</span> <span class="n">NOT_GIVEN</span><span class="p">,</span>
        <span class="c1"># that&#39;s memory_mask in torch code base and is ensured not used here</span>
        <span class="c1"># fmt: on</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">encoder_hidden_states</span> <span class="ow">is</span> <span class="n">NOT_GIVEN</span><span class="p">,</span> <span class="s2">&quot;GPTDecoderBlock does not have encoder-decoder cross-attention&quot;</span>
        <span class="k">assert</span> <span class="n">encoder_hidden_states_masks</span> <span class="ow">is</span> <span class="n">NOT_GIVEN</span><span class="p">,</span> <span class="s2">&quot;GPTDecoderBlock does not have encoder-decoder cross-attention&quot;</span>

        <span class="c1"># fmt: off</span>
        <span class="n">batch_size</span>  <span class="p">:</span> <span class="nb">int</span>              <span class="o">=</span> <span class="n">input_tokens</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">seq_len</span>     <span class="p">:</span> <span class="nb">int</span>              <span class="o">=</span> <span class="n">input_tokens</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># note seq_len &lt;= context_length in decoder</span>
        <span class="n">target_masks</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_target_masks</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="o">=</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">target_padding_masks</span><span class="o">=</span><span class="n">target_padding_masks</span><span class="p">,</span> <span class="n">future_masks</span><span class="o">=</span><span class="n">future_masks</span><span class="p">)</span>

        <span class="n">target_masks</span> <span class="o">=</span> <span class="n">target_masks</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">input_tokens</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="c1"># type: ignore[assignment]</span>

        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tok_embed</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">)</span> <span class="c1"># * math.sqrt(self.d_model) for better optimization landscape</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">decoder_block</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder_blocks</span><span class="p">:</span>
            <span class="n">z</span>  <span class="o">=</span> <span class="n">decoder_block</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">target_masks</span><span class="o">=</span><span class="n">target_masks</span><span class="p">)</span>

        <span class="n">z</span>      <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">logits</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="c1"># fmt: on</span>
        <span class="k">return</span> <span class="n">logits</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">starting_tokens</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span> <span class="o">|</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>  <span class="c1"># alias is starting_tokens</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>  <span class="c1"># max tokens to generate</span>
        <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>  <span class="c1"># temperature for sampling</span>
        <span class="n">greedy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># if True, sample greedily</span>
        <span class="n">top_k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># if not None, sample from top k tokens</span>
        <span class="n">top_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># neclueus sampling</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="c1"># a safety check to make sure we are not in training mode</span>
            <span class="c1"># this generate could be called outside after training, or during</span>
            <span class="c1"># training as a form of validation/evaluation.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

        <span class="c1"># NOTE: `starting_tokens` is a list of integers, or a torch.LongTensor of shape (S or T).</span>
        <span class="c1"># the distinction between this `starting_tokens` versus the one in `forward` is this is</span>
        <span class="c1"># not batched! It is a single sequence of tokens so in order for it to be compatible</span>
        <span class="c1"># with the model, we need to expand the first dimension to 1 - making it a batch.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">starting_tokens</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">starting_tokens</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">starting_tokens</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">starting_tokens</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">starting_tokens</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">starting_tokens</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="o">...</span><span class="p">])</span>  <span class="c1"># type: ignore[no-redef]</span>
        <span class="k">assert</span> <span class="n">starting_tokens</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;starting_tokens must be a 1D or 2D tensor&quot;</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_tokens</span><span class="p">):</span>
            <span class="c1"># if the sequence context is growing too long we must crop it at context_length</span>
            <span class="n">starting_tokens_cropped</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">starting_tokens</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">context_length</span> <span class="p">:]</span>
                <span class="k">if</span> <span class="n">starting_tokens</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">context_length</span>
                <span class="k">else</span> <span class="n">starting_tokens</span>
            <span class="p">)</span>

            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">starting_tokens_cropped</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">seq_len</span> <span class="o">=</span> <span class="n">starting_tokens_cropped</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># this must be less than or equal to self.config.context_length</span>

            <span class="n">target_padding_masks</span> <span class="o">=</span> <span class="n">construct_dummy_batch_target_padding_masks</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
            <span class="n">future_masks</span> <span class="o">=</span> <span class="n">construct_dummy_batch_future_masks</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>

            <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span>
                <span class="n">starting_tokens_cropped</span><span class="p">,</span>
                <span class="n">target_padding_masks</span><span class="o">=</span><span class="n">target_padding_masks</span><span class="p">,</span>
                <span class="n">future_masks</span><span class="o">=</span><span class="n">future_masks</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>

            <span class="c1"># NOTE: we are only interested in the last token&#39;s logits because in</span>
            <span class="c1"># autoregressive models, the last token&#39;s logits holds the contextual</span>
            <span class="c1"># information of all previous tokens (because it is the only token</span>
            <span class="c1"># not masked). But in any case, we need this last token&#39;s logits to</span>
            <span class="c1"># sample the next token.</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># shape: (batch_size, vocab_size)</span>
            <span class="k">assert</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>

            <span class="c1"># now scale by temperature</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">/</span> <span class="p">(</span><span class="n">temperature</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>  <span class="c1"># add epsilon to prevent division by zero</span>

            <span class="c1"># optional cropping of logits to top k</span>
            <span class="k">if</span> <span class="n">top_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">top_k_values</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">top_k</span><span class="p">)</span>
                <span class="c1"># The masking out to -inf is to prevent the sampling from</span>
                <span class="c1"># non-top k values, effectively making the sampling pool</span>
                <span class="c1"># to be only the top k values. We are zeroing out the</span>
                <span class="c1"># probabilities of non-top k values.</span>
                <span class="n">logits</span><span class="p">[</span><span class="n">logits</span> <span class="o">&lt;</span> <span class="n">top_k_values</span><span class="p">[:,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">top_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>

                <span class="k">def</span> <span class="nf">top_p_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
                    <span class="n">sorted_logits</span><span class="p">,</span> <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                    <span class="n">cumulative_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">sorted_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

                    <span class="c1"># Remove tokens with cumulative probability above the threshold</span>
                    <span class="n">sorted_indices_to_remove</span> <span class="o">=</span> <span class="n">cumulative_probs</span> <span class="o">&gt;</span> <span class="n">p</span>
                    <span class="c1"># Shift the indices to the right to keep also the first token above the threshold</span>
                    <span class="n">sorted_indices_to_remove</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="n">sorted_indices_to_remove</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                    <span class="n">sorted_indices_to_remove</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

                    <span class="c1"># Scatter sorted tensors to original indexing</span>
                    <span class="n">indices_to_remove</span> <span class="o">=</span> <span class="n">sorted_indices</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span>
                        <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">sorted_indices</span><span class="p">,</span> <span class="n">src</span><span class="o">=</span><span class="n">sorted_indices_to_remove</span>
                    <span class="p">)</span>
                    <span class="n">logits</span><span class="p">[</span><span class="n">indices_to_remove</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">)</span>
                    <span class="k">return</span> <span class="n">logits</span>

                <span class="n">logits</span> <span class="o">=</span> <span class="n">top_p_logits</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">top_p</span><span class="p">)</span>

            <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">next_token</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">greedy</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">indices</span>
            <span class="p">)</span>

            <span class="c1"># append the next token to the input tokens, aka append sampled index</span>
            <span class="c1"># to the running sequence context and continue the generation</span>
            <span class="n">starting_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">starting_tokens</span><span class="p">,</span> <span class="n">next_token</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># type: ignore[assignment]</span>
        <span class="k">return</span> <span class="n">starting_tokens</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">generator</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>
<span class="n">seed_all</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">set_torch_deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">train_batch</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="n">composer</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">train_batch</span>
<span class="k">assert</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GPTDecoder</span><span class="p">(</span><span class="n">model_config</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">total_trainable_parameters</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">block_size</span><span class="p">,</span> <span class="n">composer</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">452473</span>
</pre>
</div></div>
</div>
<p>To this end, we have covered the implementations of the GPT-2 architecture, for
the training and inference mechanism, please see the next
<a class="reference external" href="https://www.gaohongnan.com/transformer/decoder/adder.html">series</a>.</p>
</section>
</section>
<section id="references-and-further-readings">
<h2><a class="toc-backref" href="#id129" role="doc-backlink">References and Further Readings</a><a class="headerlink" href="#references-and-further-readings" title="Link to this heading">#</a></h2>
<div class="seealso admonition">
<p class="admonition-title">References</p>
<ul class="simple">
<li><p><a class="reference external" href="https://d2l.ai/chapter_convolutional-modern/batch-norm.html#layer-normalization">Layer Normalization - Dive into Deep Learning</a></p></li>
<li><p><a class="reference external" href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#residual-connection-and-layer-normalization">Residual Connection and Layer Normalization - Dive into Deep Learning</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1607.06450">Layer Normalization - arXiv</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html">PyTorch Documentation: torch.nn.LayerNorm</a></p></li>
<li><p><a class="reference external" href="https://aman.ai/primers/ai/transformers/#positional-encoding">Positional Encodings - Aman Chadha</a></p></li>
<li><p><a class="reference external" href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html">Self-Attention and Positional Encoding - Dive into Deep Learning</a></p></li>
<li><p><a class="reference external" href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">Transformer Architecture: The Positional Encoding</a></p></li>
<li><p><a class="reference external" href="https://e2eml.school/transformers.html#positional_encoding">Positional Encoding - Brandon Rohrer</a></p></li>
<li><p><a class="reference external" href="https://e2eml.school/transformers.html">Transformers from Scratch - Brandon Rohrer</a></p></li>
<li><p><a class="reference external" href="https://aman.ai/primers/ai/transformers/#positional-encoding">Primers • Transformers - Aman Chadha</a></p></li>
<li><p><a class="reference external" href="https://jalammar.github.io/illustrated-gpt2/">The Illustrated GPT-2 (Visualizing Transformer Language Models) - Jay Alammar</a></p></li>
<li><p><a class="reference external" href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2">The Transformer Family Version 2.0 - Lilian Weng</a></p></li>
<li><p><a class="reference external" href="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer">The Random Transformer - hackerllama</a></p></li>
<li><p><a class="reference external" href="https://blog.matdmiller.com/posts/2023-06-10_transformers/notebook.html">Transformers From Scratch - Mat Miller</a></p></li>
<li><p><a class="reference external" href="https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=JB82yzt44REI">Andrej Karpathy GPT Implementation Google Colab</a></p></li>
<li><p><a class="reference external" href="http://www.columbia.edu/~jsl2239/transformers.html">Transformers: a Primer - Justin Seonyong Lee</a></p></li>
<li><p><a class="reference external" href="https://leimao.github.io/article/OpenAI-GPT-Models/">OpenAI GPT Models - Lei Mao</a></p></li>
<li><p><a class="reference external" href="https://leimao.github.io/blog/Transformer-Explained/">Transformer Explained in One Single Page - Lei Mao</a></p></li>
<li><p><a class="reference external" href="https://deepgenerativemodels.github.io/notes/autoregressive/">Autoregressive models</a></p></li>
<li><p><a class="reference external" href="https://nlp.seas.harvard.edu/annotated-transformer/">The Annotated Transformer - Harvard NLP</a></p></li>
<li><p><a class="reference external" href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html">Tutorial 6: Transformers and Multi-Head Attention - UvA</a></p></li>
<li><p><a class="reference external" href="https://eugeneyan.com/writing/attention/">Some Intuition on Attention and the Transformer - Eugene Yan</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Let’s build GPT: from scratch, in code, spelled out - Andrej Karpathy</a></p></li>
<li><p><a class="reference external" href="https://ai.stackexchange.com/questions/20075/why-does-the-transformer-do-better-than-rnn-and-lstm-in-long-range-context-depen">Why does the transformer do better than RNN and LSTM in long-range context dependencies?</a></p></li>
<li><p><a class="reference external" href="https://stackoverflow.com/questions/55158554/how-transformer-is-bidirectional-machine-learning">How Transformer is Bidirectional - Machine Learning</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate - arXiv</a></p></li>
<li><p><a class="reference external" href="https://github.com/rasbt/LLMs-from-scratch">Building Language Models from Scratch - Sebastian Raschka</a></p></li>
<li><p><a class="reference external" href="https://d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#breaking-the-symmetry">Numerical Stability and Initialization - Dive into Deep Learning</a></p></li>
<li><p><a class="reference external" href="https://developers.google.com/machine-learning/gan/generative">Background: What is a Generative Model? - Google Developers</a></p></li>
<li><p><a class="reference external" href="https://ai.stackexchange.com/questions/12579/why-can-we-approximate-the-joint-probability-distribution-using-the-output-vecto">Why can we approximate the joint probability distribution using the output vector of the GPT model?</a></p></li>
<li><p><a class="reference external" href="https://datascience.stackexchange.com/questions/65806/why-joint-probability-in-generative-models">Why Joint Probability in Generative Models? - Data Science Stack Exchange</a></p></li>
<li><p><a class="reference external" href="https://probmlcourse.github.io/csc412/lectures/week_2/">CSC412 Winter 2020: Probabilsitic Machine Learning - University of Toronto</a></p></li>
<li><p><a class="reference external" href="https://stanford-cs324.github.io/winter2022/lectures/introduction/">CS324 - Large Language Models - Stanford University</a></p></li>
<li><p><a class="reference external" href="https://www.probabilitycourse.com/chapter5/5_1_1_joint_pmf.php">Joint Probability Mass Function (PMF)</a></p></li>
<li><p><a class="reference external" href="https://math.stackexchange.com/questions/1566215/difference-between-joint-probability-distribution-and-conditional-probability-di">Difference Between Joint Probability Distribution and Conditional Probability Distribution - Math Stack Exchange</a></p></li>
<li><p><a class="reference external" href="https://d2l.ai/chapter_convolutional-modern/resnet.html">Residual Networks (ResNets) - Dive into Deep Learning</a></p></li>
<li><p><a class="reference external" href="https://songhuiming.github.io/pages/2023/05/28/gpt-1-gpt-2-gpt-3-instructgpt-chatgpt-and-gpt-4-summary/">GPT-1, GPT-2, GPT-3, InstructGPT, ChatGPT, and GPT-4 Summary</a></p></li>
<li><p><a class="reference external" href="https://devblogs.nvidia.com/how-optimize-data-transfers-cuda-cc/">How to Optimize Data Transfers in CUDA C/C++</a></p></li>
<li><p><a class="reference external" href="https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/">How to Overlap Data Transfers in CUDA C/C++</a></p></li>
</ul>
</div>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id40" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://github.com/openai/tiktoken">OpenAI tiktoken</a></p>
</aside>
<aside class="footnote brackets" id="id41" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">2</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://devblogs.nvidia.com/how-optimize-data-transfers-cuda-cc/">How to Optimize Data Transfers in CUDA C/C++</a></p>
</aside>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./influential/generative_pretrained_transformer"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="03_concept.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">The Concept of Generative Pre-trained Transformers (GPT)</p>
      </div>
    </a>
    <a class="right-next"
       href="05_adder.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Training a Mini-GPT to Learn Two-Digit Addition</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dependencies">Dependencies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#composing-the-configurations">Composing the Configurations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducibility">Reproducibility</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#utilities">Utilities</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization-and-vocabulary">Tokenization and Vocabulary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-and-dataloading-poor-man-s-dataloader">Dataset and Dataloading (Poor Man’s Dataloader)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-mapping">Memory Mapping</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-sequence">Input Sequence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#context-length-context-window-block-size">Context Length / Context Window / Block Size</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shuffling-and-discrete-uniform-sampling">Shuffling and Discrete Uniform Sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#construction-input-sequences">Construction Input Sequences</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#construction-target-sequences">Construction Target Sequences</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#asynchronous-data-loading-and-prefetching">Asynchronous Data Loading and Prefetching</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#collating-everything-together">Collating Everything Together</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-pytorch-s-dataset-and-dataloader">Using PyTorch’s Dataset and Dataloader</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generative-pre-trained-transformer-gpt">Generative Pre-trained Transformer (GPT)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#modifications-from-gpt-1-and-model-stability">Modifications from GPT-1 and Model Stability</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-2-variants">GPT-2 Variants</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-2-model-architecture-huggingface">GPT-2 Model Architecture (HuggingFace)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#token-embeddings">Token Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#output-of-the-embedding-layer">Output Of The Embedding Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-representation-of-input-sequence-mathbf-x">One-Hot Representation of Input Sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding-process">One-Hot Encoding Process</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-layer-is-matmul-of-one-hot-encoded-input-matrix-and-embedding-matrix-weights">Embedding Layer Is Matmul Of One-Hot Encoded Input Matrix And Embedding Matrix Weights</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Definition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#lookup">Lookup</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#semantic-representation">Semantic Representation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-embeddings">Positional Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualising-positional-encodings">Visualising Positional Encodings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-of-positional-encoding">An Example of Positional Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encodings-via-embeddings">Positional Encodings Via Embeddings</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">Layer Normalization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learnable-affine-transformation">Learnable Affine Transformation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-norm-stabilises-activation-distributions">Layer Norm Stabilises Activation Distributions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-connection">Residual Connection</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layernorm-and-residual-connection">LayerNorm and Residual Connection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-of-residual-block-and-addnorm">Implementation of Residual Block and AddNorm</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention">Self-Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition-of-attention-mechanism">Intuition of Attention Mechanism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-embedding-and-vector-representation-process">Token Embedding and Vector Representation Process</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#queries-keys-and-values">Queries, Keys, and Values</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#database-analogy">Database Analogy</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#queries-keys-and-values-in-attention-mechanism">Queries, Keys, and Values in Attention Mechanism</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-projections">Linear Projections</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaled-dot-product-attention">Scaled Dot-Product Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">Definition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-scoring-function">Attention Scoring Function</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-down-the-dot-product-of-query-and-key-vectors">Scaling Down the Dot Product of Query and Key Vectors</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">Softmax</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#context-vector-matrix">Context Vector/Matrix</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-stability-and-gradient-saturation">Numerical Stability and Gradient Saturation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-variance-of-dot-product">Visualizing Variance of Dot Product</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#projections-lead-to-dynamic-context-vectors">Projections Lead to Dynamic Context Vectors</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">Implementation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#heatmap">Heatmap</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#masked-causal-self-attention">Masked/Causal Self-Attention</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#last-token-has-full-context">Last Token has Full Context</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computational-complexity-of-self-attention">Computational Complexity of Self-Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention-enables-parallelism">Self-Attention Enables Parallelism</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#complexity-per-layer">Complexity per Layer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sequential-operations">Sequential Operations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-path-length">Maximum Path Length</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention">Multi-Head Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#intuition">Intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id34">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id35">Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#permutation-invariance">Permutation Invariance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applying-layernorm-and-residual-connections-to-multi-head-attention-output">Applying LayerNorm and Residual Connections to Multi-Head Attention Output</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positionwise-feed-forward-networks">Positionwise Feed-Forward Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#independent-processing">Independent Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#identical-application">Identical Application</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id37">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#projection-to-a-higher-dimension-space">Projection to a Higher Dimension Space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-error-linear-unit-gelu">Gaussian Error Linear Unit (GELU)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id39">Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#applying-layernorm-and-residual-connections-to-positionwise-ffn-output">Applying LayerNorm and Residual Connections to Positionwise FFN Output</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-head">Softmax Head</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#putting-it-all-together-to-form-the-gpt">Putting it all Together to form the GPT</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#patch-composer-configuration-with-model-config">Patch Composer Configuration with Model Config</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder-blocks">Decoder Blocks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder">Decoder</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>


  <footer class="bd-footer">
  </footer>
<div class="share-buttons" style="padding: 15px; text-align: center; background-color: #f5f5f5; margin: 15px 0;">
    <div class="container" style="max-width: 500px; margin: 0 auto;">
        <h3 style="margin: 0 0 10px 0; font-size: 1.1em;">Connect with me!</h3>
        <div style="display: flex; justify-content: center; gap: 15px;">
            <!-- LinkedIn Follow Button -->
            <a href="https://www.linkedin.com/in/gao-hongnan/" target="_blank" style="text-decoration: none; flex: 1; max-width: 160px;">
                <button style="background-color: #0077b5; color: white; border: none; padding: 8px 15px; border-radius: 5px; cursor: pointer; width: 100%; font-size: 0.9em;">
                    Follow on LinkedIn
                </button>
            </a>

            <!-- Twitter/X Follow Button -->
            <a href="https://x.com/gaohongnan" target="_blank" style="text-decoration: none; flex: 1; max-width: 160px;">
                <button style="background-color: #000000; color: white; border: none; padding: 8px 15px; border-radius: 5px; cursor: pointer; width: 100%; font-size: 0.9em;">
                    Follow on X
                </button>
            </a>
        </div>

        <h3 style="margin: 12px 0 10px 0; font-size: 1.1em;">Share this page!</h3>
        <div style="display: flex; justify-content: center; gap: 15px;">
            <!-- LinkedIn Share Button -->
            <a href="javascript:void(0);" onclick="window.open('https://www.linkedin.com/sharing/share-offsite/?url=' + encodeURIComponent(window.location.href), 'linkedin-share', 'width=600,height=400')" style="text-decoration: none; flex: 1; max-width: 160px;">
                <button style="background-color: #0077b5; color: white; border: none; padding: 8px 15px; border-radius: 5px; cursor: pointer; width: 100%; font-size: 0.9em;">
                    Share on LinkedIn
                </button>
            </a>

            <!-- Twitter/X Share Button -->
            <a href="javascript:void(0);" onclick="window.open('https://twitter.com/intent/tweet?text=' + encodeURIComponent('Check out this page by @gaohongnan!') + '&url=' + encodeURIComponent(window.location.href), 'twitter-share', 'width=600,height=400')" style="text-decoration: none; flex: 1; max-width: 160px;">
                <button style="background-color: #000000; color: white; border: none; padding: 8px 15px; border-radius: 5px; cursor: pointer; width: 100%; font-size: 0.9em;">
                    Share on X
                </button>
            </a>
        </div>
    </div>
</div>

  </body>
</html>