
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Notations &#8212; Omniverse</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=bb35926c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MYW5YKC2WF"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"defeq": "\\overset{\\text{def}}{=}", "defa": "\\overset{\\text{(a)}}{=}", "defb": "\\overset{\\text{(b)}}{=}", "defc": "\\overset{\\text{(c)}}{=}", "defd": "\\overset{\\text{(d)}}{=}", "st": "\\mid", "mod": "\\mid", "S": "\\Omega", "s": "\\omega", "e": "\\exp", "P": "\\mathbb{P}", "R": "\\mathbb{R}", "expectation": "\\mathbb{E}", "v": "\\mathbf{v}", "a": "\\mathbf{a}", "b": "\\mathbf{b}", "c": "\\mathbf{c}", "u": "\\mathbf{u}", "w": "\\mathbf{w}", "x": "\\mathbf{x}", "y": "\\mathbf{y}", "z": "\\mathbf{z}", "0": "\\mathbf{0}", "1": "\\mathbf{1}", "A": "\\mathbf{A}", "B": "\\mathbf{B}", "C": "\\mathbf{C}", "E": "\\mathcal{F}", "eventA": "\\mathcal{A}", "lset": "\\left\\{", "rset": "\\right\\}", "lsq": "\\left[", "rsq": "\\right]", "lpar": "\\left(", "rpar": "\\right)", "lcurl": "\\left\\{", "rcurl": "\\right\\}", "pmf": "p_X", "pdf": "f_X", "pdftwo": "f_{X,Y}", "pdfjoint": "f_{\\mathbf{X}}", "pmfjointxy": "p_{X, Y}", "pdfjointxy": "f_{X, Y}", "cdf": "F_X", "pspace": "(\\Omega, \\mathcal{F}, \\mathbb{P})", "var": "\\operatorname{Var}", "std": "\\operatorname{Std}", "bern": "\\operatorname{Bernoulli}", "binomial": "\\operatorname{Binomial}", "geometric": "\\operatorname{Geometric}", "poisson": "\\operatorname{Poisson}", "uniform": "\\operatorname{Uniform}", "normal": "\\operatorname{Normal}", "gaussian": "\\operatorname{Gaussian}", "gaussiansymbol": "\\mathcal{N}", "exponential": "\\operatorname{Exponential}", "iid": "\\textbf{i.i.d.}", "and": "\\text{and}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'influential/generative_pretrained_transformer/02_notations';</script>
    <link rel="canonical" href="https://www.gaohongnan.com/influential/generative_pretrained_transformer/02_notations.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="The Concept of Generative Pre-trained Transformers (GPT)" href="03_concept.html" />
    <link rel="prev" title="Generative Pre-trained Transformers" href="01_intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Omniverse - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Omniverse - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    🌌 Omniverse: A Journey Through Knowledge
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notations/machine_learning.html">Machine Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Influential Ideas and Papers</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="01_intro.html">Generative Pre-trained Transformers</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_concept.html">The Concept of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="04_implementation.html">The Implementation of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="05_adder.html">Training a Mini-GPT to Learn Two-Digit Addition</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../low_rank_adaptation/01_intro.html">Low-Rank Adaptation Of Large Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../low_rank_adaptation/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../low_rank_adaptation/03_implementation.html">Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../kmeans_clustering/01_intro.html">K-Means</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/02_concept.html">Concept: K-Means Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/03_implementation.html">Implementation: K-Means (Lloyd)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kmeans_clustering/04_image_segmentation.html">Application: Image Compression and Segmentation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../empirical_risk_minimization/01_intro.html">Empirical Risk Minimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../empirical_risk_minimization/02_concept.html">Concept: Empirical Risk Minimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../empirical_risk_minimization/03_bayes_optimal_classifier.html">Bayes Optimal Classifier</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../learning_theory/01_intro.html">Is The Learning Problem Solvable?</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../learning_theory/02_concept.html">Concept: Learning Theory</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Playbook</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_calculate_flops_in_transformer_based_models.html">How to Calculate the Number of FLOPs in Transformer Based Models?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_inspect_function_and_class_signatures.html">How to Inspect Function and Class Signatures in Python?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/why_cosine_annealing_warmup_stabilize_training.html">Why Does Cosine Annealing With Warmup Stabilize Training?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html">Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling.</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_finetune_decoder_with_last_token_pooling.html">How To Fine-Tune Decoder-Only Models For Sequence Classification Using Last Token Pooling?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_finetune_decoder_with_cross_attention.html">How To Fine-Tune Decoder-Only Models For Sequence Classification With Cross-Attention?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_teacher_student_knowledge_distillation.html">How To Do Teacher-Student Knowledge Distillation?</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Operations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../operations/distributed/intro.html">Distributed Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/01_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/02_basics.html">Basics Of Distributed Data Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/03_how_to_setup_slurm_in_aws.html">How to Setup SLURM and ParallelCluster in AWS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/distributed/04_ablation.html">Ablations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../operations/profiling/intro.html">Profiling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/01_synchronize.html">Synchronize CUDA To Time CUDA Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/02_timeit.html">Profiling Code With Timeit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/03_time_profiler.html">PyTorch’s Event And Profiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/04_small_gpt_profile.html">Profile GPT Small Time And Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/profiling/05_memory_leak.html">CUDA Memory Allocations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../operations/machine_learning_lifecycle/00_intro.html">The Lifecycle of an AIOps System</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/01_problem_formulation.html">Stage 1. Problem Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/02_project_scoping.html">Stage 2. Project Scoping And Framing The Problem</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/03_dataops_pipeline.html">Stage 3. Data Pipeline (Data Engineering and DataOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/031_data_source_and_format.html">Stage 3.1. Data Source and Formats</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/032_data_model_and_storage.html">Stage 3.2. Data Model and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/03_dataops_pipeline/033_etl.html">Stage 3.3. Extract, Transform, Load (ETL)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/04_mlops_data_pipeline.html">Stage 4. Data Extraction (MLOps), Data Analysis (Data Science), Data Preparation (Data Science)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/05_ml_training_pipeline.html">Stage 5. Model Development and Training (MLOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/051_model_selection.html">Stage 5.1. Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/052_metric_selection.html">Stage 5.2. Metric Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/053_experiment_tracking.html">Stage 5.3. Experiment Tracking And Versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/054_model_testing.html">Stage 5.4. Model Testing</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/06_model_evaluation.html">Stage 6. Model Evaluation (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/07_model_validation_registry_and_pushing_model_to_production.html">Stage 7. Model Validation, Registry and Pushing Model to Production (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/08_model_deployment_and_serving.html">Stage 8. Model Serving (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/09_model_monitoring.html">Stage 9. Model Monitoring (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/010_continuous_integration_deployment_learning_and_training.html">Stage 10. Continuous Integration, Deployment, Learning and Training (DevOps, DataOps, MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/machine_learning_lifecycle/011_infrastructure_and_tooling_for_mlops.html">Stage 11. Infrastructure and Tooling for MLOps</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/config_management/concept.html">Configuration Management</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/config_management/01-pydra.html">Pydantic And Hydra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/config_management/02-state.html">State And Metadata Management</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/design_patterns/intro.html">Design Patterns</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/dependency-inversion-principle.html">Dependency Inversion Principle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/strategy.html">Strategy Pattern</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/design_patterns/registry.html">Registry Design Pattern</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/intro.html">Concurrency, Parallelism and Asynchronous Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/generator_yield.html">A Rudimentary Introduction to Generator and Yield in Python</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../computer_science/type_theory/intro.html">Type Theory, A Very Rudimentary Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/01-subtypes.html">Subtypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/02-type-safety.html">Type Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/03-subsumption.html">Subsumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/04-generics.html">Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/05-typevar-bound-constraints.html">Bound and Constraint in Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/06-invariance-covariance-contravariance.html">Invariance, Covariance and Contravariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/07-pep-3124-overloading.html">Function Overloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/08-pep-661-sentinel-values.html">Sentinel Types</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Structures and Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/complexity_analysis/intro.html">Complexity Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/complexity_analysis/master_theorem.html">Master Theorem </a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/stack/intro.html">Stack</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/stack/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/intro.html">Linear Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/intro.html">Binary Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/problems/875-koko-eating-bananas.html">Koko Eating Bananas</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/01_preliminaries/intro.html">Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/01-fields.html">Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/02-systems-of-linear-equations.html">Systems of Linear Equations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/02_vectors/intro.html">Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/01-vector-definition.html">Vector and Its Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/02-vector-operation.html">Vector and Its Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/03-vector-norm.html">Vector Norm and Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/04-vector-products.html">A First Look at Vector Products</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Probability Theory</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/intro.html">Chapter 1. Mathematical Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/01_combinatorics.html">Permutations and Combinations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/02_calculus.html">Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/03_contours.html">Contour Maps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/01_mathematical_preliminaries/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/02_probability/intro.html">Chapter 2. Probability</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0202_probability_space.html">Probability Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0203_probability_axioms.html">Probability Axioms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0204_conditional_probability.html">Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0205_independence.html">Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/0206_bayes_theorem.html">Baye’s Theorem and the Law of Total Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/02_probability/summary.html">Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/intro.html">Chapter 3. Discrete Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0301_random_variables.html">Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0302_discrete_random_variables.html">Discrete Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0303_probability_mass_function.html">Probability Mass Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0304_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0305_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/0306_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/intro.html">Discrete Uniform Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/intro.html">Bernoulli Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/iid.html">Independent and Identically Distributed (IID)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/intro.html">Binomial Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_implementation.html">Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_application.html">Real World Examples</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/geometric/intro.html">Geometric Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/geometric/0310_geometric_distribution_concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/intro.html">Poisson Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_implementation.html">Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/summary.html">Important</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/03_discrete_random_variables/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/intro.html">Chapter 4. Continuous Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/from_discrete_to_continuous.html">From Discrete to Continuous</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0401_continuous_random_variables.html">Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0402_probability_density_function.html">Probability Density Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0403_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0404_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0405_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0406_mean_median_mode.html">Mean, Median and Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0407_continuous_uniform_distribution.html">Continuous Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0408_exponential_distribution.html">Exponential Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0409_gaussian_distribution.html">Gaussian Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0410_skewness_and_kurtosis.html">Skewness and Kurtosis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0411_convolve_and_sum_of_random_variables.html">Convolution and Sum of Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/04_continuous_random_variables/0412_functions_of_random_variables.html">Functions of Random Variables</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/intro.html">Chapter 5. Joint Distributions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../probability_theory/05_joint_distributions/from_single_variable_to_joint_distributions.html">From Single Variable to Joint Distributions</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/intro.html">Joint PMF and PDF</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/intro.html">Joint Expectation and Correlation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/intro.html">Conditional PMF and PDF</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/intro.html">Conditional Expectation and Variance</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0505_sum_of_random_variables/intro.html">Sum of Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0505_sum_of_random_variables/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0506_random_vectors/intro.html">Random Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0506_random_vectors/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/intro.html">Multivariate Gaussian Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/application_transformation.html">Application: Plots and Transformations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/psd.html">Covariance Matrix is Positive Semi-Definite</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/eigendecomposition.html">Eigendecomposition and Covariance Matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/05_joint_distributions/0507_multivariate_gaussian/geometry_of_multivariate_gaussian.html">The Geometry of Multivariate Gaussians</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/intro.html">Chapter 6. Sample Statistics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/intro.html">Moment Generating and Characteristic Functions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function.html">Moment Generating Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function_application_sum_of_rv.html">Application: Moment Generating Function and the Sum of Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/characteristic_function.html">Characteristic Function</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/intro.html">Probability Inequalities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/concept.html">Probability Inequalities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0602_probability_inequalities/application.html">Application: Learning Theory</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/intro.html">Law of Large Numbers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/convergence.html">Convergence of Sample Average</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/06_sample_statistics/0603_law_of_large_numbers/application.html">Application: Learning Theory</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../probability_theory/08_estimation_theory/intro.html">Chapter 8. Estimation Theory</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../probability_theory/08_estimation_theory/maximum_likelihood_estimation/intro.html">Maximum Likelihood Estimation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html">Concept</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../citations.html">IEEE (Style) Citations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../api/reproducibility.html">API Reference</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse/issues/new?title=Issue%20on%20page%20%2Finfluential/generative_pretrained_transformer/02_notations.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/influential/generative_pretrained_transformer/02_notations.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Notations</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensions-and-indexing">Dimensions and Indexing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-notations">General Notations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elementwise-and-vectorwise-operations">Elementwise and Vectorwise Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vocabulary">Vocabulary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-sequence">Input Sequence</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#batched-input-sequences">Batched Input Sequences</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-to-index-and-index-to-token-mappings">Token to Index, and Index to Token Mappings</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#string-to-index-mapping">String-to-Index Mapping</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#index-to-string-mapping">Index-to-String Mapping</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-representation-of-input-sequence-mathbf-x">One-Hot Representation of Input Sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding-process">One-Hot Encoding Process</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#batched">Batched</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#weights-and-embeddings">Weights And Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-multiplication-primer">Matrix Multiplication Primer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathbf-x-output-of-the-embedding-layer"><span class="math notranslate nohighlight">\(\mathbf{X}\)</span>: Output of the Embedding Layer</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Definition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#lookup">Lookup</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#semantic-representation">Semantic Representation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathbf-w-e-embedding-matrix"><span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span>: Embedding Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pe-positional-encoding-layer"><span class="math notranslate nohighlight">\(PE\)</span>: Positional Encoding Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tilde-mathbf-x-output-of-the-positional-encoding-layer"><span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span>: Output of the Positional Encoding Layer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">Layer Normalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-notations">Attention Notations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensions">Dimensions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#query-key-and-values">Query, Key and Values</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-attention-mechanism">General Attention Mechanism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention-for-layer-ell">Multi-Head Attention for Layer <span class="math notranslate nohighlight">\(\ell\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#masked-multi-head-attention-for-decoder-layer-ell">Masked Multi-Head Attention for Decoder Layer <span class="math notranslate nohighlight">\(\ell\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#updated-matrix-description-table-with-batch-and-head-dimensions">Updated Matrix Description Table with Batch and Head Dimensions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positionwise-feed-forward-networks">Positionwise Feed-Forward Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#independent-processing">Independent Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#identical-application">Identical Application</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#projection-to-a-higher-dimension-space">Projection to a Higher Dimension Space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-error-linear-unit-gelu">Gaussian Error Linear Unit (GELU)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-training-phase">The Training Phase</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autoregressive-self-supervised-learning-paradigm">Autoregressive Self-Supervised Learning Paradigm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#corpus-and-tokenization">Corpus and Tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-embedding-and-positional-encoding">Token Embedding and Positional Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backbone-architecture">Backbone Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-process-through-l-decoder-blocks">Iterative Process Through L Decoder Blocks</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#first-decoder-block-ell-1">First Decoder Block (<span class="math notranslate nohighlight">\(\ell = 1\)</span>)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#subsequent-decoder-blocks-ell-1">Subsequent Decoder Blocks (<span class="math notranslate nohighlight">\(\ell &gt; 1\)</span>)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization-before-projection">Layer Normalization Before Projection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#head">Head</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-layer">Softmax Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-entropy-loss-function">Cross-Entropy Loss Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#table-of-notations">Table of Notations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="notations">
<h1>Notations<a class="headerlink" href="#notations" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://twitter.com/gaohongnan"><img alt="Twitter Handle" src="https://img.shields.io/badge/Twitter-&#64;gaohongnan-blue?style=social&amp;logo=twitter" /></a>
<a class="reference external" href="https://linkedin.com/in/gao-hongnan"><img alt="LinkedIn Profile" src="https://img.shields.io/badge/&#64;gaohongnan-blue?style=social&amp;logo=linkedin" /></a>
<a class="reference external" href="https://github.com/gao-hongnan"><img alt="GitHub Profile" src="https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&amp;logo=github" /></a>
<img alt="Tag" src="https://img.shields.io/badge/Tag-Organized_Chaos-orange" /></p>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#dimensions-and-indexing" id="id5">Dimensions and Indexing</a></p></li>
<li><p><a class="reference internal" href="#general-notations" id="id6">General Notations</a></p>
<ul>
<li><p><a class="reference internal" href="#elementwise-and-vectorwise-operations" id="id7">Elementwise and Vectorwise Operations</a></p></li>
<li><p><a class="reference internal" href="#vocabulary" id="id8">Vocabulary</a></p></li>
<li><p><a class="reference internal" href="#input-sequence" id="id9">Input Sequence</a></p>
<ul>
<li><p><a class="reference internal" href="#batched-input-sequences" id="id10">Batched Input Sequences</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#token-to-index-and-index-to-token-mappings" id="id11">Token to Index, and Index to Token Mappings</a></p>
<ul>
<li><p><a class="reference internal" href="#string-to-index-mapping" id="id12">String-to-Index Mapping</a></p></li>
<li><p><a class="reference internal" href="#index-to-string-mapping" id="id13">Index-to-String Mapping</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#one-hot-representation-of-input-sequence-mathbf-x" id="id14">One-Hot Representation of Input Sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span></a></p>
<ul>
<li><p><a class="reference internal" href="#definition" id="id15">Definition</a></p></li>
<li><p><a class="reference internal" href="#one-hot-encoding-process" id="id16">One-Hot Encoding Process</a></p></li>
<li><p><a class="reference internal" href="#batched" id="id17">Batched</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#weights-and-embeddings" id="id18">Weights And Embeddings</a></p>
<ul>
<li><p><a class="reference internal" href="#matrix-multiplication-primer" id="id19">Matrix Multiplication Primer</a></p></li>
<li><p><a class="reference internal" href="#mathbf-x-output-of-the-embedding-layer" id="id20"><span class="math notranslate nohighlight">\(\mathbf{X}\)</span>: Output of the Embedding Layer</a></p>
<ul>
<li><p><a class="reference internal" href="#id1" id="id21">Definition</a></p></li>
<li><p><a class="reference internal" href="#lookup" id="id22">Lookup</a></p></li>
<li><p><a class="reference internal" href="#semantic-representation" id="id23">Semantic Representation</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#mathbf-w-e-embedding-matrix" id="id24"><span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span>: Embedding Matrix</a></p></li>
<li><p><a class="reference internal" href="#pe-positional-encoding-layer" id="id25"><span class="math notranslate nohighlight">\(PE\)</span>: Positional Encoding Layer</a></p></li>
<li><p><a class="reference internal" href="#tilde-mathbf-x-output-of-the-positional-encoding-layer" id="id26"><span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span>: Output of the Positional Encoding Layer</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#layer-normalization" id="id27">Layer Normalization</a></p></li>
<li><p><a class="reference internal" href="#attention-notations" id="id28">Attention Notations</a></p>
<ul>
<li><p><a class="reference internal" href="#dimensions" id="id29">Dimensions</a></p></li>
<li><p><a class="reference internal" href="#query-key-and-values" id="id30">Query, Key and Values</a></p></li>
<li><p><a class="reference internal" href="#general-attention-mechanism" id="id31">General Attention Mechanism</a></p></li>
<li><p><a class="reference internal" href="#multi-head-attention-for-layer-ell" id="id32">Multi-Head Attention for Layer <span class="math notranslate nohighlight">\(\ell\)</span></a></p></li>
<li><p><a class="reference internal" href="#masked-multi-head-attention-for-decoder-layer-ell" id="id33">Masked Multi-Head Attention for Decoder Layer <span class="math notranslate nohighlight">\(\ell\)</span></a></p></li>
<li><p><a class="reference internal" href="#updated-matrix-description-table-with-batch-and-head-dimensions" id="id34">Updated Matrix Description Table with Batch and Head Dimensions</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#positionwise-feed-forward-networks" id="id35">Positionwise Feed-Forward Networks</a></p>
<ul>
<li><p><a class="reference internal" href="#independent-processing" id="id36">Independent Processing</a></p></li>
<li><p><a class="reference internal" href="#identical-application" id="id37">Identical Application</a></p></li>
<li><p><a class="reference internal" href="#id2" id="id38">Definition</a></p></li>
<li><p><a class="reference internal" href="#projection-to-a-higher-dimension-space" id="id39">Projection to a Higher Dimension Space</a></p></li>
<li><p><a class="reference internal" href="#gaussian-error-linear-unit-gelu" id="id40">Gaussian Error Linear Unit (GELU)</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#the-training-phase" id="id41">The Training Phase</a></p>
<ul>
<li><p><a class="reference internal" href="#autoregressive-self-supervised-learning-paradigm" id="id42">Autoregressive Self-Supervised Learning Paradigm</a></p></li>
<li><p><a class="reference internal" href="#corpus-and-tokenization" id="id43">Corpus and Tokenization</a></p></li>
<li><p><a class="reference internal" href="#token-embedding-and-positional-encoding" id="id44">Token Embedding and Positional Encoding</a></p></li>
<li><p><a class="reference internal" href="#backbone-architecture" id="id45">Backbone Architecture</a></p></li>
<li><p><a class="reference internal" href="#iterative-process-through-l-decoder-blocks" id="id46">Iterative Process Through L Decoder Blocks</a></p>
<ul>
<li><p><a class="reference internal" href="#first-decoder-block-ell-1" id="id47">First Decoder Block (<span class="math notranslate nohighlight">\(\ell = 1\)</span>)</a></p></li>
<li><p><a class="reference internal" href="#subsequent-decoder-blocks-ell-1" id="id48">Subsequent Decoder Blocks (<span class="math notranslate nohighlight">\(\ell &gt; 1\)</span>)</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#layer-normalization-before-projection" id="id49">Layer Normalization Before Projection</a></p></li>
<li><p><a class="reference internal" href="#head" id="id50">Head</a></p></li>
<li><p><a class="reference internal" href="#softmax-layer" id="id51">Softmax Layer</a></p></li>
<li><p><a class="reference internal" href="#cross-entropy-loss-function" id="id52">Cross-Entropy Loss Function</a></p></li>
<li><p><a class="reference internal" href="#table-of-notations" id="id53">Table of Notations</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#references" id="id54">References</a></p></li>
</ul>
</nav>
<section id="dimensions-and-indexing">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Dimensions and Indexing</a><a class="headerlink" href="#dimensions-and-indexing" title="Link to this heading">#</a></h2>
<p>This section outlines the common dimensions and indexing conventions utilized in
the Transformer model.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{B}\)</span>: The minibatch size.</p></li>
<li><p><span class="math notranslate nohighlight">\(D\)</span>: Embedding dimension. In the original Transformer paper, this is
represented as <span class="math notranslate nohighlight">\(d_{\text{model}}\)</span>.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(d\)</span>: Index within the embedding vector, where <span class="math notranslate nohighlight">\(0 \leq d &lt; D\)</span>.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(T\)</span>: Sequence length.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(t\)</span>: Positional index of a token within the sequence, where
<span class="math notranslate nohighlight">\(0 \leq t &lt; T\)</span>.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(V\)</span>: Size of the vocabulary.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(j\)</span>: Index of a word in the vocabulary, where <span class="math notranslate nohighlight">\(0 \leq j &lt; V\)</span>.</p></li>
</ul>
</li>
</ul>
</section>
<section id="general-notations">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">General Notations</a><a class="headerlink" href="#general-notations" title="Link to this heading">#</a></h2>
<section id="elementwise-and-vectorwise-operations">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">Elementwise and Vectorwise Operations</a><a class="headerlink" href="#elementwise-and-vectorwise-operations" title="Link to this heading">#</a></h3>
<p>Element-wise operations like dropout or activation functions are applied to each
element of a tensor independently. For example, applying the ReLU activation
function to a tensor <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{B \times T \times D}\)</span> results
in a tensor of the same shape, where the ReLU function is applied to each
element of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> independently (i.e. you can think of it as applying the
ReLU a total of <span class="math notranslate nohighlight">\(B \times T \times D\)</span> times).</p>
<p>For vector-wise operations, the operation is applied to each vector along a
specific <em>dimension</em> or <em>axis</em> of the tensor. For example, applying layer
normalization to a tensor <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{B \times T \times D}\)</span> will
apply the normalization operation to each vector along the feature dimension <span class="math notranslate nohighlight">\(D\)</span>
independently. This means that the normalization operation is applied to each
vector of size <span class="math notranslate nohighlight">\(D\)</span> independently across all batches and sequence positions. You
can then think of the normalization operation as being applied a total of
<span class="math notranslate nohighlight">\(B \times T\)</span> times.</p>
</section>
<section id="vocabulary">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">Vocabulary</a><a class="headerlink" href="#vocabulary" title="Link to this heading">#</a></h3>
<p><span class="math notranslate nohighlight">\(\mathcal{V}\)</span>: The set of all unique words in the vocabulary, defined as:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{V} = \{w_1, w_2, \ldots, w_V\}
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(V\)</span> (denoted as <span class="math notranslate nohighlight">\(|\mathcal{V}|\)</span>): The size of the vocabulary.</p></li>
<li><p><span class="math notranslate nohighlight">\(w_j\)</span>: A unique word in the vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>, where
<span class="math notranslate nohighlight">\(w_j \in \mathcal{V}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(j\)</span>: The index of a word in <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>, explicitly defined as
<span class="math notranslate nohighlight">\(0 \leq j \leq V\)</span>.</p></li>
</ul>
<p>For example, consider the following sentences in the training set:</p>
<ul class="simple">
<li><p>“cat eat mouse”</p></li>
<li><p>“dog chase cat”</p></li>
<li><p>“mouse eat cheese”</p></li>
</ul>
<p>The resulting vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{V} = \{\text{cat}, \text{eat}, \text{mouse}, \text{dog}, \text{chase}, \text{cheese}\}
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(V = 6\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(w_1 = \text{cat}, w_2 = \text{eat}, w_3 = \text{mouse}, w_4 = \text{dog}, w_5 = \text{chase}, w_6 = \text{cheese}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(j = 1, 2, \ldots, 6\)</span>.</p></li>
</ul>
<p>Note: Depending on the transformer model, special tokens (e.g., <code class="docutils literal notranslate"><span class="pre">[PAD]</span></code>,
<code class="docutils literal notranslate"><span class="pre">[CLS]</span></code>, <code class="docutils literal notranslate"><span class="pre">[BOS]</span></code>, <code class="docutils literal notranslate"><span class="pre">[EOS]</span></code>, <code class="docutils literal notranslate"><span class="pre">[UNK]</span></code>, etc.) may also be included in <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p>
</section>
<section id="input-sequence">
<h3><a class="toc-backref" href="#id9" role="doc-backlink">Input Sequence</a><a class="headerlink" href="#input-sequence" title="Link to this heading">#</a></h3>
<p>The input sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> for a GPT model is defined as a sequence of <span class="math notranslate nohighlight">\(T\)</span>
tokens. Each token in this sequence is typically represented as an integer that
corresponds to a position in the vocabulary set <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>. The sequence is
represented as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x} = (x_1, x_2, \ldots, x_T) \in \mathbb{Z}^{1 \times T}
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(T\)</span>: Total length of the sequence. It denotes the number of tokens in the
sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_t\)</span>: Represents a token at position <span class="math notranslate nohighlight">\(t\)</span> within the sequence. Each token
<span class="math notranslate nohighlight">\(x_t\)</span> is an integer where <span class="math notranslate nohighlight">\(0 \leq x_t &lt; V\)</span>. Here, <span class="math notranslate nohighlight">\(V\)</span> is the size of the
vocabulary, and each integer corresponds to a unique word or symbol in
<span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(t\)</span>: The index of a token within the sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, where
<span class="math notranslate nohighlight">\(1 \leq t \leq T\)</span>.</p></li>
</ul>
<section id="batched-input-sequences">
<h4><a class="toc-backref" href="#id10" role="doc-backlink">Batched Input Sequences</a><a class="headerlink" href="#batched-input-sequences" title="Link to this heading">#</a></h4>
<p>In practice, GPT models are often trained on batches of sequences to improve
computational efficiency. A batched input is represented as
<span class="math notranslate nohighlight">\(\mathbf{x}^{\mathcal{B}}\)</span>, where <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> denotes the batch size. The
batched input <span class="math notranslate nohighlight">\(\mathbf{x}^{\mathcal{B}}\)</span> can be visualized as a matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{x}^{\mathcal{B}} = \begin{bmatrix}
x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,T} \\
x_{2,1} &amp; x_{2,2} &amp; \cdots &amp; x_{2,T} \\
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
x_{\mathcal{B},1} &amp; x_{\mathcal{B},2} &amp; \cdots &amp; x_{\mathcal{B},T}
\end{bmatrix} \in \mathbb{Z}^{\mathcal{B} \times T}
\end{split}\]</div>
<p>In this matrix:</p>
<ul class="simple">
<li><p>Each row corresponds to a sequence in the batch.</p></li>
<li><p>Each column corresponds to a token position across all sequences in the
batch.</p></li>
<li><p><span class="math notranslate nohighlight">\(x_{b,t}\)</span> refers to the token at position <span class="math notranslate nohighlight">\(t\)</span> in sequence <span class="math notranslate nohighlight">\(b\)</span>, with
<span class="math notranslate nohighlight">\(1 \leq b \leq \mathcal{B}\)</span> and <span class="math notranslate nohighlight">\(1 \leq t \leq T\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(T\)</span> and <span class="math notranslate nohighlight">\(V\)</span> are as defined previously.</p></li>
</ul>
</section>
</section>
<section id="token-to-index-and-index-to-token-mappings">
<h3><a class="toc-backref" href="#id11" role="doc-backlink">Token to Index, and Index to Token Mappings</a><a class="headerlink" href="#token-to-index-and-index-to-token-mappings" title="Link to this heading">#</a></h3>
<section id="string-to-index-mapping">
<h4><a class="toc-backref" href="#id12" role="doc-backlink">String-to-Index Mapping</a><a class="headerlink" href="#string-to-index-mapping" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
f_{\text{stoi}} : \mathcal{V} \to \{0, 1, \ldots, V-1\}
\]</div>
<ul class="simple">
<li><p><strong>Function</strong>: <span class="math notranslate nohighlight">\(f_{\text{stoi}}\)</span></p></li>
<li><p><strong>Domain</strong>: <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>, the set of all tokens in the vocabulary.</p></li>
<li><p><strong>Codomain</strong>: <span class="math notranslate nohighlight">\(\{0, 1, \ldots, V-1\}\)</span>, where <span class="math notranslate nohighlight">\(V\)</span> is the size of the
vocabulary.</p></li>
<li><p><strong>Purpose</strong>: This function maps each token (word) from the vocabulary to a
unique index. For a token <span class="math notranslate nohighlight">\(w \in \mathcal{V}\)</span>, the value
<span class="math notranslate nohighlight">\(f_{\text{stoi}}(w) = j\)</span> indicates that the token <span class="math notranslate nohighlight">\(w\)</span> corresponds to the
<span class="math notranslate nohighlight">\(j\)</span>-th position in the vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p></li>
<li><p><strong>Example</strong>: If <span class="math notranslate nohighlight">\(\mathcal{V} = \{\text{cat}, \text{dog}, \text{mouse}\}\)</span> and
<span class="math notranslate nohighlight">\(V = 3\)</span>, then <span class="math notranslate nohighlight">\(f_{\text{stoi}}(\text{cat}) = 0\)</span>,
<span class="math notranslate nohighlight">\(f_{\text{stoi}}(\text{dog}) = 1\)</span>, and <span class="math notranslate nohighlight">\(f_{\text{stoi}}(\text{mouse}) = 2\)</span>.</p></li>
</ul>
</section>
<section id="index-to-string-mapping">
<h4><a class="toc-backref" href="#id13" role="doc-backlink">Index-to-String Mapping</a><a class="headerlink" href="#index-to-string-mapping" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[
f_{\text{itos}} : \{0, 1, \ldots, V-1\} \to \mathcal{V}
\]</div>
<ul class="simple">
<li><p><strong>Function</strong>: <span class="math notranslate nohighlight">\(f_{\text{itos}}\)</span></p></li>
<li><p><strong>Domain</strong>: <span class="math notranslate nohighlight">\(\{0, 1, \ldots, V-1\}\)</span></p></li>
<li><p><strong>Codomain</strong>: <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>, the set of all tokens in the vocabulary.</p></li>
<li><p><strong>Purpose</strong>: This function maps each index back to its corresponding token
(word) in the vocabulary. For an index <span class="math notranslate nohighlight">\(j\)</span>, the value
<span class="math notranslate nohighlight">\(f_{\text{itos}}(j) = w\)</span> indicates that the index <span class="math notranslate nohighlight">\(j\)</span> corresponds to the
token <span class="math notranslate nohighlight">\(w\)</span> in the vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p></li>
<li><p><strong>Example</strong>: Continuing the previous example,
<span class="math notranslate nohighlight">\(f_{\text{itos}}(0) = \text{cat}\)</span>, <span class="math notranslate nohighlight">\(f_{\text{itos}}(1) = \text{dog}\)</span>, and
<span class="math notranslate nohighlight">\(f_{\text{itos}}(2) = \text{mouse}\)</span>.</p></li>
</ul>
</section>
</section>
<section id="one-hot-representation-of-input-sequence-mathbf-x">
<h3><a class="toc-backref" href="#id14" role="doc-backlink">One-Hot Representation of Input Sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span></a><a class="headerlink" href="#one-hot-representation-of-input-sequence-mathbf-x" title="Link to this heading">#</a></h3>
<p>The one-hot representation of the input sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is denoted as
<span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span>. This representation converts each token in the
sequence to a one-hot encoded vector, where each vector has a length equal to
the size of the vocabulary <span class="math notranslate nohighlight">\(V\)</span>.</p>
<section id="definition">
<h4><a class="toc-backref" href="#id15" role="doc-backlink">Definition</a><a class="headerlink" href="#definition" title="Link to this heading">#</a></h4>
<p>The one-hot encoded matrix <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X}^{\text{ohe}} = \begin{bmatrix}
o_{1,1} &amp; o_{1,2} &amp; \cdots &amp; o_{1,V} \\
o_{2,1} &amp; o_{2,2} &amp; \cdots &amp; o_{2,V} \\
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
o_{T,1} &amp; o_{T,2} &amp; \cdots &amp; o_{T,V}
\end{bmatrix} \in \{0, 1\}^{T \times V}
\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(T\)</span>: Total length of the sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(V\)</span>: Size of the vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(o_{t,j}\)</span>: Element of the one-hot encoded matrix <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span>
at row <span class="math notranslate nohighlight">\(t\)</span> and column <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
</ul>
<p>In addition, we have:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> is a <span class="math notranslate nohighlight">\(T \times V\)</span> matrix.</p></li>
<li><p>Elements of <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> are binary, i.e., they belong to
<span class="math notranslate nohighlight">\(\{0, 1\}\)</span>.</p></li>
<li><p>The row vector <span class="math notranslate nohighlight">\(\mathbf{o}_{t, :}\)</span> represents the one-hot encoded vector for
the token at position <span class="math notranslate nohighlight">\(t\)</span> in the sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p></li>
</ul>
</section>
<section id="one-hot-encoding-process">
<h4><a class="toc-backref" href="#id16" role="doc-backlink">One-Hot Encoding Process</a><a class="headerlink" href="#one-hot-encoding-process" title="Link to this heading">#</a></h4>
<p>For each token <span class="math notranslate nohighlight">\(x_t\)</span> at position <span class="math notranslate nohighlight">\(t\)</span> in the sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>
(<span class="math notranslate nohighlight">\(1 \leq t \leq T\)</span>), the corresponding row vector <span class="math notranslate nohighlight">\(\mathbf{o}_{t, :}\)</span> in
<span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{o}_{t, j} = \begin{cases}
1 &amp; \text{if } f_{\text{stoi}}(x_t) = j-1\\
0 &amp; \text{otherwise}
\end{cases}
\end{split}\]</div>
<p>for <span class="math notranslate nohighlight">\(j = 1, 2, \ldots, V\)</span>.</p>
<p>Here, <span class="math notranslate nohighlight">\(f_{\text{stoi}}(x_t)\)</span> maps the token <span class="math notranslate nohighlight">\(x_t\)</span> to its index <span class="math notranslate nohighlight">\(j-1\)</span> in the
vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>, the <span class="math notranslate nohighlight">\(j-1\)</span> is because zero-based indexing used in
python (where <span class="math notranslate nohighlight">\(0 \leq j-1 &lt; V\)</span>). Each row <span class="math notranslate nohighlight">\(\mathbf{o}_{t, :}\)</span> in
<span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> contains a single ‘1’ at the column <span class="math notranslate nohighlight">\(j\)</span> corresponding
to the vocabulary index of <span class="math notranslate nohighlight">\(x_t\)</span>, and ‘0’s elsewhere.</p>
<div class="proof example admonition" id="gpt-notations-one-hot-example">
<p class="admonition-title"><span class="caption-number">Example 1 </span> (Example)</p>
<section class="example-content" id="proof-content">
<p>For example, if the vocabulary
<span class="math notranslate nohighlight">\(\mathcal{V} = \{\text{cat}, \text{dog}, \text{mouse}\}\)</span> and the sequence
<span class="math notranslate nohighlight">\(\mathbf{x} = (\text{mouse}, \text{dog})\)</span>, then the one-hot encoded matrix
<span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> will be:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X}^{\text{ohe}} = \begin{bmatrix}
0 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0
\end{bmatrix} \in \{0, 1\}^{2 \times 3}
\end{split}\]</div>
<p>In this example:</p>
<ul class="simple">
<li><p>The sequence length <span class="math notranslate nohighlight">\(T = 2\)</span>.</p></li>
<li><p>The vocabulary size <span class="math notranslate nohighlight">\(V = 3\)</span>.</p></li>
<li><p>“mouse” corresponds to the third position in the vocabulary, and “dog” to
the second, which is seen in their respective one-hot vectors.</p></li>
</ul>
</section>
</div></section>
<section id="batched">
<h4><a class="toc-backref" href="#id17" role="doc-backlink">Batched</a><a class="headerlink" href="#batched" title="Link to this heading">#</a></h4>
<p>The batched one-hot encoded matrix <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe, }\mathcal{B}}\)</span> for a
batch of size <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> is defined as a three-dimensional tensor, where each
“slice” (or matrix) along the first dimension corresponds to the one-hot encoded
representation of a sequence in the batch:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X}^{\text{ohe, }\mathcal{B}} = \begin{bmatrix}
\mathbf{X}^{\text{ohe}}_1 \\
\mathbf{X}^{\text{ohe}}_2 \\
\vdots \\
\mathbf{X}^{\text{ohe}}_{\mathcal{B}}
\end{bmatrix} \in \{0, 1\}^{\mathcal{B} \times T \times V}
\end{split}\]</div>
<p>Here:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{B}\)</span>: Batch size, the number of sequences processed together.</p></li>
<li><p><span class="math notranslate nohighlight">\(T\)</span>: Length of each sequence, assumed uniform across the batch.</p></li>
<li><p><span class="math notranslate nohighlight">\(V\)</span>: Size of the vocabulary.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}_b\)</span>: One-hot encoded matrix of the <span class="math notranslate nohighlight">\(b^{th}\)</span> sequence
in the batch.</p></li>
</ul>
</section>
</section>
</section>
<section id="weights-and-embeddings">
<h2><a class="toc-backref" href="#id18" role="doc-backlink">Weights And Embeddings</a><a class="headerlink" href="#weights-and-embeddings" title="Link to this heading">#</a></h2>
<section id="matrix-multiplication-primer">
<h3><a class="toc-backref" href="#id19" role="doc-backlink">Matrix Multiplication Primer</a><a class="headerlink" href="#matrix-multiplication-primer" title="Link to this heading">#</a></h3>
<p>See
<a class="reference external" href="https://math.stackexchange.com/questions/2063241/matrix-multiplication-notation">source</a>.</p>
<blockquote>
<div><p>If <span class="math notranslate nohighlight">\(A=(a_{ij})\in M_{mn}(\Bbb F), B=(b_{ij})\in M_{np}(\Bbb F)\)</span> then
<span class="math notranslate nohighlight">\(C=A\times B=(c_{ij})\in M_{mp}(\Bbb F)\)</span>. <span class="math notranslate nohighlight">\(c_{ij}=\sum_{k=1}^{n} a_{ik}b_{kj}\)</span>
where <span class="math notranslate nohighlight">\(i=1,...m, j=1,...p\)</span></p>
</div></blockquote>
<p>Let’s take a look at one specific element in the product <span class="math notranslate nohighlight">\(C=AB\)</span>, namely the
element on position <span class="math notranslate nohighlight">\((i,j)\)</span>, i.e. in the <span class="math notranslate nohighlight">\(i\)</span>th row and <span class="math notranslate nohighlight">\(j\)</span>th column.</p>
<p>To obtain this element, you:</p>
<ul class="simple">
<li><p>first <strong>multiply</strong> all elements of the <em><span class="math notranslate nohighlight">\(i\)</span>th row</em> of the matrix <span class="math notranslate nohighlight">\(A\)</span>
<em>pairwise</em> with all the elements of the <em><span class="math notranslate nohighlight">\(j\)</span>th column</em> of the matrix <span class="math notranslate nohighlight">\(B\)</span>;</p></li>
<li><p>and then you <strong>add</strong> these <span class="math notranslate nohighlight">\(n\)</span> products.</p></li>
</ul>
<p>You have to repeat this procedure for every element of <span class="math notranslate nohighlight">\(C\)</span>, but let’s zoom in on
that one specific (but arbitrary) element on position <span class="math notranslate nohighlight">\((i,j)\)</span> for now:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
a_{11} &amp;\ldots  &amp;a_{1n}\\
\vdots&amp; \ddots &amp;\vdots\\
\color{blue}{\mathbf{a_{i1}}} &amp;\color{blue}{\rightarrow}  &amp;\color{blue}{\mathbf{a_{in}}}\\
\vdots&amp;  \ddots &amp;\vdots\\
a_{m1} &amp;\ldots &amp;a_{mn}
\end{pmatrix}
\cdot
\begin{pmatrix}
b_{11}&amp;\ldots &amp;\color{red}{\mathbf{b_{1j}}} &amp;\ldots &amp;b_{1p}\\
\vdots&amp; \ddots &amp;\color{red}{\downarrow} &amp;  \ddots  &amp;\vdots\\
b_{n1}&amp;\ldots &amp;\color{red}{\mathbf{b_{nj}}}&amp;\ldots &amp;b_{np}
\end{pmatrix}
=
\begin{pmatrix}
c_{11}&amp;\ldots&amp; c_{1j} &amp;\ldots &amp;c_{1p}\\
\vdots&amp;  \ddots &amp; &amp; &amp;\vdots\\
c_{i1}&amp; &amp; \color{purple}{\mathbf{c_{ij}}} &amp; &amp;c_{ip}\\
\vdots&amp; &amp;  &amp; \ddots &amp;\vdots\\
c_{m1} &amp;\ldots&amp; c_{mj} &amp;\ldots &amp;c_{mp}
\end{pmatrix}
\end{split}\]</div>
<p>with element <span class="math notranslate nohighlight">\(\color{purple}{\mathbf{c_{ij}}}\)</span> equal to:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{\color{purple}{c_{ij}}  =  \color{blue}{a_{i1}} \color{red}{b_{1j}}  + \color{blue}{a_{i2}} \color{red}{b_{2j}}  +  \cdots  + \color{blue}{a_{in}} \color{red}{b_{nj}}}
\]</div>
<p>Now notice that in the sum above, the left outer index is always <span class="math notranslate nohighlight">\(i\)</span> (<span class="math notranslate nohighlight">\(i\)</span>th row
of <span class="math notranslate nohighlight">\(A\)</span>) and the right outer index is always <span class="math notranslate nohighlight">\(j\)</span> (<span class="math notranslate nohighlight">\(j\)</span>th column of <span class="math notranslate nohighlight">\(B\)</span>). The inner
indices run from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(n\)</span> so you can introduce a summation index <span class="math notranslate nohighlight">\(k\)</span> and write
this sum compactly using summation notation:</p>
<div class="math notranslate nohighlight">
\[
\color{purple}{\mathbf{c_{ij}}}=\sum_{k=1}^{n} \color{blue}{\mathbf{a_{ik}}}\color{red}{\mathbf{b_{kj}}}
\]</div>
<p>The formule above thus gives you the element on position <span class="math notranslate nohighlight">\((i,j)\)</span> in the product
matrix <span class="math notranslate nohighlight">\(C=AB\)</span> and therefore completely defines <span class="math notranslate nohighlight">\(C\)</span> by letting <span class="math notranslate nohighlight">\(i=1,...,m\)</span> and
<span class="math notranslate nohighlight">\(j=1,...,p\)</span>.</p>
</section>
<section id="mathbf-x-output-of-the-embedding-layer">
<h3><a class="toc-backref" href="#id20" role="doc-backlink"><span class="math notranslate nohighlight">\(\mathbf{X}\)</span>: Output of the Embedding Layer</a><a class="headerlink" href="#mathbf-x-output-of-the-embedding-layer" title="Link to this heading">#</a></h3>
<p>Once the one hot encoding representation <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> is well
defined, we can then pass it as input through our GPT model, in which the first
layer is a embedding lookup table. In the GPT model architecture, the first
layer typically involves mapping the one-hot encoded input vectors into a
lower-dimensional, dense embedding space using the embedding matrix
<span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span>.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Matrix Description</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Dimensions</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>One-Hot Encoded Input Matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times V\)</span></p></td>
<td><p>Each row corresponds to a one-hot encoded vector representing a token in the sequence.</p></td>
</tr>
<tr class="row-odd"><td><p>Embedding Matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(V \times D\)</span></p></td>
<td><p>Each row is the embedding vector of the corresponding token in the vocabulary.</p></td>
</tr>
<tr class="row-even"><td><p>Embedded Input Matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times D\)</span></p></td>
<td><p>Each row is the embedding vector of the corresponding token in the input sequence.</p></td>
</tr>
<tr class="row-odd"><td><p>Embedding Vector for Token <span class="math notranslate nohighlight">\(t\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}_t\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1 \times D\)</span></p></td>
<td><p>The embedding vector for the token at position <span class="math notranslate nohighlight">\(t\)</span> in the input sequence.</p></td>
</tr>
<tr class="row-even"><td><p>Batched Input Tensor</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}^{\mathcal{B}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(B \times T \times D\)</span></p></td>
<td><p>A batched tensor containing <span class="math notranslate nohighlight">\(B\)</span> input sequences, each sequence is of shape <span class="math notranslate nohighlight">\(T \times D\)</span>.</p></td>
</tr>
</tbody>
</table>
</div>
<p>More concretely, we create an embedding matrix <span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span> of size
<span class="math notranslate nohighlight">\(V \times D\)</span>, where <span class="math notranslate nohighlight">\(V\)</span> is the vocabulary size, <span class="math notranslate nohighlight">\(D\)</span> is the dimensions of the
embeddings, we would then matrix multiply <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> with
<span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span> to get the output tensor <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X} = \mathbf{X}^{\text{ohe}} \cdot \mathbf{W}_{e}
\]</div>
<section id="id1">
<h4><a class="toc-backref" href="#id21" role="doc-backlink">Definition</a><a class="headerlink" href="#id1" title="Link to this heading">#</a></h4>
<p>The embedding matrix <span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span> is structured as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{W}_e &amp;= \left[\begin{array}{cccc}
w_{1,1} &amp; w_{1,2} &amp; \cdots &amp; w_{1, D} \\
w_{2,1} &amp; w_{2,2} &amp; \cdots &amp; w_{2, D} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{V, 1} &amp; w_{V, 2} &amp; \cdots &amp; w_{V, D}
\end{array}\right] \in \mathbb{R}^{V \times D}
\end{aligned}
\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{w}_j = (w_{j,1}, w_{j,2}, \ldots, w_{j,D}) \in \mathbb{R}^{1 \times D}\)</span>:</p>
<ul>
<li><p>Each row vector <span class="math notranslate nohighlight">\(\mathbf{w}_j\)</span> of the matrix <span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span> represents
the <span class="math notranslate nohighlight">\(D\)</span>-dimensional embedding vector for the <span class="math notranslate nohighlight">\(j\)</span>-th token in the
vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p></li>
<li><p>The subscript <span class="math notranslate nohighlight">\(j\)</span> ranges from 1 to <span class="math notranslate nohighlight">\(V\)</span>, indexing the tokens.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(V\)</span> is the vocabulary size.</p></li>
<li><p><span class="math notranslate nohighlight">\(D\)</span> is the hidden embedding dimension.</p></li>
</ul>
<p>Here is a visual representation of how each embedding vector is selected through
matrix multiplication:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{X}^{\text{ohe}} \cdot \mathbf{W}_{e} &amp;=
\begin{bmatrix}
0 &amp; 1 &amp; \cdots &amp; 0 \\
1 &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 1
\end{bmatrix}_{T \times V}
\cdot
\begin{bmatrix}
w_{1,1} &amp; w_{1,2} &amp; \cdots &amp; w_{1,D} \\
w_{2,1} &amp; w_{2,2} &amp; \cdots &amp; w_{2,D} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{V,1} &amp; w_{V,2} &amp; \cdots &amp; w_{V,D}
\end{bmatrix}_{V \times D} \\
&amp;=
\begin{bmatrix}
w_{2,1} &amp; w_{2,2} &amp; \cdots &amp; w_{2,D} \\
w_{1,1} &amp; w_{1,2} &amp; \cdots &amp; w_{1,D} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{T,1} &amp; w_{T,2} &amp; \cdots &amp; w_{T,D}
\end{bmatrix}_{T \times D}
\end{aligned}
\end{split}\]</div>
<p>Each row in the resulting matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is the embedding of the
corresponding token in the input sequence, picked directly from <span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span>
by the one-hot vectors. In other words, the matrix <span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span> can be
visualized as a table where each row corresponds to a token’s embedding vector:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{c|cccc}
\text{Token Index} &amp; \text{Dimension 1} &amp; \text{Dimension 2} &amp; \cdots &amp; \text{Dimension } D \\
\hline
1 &amp; w_{1,1} &amp; w_{1,2} &amp; \cdots &amp; w_{1,D} \\
2 &amp; w_{2,1} &amp; w_{2,2} &amp; \cdots &amp; w_{2,D} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
V &amp; w_{V,1} &amp; w_{V,2} &amp; \cdots &amp; w_{V,D} \\
\end{array}
\end{split}\]</div>
</section>
<section id="lookup">
<h4><a class="toc-backref" href="#id22" role="doc-backlink">Lookup</a><a class="headerlink" href="#lookup" title="Link to this heading">#</a></h4>
<p>When the one-hot encoded input matrix <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> multiplies with
the embedding matrix <span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span>, each row of <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span>
effectively selects a corresponding row from <span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span>. This operation
simplifies to row selection because each row of <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span>
contains exactly one ‘1’ and the rest are ‘0’s.</p>
</section>
<section id="semantic-representation">
<h4><a class="toc-backref" href="#id23" role="doc-backlink">Semantic Representation</a><a class="headerlink" href="#semantic-representation" title="Link to this heading">#</a></h4>
<p>Now each row of the output tensor, indexed by <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(\mathbf{X}_{t, :}\)</span>: is the
<span class="math notranslate nohighlight">\(D\)</span> dimensional embedding vector for the token <span class="math notranslate nohighlight">\(x_t\)</span> at the <span class="math notranslate nohighlight">\(t\)</span>-th position in
the sequence. In this context, each token in the sequence is represented by a
<span class="math notranslate nohighlight">\(D\)</span> dimensional vector. So, the output tensor <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> captures the dense
representation of the sequence. Each token in the sequence is replaced by its
corresponding embedding vector from the embedding matrix <span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span>. As
before, the output tensor <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> carries semantic information about the
tokens in the sequence. The closer two vectors are in this embedding space, the
more semantically similar they are.</p>
</section>
</section>
<section id="mathbf-w-e-embedding-matrix">
<h3><a class="toc-backref" href="#id24" role="doc-backlink"><span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span>: Embedding Matrix</a><a class="headerlink" href="#mathbf-w-e-embedding-matrix" title="Link to this heading">#</a></h3>
<p>The embedding matrix <span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span> is structured as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{W}_e &amp;= \left[\begin{array}{cccc}
w_{1,1} &amp; w_{1,2} &amp; \cdots &amp; w_{1, D} \\
w_{2,1} &amp; w_{2,2} &amp; \cdots &amp; w_{2, D} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{V, 1} &amp; w_{V, 2} &amp; \cdots &amp; w_{V, D}
\end{array}\right] \in \mathbb{R}^{V \times D}
\end{aligned}
\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{w}_j = (w_{j,1}, w_{j,2}, \ldots, w_{j,D}) \in \mathbb{R}^{1 \times D}\)</span>:</p>
<ul>
<li><p>Each row vector <span class="math notranslate nohighlight">\(\mathbf{w}_j\)</span> of the matrix <span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span> represents
the <span class="math notranslate nohighlight">\(D\)</span>-dimensional embedding vector for the <span class="math notranslate nohighlight">\(j\)</span>-th token in the
vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p></li>
<li><p>The subscript <span class="math notranslate nohighlight">\(j\)</span> ranges from 1 to <span class="math notranslate nohighlight">\(V\)</span>, indexing the tokens.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(V\)</span> is the vocabulary size.</p></li>
<li><p><span class="math notranslate nohighlight">\(D\)</span> is the hidden embedding dimension.</p></li>
</ul>
</section>
<section id="pe-positional-encoding-layer">
<h3><a class="toc-backref" href="#id25" role="doc-backlink"><span class="math notranslate nohighlight">\(PE\)</span>: Positional Encoding Layer</a><a class="headerlink" href="#pe-positional-encoding-layer" title="Link to this heading">#</a></h3>
<p>For a given input matrix <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{T \times D}\)</span>, where <span class="math notranslate nohighlight">\(T\)</span> is
the sequence length and <span class="math notranslate nohighlight">\(D\)</span> is the embedding dimension (denoted as
<span class="math notranslate nohighlight">\(d_{\text{model}}\)</span> in typical Transformer literature), the positional encoding
<span class="math notranslate nohighlight">\(\operatorname{PE}\)</span> is applied to integrate sequence positional information into
the embeddings. The resultant matrix <span class="math notranslate nohighlight">\(\mathbf{X}'\)</span> after applying positional
encoding can be expressed as follows:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}' = \operatorname{PE}(\mathbf{X}),
\]</div>
<p>where each element of <span class="math notranslate nohighlight">\(\mathbf{X}'\)</span>, denoted as <span class="math notranslate nohighlight">\(x'_{i, j}\)</span>, is calculated based
on the sinusoidal function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
x'_{i, j} =
\begin{cases}
\sin\left(\frac{i}{10000^{j/D}}\right) &amp; \text{if } j \mod 2 = 0 \\
\cos\left(\frac{i}{10000^{(j-1)/D}}\right) &amp; \text{otherwise}
\end{cases}
\end{split}\]</div>
<p>for <span class="math notranslate nohighlight">\(i = 1, \ldots, T\)</span> and <span class="math notranslate nohighlight">\(j = 1, \ldots, D\)</span>.</p>
</section>
<section id="tilde-mathbf-x-output-of-the-positional-encoding-layer">
<h3><a class="toc-backref" href="#id26" role="doc-backlink"><span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span>: Output of the Positional Encoding Layer</a><a class="headerlink" href="#tilde-mathbf-x-output-of-the-positional-encoding-layer" title="Link to this heading">#</a></h3>
<p>We can update our original embeddings tensor <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> to include positional
information:</p>
<div class="math notranslate nohighlight">
\[
\tilde{\mathbf{X}} := \mathbf{X} + \operatorname{PE}(X)
\]</div>
<p>This operation adds the positional encodings to the original embeddings, giving
the final embeddings that are passed to subsequent layers in the Transformer
model.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Matrix Description</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Dimensions</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Positional Encoding Matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}_{p}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times D\)</span></p></td>
<td><p>Matrix with positional encoding vectors for each position in the sequence, computed using sinusoidal functions.</p></td>
</tr>
<tr class="row-odd"><td><p>Output of Positional Encoding Layer</p></td>
<td><p><span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times D\)</span></p></td>
<td><p>The resultant embeddings matrix after adding positional encoding <span class="math notranslate nohighlight">\(\mathbf{W}_{p}\)</span> to the embedded input matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. Each row now includes positional information.</p></td>
</tr>
<tr class="row-even"><td><p>Embedding Vector for Token <span class="math notranslate nohighlight">\(t\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}_t\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1 \times D\)</span></p></td>
<td><p>The token and positional embedding vector for the token at position <span class="math notranslate nohighlight">\(t\)</span> in the input sequence.</p></td>
</tr>
<tr class="row-odd"><td><p>Batched Input Tensor</p></td>
<td><p><span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}^{\mathcal{B}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(B \times T \times D\)</span></p></td>
<td><p>A batched tensor containing <span class="math notranslate nohighlight">\(B\)</span> input sequences, each sequence is of shape <span class="math notranslate nohighlight">\(T \times D\)</span>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="layer-normalization">
<h2><a class="toc-backref" href="#id27" role="doc-backlink">Layer Normalization</a><a class="headerlink" href="#layer-normalization" title="Link to this heading">#</a></h2>
<p>Layer normalization modifies the activations within each layer to have zero mean
and unit variance across the features for each data point in a batch
independently, which helps in stabilizing the learning process. It then applies
a learnable affine transformation to each normalized activation, allowing the
network to scale and shift these values where beneficial.</p>
<p>For a given layer with inputs <span class="math notranslate nohighlight">\(\mathbf{Z} \in \mathbb{R}^{B \times T \times D}\)</span>
(where <span class="math notranslate nohighlight">\(B\)</span> is the batch size, <span class="math notranslate nohighlight">\(T\)</span> is the sequence length, and <span class="math notranslate nohighlight">\(D\)</span> is the feature
dimension or hidden dimension size), the layer normalization of <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> is
computed as follows:</p>
<ol class="arabic">
<li><p><strong>Mean and Variance Calculation</strong>: Calculate the mean <span class="math notranslate nohighlight">\(\mu_t\)</span> and variance
<span class="math notranslate nohighlight">\(\sigma_t^2\)</span> for each feature vector across the feature dimension <span class="math notranslate nohighlight">\(D\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    \mu_t = \frac{1}{D} \sum_{d=1}^D \mathbf{Z}_{t, d}, \quad \sigma_t^2 = \frac{1}{D} \sum_{d=1}^D (\mathbf{Z}_{t, d} - \mu_t)^2
    \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu_t\)</span> and <span class="math notranslate nohighlight">\(\sigma_t^2\)</span> are computed for each token <span class="math notranslate nohighlight">\(t\)</span> across all
batches <span class="math notranslate nohighlight">\(B\)</span> and sequence positions <span class="math notranslate nohighlight">\(T\)</span>, but independently for each batch
and sequence position.</p></li>
</ul>
</li>
<li><p><strong>Normalization</strong>: Normalize the activations for each feature dimension:</p>
<div class="math notranslate nohighlight">
\[
    \hat{\mathbf{Z}}_{t, d} = \frac{\mathbf{Z}_{t, d} - \mu_t}{\sqrt{\sigma_t^2 + \epsilon}}
    \]</div>
<ul class="simple">
<li><p>Where <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant (e.g., <span class="math notranslate nohighlight">\(10^{-5}\)</span>) added for numerical
stability.</p></li>
</ul>
</li>
<li><p><strong>Affine Transformation</strong>: Apply a learnable affine transformation to each
normalized feature:</p>
<div class="math notranslate nohighlight">
\[
    \overline{\mathbf{Z}}_{t, d} = \hat{\mathbf{Z}}_{t, d} \cdot \gamma_d + \beta_d
    \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\gamma_d\)</span> and <span class="math notranslate nohighlight">\(\beta_d\)</span> are learnable parameters that scale and shift the
normalized feature respectively. They are of the same dimensionality <span class="math notranslate nohighlight">\(D\)</span>
as the features and are shared across all tokens and batches.</p></li>
</ul>
</li>
</ol>
<p>In practice, these operations are implemented vector-wise across the feature
dimension <span class="math notranslate nohighlight">\(D\)</span>, and can be compactly expressed as:</p>
<div class="math notranslate nohighlight">
\[
\overline{\mathbf{Z}}_t = \dfrac{\mathbf{Z}_t - \mu_t}{\sqrt{\sigma_t^2 + \epsilon}} \odot \gamma + \beta
\]</div>
<ul class="simple">
<li><p>Here, <span class="math notranslate nohighlight">\(\odot\)</span> denotes element-wise multiplication, emphasizing that <span class="math notranslate nohighlight">\(\gamma\)</span>
and <span class="math notranslate nohighlight">\(\beta\)</span> scale and shift each normalized feature dimension identically
across all tokens and batches.</p></li>
<li><p>For better understanding, we can calculate in a loop all <span class="math notranslate nohighlight">\(T\)</span> rows of
<span class="math notranslate nohighlight">\(\overline{\mathbf{Z}}_t\)</span>, and we stack them together to get the final
output tensor <span class="math notranslate nohighlight">\(\overline{\mathbf{Z}}\)</span>.</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Input/Output</p></th>
<th class="head"><p>Shape</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathbf{Z}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The input tensor to the layer normalization operation.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\overline{\mathbf{Z}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The output tensor after applying layer normalization. The dimensionality remains the same as the input, only the scale and shift of the activations within each feature vector are adjusted.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\operatorname{LayerNorm}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbb{R}^{B \times T \times D} \to \mathbb{R}^{B \times T \times D}\)</span></p></td>
<td><p>The layer normalization function that takes an input tensor <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> and returns the normalized tensor <span class="math notranslate nohighlight">\(\overline{\mathbf{Z}}\)</span> with the same shape.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="attention-notations">
<h2><a class="toc-backref" href="#id28" role="doc-backlink">Attention Notations</a><a class="headerlink" href="#attention-notations" title="Link to this heading">#</a></h2>
<section id="dimensions">
<h3><a class="toc-backref" href="#id29" role="doc-backlink">Dimensions</a><a class="headerlink" href="#dimensions" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Symbol</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(H\)</span></p></td>
<td><p>Number of attention heads.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(h\)</span></p></td>
<td><p>Index of the attention head.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(d_k = D/H\)</span></p></td>
<td><p>Dimension of the keys. In the multi-head attention case, this would typically be <span class="math notranslate nohighlight">\(D/H\)</span> where <span class="math notranslate nohighlight">\(D\)</span> is the dimensionality of input embeddings and <span class="math notranslate nohighlight">\(H\)</span> is the number of attention heads.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(d_q = D/H\)</span></p></td>
<td><p>Dimension of the queries. Also usually set equal to <span class="math notranslate nohighlight">\(d_k\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(d_v = D/H\)</span></p></td>
<td><p>Dimension of the values. Usually set equal to <span class="math notranslate nohighlight">\(d_k\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(L\)</span></p></td>
<td><p>Total number of decoder blocks in the GPT architecture.</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p>Index of the decoder block, ranging from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(L\)</span>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="query-key-and-values">
<h3><a class="toc-backref" href="#id30" role="doc-backlink">Query, Key and Values</a><a class="headerlink" href="#query-key-and-values" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Matrix Description</strong></p></th>
<th class="head"><p><strong>Symbol</strong></p></th>
<th class="head"><p><strong>Dimensions</strong></p></th>
<th class="head"><p><strong>Description</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Generic Query Matrix for All Heads</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Q}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times D\)</span></p></td>
<td><p>Contains the query representations for all tokens in the sequence using combined weights of all heads.</p></td>
</tr>
<tr class="row-odd"><td><p>Generic Key Matrix for All Heads</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{K}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times D\)</span></p></td>
<td><p>Contains the key representations for all tokens in the sequence using combined weights of all heads.</p></td>
</tr>
<tr class="row-even"><td><p>Generic Value Matrix for All Heads</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{V}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times D\)</span></p></td>
<td><p>Contains the value representations for all tokens in the sequence using combined weights of all heads.</p></td>
</tr>
<tr class="row-odd"><td><p>Query Matrix for All Heads in Layer <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Q}^{(\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times D\)</span></p></td>
<td><p>Contains the query representations for all tokens in the sequence using combined weights of all heads in the <span class="math notranslate nohighlight">\(\ell\)</span>-th layer.</p></td>
</tr>
<tr class="row-even"><td><p>Key Matrix for All Heads in Layer <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{K}^{(\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times D\)</span></p></td>
<td><p>Contains the key representations for all tokens in the sequence using combined weights of all heads in the <span class="math notranslate nohighlight">\(\ell\)</span>-th layer.</p></td>
</tr>
<tr class="row-odd"><td><p>Value Matrix for All Heads in Layer <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{V}^{(\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times D\)</span></p></td>
<td><p>Contains the value representations for all tokens in the sequence using combined weights of all heads in the <span class="math notranslate nohighlight">\(\ell\)</span>-th layer.</p></td>
</tr>
<tr class="row-even"><td><p>Query Weight Matrix for All Heads in Layer <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}^{\mathbf{Q}, (\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(D \times D\)</span></p></td>
<td><p>The query transformation matrix applicable to all heads in the <span class="math notranslate nohighlight">\(\ell\)</span>-th layer. It transforms the embeddings <span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span> into query representations.</p></td>
</tr>
<tr class="row-odd"><td><p>Key Weight Matrix for All Heads in Layer <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}^{\mathbf{K}, (\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(D \times D\)</span></p></td>
<td><p>The key transformation matrix applicable to all heads in the <span class="math notranslate nohighlight">\(\ell\)</span>-th layer. It transforms the embeddings <span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span> into key representations.</p></td>
</tr>
<tr class="row-even"><td><p>Value Weight Matrix for All Heads in Layer <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}^{\mathbf{V}, (\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(D \times D\)</span></p></td>
<td><p>The value transformation matrix applicable to all heads in the <span class="math notranslate nohighlight">\(\ell\)</span>-th layer. It transforms the embeddings <span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span> into value representations.</p></td>
</tr>
<tr class="row-odd"><td><p>Query Matrix for Head <span class="math notranslate nohighlight">\(h\)</span> in Layer <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Q}_h^{(\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times d_q\)</span></p></td>
<td><p>Contains the query representations for all tokens in the sequence specific to head <span class="math notranslate nohighlight">\(h\)</span> in the <span class="math notranslate nohighlight">\(\ell\)</span>-th layer.</p></td>
</tr>
<tr class="row-even"><td><p>Key Matrix for Head <span class="math notranslate nohighlight">\(h\)</span> in Layer <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{K}_h^{(\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times d_k\)</span></p></td>
<td><p>Contains the key representations for all tokens in the sequence specific to head <span class="math notranslate nohighlight">\(h\)</span> in the <span class="math notranslate nohighlight">\(\ell\)</span>-th layer.</p></td>
</tr>
<tr class="row-odd"><td><p>Value Matrix for Head <span class="math notranslate nohighlight">\(h\)</span> in Layer <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{V}_h^{(\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times d_v\)</span></p></td>
<td><p>Contains the value representations for all tokens in the sequence specific to head <span class="math notranslate nohighlight">\(h\)</span> in the <span class="math notranslate nohighlight">\(\ell\)</span>-th layer.</p></td>
</tr>
<tr class="row-even"><td><p>Query Weight Matrix for Head <span class="math notranslate nohighlight">\(h\)</span> in Layer <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}_h^{\mathbf{Q}, (\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(D \times d_q\)</span></p></td>
<td><p>Linear transformation matrix for queries in masked attention head <span class="math notranslate nohighlight">\(h \in \{1, \ldots, H\}\)</span> of decoder block <span class="math notranslate nohighlight">\(\ell \in \{1, \ldots, L\}\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p>Key Weight Matrix for Head <span class="math notranslate nohighlight">\(h\)</span> in Layer <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}_h^{\mathbf{K}, (\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(D \times d_k\)</span></p></td>
<td><p>Linear transformation matrix for keys in masked attention head <span class="math notranslate nohighlight">\(h \in \{1, \ldots, H\}\)</span> of decoder block <span class="math notranslate nohighlight">\(\ell \in \{1, \ldots, L\}\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p>Value Weight Matrix for Head <span class="math notranslate nohighlight">\(h\)</span> in Layer <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}_h^{\mathbf{V}, (\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(D \times d_v\)</span></p></td>
<td><p>Linear transformation matrix for values in masked attention head <span class="math notranslate nohighlight">\(h \in \{1, \ldots, H\}\)</span> of decoder block <span class="math notranslate nohighlight">\(\ell \in \{1, \ldots, L\}\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p>Projection Weight Matrix for Layer <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}^{O, (\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(D \times D\)</span></p></td>
<td><p>The projection matrix used to combine and transform the concatenated outputs from all heads in the <span class="math notranslate nohighlight">\( \ell \)</span>-th layer back to the original dimension <span class="math notranslate nohighlight">\(D\)</span>.</p></td>
</tr>
</tbody>
</table>
</div>
<p>We will talk about the relevant shapes in the last section.</p>
</section>
<section id="general-attention-mechanism">
<h3><a class="toc-backref" href="#id31" role="doc-backlink">General Attention Mechanism</a><a class="headerlink" href="#general-attention-mechanism" title="Link to this heading">#</a></h3>
<p>To calculate the embeddings after attention in the GPT model:</p>
<div class="math notranslate nohighlight">
\[
\operatorname{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\operatorname{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^{\top}}{\sqrt{d_k}}\right) \mathbf{V}
\]</div>
</section>
<section id="multi-head-attention-for-layer-ell">
<h3><a class="toc-backref" href="#id32" role="doc-backlink">Multi-Head Attention for Layer <span class="math notranslate nohighlight">\(\ell\)</span></a><a class="headerlink" href="#multi-head-attention-for-layer-ell" title="Link to this heading">#</a></h3>
<p>For the multi-head attention in layer <span class="math notranslate nohighlight">\(\ell\)</span> of the Transformer (applicable to
both encoder and decoder in architectures that have both components, but here
tailored for GPT which primarily uses decoder stacks):</p>
<div class="math notranslate nohighlight">
\[
\operatorname{MultiHead}^{(\ell)}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \operatorname{Concat}\left(\operatorname{head}_{\ell, 1}, \operatorname{head}_{\ell, 2}, \cdots, \operatorname{head}_{\ell, H}\right) \mathbf{W}^{O, (\ell)}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\operatorname{head}_{\ell, h} = \operatorname{Attention}\left(\mathbf{Q} \mathbf{W}_{h}^{\mathbf{Q}, (\ell)}, \mathbf{K} \mathbf{W}_{h}^{\mathbf{K}, (\ell)}, \mathbf{V} \mathbf{W}_{h}^{\mathbf{V}, (\ell)}\right)
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}_{h}^{\mathbf{Q}, (\ell)}, \mathbf{W}_{h}^{\mathbf{K}, (\ell)}, \mathbf{W}_{h}^{\mathbf{V}, (\ell)}\)</span>
are the weight matrices for queries, keys, and values for the <span class="math notranslate nohighlight">\(h\)</span>-th head in
the <span class="math notranslate nohighlight">\(\ell\)</span>-th layer, respectively.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{O, (\ell)}\)</span> is the output transformation matrix for the
<span class="math notranslate nohighlight">\(\ell\)</span>-th layer.</p></li>
</ul>
</section>
<section id="masked-multi-head-attention-for-decoder-layer-ell">
<h3><a class="toc-backref" href="#id33" role="doc-backlink">Masked Multi-Head Attention for Decoder Layer <span class="math notranslate nohighlight">\(\ell\)</span></a><a class="headerlink" href="#masked-multi-head-attention-for-decoder-layer-ell" title="Link to this heading">#</a></h3>
<p>Masked multi-head attention, used in the decoder to ensure that the predictions
for position <span class="math notranslate nohighlight">\(t\)</span> can only depend on known outputs at positions less than <span class="math notranslate nohighlight">\(t\)</span>
(auto-regressive property):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\operatorname{MaskedMultiHead}^{(\ell)}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) &amp;= \operatorname{Concat}\left(\operatorname{head}_{\ell, 1}^{M}, \operatorname{head}_{\ell, 2}^{M}, \cdots, \operatorname{head}_{\ell, H}^{M}\right) \mathbf{W}^{O, M, (\ell)} \\
&amp;= \left( \operatorname{{head}_{\ell, 1}^{M}} \oplus \operatorname{{head}_{\ell, 2}^{M}} \oplus \cdots \oplus \operatorname{{head}_{\ell, H}^{M}} \right) \mathbf{W}^{O, M, (\ell)}
\end{aligned}
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\operatorname{head}_{\ell, h}^{M} = \operatorname{softmax}\left(\operatorname{Mask}\left(\frac{\left(\mathbf{Q} \mathbf{W}_{h}^{\mathbf{Q}, M, (\ell)}\right)\left(\mathbf{K} \mathbf{W}_{h}^{\mathbf{K}, M, (\ell)}\right)^{\top}}{\sqrt{d_k}}\right)\right)\left(\mathbf{V} \mathbf{W}_{h}^{\mathbf{V}, M, (\ell)}\right)
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\operatorname{Mask}(\mathbf{x})_{i, j}= \begin{cases}\mathbf{x}_{i, j} &amp; \text{if } i \geq j \\ -\infty &amp; \text{otherwise}\end{cases}
\end{split}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(M\)</span> denotes the masked condition.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}_{h}^{\mathbf{Q}, M, (\ell)}, \mathbf{W}_{h}^{\mathbf{K}, M, (\ell)}, \mathbf{W}_{h}^{\mathbf{V}, M, (\ell)}\)</span>
are the masked weight matrices for queries, keys, and values for the <span class="math notranslate nohighlight">\(h\)</span>-th
head in the <span class="math notranslate nohighlight">\(\ell\)</span>-th layer, specifically used under the masked condition.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{O, M, (\ell)}\)</span> is the masked output transformation matrix for
the <span class="math notranslate nohighlight">\(\ell\)</span>-th layer, ensuring that future tokens do not influence the
predictions of the current token in an auto-regressive manner.</p></li>
</ul>
</section>
<section id="updated-matrix-description-table-with-batch-and-head-dimensions">
<h3><a class="toc-backref" href="#id34" role="doc-backlink">Updated Matrix Description Table with Batch and Head Dimensions</a><a class="headerlink" href="#updated-matrix-description-table-with-batch-and-head-dimensions" title="Link to this heading">#</a></h3>
<p>To accurately reflect the practical shapes of the Query (Q), Key (K), and Value
(V) matrices in implementations like GPT, where batch processing and multi-head
attention are used, we should adjust the notation to include batch size <span class="math notranslate nohighlight">\(B\)</span>,
number of heads <span class="math notranslate nohighlight">\(H\)</span>, and the dimensions <span class="math notranslate nohighlight">\(d_k, d_q, d_v\)</span> corresponding to each
head.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Matrix Description</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Dimensions</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Query Matrix for All Heads in Layer <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Q}^{(\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D \rightarrow B \times T \times \underset{D}{\underbrace{H \times d_q}} \xrightarrow[]{\text{transpose 1-2}} B \times H \times T \times d_q\)</span></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>Contains the query representations for all tokens in all sequences of a batch, separated by heads in the <span class="math notranslate nohighlight">\(\ell\)</span>-th layer.</p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>Key Matrix for All Heads in Layer <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{K}^{(\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D \rightarrow B \times T \times \underset{D}{\underbrace{H \times d_k}} \xrightarrow[]{\text{transpose 1-2}} B \times H \times T \times d_k\)</span></p></td>
<td><p>Contains the key representations for all tokens in all sequences of a batch, separated by heads in the <span class="math notranslate nohighlight">\(\ell\)</span>-th layer.</p></td>
</tr>
<tr class="row-odd"><td><p>Value Matrix for All Heads in Layer <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{V}^{(\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D \rightarrow B \times T \times \underset{D}{\underbrace{H \times d_v}} \xrightarrow[]{\text{transpose 1-2}} B \times H \times T \times d_v\)</span></p></td>
<td><p>Contains the value representations for all tokens in all sequences of a batch, separated by heads in the <span class="math notranslate nohighlight">\(\ell\)</span>-th layer.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="positionwise-feed-forward-networks">
<h2><a class="toc-backref" href="#id35" role="doc-backlink">Positionwise Feed-Forward Networks</a><a class="headerlink" href="#positionwise-feed-forward-networks" title="Link to this heading">#</a></h2>
<p>The term “positionwise feed-forward network” (FFN) in the context of Transformer
models refers to a dense neural network (otherwise known as multilayer
perceptron) that operates on the output of the Multi-Head Attention mechanism.
This component is called “positionwise” because it applies the <strong>same</strong>
feed-forward neural network (FFN) <strong>independently</strong> and <strong>identically</strong> to each
position <span class="math notranslate nohighlight">\(t\)</span> in the sequence of length <span class="math notranslate nohighlight">\(T\)</span>.</p>
<section id="independent-processing">
<h3><a class="toc-backref" href="#id36" role="doc-backlink">Independent Processing</a><a class="headerlink" href="#independent-processing" title="Link to this heading">#</a></h3>
<p>In the Transformer architecture, after the Multi-Head Attention mechanism
aggregates information from different positions in the sequence based on
attention scores, each element (or position) <span class="math notranslate nohighlight">\(t\)</span> in the sequence has an updated
representation. The positionwise FFN then processes each of these updated
representations. However, rather than considering the sequence as a whole or how
elements relate to each other at this stage, the FFN operates on each position
separately. This means that for a sequence of length <span class="math notranslate nohighlight">\(T\)</span>, the same FFN is
applied <span class="math notranslate nohighlight">\(T\)</span> times independently, and by extension, given a batch of sequences,
the FFN is applied <span class="math notranslate nohighlight">\(T \times \mathcal{B}\)</span> times, where <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> is the
batch size.</p>
</section>
<section id="identical-application">
<h3><a class="toc-backref" href="#id37" role="doc-backlink">Identical Application</a><a class="headerlink" href="#identical-application" title="Link to this heading">#</a></h3>
<p>The term “using the same FFN” signifies that the same set of parameters (weights
and biases) of the feed-forward neural network is used for each position in the
sequence. The rationale is that the transformation is consistent across all
sequence positions, so each element is transformed by the same learned function.
This means the weight matrices and bias vectors of the FFN are shared across all
positions in the sequence. In other words, if a sequence has <span class="math notranslate nohighlight">\(T=3\)</span>
positions/tokens, the weight matrices and bias vectors of the FFN are the same
for all three positions.</p>
</section>
<section id="id2">
<h3><a class="toc-backref" href="#id38" role="doc-backlink">Definition</a><a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>Typically, a positionwise FFN consists of two linear transformations with a
non-linear activation function in between. The general form can be represented
as follows.</p>
<div class="proof definition admonition" id="def-positionwise-ffn-notation">
<p class="admonition-title"><span class="caption-number">Definition 1 </span> (Position-wise Feedforward Networks)</p>
<section class="definition-content" id="proof-content">
<p>Given an input matrix <span class="math notranslate nohighlight">\(\mathbf{Z} \in \mathbb{R}^{T \times D}\)</span>, the
position-wise feedforward network computes the output matrix
<span class="math notranslate nohighlight">\(\mathbf{Z}^{\prime} \in \mathbb{R}^{T \times D}\)</span> via the following operations:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z}^{\prime}=\sigma_Z\left(\mathbf{Z} \mathbf{W}^{\text{FF}}_1 + \mathbf{b}^{\text{FF}}_1\right) \mathbf{W}^{\text{FF}}_2 + \mathbf{b}^{\text{FF}}_2
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{\text{FF}}_1 \in \mathbb{R}^{D \times d_{\text{ff}}}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{W}^{\text{FF}}_2 \in \mathbb{R}^{d_{\text{ff}} \times D}\)</span> are
learnable weight matrices.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{b}^{\text{FF}}_1 \in \mathbb{R}^{d_{\text{ff}}}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{b}^{\text{FF}}_2 \in \mathbb{R}^{D}\)</span> are learnable bias vectors.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma_Z\)</span> is a non-linear activation function, such as the Gaussian Error
Linear Unit (GELU) or the Rectified Linear Unit (ReLU).</p></li>
</ul>
</section>
</div></section>
<section id="projection-to-a-higher-dimension-space">
<h3><a class="toc-backref" href="#id39" role="doc-backlink">Projection to a Higher Dimension Space</a><a class="headerlink" href="#projection-to-a-higher-dimension-space" title="Link to this heading">#</a></h3>
<p>In the Transformer architecture, the dimensionality of the hidden layer in the
positionwise FFN, denoted as <span class="math notranslate nohighlight">\(d_{\text{ff}}\)</span>, is often chosen to be larger than
the dimensionality of the input and output embeddings, <span class="math notranslate nohighlight">\(D\)</span>. This means that the
FFN projects the input embeddings into a higher-dimensional space before
projecting them back to the original dimensionality.</p>
<p>The motivation behind this design choice is to allow the model to learn more
complex and expressive representations. By projecting the input embeddings into
a higher-dimensional space, the model capacity is increased, and the FFN can
capture more intricate patterns and relationships among the features. We then
project back (“unembedding”) the higher-dimensional representations to the
original dimensionality to maintain the consistency of the model.</p>
<p>In practice, a common choice for the dimensionality of the hidden layer is to
set <span class="math notranslate nohighlight">\(d_{\text{ff}}\)</span> to be a multiple of the input and output dimensionality <span class="math notranslate nohighlight">\(D\)</span>.
For example, in the original Transformer paper <span id="id3">[<a class="reference internal" href="../../bibliography.html#id18" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017. arXiv:1706.03762.">Vaswani <em>et al.</em>, 2017</a>]</span>, the
authors used <span class="math notranslate nohighlight">\(d_{\text{ff}} = 4 \times D\)</span>.</p>
</section>
<section id="gaussian-error-linear-unit-gelu">
<h3><a class="toc-backref" href="#id40" role="doc-backlink">Gaussian Error Linear Unit (GELU)</a><a class="headerlink" href="#gaussian-error-linear-unit-gelu" title="Link to this heading">#</a></h3>
<p>The Gaussian Error Linear Unit (GELU) is a non-linear activation function used
in the context of neural networks, which allows the model to capture more
complex patterns in the data compared to traditional activation functions like
ReLU. The GELU activation function is defined as:</p>
<div class="math notranslate nohighlight">
\[
\text{GELU}(x) = x \cdot \Phi(x)
\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is the input to the activation function, and <span class="math notranslate nohighlight">\(\Phi(x)\)</span> represents the
cumulative distribution function (CDF) of the standard Gaussian distribution.
The GELU function, effectively, models inputs with a non-linear transformation
that weights inputs by their value, with a probabilistic gating mechanism
derived from the Gaussian distribution.</p>
<p>The cumulative distribution function <span class="math notranslate nohighlight">\(\Phi(x)\)</span> for a standard Gaussian
distribution is given by:</p>
<div class="math notranslate nohighlight">
\[
\Phi(x) = \frac{1}{2} \left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]
\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{erf}\)</span> denotes the error function, which is a special function
integral of the Gaussian distribution. Combining these, the GELU function can be
expressed as:</p>
<div class="math notranslate nohighlight">
\[
\text{GELU}(x) = x \cdot \frac{1}{2} \left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]
\]</div>
<p>I will not pretend I have went through the entire paper and motivation of GELU,
but usually, when new and “better” activation functions are proposed, they
usually serve as an alternative to the common activation functions such as ReLU
etc, where they solve some of the problems that the common activation functions
have. From the formulation, we can see that GELU obeys the following properties:</p>
<ul class="simple">
<li><p><strong>Non-linearity</strong>: GELU introduces non-linearity to the model, a given
requirement.</p></li>
<li><p><strong>Differentiability</strong>: GELU is smooth and differentiable everywhere, which
is beneficial for gradient-based optimization methods.</p></li>
<li><p><strong>Boundedness</strong>: GELU seems to be bounded below by <span class="math notranslate nohighlight">\(-0.17\)</span> and not upper
bounded, but practice we can show there is an upper bound if we normalize
the input.</p></li>
</ul>
<div class="proof remark admonition" id="remark-approx-gelu-notation">
<p class="admonition-title"><span class="caption-number">Remark 1 </span> (Approximation of GELU)</p>
<section class="remark-content" id="proof-content">
<p>To further simplify the GELU function and enhance computational efficiency, an
approximation of the Gaussian CDF is commonly used in practice (extracted from
<a class="reference external" href="https://www.hindawi.com/journals/jmath/2023/4229924/">Mathematical Analysis and Performance Evaluation of the GELU Activation Function in Deep Learning</a>):</p>
<div class="math notranslate nohighlight">
\[
\Phi(\alpha x) \approx \frac{1}{2}\left(1+\tanh \left(\beta\left(\alpha x+\gamma(\alpha x)^3\right)\right)\right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta&gt;0\)</span> and <span class="math notranslate nohighlight">\(\gamma \in \mathbb{R}\)</span> are constants, selected to minimize
approximation error. Substituting this approximation into the GELU function, we
arrive at the final approximate form of the GELU activation function (Figure 1):</p>
<div class="math notranslate nohighlight">
\[
\operatorname{GELU}(x)=0.5 x\left(1+\tanh \left(\sqrt{\frac{2}{\pi}}\left(x+0.044715 x^3\right)\right)\right) .
\]</div>
</section>
</div><div class="proof definition admonition" id="def-gelu-notation">
<p class="admonition-title"><span class="caption-number">Definition 2 </span> (GELU Activation Function)</p>
<section class="definition-content" id="proof-content">
<p>For a matrix <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> with elements <span class="math notranslate nohighlight">\(\mathbf{Z}_{t d}\)</span> where <span class="math notranslate nohighlight">\(t\)</span> indexes the
sequence (from 1 to <span class="math notranslate nohighlight">\(T\)</span> ) and <span class="math notranslate nohighlight">\(d\)</span> indexes the feature dimension (from 1 to <span class="math notranslate nohighlight">\(D\)</span>
), the GELU activation is applied <strong>element-wise</strong> to each element
<span class="math notranslate nohighlight">\(\mathbf{Z}_{t d}\)</span> independently:</p>
<div class="math notranslate nohighlight">
\[
\operatorname{GELU}\left(x_{t d}\right)=x_{t d} \cdot \frac{1}{2}\left[1+\operatorname{erf}\left(\frac{x_{t d}}{\sqrt{2}}\right)\right]
\]</div>
</section>
</div><div class="seealso admonition">
<p class="admonition-title">References</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.hindawi.com/journals/jmath/2023/4229924/">Mathematical Analysis and Performance Evaluation of the GELU Activation Function in Deep Learning</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1606.08415">Gaussian Error Linear Units (GELUs) </a></p></li>
</ul>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Matrix Description</strong></p></th>
<th class="head"><p><strong>Symbol</strong></p></th>
<th class="head"><p><strong>Dimensions</strong></p></th>
<th class="head"><p><strong>Description</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Input to FFN in Layer <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_4\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>Output from the residual connection that adds the normalized self-attention outputs to the initial input embeddings.</p></td>
</tr>
<tr class="row-odd"><td><p>First Linear Transformation in FFN</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{FF, (\ell)}_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times d_{\text{ff}}\)</span></p></td>
<td><p>Applies the first linear transformation to each position’s embedding, projecting it to a higher dimensional space (<span class="math notranslate nohighlight">\(d_{\text{ff}}\)</span>).</p></td>
</tr>
<tr class="row-even"><td><p>Activation (e.g., GELU) Applied to First Linear Output</p></td>
<td><p><span class="math notranslate nohighlight">\(\sigma\left(\mathbf{Z}^{FF, (\ell)}_1\right)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times d_{\text{ff}}\)</span></p></td>
<td><p>Applies the GELU non-linear activation function to the output of the first linear transformation.</p></td>
</tr>
<tr class="row-odd"><td><p>Second Linear Transformation in FFN</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_5\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>Transforms the activated output back down to the original dimensionality <span class="math notranslate nohighlight">\(D\)</span> of the embeddings.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Weights and Biases</strong></p></td>
<td><p></p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>Weights for First Linear Transformation</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}^{FF, (\ell)}_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(D \times d_{\text{ff}}\)</span></p></td>
<td><p>Weights used to transform the input embeddings from dimension <span class="math notranslate nohighlight">\(D\)</span> to <span class="math notranslate nohighlight">\(d_{\text{ff}}\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p>Biases for First Linear Transformation</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{b}^{FF, (\ell)}_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(d_{\text{ff}}\)</span></p></td>
<td><p>Biases added to the linearly transformed embeddings in the first FFN layer.</p></td>
</tr>
<tr class="row-odd"><td><p>Weights for Second Linear Transformation</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}^{FF, (\ell)}_2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(d_{\text{ff}} \times D\)</span></p></td>
<td><p>Weights used to project the activated embeddings from dimension <span class="math notranslate nohighlight">\(d_{\text{ff}}\)</span> back to <span class="math notranslate nohighlight">\(D\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p>Biases for Second Linear Transformation</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{b}^{FF, (\ell)}_2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(D\)</span></p></td>
<td><p>Biases added to the output of the second linear transformation in the FFN, shaping it back to the original embedding dimension.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="the-training-phase">
<h2><a class="toc-backref" href="#id41" role="doc-backlink">The Training Phase</a><a class="headerlink" href="#the-training-phase" title="Link to this heading">#</a></h2>
<section id="autoregressive-self-supervised-learning-paradigm">
<h3><a class="toc-backref" href="#id42" role="doc-backlink">Autoregressive Self-Supervised Learning Paradigm</a><a class="headerlink" href="#autoregressive-self-supervised-learning-paradigm" title="Link to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> be the true but unknown distribution of the natural language
space. In the context of unsupervised learning with self-supervision, such as
language modeling, we consider both the inputs and the implicit labels derived
from the same data sequence. Thus, while traditionally we might decompose the
distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> of a supervised learning task into input space
<span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and label space <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>, in this scenario, <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and
<span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> are intrinsically linked, because <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> is a shifted
version of <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>, and so we can consider <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> as a distribution
over <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> only.</p>
<p>Since <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is a distribution, we also define it as a probability
distribution over <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>, and we can write it as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{D} &amp;= \mathbb{P}(\mathcal{X} ; \boldsymbol{\Theta}) \\
            &amp;= \mathbb{P}_{\{\mathcal{X} ; \boldsymbol{\Theta}\}}(\mathbf{x})
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\Theta}\)</span> is the parameter space that defines the distribution
<span class="math notranslate nohighlight">\(\mathbb{P}(\mathcal{X} ; \boldsymbol{\Theta})\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is a sample
from <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> generated by the distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. It is common to
treat <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> as a sequence of tokens (i.e. a sentence is a sequence of
tokens), and we can write <span class="math notranslate nohighlight">\(\mathbf{x} = \left(x_1, x_2, \ldots, x_T\right)\)</span>,
where <span class="math notranslate nohighlight">\(T\)</span> is the length of the sequence.</p>
<p>Given such a sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, the joint probability of the sequence can be
factorized into the product of the conditional probabilities of each token in
the sequence via the
<a class="reference external" href="https://en.wikipedia.org/wiki/Chain_rule_(probability)">chain rule of probability</a>:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(\mathbf{x} ; \boldsymbol{\Theta}) = \prod_{t=1}^T \mathbb{P}(x_t \mid x_1, x_2, \ldots, x_{t-1} ; \boldsymbol{\Theta})
\]</div>
<p>We can do this because natural language are <em>inherently ordered</em>. Such
decomposition allows for <em>tractable sampling</em> from and <em>estimation</em> of the
distribution <span class="math notranslate nohighlight">\(\mathbb{P}(\mathbf{x} ; \boldsymbol{\Theta})\)</span> as well as any
conditionals in the form of
<span class="math notranslate nohighlight">\(\mathbb{P}(x_{t-k}, x_{t-k+1}, \ldots, x_{t} \mid x_{1}, x_{2}, \ldots, x_{t-k-1} ; \boldsymbol{\Theta})\)</span>
<span id="id4">[<a class="reference internal" href="../../bibliography.html#id16" title="Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.">Radford <em>et al.</em>, 2019</a>]</span>.</p>
<p>To this end, consider a corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> with <span class="math notranslate nohighlight">\(N\)</span> sequences
<span class="math notranslate nohighlight">\(\left\{\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{N}\right\}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{S} = \left\{\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{N}\right\} \underset{\text{i.i.d.}}{\sim} \mathcal{D}
\]</div>
<p>where each sequence <span class="math notranslate nohighlight">\(\mathbf{x}_{n}\)</span> is a sequence of tokens that are sampled
<span class="math notranslate nohighlight">\(\text{i.i.d.}\)</span> from the distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</p>
<p>Then, we can frame the
<a class="reference external" href="https://gao-hongnan.github.io/gaohn-galaxy/probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html">likelihood function</a>
<span class="math notranslate nohighlight">\(\hat{\mathcal{L}}(\cdot)\)</span> as the likelihood of observing the sequences in the
corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathcal{L}}\left(\mathcal{S} ; \hat{\boldsymbol{\Theta}}\right) = \prod_{n=1}^N \mathbb{P}(\mathbf{x}_{n} ; \hat{\boldsymbol{\Theta}})
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Theta}}\)</span> is the estimated parameter space that
approximates the true parameter space <span class="math notranslate nohighlight">\(\boldsymbol{\Theta}\)</span>.</p>
<p>Subsequently, the objective function is now well-defined, to be the maximization
of the likelihood of the sequences in the corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\hat{\boldsymbol{\theta}}^{*} &amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmax}} \hat{\mathcal{L}}\left(\mathcal{S} ; \hat{\boldsymbol{\Theta}}\right) \\
                              &amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmax}} \prod_{n=1}^N \mathbb{P}(\mathbf{x}_{n} ; \hat{\boldsymbol{\Theta}}) \\
                              &amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmax}} \prod_{n=1}^N \prod_{t=1}^{T_n} \mathbb{P}(x_{n, t} \mid x_{n, 1}, x_{n, 2}, \ldots, x_{n, t-1} ; \hat{\boldsymbol{\Theta}}) \\
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(T_n\)</span> is the length of the sequence <span class="math notranslate nohighlight">\(\mathbf{x}_{n}\)</span>.</p>
<p>Owing to the fact that multiplying many probabilities together can lead to
<a class="reference external" href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html#numerical-optimization-and-the-negative-log-likelihood">numerical instability</a>
because the product of many probabilities can be very small, it is common and
necessary to use the log-likelihood as the objective function, because it can be
proven that maximizing the log-likelihood is equivalent to maximizing the
likelihood itself.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\hat{\boldsymbol{\theta}}^{*} &amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmax}} \log\left(\hat{\mathcal{L}}\left(\mathcal{S} ; \hat{\boldsymbol{\Theta}}\right)\right) \\
&amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmax}} \sum_{n=1}^N \sum_{t=1}^{T_n} \log \mathbb{P}(x_{n, t} \mid x_{n, 1}, x_{n, 2}, \ldots, x_{n, t-1} ; \hat{\boldsymbol{\Theta}}) \\
\end{aligned}
\end{split}\]</div>
<p>Furthermore, since we are treating the the loss function as a form of
minimization, we can simply negate the log-likelihood to obtain the negative
log-likelihood as the objective function to be minimized,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\hat{\boldsymbol{\theta}}^{*} &amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmin}} \left(-\log\left(\hat{\mathcal{L}}\left(\mathcal{S} ; \hat{\boldsymbol{\Theta}}\right)\right)\right) \\
&amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmin}} \left(-\sum_{n=1}^N \sum_{t=1}^{T_n} \log \mathbb{P}(x_{n, t} \mid x_{n, 1}, x_{n, 2}, \ldots, x_{n, t-1} ; \hat{\boldsymbol{\Theta}})\right) \\
\end{aligned}
\end{split}\]</div>
<p>It is worth noting that the objective function is a function of the parameter
space <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Theta}}\)</span>, and not the data <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, so all
analysis such as convergence and consistency will be with respect to the
parameter space <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Theta}}\)</span>.</p>
<p>To this end, we denote the GPT model <span class="math notranslate nohighlight">\(\mathcal{G}\)</span> to be an <em>autoregressive</em> and
<em>self-supervised learning</em> model that is trained to maximize the likelihood of
observing all data points <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{S}\)</span> via the objective
function <span class="math notranslate nohighlight">\(\hat{\mathcal{L}}\left(\mathcal{S} ; \hat{\boldsymbol{\Theta}}\right)\)</span>
by learning the conditional probability distribution
<span class="math notranslate nohighlight">\(\mathbb{P}(x_t \mid x_{&lt;t} ; \hat{\boldsymbol{\Theta}})\)</span> over the vocabulary
<span class="math notranslate nohighlight">\(\mathcal{V}\)</span> of tokens, conditioned on the contextual preciding tokens
<span class="math notranslate nohighlight">\(x_{&lt;t} = \left(x_1, x_2, \ldots, x_{t-1}\right)\)</span>. We are clear that although
the goal is to model the joint probability distribution of the token sequences,
we can do so by estimating the joint probability distribution via the
conditional probability distributions.</p>
</section>
<section id="corpus-and-tokenization">
<h3><a class="toc-backref" href="#id43" role="doc-backlink">Corpus and Tokenization</a><a class="headerlink" href="#corpus-and-tokenization" title="Link to this heading">#</a></h3>
<div class="note admonition">
<p class="admonition-title">Step 1. Corpus</p>
<p>Consider a corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> consisting of <span class="math notranslate nohighlight">\(N\)</span> sequences, denoted as
<span class="math notranslate nohighlight">\({\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N}\)</span>, where each sequence
<span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, x_2, \ldots, x_T) \in \mathcal{S}\)</span> is a sequence of <span class="math notranslate nohighlight">\(T\)</span>
tokens. These tokens are sampled i.i.d. from a true, unknown distribution
<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{S}=\left\{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\right\} \underset{\text { i.i.d. }}{\sim} \mathcal{D}
\]</div>
<p>Each sequence <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{S}\)</span> represents a collection of tokenized
elements (e.g., words or characters), where each token <span class="math notranslate nohighlight">\(x_t\)</span> comes from a finite
vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Step 2. Vocabulary and Tokenization</p>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{V} = \{w_1, w_2, \ldots, w_V\}\)</span> be the vocabulary set, where <span class="math notranslate nohighlight">\(w_j\)</span>
is the <span class="math notranslate nohighlight">\(j\)</span>-th token in the vocabulary and <span class="math notranslate nohighlight">\(V = |\mathcal{V}|\)</span> is the size of the
vocabulary. It is worth noting that it is common to train one’s own vocabulary
and tokenizer on the corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, but for simplicity, we assume that
the vocabulary set <span class="math notranslate nohighlight">\(\mathcal{V}\)</span> is predefined.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> be the set of all possible sequences that can be formed by
concatenating tokens from the vocabulary set <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>. Each sequence
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{X}\)</span> is a finite sequence of tokens, and the length of
each sequence is denoted by <span class="math notranslate nohighlight">\(\tau\)</span>. Formally:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{X} = \bigcup_{\tau=1}^{T} \mathcal{V}^{\tau}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{V}^\tau\)</span> represents the set of all sequences of length <span class="math notranslate nohighlight">\(\tau\)</span>
formed by concatenating tokens from <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>, and <span class="math notranslate nohighlight">\(T\)</span> is the maximum
sequence length.</p>
<p>Now, let
<span class="math notranslate nohighlight">\(\mathcal{S} = \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\} \subset \mathcal{X}\)</span>
be a corpus of <span class="math notranslate nohighlight">\(N\)</span> sequences, where each sequence <span class="math notranslate nohighlight">\(\mathbf{x}_n \in \mathcal{X}\)</span>
is a finite sequence of tokens from the vocabulary set <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p>
<p>The tokenizer algorithm <span class="math notranslate nohighlight">\(\mathcal{T}\)</span> is a function that operates on individual
sequences <span class="math notranslate nohighlight">\(\mathbf{x}_n\)</span> from the corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> and maps the tokens to
their corresponding integer indices using the vocabulary set <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{T}: \mathcal{X} \rightarrow \mathbb{N}^{\leq T}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{N}^{\leq T}\)</span> represents the set of all finite sequences of
natural numbers (non-negative integers) with lengths up to <span class="math notranslate nohighlight">\(T\)</span>. The output of
<span class="math notranslate nohighlight">\(\mathcal{T}\)</span> is a tokenized sequence, which is a finite sequence of integer
indices corresponding to the tokens in the input sequence.</p>
<p>To map the tokens to their corresponding integer indices, we define a bijective
mapping function <span class="math notranslate nohighlight">\(f: \mathcal{V} \rightarrow \{1, 2, \ldots, V\}\)</span> such that:</p>
<div class="math notranslate nohighlight">
\[
f(w_j) = j, \quad \forall j \in \{1, 2, \ldots, V\}
\]</div>
<p>where <span class="math notranslate nohighlight">\(f(w_j)\)</span> represents the integer index assigned to the token
<span class="math notranslate nohighlight">\(w_j \in \mathcal{V}\)</span>.</p>
<p>Given a sequence <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, x_2, \ldots, x_\tau) \in \mathcal{X}\)</span>,
where <span class="math notranslate nohighlight">\(\tau \leq T\)</span> is the length of the sequence, the tokenizer algorithm
<span class="math notranslate nohighlight">\(\mathcal{T}\)</span> maps each token <span class="math notranslate nohighlight">\(x_t\)</span> to its corresponding integer index using the
bijective mapping function <span class="math notranslate nohighlight">\(f\)</span>. The tokenized representation of the sequence
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> can be defined as:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{T}(\mathbf{x}) = \left(f(x_1), f(x_2), \ldots, f(x_\tau)\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(f(x_t)\)</span> is the integer index assigned to the token <span class="math notranslate nohighlight">\(x_t\)</span> based on <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p>In the case where a token <span class="math notranslate nohighlight">\(x_t\)</span> is not present in the vocabulary set
<span class="math notranslate nohighlight">\(\mathcal{V}\)</span>, a special token index, such as <span class="math notranslate nohighlight">\(f(\text{&lt;UNK&gt;})\)</span>, can be assigned
to represent an unknown token.</p>
<p>The tokenizer algorithm <span class="math notranslate nohighlight">\(\mathcal{T}\)</span> can be applied to each sequence
<span class="math notranslate nohighlight">\(\mathbf{x}_n\)</span> in the corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> to obtain the tokenized corpus
<span class="math notranslate nohighlight">\(\mathcal{S}^{\mathcal{T}}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{S}^{\mathcal{T}} = \left\{\mathcal{T}(\mathbf{x}_1), \mathcal{T}(\mathbf{x}_2), \ldots, \mathcal{T}(\mathbf{x}_N)\right\} \subset \mathbb{N}^{\leq T}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{T}(\mathbf{x}_n)\)</span> is the tokenized representation of the
sequence <span class="math notranslate nohighlight">\(\mathbf{x}_n \in \mathcal{S}\)</span>.</p>
<p>The tokenized corpus <span class="math notranslate nohighlight">\(\mathcal{S}^{\mathcal{T}}\)</span> is a set of sequences, where
each sequence is a finite sequence of integer indices representing the tokens in
the original sequences from the corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>.</p>
</div>
</section>
<section id="token-embedding-and-positional-encoding">
<h3><a class="toc-backref" href="#id44" role="doc-backlink">Token Embedding and Positional Encoding</a><a class="headerlink" href="#token-embedding-and-positional-encoding" title="Link to this heading">#</a></h3>
<div class="note admonition">
<p class="admonition-title">Step 3. One Hot Encoding</p>
<p>For each sequence <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{S}^{\mathcal{T}}\)</span> in the corpus, we
would apply one hot encoding so that each sample/sequence is transformed to
<span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}} \in \{0, 1\}^{T \times V}\)</span> where <span class="math notranslate nohighlight">\(V\)</span> is the vocabulary
size and <span class="math notranslate nohighlight">\(T\)</span> the pre-defined context window size.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X}^{\text{ohe}} = \begin{bmatrix}
o_{1,1} &amp; o_{1,2} &amp; \cdots &amp; o_{1,V} \\
o_{2,1} &amp; o_{2,2} &amp; \cdots &amp; o_{2,V} \\
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
o_{T,1} &amp; o_{T,2} &amp; \cdots &amp; o_{T,V}
\end{bmatrix} \in \{0, 1\}^{T \times V}
\end{split}\]</div>
<p>Each row <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}_{t} \in \mathbb{R}^{1 \times V}\)</span> represents
the one-hot encoded representation of the token at position <span class="math notranslate nohighlight">\(t\)</span> in the sequence.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Step 4. Token Embedding</p>
<p>Given the one-hot encoded input
<span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}} \in \{0, 1\}^{T \times |\mathcal{V}|}\)</span>, where <span class="math notranslate nohighlight">\(T\)</span> is
the sequence length and <span class="math notranslate nohighlight">\(V = |\mathcal{V}|\)</span> is the vocabulary size, we obtain
the token embedding matrix <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{T \times D}\)</span> by matrix
multiplying <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> with the token embedding weight matrix
<span class="math notranslate nohighlight">\(\mathbf{W}_e \in \mathbb{R}^{V \times D}\)</span>, where <span class="math notranslate nohighlight">\(D\)</span> is the embedding
dimension:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{X} &amp;= \mathbf{X}^{\text{ohe}} \operatorname{&#64;} \mathbf{W}_{e} \\
T \times D                      &amp;\leftarrow T \times V \operatorname{&#64;} V \times D  \\
\mathcal{B} \times T \times D   &amp;\leftarrow\mathcal{B} \times T \times V \operatorname{&#64;} V \times D
\end{aligned}
\end{split}\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Weight Sharing</p>
<p>Note carefully that with the addition of batch dimension <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> the
matrix multiplication is still well-defined for such tensor in PyTorch because
we are essentially just performing matrix multiplication in <span class="math notranslate nohighlight">\(T \times D\)</span> for
each sequence <span class="math notranslate nohighlight">\(\mathbf{X}_b \in \mathbf{X}^{\mathcal{B}}\)</span> with the same weight
matrix <span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span>.</p>
<p>The token embedding weight matrix <span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span> with dimensions <span class="math notranslate nohighlight">\(V \times D\)</span> is
shared across all sequences in the batch. Each sequence <span class="math notranslate nohighlight">\(\mathbf{X}^{(b)}\)</span> in the
batched input tensor <span class="math notranslate nohighlight">\(\mathbf{X}^{\mathcal{B}}\)</span> undergoes the same matrix
multiplication with <span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span> to obtain the corresponding embedded sequence
representation.</p>
<p>The idea of weight sharing is that the same set of parameters (in this case, the
embedding weights) is used for processing multiple instances of the input
(sequences in the batch). Instead of having separate embedding weights for each
sequence, the same embedding matrix is applied to all sequences. This parameter
sharing allows the model to learn a common representation for the tokens across
different sequences.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Step 5. Positional Embedding</p>
<p>In addition to the token embeddings, we incorporate positional information into
the input representation to capture the sequential nature of the input
sequences. Let <span class="math notranslate nohighlight">\(\operatorname{PE}(\cdot)\)</span> denote the positional encoding
function that maps the token positions to their corresponding positional
embeddings.</p>
<p>Given the token embedding matrix <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{T \times D}\)</span>, where
<span class="math notranslate nohighlight">\(T\)</span> is the sequence length and <span class="math notranslate nohighlight">\(D\)</span> is the embedding dimension, we add the
positional embeddings to obtain the position-aware input representation
<span class="math notranslate nohighlight">\(\tilde{\mathbf{X}} \in \mathbb{R}^{T \times D}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\tilde{\mathbf{X}} &amp;= \operatorname{PE}(\mathbf{X}) + \mathbf{X} \\
T \times D                      &amp;\leftarrow T \times D \operatorname{+} T \times D  \\
\mathcal{B} \times T \times D   &amp;\leftarrow \mathcal{B} \times T \times D \operatorname{+} \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>The positional encoding function <span class="math notranslate nohighlight">\(\operatorname{PE}(\cdot)\)</span> can be implemented
in various ways, such as using fixed sinusoidal functions or learned positional
embeddings. For the latter, we can easily replace <span class="math notranslate nohighlight">\(\operatorname{PE}(\cdot)\)</span>
with a learnable positional embedding layer in the model architecture
(<span class="math notranslate nohighlight">\(\mathbf{W}_{p}\)</span>).</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Dropout And Elementwise Operation</p>
<p>At this stage, it is common practice to apply a dropout layer
<span class="math notranslate nohighlight">\(\operatorname{Dropout}(\cdot)\)</span> to the position-aware input representation
<span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span> (or <span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}_{\text{batch}}\)</span> in the case of a
batch). Dropout is a regularization technique that randomly sets a fraction of
the elements in the input tensor to zero during training and is an
<strong><em>element-wise</em></strong> operation that acts <strong><em>independently</em></strong> on each element in
the tensor. This means that each element has a fixed probability (usually
denoted as <span class="math notranslate nohighlight">\(p\)</span>) of being set to zero, regardless of its position or the values
of other elements in the tensor.</p>
<p>Mathematically, for an input tensor <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{T \times D}\)</span>,
elementwise dropout can be expressed as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{X}^{\text{dropout}} &amp;= \mathbf{X} \odot \mathbf{M} \\
T \times D &amp;= T \times D \odot T \times D
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\odot\)</span> denotes the elementwise (Hadamard) product, and
<span class="math notranslate nohighlight">\(\mathbf{M} \in \{0, 1\}^{T \times D}\)</span> is a binary mask tensor of the same shape
as <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. Each element in <span class="math notranslate nohighlight">\(\mathbf{M}\)</span> is independently sampled from a
Bernoulli distribution with probability <span class="math notranslate nohighlight">\(p\)</span> of being 0 (i.e., dropped) and
probability <span class="math notranslate nohighlight">\(1-p\)</span> of being 1 (i.e., retained).</p>
</div>
</section>
<section id="backbone-architecture">
<h3><a class="toc-backref" href="#id45" role="doc-backlink">Backbone Architecture</a><a class="headerlink" href="#backbone-architecture" title="Link to this heading">#</a></h3>
<div class="note admonition">
<p class="admonition-title">Step 6. Pre-Layer Normalization For Masked Multi-Head Attention</p>
<p>Before passing the input through the Multi-Head Attention (MHA) layer, we apply
Layer Normalization to the positionally encoded embeddings <span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span>.
This is known as pre-layer Normalization in the more modern GPT architecture (as
opposed to post-layer Normalization, which is applied after the MHA layer).</p>
<p>The Layer Normalization function <span class="math notranslate nohighlight">\(\operatorname{LayerNorm}(\cdot)\)</span> is a
<strong><em>vectorwise</em></strong> operation that operates on the feature dimension <span class="math notranslate nohighlight">\(D\)</span> of the
input tensor. It normalizes the activations to have zero mean and unit variance
across the features for each token independently. The vectorwise nature of Layer
Normalization arises from the fact that it computes the mean and standard
deviation along the feature dimension, requiring <strong>aggregation</strong> of information
across the entire feature vector for each token.</p>
<p>Mathematically, for an input tensor <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{T \times D}\)</span>,
Layer Normalization is applied independently to each row
<span class="math notranslate nohighlight">\(\mathbf{x}_t \in \mathbb{R}^{1 \times D}\)</span>, where <span class="math notranslate nohighlight">\(t \in \{1, 2, \ldots, T\}\)</span>.
The normalization is performed using the following formula:</p>
<div class="math notranslate nohighlight">
\[
\operatorname{LayerNorm}(\mathbf{x}_t) = \frac{\mathbf{x}_t - \mu_t}{\sqrt{\sigma_t^2 + \epsilon}} \odot \gamma + \beta
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu_t \in \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(\sigma_t^2 \in \mathbb{R}\)</span> are the mean and
variance of the features in <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> (broadcasted), respectively,
<span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant for numerical stability,
<span class="math notranslate nohighlight">\(\gamma \in \mathbb{R}^D\)</span> and <span class="math notranslate nohighlight">\(\beta \in \mathbb{R}^D\)</span> are learnable affine
parameters (scale and shift), and <span class="math notranslate nohighlight">\(\odot\)</span> denotes the elementwise product.</p>
<p>Applying Layer Normalization to the positionally encoded embeddings
<span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span> at layer <span class="math notranslate nohighlight">\(\ell\)</span> results in the normalized embeddings
<span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(\ell)}_1 &amp;= \operatorname{LayerNorm}\left(\tilde{\mathbf{X}}\right) \\
T \times D &amp;\leftarrow T \times D \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span> represents the normalized embeddings at layer
<span class="math notranslate nohighlight">\(\ell\)</span>, and the index <span class="math notranslate nohighlight">\(1\)</span> refers to the first sub-layer/sub-step in the decoder
block.</p>
<p>For the first layer (<span class="math notranslate nohighlight">\(\ell = 1\)</span>), <span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span> is the output from Step 4
(Positional Embedding). So we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(1)}_1 &amp;= \operatorname{LayerNorm}\left(\tilde{\mathbf{X}}\right) \\
T \times D &amp;\leftarrow T \times D \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>In code we have:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># [1]</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># [2]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">elementwise_affine</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>  <span class="c1"># [3]</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>  <span class="c1"># [4]</span>
</pre></div>
</div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Line</strong></p></th>
<th class="head"><p><strong>Code</strong></p></th>
<th class="head"><p><strong>Operation Description</strong></p></th>
<th class="head"><p><strong>Input Shape</strong></p></th>
<th class="head"><p><strong>Output Shape</strong></p></th>
<th class="head"><p><strong>Notes</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>[1]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">mean</span> <span class="pre">=</span> <span class="pre">x.mean(dim=-1,</span> <span class="pre">keepdim=True)</span></code></p></td>
<td><p>Computes the mean of <code class="docutils literal notranslate"><span class="pre">x</span></code> along the last dimension.</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times 1\)</span></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">keepdim=True</span></code> ensures the number of dimensions is preserved, facilitating broadcasting in subsequent operations. Mean is computed for each feature vector.</p></td>
</tr>
<tr class="row-odd"><td><p>[2]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">std</span> <span class="pre">=</span> <span class="pre">x.std(dim=-1,</span> <span class="pre">keepdim=True,</span> <span class="pre">unbiased=False)</span></code></p></td>
<td><p>Computes the standard deviation along the last dimension.</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times 1\)</span></p></td>
<td><p>Similar to the mean, <code class="docutils literal notranslate"><span class="pre">std</span></code> is computed per feature vector with unbiased variance estimation disabled (appropriate for normalization purposes).</p></td>
</tr>
<tr class="row-even"><td><p>[3]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">return</span> <span class="pre">self.gamma</span> <span class="pre">*</span> <span class="pre">(x</span> <span class="pre">-</span> <span class="pre">mean)</span> <span class="pre">/</span> <span class="pre">(std</span> <span class="pre">+</span> <span class="pre">self.eps)</span> <span class="pre">+</span> <span class="pre">self.beta</span></code></p></td>
<td><p>Applies the normalization formula with learnable parameters gamma (<span class="math notranslate nohighlight">\(\gamma\)</span>) and beta (<span class="math notranslate nohighlight">\(\beta\)</span>).</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>Element-wise operations are used. <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are of shape <span class="math notranslate nohighlight">\(D\)</span>, and are broadcasted to match the input shape. This line only executes if <code class="docutils literal notranslate"><span class="pre">elementwise_affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p>[4]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">return</span> <span class="pre">(x</span> <span class="pre">-</span> <span class="pre">mean)</span> <span class="pre">/</span> <span class="pre">(std</span> <span class="pre">+</span> <span class="pre">self.eps)</span></code></p></td>
<td><p>Applies the normalization formula without learnable parameters.</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>Simple normalization where each element in the feature vector <span class="math notranslate nohighlight">\(x\)</span> is normalized by the corresponding mean and standard deviation.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="note admonition">
<p class="admonition-title">Step 7. Masked Multi-Head Self-Attention</p>
<p>Given the normalized input embeddings
<span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1 \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span> from Step
6 (Pre-Layer Normalization), we apply the masked multi-head self-attention
mechanism to compute the output embeddings <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span>, where the
index <span class="math notranslate nohighlight">\(2\)</span> denotes the second sub-layer within the <span class="math notranslate nohighlight">\(\ell\)</span>-th decoder layer
(multi-head attention).</p>
<p>Let <span class="math notranslate nohighlight">\(\operatorname{MaskedMultiHead}^{(\ell)}(\cdot)\)</span> denote the masked
multi-head self-attention function at layer <span class="math notranslate nohighlight">\(\ell\)</span>. The masked multi-head
self-attention operation takes the normalized input embeddings
<span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span> as the query, key, and value matrices, and produces the
output embeddings <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span>.</p>
<p>For the first layer (<span class="math notranslate nohighlight">\(\ell = 1\)</span>), the masked multi-head self-attention operation
can be expressed as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(1)}_2 &amp;= \operatorname{MaskedMultiHead}^{(1)}\left(\mathbf{Z}^{(1)}_1, \mathbf{Z}^{(1)}_1, \mathbf{Z}^{(1)}_1\right) \\
\mathcal{B} \times T \times D &amp;\leftarrow \operatorname{MaskedMultiHead}^{(1)}\left(\mathcal{B} \times T \times D, \mathcal{B} \times T \times D, \mathcal{B} \times T \times D\right) \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\mathbf{Z}^{(1)}_2 \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span>
represents the output embeddings of the masked multi-head self-attention
operation at layer <span class="math notranslate nohighlight">\(1\)</span>, and
<span class="math notranslate nohighlight">\(\mathbf{Z}^{(1)}_1 \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span> represents
the normalized input embeddings from Step 6.</p>
<p>The <span class="math notranslate nohighlight">\(\operatorname{MaskedMultiHead}^{(\ell)}(\cdot)\)</span> function internally
performs the following steps:</p>
<ol class="arabic simple">
<li><p>Linearly projects the input embeddings <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span> into query,
key, and value matrices for each attention head.</p></li>
<li><p>Computes the scaled dot-product attention scores between the query and key
matrices, and applies the attention mask to prevent attending to future
tokens.</p></li>
<li><p>Applies the softmax function to the masked attention scores to obtain the
attention weights.</p></li>
<li><p>Multiplies the attention weights with the value matrices to produce the
output embeddings for each attention head.</p></li>
<li><p>Concatenates the output embeddings from all attention heads and linearly
projects them to obtain the final output embeddings <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span>.</p></li>
</ol>
<p>The specifics of the scaled dot-product attention mechanism and the multi-head
attention operation will be discussed in the next few steps.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Step 7.1. Linear Projections, Query, Key, and Value Matrices</p>
<p>In the masked multi-head self-attention mechanism, the first step is to linearly
project the normalized input embeddings <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span> into query, key,
and value matrices for each attention head. This step is performed using
learnable weight matrices <span class="math notranslate nohighlight">\(\mathbf{W}^{Q, (\ell)}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{W}^{K, (\ell)}\)</span>,
and <span class="math notranslate nohighlight">\(\mathbf{W}^{V, (\ell)}\)</span>.</p>
<p>Mathematically, the linear projections can be expressed as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Q}^{(\ell)} &amp;= \mathbf{Z}^{(\ell)}_1 \mathbf{W}^{Q, (\ell)} ,\quad \mathcal{B} \times T \times D \leftarrow \mathcal{B} \times T \times D \times D \\
\mathbf{K}^{(\ell)} &amp;= \mathbf{Z}^{(\ell)}_1 \mathbf{W}^{K, (\ell)} ,\quad \mathcal{B} \times T \times D \leftarrow \mathcal{B} \times T \times D \times D \\
\mathbf{V}^{(\ell)} &amp;= \mathbf{Z}^{(\ell)}_1 \mathbf{W}^{V, (\ell)} ,\quad \mathcal{B} \times T \times D \leftarrow \mathcal{B} \times T \times D \times D
\end{aligned}
\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{Q}^{(\ell)} \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span> is the
query matrix for the <span class="math notranslate nohighlight">\(\ell\)</span>-th decoder layer.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{K}^{(\ell)} \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span> is the
key matrix for the <span class="math notranslate nohighlight">\(\ell\)</span>-th decoder layer.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{V}^{(\ell)} \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span> is the
value matrix for the <span class="math notranslate nohighlight">\(\ell\)</span>-th decoder layer.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{Q, (\ell)} \in \mathbb{R}^{D \times D}\)</span>,
<span class="math notranslate nohighlight">\(\mathbf{W}^{K, (\ell)} \in \mathbb{R}^{D \times D}\)</span>, and
<span class="math notranslate nohighlight">\(\mathbf{W}^{V, (\ell)} \in \mathbb{R}^{D \times D}\)</span> are the learnable
weight matrices that transform the normalized embeddings into queries, keys,
and values, respectively.</p></li>
<li><p>Again notice that we are using the same weight matrices for all heads,
weight/parameters sharing.</p></li>
</ul>
<p>The linear projections are performed using matrix multiplication between the
normalized input embeddings <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span> and the corresponding weight
matrices. The resulting query, key, and value matrices have the same shape as
the input embeddings: <span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span>.</p>
<p>In the provided code snippet, the linear projections are implemented using the
<code class="docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code> modules <code class="docutils literal notranslate"><span class="pre">self.W_Q</span></code>, <code class="docutils literal notranslate"><span class="pre">self.W_K</span></code>, and <code class="docutils literal notranslate"><span class="pre">self.W_V</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_Q</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>  <span class="c1"># Z @ W_Q = [B, T, D] @ [D, D] = [B, T, D]</span>
<span class="n">K</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_K</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>  <span class="c1"># Z @ W_K = [B, T, D] @ [D, D] = [B, T, D]</span>
<span class="n">V</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_V</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>  <span class="c1"># Z @ W_V = [B, T, D] @ [D, D] = [B, T, D]</span>
</pre></div>
</div>
</div>
<div class="note admonition">
<p class="admonition-title">Step 7.2. Reshaping and Transposing Query, Key, and Value Matrices</p>
<p>Subsequently, we have already known that instead of for loop to compute each
head, we can compute all heads in parallel using matrix operations. The query,
key, and value matrices are split into <span class="math notranslate nohighlight">\(H\)</span> heads, and the attention scores are
computed in parallel. So our aim is simple, we want to reshape the query, key,
and value matrices to include the head dimension, basically splitting the <span class="math notranslate nohighlight">\(D\)</span>
dimension into <span class="math notranslate nohighlight">\(H\)</span> heads. We can denote the reshaping and transposition
operation using tensor index notation which makes it explicit how indices are
permuted and combined:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Q}_{b,t,d} &amp; \rightarrow \mathbf{Q}_{b,t,h,d_q} \quad \text{where } d = h \cdot (D // H) + d_q, \text{ for } h \in [0, H-1] \text{ and } d_q \in [0, D//H-1] \\
\mathbf{Q}_{b,t,h,d_q} &amp; \rightarrow \mathbf{Q}_{b,h,t,d_q} \quad \text{(transpose dimensions)}
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{K}_{b,t,d} &amp; \rightarrow \mathbf{K}_{b,t,h,d_k} \quad \text{where } d = h \cdot (D // H) + d_k, \text{ for } h \in [0, H-1] \text{ and } d_k \in [0, D//H-1] \\
\mathbf{K}_{b,t,h,d_k} &amp; \rightarrow \mathbf{K}_{b,h,t,d_k} \quad \text{(transpose dimensions)}
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{V}_{b,t,d} &amp; \rightarrow \mathbf{V}_{b,t,h,d_v} \quad \text{where } d = h \cdot (D // H) + d_v, \text{ for } h \in [0, H-1] \text{ and } d_v \in [0, D//H-1] \\
\mathbf{V}_{b,t,h,d_v} &amp; \rightarrow \mathbf{V}_{b,h,t,d_v} \quad \text{(transpose dimensions)}
\end{aligned}
\end{split}\]</div>
<p>To this end, we have reshaped and transposed the query, key, and value matrices
as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Q}^{(\ell)} &amp;\in \mathbb{R}^{\mathcal{B} \times T \times D} \rightarrow \mathbf{Q}^{(\ell)}  \in \mathbb{R}^{\mathcal{B} \times T \times H \times D // H} \rightarrow \mathbf{Q}^{(\ell)}  \in \mathbb{R}^{\mathcal{B} \times H \times T \times D // H} \\
\mathbf{K}^{(\ell)}  &amp;\in \mathbb{R}^{\mathcal{B} \times T \times D} \rightarrow \mathbf{K}^{(\ell)}  \in \mathbb{R}^{\mathcal{B} \times T \times H \times D // H} \rightarrow \mathbf{K}^{(\ell)}  \in \mathbb{R}^{\mathcal{B} \times H \times T \times D // H} \\
\mathbf{V}^{(\ell)}  &amp;\in \mathbb{R}^{\mathcal{B} \times T \times D} \rightarrow \mathbf{V}^{(\ell)}  \in \mathbb{R}^{\mathcal{B} \times T \times H \times D // H} \rightarrow \mathbf{V}^{(\ell)}  \in \mathbb{R}^{\mathcal{B} \times H \times T \times D // H}
\end{aligned}
\end{split}\]</div>
<p>In code, the reshaping and transposition operations are performed as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">,</span> <span class="n">D</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">dim0</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># [B, T, D] -&gt; [B, T, H, D // H] -&gt; [B, H, T, D//H]</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">,</span> <span class="n">D</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">dim0</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">,</span> <span class="n">D</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">dim0</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">view</span></code> operation reshapes the matrices to include the head dimension, and
the <code class="docutils literal notranslate"><span class="pre">transpose</span></code> operation swaps the sequence and head dimensions to obtain the
desired ordering of dimensions.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Step 7.3. Scaled Dot-Product Attention and Masking</p>
<p>The attention mask matrix <span class="math notranslate nohighlight">\(\mathbf{M} \in \{0, -\infty\}^{T \times T}\)</span> is
initially constructed as a lower triangular matrix of ones and zeros:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{M} = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
1 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\
1 &amp; 1 &amp; 1 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; 1 &amp; 1 &amp; \cdots &amp; 1
\end{bmatrix}
\end{split}\]</div>
<p>In this matrix, “1” (conceptually) allows attention, and “0” blocks it. This
mask is broadcastable to the attention scores tensor
<span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span> since this is a typical shape for
multi-head self-attention.</p>
<p>However, before applying the mask, the ones in <span class="math notranslate nohighlight">\(\mathbf{M}\)</span> are typically
replaced with zeros, and the zeros are replaced with a large negative value
(e.g., <span class="math notranslate nohighlight">\(-\infty\)</span>) to effectively set the attention weights of the masked
positions to zero after the softmax operation (which is to mimic the
<code class="docutils literal notranslate"><span class="pre">masked_fill</span></code> operation in the code). We define
<span class="math notranslate nohighlight">\(\mathbf{M}^{-\infty} \in \{0, -\infty\}^{T \times T} = 1 - \mathbf{M}\)</span>, where:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{M}^{-\infty}_{ij} = \left\{\begin{array}{ll}
0 &amp; \text{if } i \geq j \\
-\infty &amp; \text{if } i &lt; j
\end{array} \right\} \quad \text{forms a triangular matrix:} \quad \begin{bmatrix}
0 &amp; -\infty &amp; -\infty &amp; \cdots &amp; -\infty \\
0 &amp; 0 &amp; -\infty &amp; \cdots &amp; -\infty \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; -\infty \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0
\end{bmatrix}
\end{split}\]</div>
<div class="warning admonition">
<p class="admonition-title">Notation Abuse</p>
<p>For the ease of notation, we abuse notation by using <span class="math notranslate nohighlight">\(\mathbf{M}\)</span> to denote the
final mask matrix <span class="math notranslate nohighlight">\(\mathbf{M}^{-\infty}\)</span>.</p>
</div>
<p>The masking operation will then be applied elementwise to the attention scores
tensor <span class="math notranslate nohighlight">\(\mathbf{A}_{s}^{(\ell)}\)</span>, which is first obtained by computing the
scaled dot product between the query matrix <span class="math notranslate nohighlight">\(\mathbf{Q}^{(\ell)}\)</span> and the key
matrix <span class="math notranslate nohighlight">\(\mathbf{K}^{(\ell)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{A}_{s}^{(\ell)} &amp;= \frac{\mathbf{Q}^{(\ell)} (\mathbf{K}^{(\ell)})^T}{\sqrt{D//H}} \\
\mathcal{B} \times H \times T \times T &amp;\leftarrow \frac{\mathcal{B} \times H \times T \times D//H \operatorname{&#64;} \mathcal{B} \times H \times D//H \times T}{\sqrt{D//H}}
\end{aligned}
\end{split}\]</div>
<p>The masking operation is performed using the elementwise sum (<span class="math notranslate nohighlight">\(\oplus\)</span>) between
the attention scores tensor
<span class="math notranslate nohighlight">\(\mathbf{A}_{s}^{(\ell)} \in \mathbb{R}^{\mathcal{B} \times H \times T \times T}\)</span>
and the <strong><em>broadcasted</em></strong> mask matrix
<span class="math notranslate nohighlight">\(\mathbf{M} \in \{0, -\infty\}^{\mathcal{B} \times H \times T \times T}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{A}_{s}^{M, (\ell)} &amp;= \mathbf{A}_{s}^{(\ell)} \oplus \mathbf{M} \\
\mathcal{B} \times H \times T \times T &amp;\leftarrow \mathcal{B} \times H \times T \times T \oplus \mathcal{B} \times H \times T \times T
\end{aligned}
\end{split}\]</div>
<p>The elementwise sum ensures that the attention scores corresponding to future
tokens (positions above the diagonal) are added by <span class="math notranslate nohighlight">\(-\infty\)</span> to effectively
block attention to those positions and added by <span class="math notranslate nohighlight">\(0\)</span> for the rest of the
positions that are allowed to attend to.</p>
<div class="dropdown admonition">
<p class="admonition-title">Alternative Masking</p>
<p>I have some quirks with the above because it “feels weird” to use <span class="math notranslate nohighlight">\(0\)</span> as
“allowed” instead of <span class="math notranslate nohighlight">\(1\)</span>. After all the above formulation neatly fits the
notation. However, an alternative way is to use <span class="math notranslate nohighlight">\(1\)</span> as “allowed” - where we
define <span class="math notranslate nohighlight">\(\mathbf{M}^{-\infty}\)</span> again but this time we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{A}_{s}^{M, (\ell)} &amp;= \mathbf{A}_{s}^{(\ell)} \odot \mathbf{M} + \mathbf{M}^{-\infty} \\
\mathcal{B} \times H \times T \times T &amp;\leftarrow (\mathcal{B} \times H \times T \times T \odot \mathcal{B} \times H \times T \times T) + \mathcal{B} \times H \times T \times T
\end{aligned}
\end{split}\]</div>
<p>The elementwise multiplication <span class="math notranslate nohighlight">\(\mathbf{A}_{s}^{(\ell)} \odot \mathbf{M}\)</span>
preserves the attention scores for the allowed positions (where <span class="math notranslate nohighlight">\(\mathbf{M}\)</span>
is 1) and sets the scores to zero for the masked positions (where <span class="math notranslate nohighlight">\(\mathbf{M}\)</span>
is 0). Then, the elementwise addition with <span class="math notranslate nohighlight">\(\mathbf{M}_{-\infty}\)</span> effectively
pushes the attention scores of the masked positions towards negative infinity,
while leaving the scores of the allowed positions unchanged.</p>
</div>
<p>Finally, the masked attention scores <span class="math notranslate nohighlight">\(\mathbf{A}_{s}^{M, (\ell)}\)</span> are passed
through the softmax function to obtain the attention weights
<span class="math notranslate nohighlight">\(\mathbf{A}_{w}^{(\ell)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{A}_{w}^{(\ell)} &amp;= \operatorname{softmax}\left(\mathbf{A}_{s}^{M, (\ell)}\right) \\
\mathcal{B} \times H \times T \times T &amp;\leftarrow \operatorname{softmax}\left(\mathcal{B} \times H \times T \times T\right) \\
\mathcal{B} \times H \times T \times T &amp;\leftarrow \mathcal{B} \times H \times T \times T
\end{aligned}
\end{split}\]</div>
<p>The softmax function is then applied to the masked attention scores tensor in a
<strong>vectorwise</strong> manner. Specifically, the softmax operation is applied
<strong>independently</strong> to each row of the last two dimensions (<span class="math notranslate nohighlight">\(T \times T\)</span>) for each
batch and head. This ensures that the attention weights for each token position
across the sequence length sum up to 1. The large negative values in the masked
positions ensure that the corresponding attention weights become close to zero
after the softmax operation.</p>
<p>Finally, the context matrix <span class="math notranslate nohighlight">\(\mathbf{C}^{(\ell)}\)</span> is obtained by multiplying the
attention weights <span class="math notranslate nohighlight">\(\mathbf{A}_{w}^{(\ell)}\)</span> with the value matrix
<span class="math notranslate nohighlight">\(\mathbf{V}^{(\ell)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{C}^{(\ell)} &amp;= \mathbf{A}_{w}^{(\ell)} \mathbf{V}^{(\ell)} \\
\mathcal{B} \times H \times T \times D//H &amp;\leftarrow \mathcal{B} \times H \times T \times T \times \mathcal{B} \times H \times T \times D//H
\end{aligned}
\end{split}\]</div>
<p>The resulting context matrix <span class="math notranslate nohighlight">\(\mathbf{C}^{(\ell)}\)</span> contains the attended values
for each head in layer <span class="math notranslate nohighlight">\(\ell\)</span>.</p>
<p>Note <span class="math notranslate nohighlight">\(\mathbf{C}^{(\ell)}\)</span> is the context matrix which is the output of the
self-attention mechanism and it contains <span class="math notranslate nohighlight">\(\operatorname{head}_{\ell, h}^{M}\)</span> for
each head <span class="math notranslate nohighlight">\(h\)</span> in the layer <span class="math notranslate nohighlight">\(\ell\)</span>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">d_q</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">dim0</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">d_q</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>         <span class="c1"># [B, H, T, d_q] @ [B, H, d_q, T] = [B, H, T, T]</span>
<span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span> <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">attention_scores</span>     <span class="c1"># [B, H, T, T]</span>

<span class="n">attention_weights</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>        <span class="c1"># [B, H, T, T]</span>
<span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">)</span>         <span class="c1"># [B, H, T, T]</span>

<span class="n">context_vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>     <span class="c1"># [B, H, T, T] @ [B, H, T, d_v] = [B, H, T, d_v]</span>
</pre></div>
</div>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Line</strong></p></th>
<th class="head"><p><strong>Code</strong></p></th>
<th class="head"><p><strong>Operation Description</strong></p></th>
<th class="head"><p><strong>Input Shape</strong></p></th>
<th class="head"><p><strong>Output Shape</strong></p></th>
<th class="head"><p><strong>Notes</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>[1]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">d_q</span> <span class="pre">=</span> <span class="pre">query.size(dim=-1)</span></code></p></td>
<td><p>Retrieves the dimension of the query vectors.</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times d_q\)</span></p></td>
<td><p>Scalar value</p></td>
<td><p>The last dimension of the query tensor represents the query vector dimension.</p></td>
</tr>
<tr class="row-odd"><td><p>[2]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">attention_scores</span> <span class="pre">=</span> <span class="pre">torch.matmul(query,</span> <span class="pre">key.transpose(dim0=-2,</span> <span class="pre">dim1=-1))</span> <span class="pre">/</span> <span class="pre">torch.sqrt(torch.tensor(d_q).float())</span></code></p></td>
<td><p>Computes the scaled dot-product attention scores by matrix multiplying the query and key matrices and scaling by the square root of the query dimension.</p></td>
<td><p>Query: <span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times d_q\)</span><br>Key: <span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times d_k\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p>The key matrix is transposed to align the dimensions for matrix multiplication. The scaling factor helps stabilize gradients during training.</p></td>
</tr>
<tr class="row-even"><td><p>[3]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">attention_scores</span> <span class="pre">=</span> <span class="pre">attention_scores.masked_fill(mask</span> <span class="pre">==</span> <span class="pre">0,</span> <span class="pre">float(&quot;-inf&quot;))</span> <span class="pre">if</span> <span class="pre">mask</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">None</span> <span class="pre">else</span> <span class="pre">attention_scores</span></code></p></td>
<td><p>Applies the attention mask to the attention scores. Positions where the mask is 0 are filled with <span class="math notranslate nohighlight">\(-\infty\)</span> to effectively block attention to those positions.</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p>The <code class="docutils literal notranslate"><span class="pre">masked_fill</span></code> operation is an elementwise operation that replaces the attention scores at masked positions with <span class="math notranslate nohighlight">\(-\infty\)</span>. This step is skipped if no mask is provided.</p></td>
</tr>
<tr class="row-odd"><td><p>[4]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">attention_weights</span> <span class="pre">=</span> <span class="pre">attention_scores.softmax(dim=-1)</span></code></p></td>
<td><p>Applies the softmax function to the masked attention scores along the last dimension to obtain the attention weights.</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p>The softmax operation is applied in a vectorwise manner, independently for each row of the last two dimensions (<span class="math notranslate nohighlight">\(T \times T\)</span>) for each batch and head. This ensures that the attention weights for each token position across the sequence length sum up to 1.</p></td>
</tr>
<tr class="row-even"><td><p>[5]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">attention_weights</span> <span class="pre">=</span> <span class="pre">self.dropout(attention_weights)</span></code></p></td>
<td><p>Applies dropout regularization to the attention weights to prevent overfitting.</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p>Dropout randomly sets a fraction of the attention weights to zero during training, which helps improve generalization. This is element-wise operation.</p></td>
</tr>
<tr class="row-odd"><td><p>[6]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">context_vector</span> <span class="pre">=</span> <span class="pre">torch.matmul(attention_weights,</span> <span class="pre">value)</span></code></p></td>
<td><p>Computes the context vector by matrix multiplying the attention weights with the value matrix.</p></td>
<td><p>Attention Weights: <span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span><br>Value: <span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times d_v\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times d_v\)</span></p></td>
<td><p>The attention weights are used to weight the importance of each token’s value vector. The resulting context vector captures the attended information from the input sequence.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="note admonition">
<p class="admonition-title">Step 7.4. Concatenation and Projection</p>
<p>Recall that the output from the masked multi-head self-attention operation,
denoted as <span class="math notranslate nohighlight">\(\mathbf{C}^{(\ell)}\)</span>, has a shape of
<span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times D//H\)</span>, where <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> is the batch
size, <span class="math notranslate nohighlight">\(H\)</span> is the number of attention heads, <span class="math notranslate nohighlight">\(T\)</span> is the sequence length, and
<span class="math notranslate nohighlight">\(D//H\)</span> is the dimension of each head.</p>
<p>To concatenate the heads and obtain a tensor of shape
<span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span>, we first need to transpose the dimensions of
<span class="math notranslate nohighlight">\(\mathbf{C}^{(\ell)}\)</span> from <span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times D//H\)</span> to
<span class="math notranslate nohighlight">\(\mathcal{B} \times T \times H \times D//H\)</span> - necessary to concatenate the heads
along the last dimension (feature dimension).</p>
<p>Using tensor index notation or semi einsum notation, we can denote the
transposition operation as follows:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbf{C}^{(\ell)}_{b,h,t,d} &amp; \rightarrow \mathbf{C}^{(\ell)}_{b,t,h,d} \quad \text{(transpose dimensions)}
\end{aligned}
\]</div>
<p>After transposition, the heads are concatenated along the last dimension
(feature dimension) to obtain a tensor of shape <span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span>.
The concatenation operation can be expressed using the direct sum notation as
follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{C}^{(\ell)}_{\text{concat}} &amp;= \bigoplus_{h=0}^{H-1} \mathbf{C}^{(\ell)}_{b,t,h,:} \\
&amp;= \mathbf{C}^{(\ell)}_{b,t,0,:} \oplus \mathbf{C}^{(\ell)}_{b,t,1,:} \oplus \cdots \oplus \mathbf{C}^{(\ell)}_{b,t,H-1,:} \\
&amp;= \operatorname{head}^{\ell}_1 \oplus \operatorname{head}^{\ell}_2 \oplus \cdots \oplus \operatorname{head}^{\ell}_H \\
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{C}^{(\ell)}_{b,t,h,:}\)</span> represents the tensor slice corresponding
to the <span class="math notranslate nohighlight">\(h\)</span>-th head at batch index <span class="math notranslate nohighlight">\(b\)</span> and time step <span class="math notranslate nohighlight">\(t\)</span>, and <span class="math notranslate nohighlight">\(\oplus\)</span> denotes
the concatenation operation along the feature dimension.</p>
<div class="dropdown admonition">
<p class="admonition-title">Direct Sum Notation Is Concatenation</p>
<p>In the concatenation of attention heads, the direct sum notation (<span class="math notranslate nohighlight">\(\oplus\)</span>) is
used to represent the concatenation operation, not elementwise addition. The
direct sum combines the output tensors from each attention head along a specific
dimension (usually the feature dimension). It is a vectorwise operation that
stacks the tensors along the specified dimension, creating a new tensor with an
increased dimension size.</p>
</div>
<p>The concatenation operation can be summarized as:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbf{C}^{(\ell)} &amp;\in \mathbb{R}^{\mathcal{B} \times T \times H \times D//H} \xrightarrow{\text{concatenate}} \mathbf{C}^{(\ell)}_{\text{concat}} \in \mathbb{R}^{\mathcal{B} \times T \times D}
\end{aligned}
\]</div>
<p>To summarize, the transposition and concatenation operations can be represented
as:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbf{C}^{(\ell)} &amp;\in \mathbb{R}^{\mathcal{B} \times H \times T \times D//H} \rightarrow \mathbf{C}^{(\ell)}  \in \mathbb{R}^{\mathcal{B} \times T \times H \times D//H} \rightarrow \mathbf{C}^{(\ell)}  \in \mathbb{R}^{\mathcal{B} \times T \times D}
\end{aligned}
\]</div>
<p>Finally, the concatenated tensor is linearly transformed using the projection
matrix <span class="math notranslate nohighlight">\(\mathbf{W}^{O, (\ell)}\)</span> to obtain the output <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(\ell)}_2 &amp;= \mathbf{C}^{(\ell)}_{\text{concat}} \mathbf{W}^{O, (\ell)} \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D \operatorname{&#64;} D \times D \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>In code, these operations can be implemented as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">context_vector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">context_vector</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">dim0</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>  <span class="c1"># merge all heads together</span>

<span class="n">projected_context_vector</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resid_dropout</span><span class="p">(</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">context_projection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">context_vector</span><span class="p">)</span>  <span class="c1"># [B, T, D] @ [D, D] = [B, T, D]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>To this end, for layer <span class="math notranslate nohighlight">\(\ell=1\)</span>, we would have the output tensor
<span class="math notranslate nohighlight">\(\mathbf{Z}^{(1)}_2\)</span> - which becomes the input to the next sub-layer within the
same block <span class="math notranslate nohighlight">\(\ell\)</span>. Optionally, we can apply a projection dropout to the output
tensor <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span> before passing it to the next sub-layer.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Step 8. Residual Connection</p>
<p>In the Transformer/GPT architecture, residual connections are used to facilitate
the flow of information and gradients throughout the network. The residual
connection in the decoder block is added between the input to the block and the
output of the Masked Multi-Head Attention layer.</p>
<p>For the first decoder block (<span class="math notranslate nohighlight">\(\ell = 1\)</span>), the residual connection is added
between the positionally encoded embeddings <span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span> and the output
of the Masked Multi-Head Attention layer <span class="math notranslate nohighlight">\(\mathbf{Z}^{(1)}_2\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(1)}_{3} &amp;= \tilde{\mathbf{X}} + \text{MaskedMultiHead}^{(1)}\left(\text{LayerNorm}\left(\tilde{\mathbf{X}}\right), \text{LayerNorm}\left(\tilde{\mathbf{X}}\right), \text{LayerNorm}\left(\tilde{\mathbf{X}}\right)\right) \\
&amp;= \tilde{\mathbf{X}} + \text{MaskedMultiHead}^{(1)}\left(\mathbf{Z}^{(1)}_{1}, \mathbf{Z}^{(1)}_{1}, \mathbf{Z}^{(1)}_{1}\right) \\
&amp;= \tilde{\mathbf{X}} + \mathbf{Z}^{(1)}_{2} \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D + \mathcal{B} \times T \times D \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{Z}^{(1)}_{3}\)</span> represents the output of the residual connection
for the first decoder block, <span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span> is the positionally encoded
embeddings, and <span class="math notranslate nohighlight">\(\mathbf{Z}^{(1)}_{2}\)</span> is the output of the Masked Multi-Head
Attention layer.</p>
<p>For subsequent decoder blocks (<span class="math notranslate nohighlight">\(\ell &gt; 1\)</span>), the residual connection is added
between the output of the previous decoder block
<span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell-1)}_{\text{out}}\)</span> and the output of the Masked Multi-Head
Attention layer <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span> of the current block:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z}^{(\ell)}_3 = \mathbf{Z}^{(\ell-1)}_{\text{out}} + \mathbf{Z}^{(\ell)}_2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_3\)</span> represents the output of the residual connection
for the <span class="math notranslate nohighlight">\(\ell\)</span>-th decoder block, <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell-1)}_{\text{out}}\)</span> is the
output of the previous decoder block after the Position-wise Feed-Forward
Network and the second residual connection, and <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span> is the
output of the Masked Multi-Head Attention layer of the current block.</p>
<p>The complete set of equations for the <span class="math notranslate nohighlight">\(\ell\)</span>-th decoder block (<span class="math notranslate nohighlight">\(\ell &gt; 1\)</span>) can
be summarized as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(\ell)}_1 &amp;= \text{LayerNorm}\left(\mathbf{Z}^{(\ell-1)}_{\text{out}}\right) \\
\mathbf{Z}^{(\ell)}_2 &amp;= \text{MaskedMultiHead}^{(\ell)}\left(\mathbf{Z}^{(\ell)}_1, \mathbf{Z}^{(\ell)}_1, \mathbf{Z}^{(\ell)}_1\right) \\
\mathbf{Z}^{(\ell)}_3 &amp;= \mathbf{Z}^{(\ell-1)}_{\text{out}} + \mathbf{Z}^{(\ell)}_2
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span> represents the output of the Layer Normalization
step, <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span> represents the output of the Masked Multi-Head
Attention layer, and <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_3\)</span> represents the output of the
residual connection.</p>
<p>The residual connection allows the model to learn the identity function more
easily, enabling the flow of information and gradients across multiple layers.
By adding the input of the block to the output of the Masked Multi-Head
Attention layer, the model can choose to either learn new information from the
attention mechanism or retain the original input information if it is already
sufficient.</p>
<p>In code, the whole series of operation up till now is simply:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">z</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ln_1</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="note admonition">
<p class="admonition-title">Step 9. Pre-Layer Normalization For Position-wise Feed-Forward Network</p>
<p>After the masked multi-head attention block and the residual connection, the
next step is to apply the position-wise feed-forward network (FFN) to the output
of the self-attention mechanism and the residual block <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_3\)</span>.
However, before applying the FFN, we perform pre-layer normalization on the
input to the FFN.</p>
<p>Mathematically, the pre-layer normalization step for the FFN can be expressed
as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(\ell)}_4 &amp;= \operatorname{LayerNorm}\left(\mathbf{Z}^{(\ell)}_3\right) \\
\mathcal{B} \times T \times D &amp;\leftarrow \operatorname{LayerNorm}\left(\mathcal{B} \times T \times D\right) \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_4\)</span> represents the normalized input to the FFN at
layer <span class="math notranslate nohighlight">\(\ell\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_3\)</span> is the output of the residual
connection from the previous step.</p>
<p>As discussed in Step 6, the Layer Normalization function
<span class="math notranslate nohighlight">\(\operatorname{LayerNorm}(\cdot)\)</span> is a vectorwise operation that operates on the
feature dimension <span class="math notranslate nohighlight">\(D\)</span> of the input tensor. It normalizes the activations to have
zero mean and unit variance across the features for each token independently.
It’s important to note that the pre-layer normalization step is applied
independently to each token
<span class="math notranslate nohighlight">\(\mathbf{Z}_{3, t}^{(\ell)} \in \mathbf{Z}_3^{(\ell)}\)</span> in the sequence with
shape <span class="math notranslate nohighlight">\(T \times D\)</span>, where <span class="math notranslate nohighlight">\(T\)</span> is the sequence length and <span class="math notranslate nohighlight">\(D\)</span> is the feature
dimension, normalizing the features across the feature dimension <span class="math notranslate nohighlight">\(D\)</span>.</p>
<p>For the first layer (<span class="math notranslate nohighlight">\(\ell = 1\)</span>), the pre-layer normalization step can be
written as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z}^{(1)}_4 = \operatorname{LayerNorm}\left(\mathbf{Z}^{(1)}_3\right)
\]</div>
</div>
<div class="note admonition">
<p class="admonition-title">Step 10. Position-wise Feed-Forward Network</p>
<p>Given the normalized input <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_4\)</span> to the Position-wise
Feed-Forward Network (FFN) in layer <span class="math notranslate nohighlight">\(\ell\)</span>, the FFN applies two linear
transformations with a GELU activation function in between. The operations
within the FFN can be mathematically represented as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{FF, (\ell)}_1 &amp;= \text{GELU}\left(\mathbf{Z}^{(\ell)}_4 \mathbf{W}^{FF, (\ell)}_1 + \mathbf{b}^{FF, (\ell)}_1\right) \\
\mathcal{B} \times T \times d_{\text{ff}} &amp;\leftarrow \text{GELU}\left(\mathcal{B} \times T \times D \operatorname{&#64;} D \times d_{\text{ff}} + d_{\text{ff}}\right) \\
\mathcal{B} \times T \times d_{\text{ff}} &amp;\leftarrow \mathcal{B} \times T \times d_{\text{ff}} \\
\mathbf{Z}^{(\ell)}_5 &amp;= \mathbf{Z}^{FF, (\ell)}_1 \mathbf{W}^{FF, (\ell)}_2 + \mathbf{b}^{FF, (\ell)}_2 \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times d_{\text{ff}} \operatorname{&#64;} d_{\text{ff}} \times D + D \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{FF, (\ell)}_1 \in \mathbb{R}^{D \times d_{\text{ff}}}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{b}^{FF, (\ell)}_1 \in \mathbb{R}^{d_{\text{ff}}}\)</span> are the weights
and biases of the first linear transformation, respectively.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{FF, (\ell)}_2 \in \mathbb{R}^{d_{\text{ff}} \times D}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{b}^{FF, (\ell)}_2 \in \mathbb{R}^{D}\)</span> are the weights and biases of
the second linear transformation, respectively.</p></li>
<li><p><span class="math notranslate nohighlight">\(d_{\text{ff}}\)</span> is the dimensionality of the hidden layer in the FFN, which
is typically larger than the input dimensionality <span class="math notranslate nohighlight">\(D\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{GELU}(\cdot)\)</span> denotes the Gaussian Error Linear Unit
activation function.</p></li>
</ul>
<p>Note the slight abuse of notation where <span class="math notranslate nohighlight">\(\mathbf{Z}^{FF, (\ell)}_1\)</span> is used to
denote the intermediate output of the first linear transformation in the FFN.
This should not be confused with the earlier notation <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span>.</p>
<p>For the first layer (<span class="math notranslate nohighlight">\(\ell = 1\)</span>), the FFN operations can be written as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{FF, (1)}_1 &amp;= \operatorname{GELU}\left(\mathbf{Z}^{(1)}_4 \mathbf{W}^{FF, (1)}_1 + \mathbf{b}^{FF, (1)}_1\right) \\
\mathbf{Z}^{(1)}_5 &amp;= \mathbf{Z}^{FF, (1)}_1 \mathbf{W}^{FF, (1)}_2 + \mathbf{b}^{FF, (1)}_2
\end{aligned}
\end{split}\]</div>
</div>
<div class="note admonition">
<p class="admonition-title">Step 11. Residual Connection</p>
<p>After obtaining the output <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_5\)</span> from the Position-wise
Feed-Forward Network (FFN) in layer <span class="math notranslate nohighlight">\(\ell\)</span>, the final step in the decoder block
is to apply a residual connection.</p>
<p>Mathematically, this step can be represented as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(\ell)}_{\text{out}} &amp;= \mathbf{Z}^{(\ell)}_3 + \mathbf{Z}^{(\ell)}_5 \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D + \mathcal{B} \times T \times D \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_3 \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span> is
the output of the residual connection from the Masked Multi-Head Attention
block (Step 8).</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_5 \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span> is
the output from the FFN (Step 10).</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_{\text{out}} \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span>
is the output of the decoder block at layer <span class="math notranslate nohighlight">\(\ell\)</span> and serves as the input
to the next decoder block (<span class="math notranslate nohighlight">\(\ell + 1\)</span>).</p></li>
</ul>
<p>The residual connection is performed by adding the output of the Masked
Multi-Head Attention block <span class="math notranslate nohighlight">\(\left(\mathbf{Z}^{(\ell)}_3\right)\)</span> and the output
of the FFN <span class="math notranslate nohighlight">\(\left(\mathbf{Z}^{(\ell)}_5\right)\)</span>. This addition operation is
element-wise, where corresponding elements of the two tensors are added
together.</p>
<p>For the first layer (<span class="math notranslate nohighlight">\(\ell = 1\)</span>), the residual connection step can be written
as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z}^{(1)}_{\text{out}} = \mathbf{Z}^{(1)}_3 + \mathbf{Z}^{(1)}_5
\]</div>
<p>The output of this step, <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_{\text{out}}\)</span>, becomes the input
to the next decoder block.</p>
</div>
</section>
<section id="iterative-process-through-l-decoder-blocks">
<h3><a class="toc-backref" href="#id46" role="doc-backlink">Iterative Process Through L Decoder Blocks</a><a class="headerlink" href="#iterative-process-through-l-decoder-blocks" title="Link to this heading">#</a></h3>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{Z}^{(1)}_{\text{out}}\)</span> be the output of the first decoder block.
This output becomes the input to the next decoder block, and the process
continues iteratively. Each decoder block builds upon the output of the previous
block through a series of mathematical transformations. The subscript notation
<span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_i\)</span> indicates the i-th step output of the <span class="math notranslate nohighlight">\(\ell\)</span>-th decoder
block.</p>
<section id="first-decoder-block-ell-1">
<h4><a class="toc-backref" href="#id47" role="doc-backlink">First Decoder Block (<span class="math notranslate nohighlight">\(\ell = 1\)</span>)</a><a class="headerlink" href="#first-decoder-block-ell-1" title="Link to this heading">#</a></h4>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(1)}_1 &amp;= \operatorname{LayerNorm}\left(\tilde{\mathbf{X}}\right) &amp; \text{(Initial normalization of inputs)} \\
\mathbf{Z}^{(1)}_2 &amp;= \operatorname{MaskedMultiHead}\left(\mathbf{Z}^{(1)}_1, \mathbf{Z}^{(1)}_1, \mathbf{Z}^{(1)}_1\right) &amp; \text{(Self-attention mechanism)} \\
\mathbf{Z}^{(1)}_3 &amp;= \tilde{\mathbf{X}} + \mathbf{Z}^{(1)}_2 &amp; \text{(Addition of the first residual connection)} \\
\mathbf{Z}^{(1)}_4 &amp;= \operatorname{LayerNorm}\left(\mathbf{Z}^{(1)}_3\right) &amp; \text{(Normalization before FFN)}\\
\mathbf{Z}^{(1)}_5 &amp;= \operatorname{FFN}\left(\mathbf{Z}^{(1)}_4\right) &amp; \text{(Feed-forward network)}\\
\mathbf{Z}^{(1)}_{\text{out}} &amp;= \mathbf{Z}^{(1)}_3 + \mathbf{Z}^{(1)}_5 &amp; \text{(Second residual connection)}
\end{aligned}
\end{split}\]</div>
</section>
<section id="subsequent-decoder-blocks-ell-1">
<h4><a class="toc-backref" href="#id48" role="doc-backlink">Subsequent Decoder Blocks (<span class="math notranslate nohighlight">\(\ell &gt; 1\)</span>)</a><a class="headerlink" href="#subsequent-decoder-blocks-ell-1" title="Link to this heading">#</a></h4>
<p>For each subsequent decoder block <span class="math notranslate nohighlight">\(\ell\)</span>, the output of the previous block’s
final step <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell-1)}_{\text{out}}\)</span> serves as the input for the
current block’s operations.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(\ell)}_1 &amp;= \operatorname{LayerNorm}\left(\mathbf{Z}^{(\ell-1)}_{\text{out}}\right) &amp; \text{(Normalization of previous block's output)} \\
\mathbf{Z}^{(\ell)}_2 &amp;= \operatorname{MaskedMultiHead}\left(\mathbf{Z}^{(\ell)}_1, \mathbf{Z}^{(\ell)}_1, \mathbf{Z}^{(\ell)}_1\right) &amp; \text{(Self-attention mechanism)} \\
\mathbf{Z}^{(\ell)}_3 &amp;= \mathbf{Z}^{(\ell-1)}_{\text{out}} + \mathbf{Z}^{(\ell)}_2 &amp; \text{(First residual connection post self-attention)} \\
\mathbf{Z}^{(\ell)}_4 &amp;= \operatorname{LayerNorm}\left(\mathbf{Z}^{(\ell)}_3\right) &amp; \text{(Normalization before FFN)}\\
\mathbf{Z}^{(\ell)}_5 &amp;= \operatorname{FFN}\left(\mathbf{Z}^{(\ell)}_4\right) &amp; \text{(Feed-forward network)}\\
\mathbf{Z}^{(\ell)}_{\text{out}} &amp;= \mathbf{Z}^{(\ell)}_3 + \mathbf{Z}^{(\ell)}_5 &amp; \text{(Second residual connection post FFN)}
\end{aligned}
\end{split}\]</div>
<p>After processing the input through a total of <span class="math notranslate nohighlight">\(L\)</span> decoder blocks, the final
output is denoted as <span class="math notranslate nohighlight">\(\mathbf{Z}^{(L)}_{\text{out}}\)</span>, which is the output of the
last decoder block. The shape of <span class="math notranslate nohighlight">\(\mathbf{Z}^{(L)}_{\text{out}}\)</span> is
<span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span>, where <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> is the batch size, <span class="math notranslate nohighlight">\(T\)</span> is
the sequence length, and <span class="math notranslate nohighlight">\(D\)</span> is the hidden dimension.</p>
</section>
</section>
<section id="layer-normalization-before-projection">
<h3><a class="toc-backref" href="#id49" role="doc-backlink">Layer Normalization Before Projection</a><a class="headerlink" href="#layer-normalization-before-projection" title="Link to this heading">#</a></h3>
<div class="note admonition">
<p class="admonition-title">Step 10. Layer Normalization Before Projection</p>
<p>The final output of the decoder block <span class="math notranslate nohighlight">\(\mathbf{Z}^{(L)}_{\text{out}}\)</span> undergoes
a layer normalization step before being projected to the vocabulary space. This
step is commonly referred to as the “pre-projection layer normalization” or
“final layer normalization” in the context of the Transformer/GPT architecture.</p>
<p>The pre-projection/head layer normalization can be represented as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}_{\text{pre-head}} &amp;= \operatorname{LayerNorm}\left(\mathbf{Z}^{(L)}_{\text{out}}\right) \\
\mathcal{B} \times T \times D &amp;\leftarrow \operatorname{LayerNorm}\left(\mathcal{B} \times T \times D\right) \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(L)}_{\text{out}} \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span>
is the output of the last decoder block (<span class="math notranslate nohighlight">\(\ell = L\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{LayerNorm}(\cdot)\)</span> denotes the Layer Normalization operation,
which normalizes the activations across the feature dimension <span class="math notranslate nohighlight">\(D\)</span> for each
token independently.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Z}_{\text{pre-proj}} \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span>
is the normalized output, which is ready to be projected to the vocabulary
space.</p></li>
</ul>
<p>The usual purpose of applying layer normalization before the projection step is
to stabilize the activations and improve training stability.</p>
</div>
</section>
<section id="head">
<h3><a class="toc-backref" href="#id50" role="doc-backlink">Head</a><a class="headerlink" href="#head" title="Link to this heading">#</a></h3>
<p>The final step in the GPT architecture is to project the normalized output of
the last decoder block, <span class="math notranslate nohighlight">\(\mathbf{Z}_{\text{pre-head}}\)</span>, to the vocabulary space.
This projection is performed using a linear transformation, where the weights of
the projection layer are denoted as
<span class="math notranslate nohighlight">\(\mathbf{W}_{s} \in \mathbb{R}^{D \times V}\)</span>. The subscript <span class="math notranslate nohighlight">\(s\)</span> indicates that
this is the projection layer before the softmax operation, and <span class="math notranslate nohighlight">\(V\)</span> represents
the size of the vocabulary.</p>
<p>Mathematically, the projection operation can be expressed as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z} &amp;= \mathbf{Z}_{\text{pre-head}} \mathbf{W}_{s} \\
\mathcal{B} \times T \times V &amp;\leftarrow \mathcal{B} \times T \times D \times \operatorname{&#64;} D \times V \\
\mathcal{B} \times T \times V &amp;\leftarrow \mathcal{B} \times T \times V
\end{aligned}
\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{Z}_{\text{pre-head}} \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span>
is the normalized output from the pre-projection layer normalization step
(Step 12).</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}_{s} \in \mathbb{R}^{D \times V}\)</span> is the weight matrix of the
projection layer, which maps the hidden dimension <span class="math notranslate nohighlight">\(D\)</span> to the vocabulary size
<span class="math notranslate nohighlight">\(V\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Z} \in \mathbb{R}^{\mathcal{B} \times T \times V}\)</span> is the resulting
logits tensor, representing the unnormalized scores for each token in the
vocabulary at each position in the sequence.</p></li>
</ul>
<p>The purpose of the projection layer is to map the hidden representations from
the decoder to the vocabulary space, allowing the model to generate probability
distributions over the vocabulary for each token position. The logits tensor
<span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> can be further processed by applying a softmax function to obtain
the final probability distribution for token prediction.</p>
</section>
<section id="softmax-layer">
<h3><a class="toc-backref" href="#id51" role="doc-backlink">Softmax Layer</a><a class="headerlink" href="#softmax-layer" title="Link to this heading">#</a></h3>
<p>The softmax function is applied to the logits tensor
<span class="math notranslate nohighlight">\(\mathbf{Z} \in \mathbb{R}^{\mathcal{B} \times T \times V}\)</span> to obtain the
predicted probability distribution over the vocabulary for each token position
in the sequence. The softmax operation is performed vector-wise along the
vocabulary dimension (i.e., the last dimension) of <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> independently
for each token position and each instance in the batch.</p>
<p>The softmax operation can be defined as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{P} &amp;= \operatorname{softmax}(\mathbf{Z}) \\
\mathbf{P}_{b,t,v} &amp;= \frac{\exp(\mathbf{Z}_{b,t,v})}{\sum_{v'=1}^{V} \exp(\mathbf{Z}_{b,t,v'})} \\
\mathcal{B} \times T \times V &amp;\leftarrow \mathcal{B} \times T \times V
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{P} \in \mathbb{R}^{\mathcal{B} \times T \times V}\)</span> is the
resulting probability tensor, and <span class="math notranslate nohighlight">\(\mathbf{P}_{b,t,v}\)</span> represents the predicted
probability of token <span class="math notranslate nohighlight">\(v\)</span> at position <span class="math notranslate nohighlight">\(t\)</span> in the sequence for batch instance <span class="math notranslate nohighlight">\(b\)</span>.
The softmax function ensures that the probabilities sum to 1 along the
vocabulary dimension for each token position and each batch instance, i.e.,
<span class="math notranslate nohighlight">\(\sum_{v=1}^{V} \mathbf{P}_{b,t,v} = 1\)</span> for all <span class="math notranslate nohighlight">\(b \in {1, \ldots, \mathcal{B}}\)</span>
and <span class="math notranslate nohighlight">\(t \in {1, \ldots, T}\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
<p>The softmax operation is applied independently to each row of the last two
dimensions <span class="math notranslate nohighlight">\((T \times V)\)</span> of the logits tensor <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>, while the batch
dimension <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> remains unchanged. This means that for each batch
instance <span class="math notranslate nohighlight">\(b\)</span> and each token position <span class="math notranslate nohighlight">\(t\)</span>, the softmax function takes the
corresponding row vector <span class="math notranslate nohighlight">\(\mathbf{Z}_{b,t,:} \in \mathbb{R}^{1 \times V}\)</span> and
computes the predicted probability distribution
<span class="math notranslate nohighlight">\(\mathbf{P}_{b,t,:} \in \mathbb{R}^{1 \times V}\)</span> over the vocabulary.</p>
</section>
<section id="cross-entropy-loss-function">
<h3><a class="toc-backref" href="#id52" role="doc-backlink">Cross-Entropy Loss Function</a><a class="headerlink" href="#cross-entropy-loss-function" title="Link to this heading">#</a></h3>
<p>The cross-entropy loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is used to measure the
dissimilarity between the predicted probability distribution <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> and
the true token distribution <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>. The true token distribution is
typically represented as a one-hot encoded tensor
<span class="math notranslate nohighlight">\(\mathbf{Y} \in \{0, 1\}^{\mathcal{B} \times T \times V}\)</span> (though in practice we
just pass in a non one-hot encoded version).</p>
<p>The one-hot encoded tensor <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> has the same shape as the predicted
probability tensor <span class="math notranslate nohighlight">\(\mathbf{P}\)</span>, i.e., <span class="math notranslate nohighlight">\(\mathcal{B} \times T \times V\)</span>. For each
batch instance <span class="math notranslate nohighlight">\(b\)</span> and each token position <span class="math notranslate nohighlight">\(t\)</span>, the corresponding row vector
<span class="math notranslate nohighlight">\(\mathbf{Y}_{b,t,:} \in {0, 1}^{1 \times V}\)</span> represents the true token
distribution, where the element corresponding to the true token is set to 1, and
all other elements are set to 0.</p>
<p>The one-hot encoded tensor <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> can be defined as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{Y}_{b,t,v} = \begin{cases}
1, &amp; \text{if } v = v^*_{b,t} \\
0, &amp; \text{otherwise}
\end{cases}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(v^*_{b,t}\)</span> denotes the true token at position <span class="math notranslate nohighlight">\(t\)</span> in the sequence for
batch instance <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p>The cross-entropy loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is then defined as:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L} = -\frac{1}{\mathcal{B} \cdot T} \sum_{b=1}^{\mathcal{B}} \sum_{t=1}^{T} \sum_{v=1}^{V} \mathbf{Y}_{b,t,v} \log(\mathbf{P}_{b,t,v})
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{Y}_{b,t,v}\)</span> is the one-hot encoded true token indicator, and
<span class="math notranslate nohighlight">\(\mathbf{P}_{b,t,v}\)</span> is the predicted probability of token <span class="math notranslate nohighlight">\(v\)</span> at position <span class="math notranslate nohighlight">\(t\)</span>
in the sequence for batch instance <span class="math notranslate nohighlight">\(b\)</span>. Note carefully that the sum is over all
batch instances, all token positions, and all tokens in the vocabulary.</p>
</section>
<section id="table-of-notations">
<h3><a class="toc-backref" href="#id53" role="doc-backlink">Table of Notations</a><a class="headerlink" href="#table-of-notations" title="Link to this heading">#</a></h3>
<p>In the table below, we will not add notational burden by adding superscript
<span class="math notranslate nohighlight">\(\mathcal{B}\)</span> to indicate a certain tensor is batched. We would just assume that
all tensors are batched unless otherwise stated.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Matrix Description</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Dimensions</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Batched Input Tensor</p></td>
<td><p><span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}^{\mathcal{B}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(B \times T \times D\)</span></p></td>
<td><p>A batched tensor containing <span class="math notranslate nohighlight">\(B\)</span> input sequences, each sequence is of shape <span class="math notranslate nohighlight">\(T \times D\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p>Batched Embedding Matrix Sequence <span class="math notranslate nohighlight">\(b\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}^{(b)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times D\)</span></p></td>
<td><p>The token and positional embedding vector for the <span class="math notranslate nohighlight">\(b\)</span>-th input sequence of the batch.</p></td>
</tr>
<tr class="row-even"><td><p>Batched Embedding Vector for Token <span class="math notranslate nohighlight">\(t\)</span> in Sequence <span class="math notranslate nohighlight">\(b\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}^{(b)}_t\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1 \times D\)</span></p></td>
<td><p>The token and positional embedding vector for the token at position <span class="math notranslate nohighlight">\(t\)</span> in the <span class="math notranslate nohighlight">\(b\)</span>-th input sequence of the batch.</p></td>
</tr>
<tr class="row-odd"><td><p>One-Hot Encoded Input Matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times V\)</span></p></td>
<td><p>Each row corresponds to a one-hot encoded vector representing a token in the sequence for a batch of size <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p>Token Embedding Matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(V \times D\)</span></p></td>
<td><p>Each row is the embedding vector of the corresponding token in the vocabulary.</p></td>
</tr>
<tr class="row-odd"><td><p>Token Embedded Input Matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>Each row is the token embedding vector of the corresponding token in the input sequence for a batch of size <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p>Positional Encoding Matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(\operatorname{PE}(\cdot)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times D\)</span></p></td>
<td><p>Matrix with positional encoding vectors for each position in the sequence, computed using sinusoidal functions or learned positional embeddings.</p></td>
</tr>
<tr class="row-odd"><td><p>Output of Positional Encoding Layer</p></td>
<td><p><span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The resultant embeddings matrix after adding positional encoding to the token embedded input matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. Each row now includes token and positional information.</p></td>
</tr>
<tr class="row-even"><td><p>First Layer Normalized Input</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(1)}_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The output of the initial layer normalization applied to <span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span>, serving as the input to the first decoder block’s masked multi-head attention mechanism.</p></td>
</tr>
<tr class="row-odd"><td><p>Query, Key, Value Matrices (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Q}^{(\ell)}, \mathbf{K}^{(\ell)}, \mathbf{V}^{(\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times D//H\)</span></p></td>
<td><p>The query, key, and value matrices obtained by linearly projecting <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span> using learned weights <span class="math notranslate nohighlight">\(\mathbf{W}^{Q, (\ell)}, \mathbf{W}^{K, (\ell)}, \mathbf{W}^{V, (\ell)}\)</span>, and then splitting into <span class="math notranslate nohighlight">\(H\)</span> attention heads. <strong>Note that the last dimension must be the same for query and key for the dot product to be valid, but can differ for values.</strong></p></td>
</tr>
<tr class="row-even"><td><p>Attention Scores (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{A}^{(\ell)}_s\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p>The scaled dot-product attention scores computed between the query and key matrices, before applying the attention mask.</p></td>
</tr>
<tr class="row-odd"><td><p>Attention Mask</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{M}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times T \overset{\text{broadcast}}{\rightarrow} \mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p>A binary mask matrix used to prevent attending to future tokens. It has a lower triangular structure with <span class="math notranslate nohighlight">\(-\infty\)</span> for future positions and <span class="math notranslate nohighlight">\(0\)</span> for allowed positions. Note that <span class="math notranslate nohighlight">\(1\)</span> for allowed positions can also be used, one just need to handle it in code.</p></td>
</tr>
<tr class="row-even"><td><p>Masked Attention Scores (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{A}^{M, (\ell)}_s\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p>The attention scores after applying the attention mask <span class="math notranslate nohighlight">\(\mathbf{M}\)</span>, which sets the scores of future tokens to <span class="math notranslate nohighlight">\(-\infty\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p>Attention Weights (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{A}^{(\ell)}_w\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p>The attention weights obtained by applying the softmax function to the masked attention scores <span class="math notranslate nohighlight">\(\mathbf{A}^{M, (\ell)}_s\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p>Context Matrix (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{C}^{(\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times D//H\)</span></p></td>
<td><p>The context matrix obtained by multiplying the attention weights <span class="math notranslate nohighlight">\(\mathbf{A}^{(\ell)}_w\)</span> with the value matrix <span class="math notranslate nohighlight">\(\mathbf{V}^{(\ell)}\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p>Concatenated Context Matrix (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{C}^{(\ell)}_{\text{concat}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The concatenated context matrix obtained by concatenating the context matrices from all attention heads in block <span class="math notranslate nohighlight">\(\ell\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p>First Self-Attention Output (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The output of the masked multi-head attention mechanism in block <span class="math notranslate nohighlight">\(\ell\)</span>, obtained by linearly projecting the concatenated context matrix <span class="math notranslate nohighlight">\(\mathbf{C}^{(\ell)}_{\text{concat}}\)</span> using learned weights <span class="math notranslate nohighlight">\(\mathbf{W}^{O, (\ell)}\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p>Output After First Residual Connection (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_3\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The resultant tensor after adding the masked multi-head attention output <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span> to the layer normalized input <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span> (for <span class="math notranslate nohighlight">\(\ell=1\)</span>) or <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell-1)}_{\text{out}}\)</span> (for <span class="math notranslate nohighlight">\(\ell&gt;1\)</span>) through a residual connection.</p></td>
</tr>
<tr class="row-even"><td><p>Normalized Input to FFN (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_4\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The output of applying layer normalization to <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_3\)</span>, serving as the input to the position-wise feed-forward network (FFN) in block <span class="math notranslate nohighlight">\(\ell\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p>Intermediate FFN Output (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{FF, (\ell)}_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times d_{\text{ff}}\)</span></p></td>
<td><p>The intermediate output of the FFN in block <span class="math notranslate nohighlight">\(\ell\)</span>, obtained by applying the first linear transformation and GELU activation to <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_4\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p>Output of FFN (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_5\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The final output of the FFN in block <span class="math notranslate nohighlight">\(\ell\)</span>, obtained by applying the second linear transformation to <span class="math notranslate nohighlight">\(\mathbf{Z}^{FF, (\ell)}_1\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p>Output of Decoder Block <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_{\text{out}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The output of decoder block <span class="math notranslate nohighlight">\(\ell\)</span>, obtained by adding the FFN output <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_5\)</span> to <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_3\)</span> through a residual connection. It serves as the input to the next decoder block (<span class="math notranslate nohighlight">\(\ell+1\)</span>) or the final output of the decoder.</p></td>
</tr>
<tr class="row-even"><td><p>Pre-Projection Layer Normalized Output</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}_{\text{pre-head}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The output of applying layer normalization to the final decoder block output <span class="math notranslate nohighlight">\(\mathbf{Z}^{(L)}_{\text{out}}\)</span>, where <span class="math notranslate nohighlight">\(L\)</span> is the total number of decoder blocks.</p></td>
</tr>
<tr class="row-odd"><td><p>Logits (Vocabulary Projection)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times V\)</span></p></td>
<td><p>The logits obtained by linearly projecting <span class="math notranslate nohighlight">\(\mathbf{Z}_{\text{pre-head}}\)</span> to the vocabulary space using learned weights <span class="math notranslate nohighlight">\(\mathbf{W}_s\)</span>. It represents the unnormalized scores for each token in the vocabulary at each position in the sequence.</p></td>
</tr>
<tr class="row-even"><td><p>FFN Layer 1 Weight Matrix (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}^{FF, (\ell)}_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(D \times d_{\text{ff}}\)</span></p></td>
<td><p>The weight matrix for the first linear transformation in the FFN of block <span class="math notranslate nohighlight">\(\ell\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p>FFN Layer 1 Bias Vector (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{b}^{FF, (\ell)}_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(d_{\text{ff}}\)</span></p></td>
<td><p>The bias vector for the first linear transformation in the FFN of block <span class="math notranslate nohighlight">\(\ell\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p>FFN Layer 2 Weight Matrix (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}^{FF, (\ell)}_2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(d_{\text{ff}} \times D\)</span></p></td>
<td><p>The weight matrix for the second linear transformation in the FFN of block <span class="math notranslate nohighlight">\(\ell\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p>FFN Layer 2 Bias Vector (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{b}^{FF, (\ell)}_2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(D\)</span></p></td>
<td><p>The bias vector for the second linear transformation in the FFN of block <span class="math notranslate nohighlight">\(\ell\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p>Projection Layer Weight Matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}_s\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(D \times V\)</span></p></td>
<td><p>The weight matrix used to linearly project <span class="math notranslate nohighlight">\(\mathbf{Z}_{\text{pre-proj}}\)</span> to the vocabulary space, mapping the hidden dimension <span class="math notranslate nohighlight">\(D\)</span> to the vocabulary size <span class="math notranslate nohighlight">\(V\)</span>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="references">
<h2><a class="toc-backref" href="#id54" role="doc-backlink">References</a><a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/">The Transformer Family v2.0 - Lilian Weng, OpenAI</a></p></li>
<li><p><a class="reference external" href="https://leimao.github.io/blog/Transformer-Explained/">Transformer Explained - Lei Mao, NVIDIA</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./influential/generative_pretrained_transformer"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="01_intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Generative Pre-trained Transformers</p>
      </div>
    </a>
    <a class="right-next"
       href="03_concept.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The Concept of Generative Pre-trained Transformers (GPT)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensions-and-indexing">Dimensions and Indexing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-notations">General Notations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elementwise-and-vectorwise-operations">Elementwise and Vectorwise Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vocabulary">Vocabulary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-sequence">Input Sequence</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#batched-input-sequences">Batched Input Sequences</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-to-index-and-index-to-token-mappings">Token to Index, and Index to Token Mappings</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#string-to-index-mapping">String-to-Index Mapping</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#index-to-string-mapping">Index-to-String Mapping</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-representation-of-input-sequence-mathbf-x">One-Hot Representation of Input Sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#one-hot-encoding-process">One-Hot Encoding Process</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#batched">Batched</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#weights-and-embeddings">Weights And Embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-multiplication-primer">Matrix Multiplication Primer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathbf-x-output-of-the-embedding-layer"><span class="math notranslate nohighlight">\(\mathbf{X}\)</span>: Output of the Embedding Layer</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Definition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#lookup">Lookup</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#semantic-representation">Semantic Representation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathbf-w-e-embedding-matrix"><span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span>: Embedding Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pe-positional-encoding-layer"><span class="math notranslate nohighlight">\(PE\)</span>: Positional Encoding Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tilde-mathbf-x-output-of-the-positional-encoding-layer"><span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span>: Output of the Positional Encoding Layer</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">Layer Normalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-notations">Attention Notations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensions">Dimensions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#query-key-and-values">Query, Key and Values</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-attention-mechanism">General Attention Mechanism</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention-for-layer-ell">Multi-Head Attention for Layer <span class="math notranslate nohighlight">\(\ell\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#masked-multi-head-attention-for-decoder-layer-ell">Masked Multi-Head Attention for Decoder Layer <span class="math notranslate nohighlight">\(\ell\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#updated-matrix-description-table-with-batch-and-head-dimensions">Updated Matrix Description Table with Batch and Head Dimensions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positionwise-feed-forward-networks">Positionwise Feed-Forward Networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#independent-processing">Independent Processing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#identical-application">Identical Application</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#projection-to-a-higher-dimension-space">Projection to a Higher Dimension Space</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-error-linear-unit-gelu">Gaussian Error Linear Unit (GELU)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-training-phase">The Training Phase</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#autoregressive-self-supervised-learning-paradigm">Autoregressive Self-Supervised Learning Paradigm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#corpus-and-tokenization">Corpus and Tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-embedding-and-positional-encoding">Token Embedding and Positional Encoding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backbone-architecture">Backbone Architecture</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-process-through-l-decoder-blocks">Iterative Process Through L Decoder Blocks</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#first-decoder-block-ell-1">First Decoder Block (<span class="math notranslate nohighlight">\(\ell = 1\)</span>)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#subsequent-decoder-blocks-ell-1">Subsequent Decoder Blocks (<span class="math notranslate nohighlight">\(\ell &gt; 1\)</span>)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization-before-projection">Layer Normalization Before Projection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#head">Head</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-layer">Softmax Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-entropy-loss-function">Cross-Entropy Loss Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#table-of-notations">Table of Notations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>