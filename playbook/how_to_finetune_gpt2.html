
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>How To Fine-Tune GPT-2 To Classify Text &#8212; Omniverse</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=bb35926c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MYW5YKC2WF"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'playbook/how_to_finetune_gpt2';</script>
    <link rel="canonical" href="https://www.gaohongnan.com/playbook/how_to_finetune_gpt2.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Training Chronicles" href="../deep_learning/training_chronicles/intro.html" />
    <link rel="prev" title="Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling" href="why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Omniverse - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Omniverse - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    🌌 Omniverse: A Journey Through Knowledge
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../notations/machine_learning.html">Machine Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Influential Ideas and Papers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../influential/generative_pretrained_transformer/01_intro.html">Generative Pre-trained Transformers</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../influential/generative_pretrained_transformer/02_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/generative_pretrained_transformer/03_concept.html">The Concept of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/generative_pretrained_transformer/04_implementation.html">The Implementation of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/generative_pretrained_transformer/05_adder.html">Training a Mini-GPT to Learn Two-Digit Addition</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Playbook</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="how_to_calculate_flops_in_gpt2.html">How to Calculate the Number of FLOPs in GPT-2</a></li>
<li class="toctree-l1"><a class="reference internal" href="how_to_inspect_function_and_class_signatures.html">How to Inspect Function and Class Signatures in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="why_cosine_annealing_warmup_stabilize_training.html">Why Does Cosine Annealing With Warmup Stabilize Training?</a></li>
<li class="toctree-l1"><a class="reference internal" href="why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html">Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">How To Fine-Tune GPT-2 To Classify Text</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../deep_learning/training_chronicles/intro.html">Training Chronicles</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../deep_learning/training_chronicles/loss.html">The Loss Landscape</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../software_engineering/devops/continuous-integration/concept.html">Continuous Integration (CI) Workflow</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/devops/continuous-integration/styling.html">Styling, Formatting, and Linting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/devops/continuous-integration/testing.html">Testing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../software_engineering/design_patterns/dependency-inversion-principle.html">Dependency Inversion Principle</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../software_engineering/concurrency_parallelism_asynchronous/intro.html">Concurrency, Parallelism and Asynchronous Programming</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/concurrency_parallelism_asynchronous/generator_yield.html">A Rudimentary Introduction to Generator and Yield in Python</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../software_engineering/serving/restful_api/intro.html">RESTful API</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/serving/restful_api/application_banking.html">Application: Designing a RESTful Banking API with FastAPI and SQLAlchemy</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../computer_science/type_theory/intro.html">Type Theory, A Very Rudimentary Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/01-subtypes.html">Subtypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/02-type-safety.html">Type Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/03-subsumption.html">Subsumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/04-generics.html">Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/05-typevar-bound-constraints.html">Bound and Constraint in Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/06-invariance-covariance-contravariance.html">Invariance, Covariance and Contravariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/07-pep-3124-overloading.html">Function Overloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/08-pep-661-sentinel-values.html">Sentinel Types</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Structures and Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/complexity_analysis/intro.html">Complexity Analysis</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/complexity_analysis/master_theorem.html">Master Theorem </a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/stack/intro.html">Stack</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/stack/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/searching_algorithms/linear_search/intro.html">Linear Search</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/searching_algorithms/linear_search/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/searching_algorithms/binary_search/intro.html">Binary Search</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/searching_algorithms/binary_search/concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dsa/searching_algorithms/binary_search/problems/875-koko-eating-bananas.html">Koko Eating Bananas</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../linear_algebra/01_preliminaries/intro.html">Preliminaries</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/01_preliminaries/01-fields.html">Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/01_preliminaries/02-systems-of-linear-equations.html">Systems of Linear Equations</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../linear_algebra/02_vectors/intro.html">Vectors</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/01-vector-definition.html">Vector and Its Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/02-vector-operation.html">Vector and Its Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/03-vector-norm.html">Vector Norm and Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/04-vector-products.html">A First Look at Vector Products</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../citations.html">IEEE (Style) Citations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../api/reproducibility.html">API Reference</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/gao-hongnan/omniverse/blob/main/omniverse/playbook/how_to_finetune_gpt2.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse/issues/new?title=Issue%20on%20page%20%2Fplaybook/how_to_finetune_gpt2.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/playbook/how_to_finetune_gpt2.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>How To Fine-Tune GPT-2 To Classify Text</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dependencies">Dependencies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducibility">Reproducibility</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constructing-a-reversal-dataset">Constructing A Reversal Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-causal-mask-to-non-causal-mask">From Causal Mask To Non-Causal Mask</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modifying-gpt-2-head-for-classification">Modifying GPT-2 Head For Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#callback">Callback</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-heatmap">Attention Heatmap</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="how-to-fine-tune-gpt-2-to-classify-text">
<h1>How To Fine-Tune GPT-2 To Classify Text<a class="headerlink" href="#how-to-fine-tune-gpt-2-to-classify-text" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://twitter.com/gaohongnan"><img alt="Twitter Handle" src="https://img.shields.io/badge/Twitter-&#64;gaohongnan-blue?style=social&amp;logo=twitter" /></a>
<a class="reference external" href="https://linkedin.com/in/gao-hongnan"><img alt="LinkedIn Profile" src="https://img.shields.io/badge/&#64;gaohongnan-blue?style=social&amp;logo=linkedin" /></a>
<a class="reference external" href="https://github.com/gao-hongnan"><img alt="GitHub Profile" src="https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&amp;logo=github" /></a>
<img alt="Tag" src="https://img.shields.io/badge/Tag-Brain_Dump-red" />
<img alt="Tag" src="https://img.shields.io/badge/Level-Beginner-green" />
<a class="reference external" href="https://github.com/gao-hongnan/omniverse/tree/main/omnivault/transformer"><img alt="Code" src="https://img.shields.io/badge/View-Code-blue?style=flat-square&amp;logo=github" /></a></p>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#dependencies" id="id1">Dependencies</a></p></li>
<li><p><a class="reference internal" href="#reproducibility" id="id2">Reproducibility</a></p></li>
<li><p><a class="reference internal" href="#constructing-a-reversal-dataset" id="id3">Constructing A Reversal Dataset</a></p></li>
<li><p><a class="reference internal" href="#from-causal-mask-to-non-causal-mask" id="id4">From Causal Mask To Non-Causal Mask</a></p></li>
<li><p><a class="reference internal" href="#modifying-gpt-2-head-for-classification" id="id5">Modifying GPT-2 Head For Classification</a></p></li>
<li><p><a class="reference internal" href="#callback" id="id6">Callback</a></p></li>
<li><p><a class="reference internal" href="#training" id="id7">Training</a></p></li>
<li><p><a class="reference internal" href="#attention-heatmap" id="id8">Attention Heatmap</a></p></li>
</ul>
</nav>
<section id="dependencies">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Dependencies</a><a class="headerlink" href="#dependencies" title="Link to this heading">#</a></h2>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">hydra.utils</span> <span class="kn">import</span> <span class="n">instantiate</span>
<span class="kn">from</span> <span class="nn">omegaconf</span> <span class="kn">import</span> <span class="n">DictConfig</span><span class="p">,</span> <span class="n">ListConfig</span>
<span class="kn">from</span> <span class="nn">omegaconf</span> <span class="kn">import</span> <span class="n">OmegaConf</span> <span class="k">as</span> <span class="n">om</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">from</span> <span class="nn">omnivault.transformer.config.composer</span> <span class="kn">import</span> <span class="n">Composer</span>
<span class="kn">from</span> <span class="nn">omnivault.transformer.config.criterion</span> <span class="kn">import</span> <span class="n">CRITERION_REGISTRY</span>
<span class="kn">from</span> <span class="nn">omnivault.transformer.config.decoder</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AddNormConfig</span><span class="p">,</span>
    <span class="n">DecoderBlockConfig</span><span class="p">,</span>
    <span class="n">DecoderConfig</span><span class="p">,</span>
    <span class="n">MultiHeadedAttentionConfig</span><span class="p">,</span>
    <span class="n">PositionwiseFeedForwardConfig</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">omnivault.transformer.config.generator</span> <span class="kn">import</span> <span class="n">GeneratorConfig</span>
<span class="kn">from</span> <span class="nn">omnivault.transformer.config.optim</span> <span class="kn">import</span> <span class="n">OPTIMIZER_REGISTRY</span>
<span class="kn">from</span> <span class="nn">omnivault.transformer.config.scheduler</span> <span class="kn">import</span> <span class="n">SCHEDULER_REGISTRY</span>
<span class="kn">from</span> <span class="nn">omnivault.transformer.config.trainer</span> <span class="kn">import</span> <span class="n">TrainerConfig</span>
<span class="kn">from</span> <span class="nn">omnivault.transformer.core.state</span> <span class="kn">import</span> <span class="n">State</span>
<span class="kn">from</span> <span class="nn">omnivault.transformer.core.trainer</span> <span class="kn">import</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">TrainerEvent</span>
<span class="kn">from</span> <span class="nn">omnivault.transformer.decoder.core</span> <span class="kn">import</span> <span class="n">GPTDecoder</span><span class="p">,</span> <span class="n">GPTDecoderBlock</span>
<span class="kn">from</span> <span class="nn">omnivault.transformer.modules.attention.core</span> <span class="kn">import</span> <span class="n">ScaledDotProductAttention</span>
<span class="kn">from</span> <span class="nn">omnivault.utils.reproducibility.seed</span> <span class="kn">import</span> <span class="n">seed_all</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">15</span>
<span class="g g-Whitespace">     </span><span class="mi">12</span> <span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span> <span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="ne">---&gt; </span><span class="mi">15</span> <span class="kn">from</span> <span class="nn">omnivault.transformer.config.composer</span> <span class="kn">import</span> <span class="n">Composer</span>
<span class="g g-Whitespace">     </span><span class="mi">16</span> <span class="kn">from</span> <span class="nn">omnivault.transformer.config.criterion</span> <span class="kn">import</span> <span class="n">CRITERION_REGISTRY</span>
<span class="g g-Whitespace">     </span><span class="mi">17</span> <span class="kn">from</span> <span class="nn">omnivault.transformer.config.decoder</span> <span class="kn">import</span> <span class="p">(</span>
<span class="g g-Whitespace">     </span><span class="mi">18</span>     <span class="n">AddNormConfig</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">19</span>     <span class="n">DecoderBlockConfig</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">22</span>     <span class="n">PositionwiseFeedForwardConfig</span><span class="p">,</span>
<span class="g g-Whitespace">     </span><span class="mi">23</span> <span class="p">)</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;omnivault&#39;
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="reproducibility">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Reproducibility</a><a class="headerlink" href="#reproducibility" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">seed_all</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">2024</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">set_torch_deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024
</pre></div>
</div>
</div>
</div>
</section>
<section id="constructing-a-reversal-dataset">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Constructing A Reversal Dataset</a><a class="headerlink" href="#constructing-a-reversal-dataset" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ReverseDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span> <span class="o">=</span> <span class="n">seq_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span> <span class="o">=</span> <span class="n">num_samples</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_samples</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">inp_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">inp_data</span><span class="p">,</span> <span class="n">dims</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,))</span>
        <span class="k">return</span> <span class="n">inp_data</span><span class="p">,</span> <span class="n">labels</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="from-causal-mask-to-non-causal-mask">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">From Causal Mask To Non-Causal Mask</a><a class="headerlink" href="#from-causal-mask-to-non-causal-mask" title="Link to this heading">#</a></h2>
<p>We can see from our line 13 below that we mask the attention scores with
<span class="math notranslate nohighlight">\(-\infty\)</span> whenever the <code class="docutils literal notranslate"><span class="pre">mask</span></code> tensor has a value of <span class="math notranslate nohighlight">\(0\)</span>.</p>
<div class="highlight-md notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span>class ScaledDotProductAttention(Attention):
<span class="linenos"> 2</span>    def forward(
<span class="linenos"> 3</span>        self,
<span class="linenos"> 4</span>        query: torch.Tensor,
<span class="linenos"> 5</span>        key: torch.Tensor,
<span class="linenos"> 6</span>        value: torch.Tensor,
<span class="linenos"> 7</span>        mask: torch.BoolTensor | None = None,
<span class="linenos"> 8</span>    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
<span class="linenos"> 9</span>        # fmt: off
<span class="linenos">10</span>        d_q               = query.size(dim=-1)
<span class="linenos">11</span>
<span class="linenos">12</span>        attention_scores  = torch.matmul(query, key.transpose(dim0=-2, dim1=-1)) / torch.sqrt(torch.tensor(d_q).float())        # [B, H, T, d<span class="ge">_q] @ [B, H, d_</span>q, T] = [B, H, T, T]
<span class="hll"><span class="linenos">13</span>        attention_scores  = attention_scores.masked_fill(mask == 0, float(&quot;-inf&quot;)) if mask is not None else attention_scores    # [B, H, T, T]
</span><span class="linenos">14</span>
<span class="linenos">15</span>        attention_weights = attention_scores.softmax(dim=-1)        # [B, H, T, T]
<span class="linenos">16</span>        attention_weights = self.dropout(attention_weights)         # [B, H, T, T]
<span class="linenos">17</span>
<span class="linenos">18</span>        context_vector    = torch.matmul(attention_weights, value)  # [B, H, T, T] @ [B, H, T, d<span class="ge">_v] = [B, H, T, d_</span>v]
<span class="linenos">19</span>        # fmt: on
<span class="linenos">20</span>        return context_vector, attention_weights
</pre></div>
</div>
<p>And the mask we pass into the <code class="docutils literal notranslate"><span class="pre">ScaledDotProductAttention</span></code> class is constructed
with lower triangular mask for each sequence in the batch as indicated in line 4
below. The lower triangular mask ensures the model does not attend to future
tokens.</p>
<div class="highlight-md notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span>def construct_dummy_batch_future_masks(batch_size: int, seq_len: int) -&gt; torch.BoolTensor:
<span class="linenos"> 2</span>    &quot;&quot;&quot;Broadcast future mask from shape (L, L) to (B, L, L) then (B, 1, L, L).&quot;&quot;&quot;
<span class="linenos"> 3</span>    # Create a lower triangular mask for a single sequence
<span class="hll"><span class="linenos"> 4</span>    future_mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool), diagonal=0).to(torch.bool)
</span><span class="linenos"> 5</span>    future_mask = future_mask.contiguous()
<span class="linenos"> 6</span>    # broadcast future mask from shape (L, L) to (B, L, L)
<span class="linenos"> 7</span>    future_masks = future_mask.unsqueeze(0).expand(batch_size, -1, -1)
<span class="linenos"> 8</span>    # broadcast future mask from shape (B, L, L) to (B, 1, L, L)
<span class="linenos"> 9</span>    future_masks = future_masks.unsqueeze(1)
<span class="linenos">10</span>    return torch.BoolTensor(future_masks)
</pre></div>
</div>
<p>However, in the context of fine-tuning GPT-2 to classify text, we need to remove
the restriction of the model not attending to future tokens. This is because the
model needs to attend to all tokens in the sequence to make a classification
decision. For example, consider the sentiment of the sentence
<code class="docutils literal notranslate"><span class="pre">&quot;I</span> <span class="pre">love</span> <span class="pre">this</span> <span class="pre">movie,</span> <span class="pre">it</span> <span class="pre">is</span> <span class="pre">so</span> <span class="pre">good&quot;</span></code> to be positive, the model should
neceesarily know the feature map of the whole sequence to make a decision.
Unlike decoder only variants, only having the last token <code class="docutils literal notranslate"><span class="pre">good</span></code> with full
contextual information with <em>every other</em> token in the sequence is not
sufficient. For now, we will first replace the lower triangular mask with a full
mask of ones, essentially making the mask a non-causal mask because when we pass
in a mask of ones, the model will <em>not</em> mask anything due to the condition
masking only 0 values (<code class="docutils literal notranslate"><span class="pre">mask</span> <span class="pre">==</span> <span class="pre">0</span></code>) in line 13 above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">construct_dummy_batch_future_masks</span><span class="p">(</span><span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Broadcast mask from shape (L, L) to (B, L, L) then (B, 1, L, L).&quot;&quot;&quot;</span>
    <span class="c1"># Create a mask for a single sequence</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="c1"># broadcast mask from shape (L, L) to (B, L, L)</span>
    <span class="n">masks</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># broadcast mask from shape (B, L, L) to (B, 1, L, L)</span>
    <span class="n">masks</span> <span class="o">=</span> <span class="n">masks</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">construct_dummy_batch_target_padding_masks</span><span class="p">(</span><span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a dummy batch of target padding masks of shape (B, 1, L, L) which</span>
<span class="sd">    assumes there is no padding token involved.&quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">custom_collate_fn</span><span class="p">(</span>
    <span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="n">sources</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)</span>

    <span class="n">sources</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">sources</span><span class="p">)</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>

    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">targets</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">future_masks</span> <span class="o">=</span> <span class="n">construct_dummy_batch_future_masks</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
    <span class="n">target_padding_masks</span> <span class="o">=</span> <span class="n">construct_dummy_batch_target_padding_masks</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">sources</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">future_masks</span><span class="p">,</span> <span class="n">target_padding_masks</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">SEQ_LEN</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">NUM_CLASSES</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ReverseDataset</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">NUM_CLASSES</span><span class="p">,</span> <span class="n">seq_len</span><span class="o">=</span><span class="n">SEQ_LEN</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">50000</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">custom_collate_fn</span><span class="p">)</span>
<span class="n">val_loader</span>   <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">custom_collate_fn</span><span class="p">)</span>
<span class="n">test_loader</span>  <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">),</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">custom_collate_fn</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inp_data</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">train_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input data:&quot;</span><span class="p">,</span> <span class="n">inp_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Labels:    &quot;</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">inp_data</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">future_masks</span><span class="p">,</span> <span class="n">target_padding_masks</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Batch&quot;</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input data:&quot;</span><span class="p">,</span> <span class="n">inp_data</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Labels:    &quot;</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Future masks:&quot;</span><span class="p">,</span> <span class="n">future_masks</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Target padding masks:&quot;</span><span class="p">,</span> <span class="n">target_padding_masks</span><span class="p">)</span>
    <span class="k">break</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Input data: tensor([2, 0, 4, 0, 3, 0, 0, 1, 3, 9, 7, 4, 1, 8, 0, 8])
Labels:     tensor([8, 0, 8, 1, 4, 7, 9, 3, 1, 0, 0, 3, 0, 4, 0, 2])
Batch 0
Input data: tensor([[9, 4, 2,  ..., 4, 5, 8],
        [5, 8, 4,  ..., 3, 8, 4],
        [7, 4, 6,  ..., 8, 2, 9],
        ...,
        [0, 2, 0,  ..., 0, 2, 0],
        [0, 2, 0,  ..., 1, 2, 9],
        [3, 2, 6,  ..., 3, 9, 0]])
Labels:     tensor([[8, 5, 4,  ..., 2, 4, 9],
        [4, 8, 3,  ..., 4, 8, 5],
        [9, 2, 8,  ..., 6, 4, 7],
        ...,
        [0, 2, 0,  ..., 0, 2, 0],
        [9, 2, 1,  ..., 0, 2, 0],
        [0, 9, 3,  ..., 6, 2, 3]])
Future masks: tensor([[[[True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          ...,
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True]]],


        [[[True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          ...,
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True]]],


        [[[True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          ...,
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True]]],


        ...,


        [[[True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          ...,
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True]]],


        [[[True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          ...,
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True]]],


        [[[True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          ...,
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True]]]])
Target padding masks: tensor([[[[True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          ...,
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True]]],


        [[[True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          ...,
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True]]],


        [[[True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          ...,
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True]]],


        ...,


        [[[True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          ...,
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True]]],


        [[[True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          ...,
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True]]],


        [[[True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          ...,
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True],
          [True, True, True,  ..., True, True, True]]]])
</pre></div>
</div>
</div>
</div>
</section>
<section id="modifying-gpt-2-head-for-classification">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Modifying GPT-2 Head For Classification</a><a class="headerlink" href="#modifying-gpt-2-head-for-classification" title="Link to this heading">#</a></h2>
<p>We are clear that the pre-head layer consist of the transformer decoder stack,
and it is clear that the output of this stack is a tensor of shape
<span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span> where <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> is the batch size, <span class="math notranslate nohighlight">\(T\)</span> is
the sequence length, and <span class="math notranslate nohighlight">\(D\)</span> is the hidden dimension of the model.</p>
<p>We can think of the pre-head layer as a feature extractor, and the head layer as
a classifier.</p>
<p>From</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>  <span class="c1"># last layer</span>
</pre></div>
</div>
<p>to</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">NUM_CLASSES</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GPTDecoderReverse</span><span class="p">(</span><span class="n">GPTDecoder</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">DecoderConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="c1"># fmt: off</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span>       <span class="p">:</span> <span class="nb">int</span>           <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tok_embed</span>     <span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span>     <span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">context_length</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder_blocks</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">GPTDecoderBlock</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_decoder_blocks</span><span class="p">)])</span> <span class="c1"># PyTorch did not make ModuleList a proper container, maybe open a PR to make it inherit Generic[T]???</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span>       <span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span>    <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span>    <span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">NUM_CLASSES</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># fmt: on</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_init_weights</span><span class="p">)</span>

        <span class="c1"># apply special scaled init to the residual projections, per GPT-2 paper</span>
        <span class="k">for</span> <span class="n">parameter_name</span><span class="p">,</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">parameter_name</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;context_projection.weight&quot;</span><span class="p">):</span>
                <span class="n">mean</span> <span class="o">=</span> <span class="mf">0.0</span>
                <span class="n">std_dev</span> <span class="o">=</span> <span class="mf">0.02</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">num_decoder_blocks</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">))</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">parameter</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="n">std_dev</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="callback">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Callback</a><a class="headerlink" href="#callback" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">evaluate_on_reverse_dataset</span><span class="p">(</span><span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">num_batches_to_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">model</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="n">dataloader</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">valid_loader</span>  <span class="c1"># Assuming you&#39;ve set your test_loader to use the ReverseDataset</span>
    <span class="k">assert</span> <span class="n">dataloader</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

    <span class="n">total_correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_samples</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Evaluating on Reverse Dataset&quot;</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">batch_index</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">target_padding_masks</span><span class="p">,</span> <span class="n">future_masks</span><span class="p">)</span> <span class="ow">in</span> <span class="n">progress_bar</span><span class="p">:</span>
        <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">target_padding_masks</span> <span class="o">=</span> <span class="n">target_padding_masks</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">future_masks</span> <span class="o">=</span> <span class="n">future_masks</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target_padding_masks</span><span class="o">=</span><span class="n">target_padding_masks</span><span class="p">,</span> <span class="n">future_masks</span><span class="o">=</span><span class="n">future_masks</span><span class="p">)</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">total_correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predictions</span> <span class="o">==</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">total_samples</span> <span class="o">+=</span> <span class="n">labels</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">num_batches_to_eval</span> <span class="ow">and</span> <span class="n">batch_index</span> <span class="o">&gt;=</span> <span class="n">num_batches_to_eval</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Early stopping evaluation.&quot;</span><span class="p">)</span>
            <span class="k">break</span>

    <span class="n">overall_accuracy</span> <span class="o">=</span> <span class="n">total_correct</span> <span class="o">/</span> <span class="n">total_samples</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Overall Accuracy on Reverse Dataset: </span><span class="si">{:.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">overall_accuracy</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="training">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Training</a><a class="headerlink" href="#training" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">masked_self_attention_mha_config</span> <span class="o">=</span> <span class="n">MultiHeadedAttentionConfig</span><span class="p">(</span>
    <span class="n">attention</span><span class="o">=</span><span class="n">ScaledDotProductAttention</span><span class="p">(),</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">H</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span>
<span class="p">)</span>

<span class="n">feed_forward_config</span> <span class="o">=</span> <span class="n">PositionwiseFeedForwardConfig</span><span class="p">(</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">32</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(</span><span class="n">approximate</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">),</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">add_norm_config_1</span> <span class="o">=</span> <span class="n">AddNormConfig</span><span class="p">(</span><span class="n">feature_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
<span class="n">add_norm_config_2</span> <span class="o">=</span> <span class="n">AddNormConfig</span><span class="p">(</span><span class="n">feature_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

<span class="c1"># Create DecoderBlockConfig</span>
<span class="n">decoder_block_config</span> <span class="o">=</span> <span class="n">DecoderBlockConfig</span><span class="p">(</span>
    <span class="n">masked_self_attention_mha</span><span class="o">=</span><span class="n">masked_self_attention_mha_config</span><span class="p">,</span>
    <span class="n">feed_forward</span><span class="o">=</span><span class="n">feed_forward_config</span><span class="p">,</span>
    <span class="n">add_norm_1</span><span class="o">=</span><span class="n">add_norm_config_1</span><span class="p">,</span>
    <span class="n">add_norm_2</span><span class="o">=</span><span class="n">add_norm_config_2</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Create the overall DecoderConfig</span>
<span class="n">model_config</span> <span class="o">=</span> <span class="n">DecoderConfig</span><span class="p">(</span>
    <span class="n">d_model</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">NUM_CLASSES</span><span class="p">,</span>
    <span class="n">context_length</span><span class="o">=</span><span class="n">SEQ_LEN</span><span class="p">,</span>
    <span class="n">num_decoder_blocks</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">decoder_block</span><span class="o">=</span><span class="n">decoder_block_config</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">optimizer_config_cls</span> <span class="o">=</span> <span class="n">OPTIMIZER_REGISTRY</span><span class="p">[</span><span class="s2">&quot;torch.optim.Adam&quot;</span><span class="p">]</span>
<span class="n">optimizer_pydantic_config</span> <span class="o">=</span> <span class="n">optimizer_config_cls</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;torch.optim.Adam&quot;</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">4e-3</span><span class="p">)</span>

<span class="n">criterion_config_cls</span> <span class="o">=</span> <span class="n">CRITERION_REGISTRY</span><span class="p">[</span><span class="s2">&quot;torch.nn.CrossEntropyLoss&quot;</span><span class="p">]</span>
<span class="n">criterion_pydantic_config</span> <span class="o">=</span> <span class="n">criterion_config_cls</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;torch.nn.CrossEntropyLoss&quot;</span><span class="p">)</span>

<span class="n">scheduler_config_cls</span> <span class="o">=</span> <span class="n">SCHEDULER_REGISTRY</span><span class="p">[</span><span class="s2">&quot;torch.optim.lr_scheduler.CosineAnnealingLR&quot;</span><span class="p">]</span>
<span class="n">scheduler_pydantic_config</span> <span class="o">=</span> <span class="n">scheduler_config_cls</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;torch.optim.lr_scheduler.CosineAnnealingLR&quot;</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">trainer_config</span> <span class="o">=</span> <span class="n">TrainerConfig</span><span class="p">(</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">max_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">eval_every_n_steps</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
    <span class="n">log_every_n_steps</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
    <span class="n">use_amp</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">autocast_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;enabled&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;cache_enabled&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">},</span>
    <span class="n">scaler_config</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;enabled&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
        <span class="s2">&quot;init_scale&quot;</span><span class="p">:</span> <span class="mf">2.0</span><span class="o">**</span><span class="mi">16</span><span class="p">,</span>
        <span class="s2">&quot;growth_factor&quot;</span><span class="p">:</span> <span class="mf">2.0</span><span class="p">,</span>
        <span class="s2">&quot;backoff_factor&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="s2">&quot;growth_interval&quot;</span><span class="p">:</span> <span class="mi">2000</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">clip_grad_norm</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;max_norm&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;norm_type&quot;</span><span class="p">:</span> <span class="mf">2.0</span><span class="p">,</span> <span class="s2">&quot;error_if_nonfinite&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;foreach&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">},</span>
    <span class="n">apply_weight_decay_to_different_param_groups</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">step_scheduler_on_batch_or_epoch</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
    <span class="n">save_dir</span><span class="o">=</span><span class="s2">&quot;./data/reversal/checkpoints&quot;</span><span class="p">,</span>
    <span class="n">save_every_epoch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;valid_this_epoch_average_loss&quot;</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">generator_config</span> <span class="o">=</span> <span class="n">GeneratorConfig</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="n">SEQ_LEN</span><span class="p">,</span> <span class="n">greedy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">composer</span> <span class="o">=</span> <span class="n">Composer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model_config</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer_pydantic_config</span><span class="p">,</span>
    <span class="n">criterion</span><span class="o">=</span><span class="n">criterion_pydantic_config</span><span class="p">,</span>
    <span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler_pydantic_config</span><span class="p">,</span>
    <span class="n">trainer</span><span class="o">=</span><span class="n">trainer_config</span><span class="p">,</span>
    <span class="n">generator</span><span class="o">=</span><span class="n">generator_config</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">GPTDecoderReverse</span><span class="p">(</span><span class="n">model_config</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">composer</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer_pydantic_config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion_pydantic_config</span><span class="o">.</span><span class="n">create_instance</span><span class="p">()</span>

<span class="n">composer</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">scheduler_pydantic_config</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">scheduler_pydantic_config</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">)</span>

<span class="n">composer</span><span class="o">.</span><span class="n">pretty_print</span><span class="p">()</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">State</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">composer</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">device</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span>
    <span class="n">composer</span><span class="o">=</span><span class="n">composer</span><span class="p">,</span>
    <span class="n">logger</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>  <span class="c1"># type: ignore[arg-type]</span>
<span class="p">)</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">add_callback</span><span class="p">(</span>
    <span class="n">TrainerEvent</span><span class="o">.</span><span class="n">ON_VALID_EPOCH_END</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">evaluate_on_reverse_dataset</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">num_batches_to_eval</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_trained_state</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_loader</span><span class="o">=</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">valid_loader</span><span class="o">=</span><span class="n">val_loader</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-04-10 22:10:13,052 - root - INFO - Initial learning rate: 0.004
2024-04-10 22:10:13,052 - root - INFO - Initial learning rate: 0.004
2024-04-10 22:10:13,054 - root - INFO - Total Parameters: 10762, Trainable Parameters: 10762
2024-04-10 22:10:13,054 - root - INFO - Total Parameters: 10762, Trainable Parameters: 10762
2024-04-10 22:10:13,056 - root - INFO - ====================================================== Starting Train Epoch: 1/10 ======================================================
2024-04-10 22:10:13,056 - root - INFO - ====================================================== Starting Train Epoch: 1/10 ======================================================
2024-04-10 22:10:13,058 - root - INFO - Learning rate:                   0.00400000000000000008
2024-04-10 22:10:13,058 - root - INFO - Learning rate:                   0.00400000000000000008
2024-04-10 22:10:17,848 - root - INFO - Total Samples:                   50000                                                                                                 
2024-04-10 22:10:17,848 - root - INFO - Total Samples:                   50000
2024-04-10 22:10:17,848 - root - INFO - Total Batches:                   390
2024-04-10 22:10:17,848 - root - INFO - Total Batches:                   390
2024-04-10 22:10:17,848 - root - INFO - Average Epoch Train Loss:        0.37160
2024-04-10 22:10:17,848 - root - INFO - Average Epoch Train Loss:        0.37160
2024-04-10 22:10:17,849 - root - INFO - Average Epoch Train Perplexity:  1.45005
2024-04-10 22:10:17,849 - root - INFO - Average Epoch Train Perplexity:  1.45005
2024-04-10 22:10:17,850 - root - INFO - 

2024-04-10 22:10:17,850 - root - INFO - 

2024-04-10 22:10:17,850 - root - INFO - ====================================================== Starting Valid Epoch: 1/10 ======================================================
2024-04-10 22:10:17,850 - root - INFO - ====================================================== Starting Valid Epoch: 1/10 ======================================================
2024-04-10 22:10:18,467 - root - INFO - Saved checkpoint at epoch 1 to ./data/reversal/checkpoints/2024-04-10_22-10-12/model_checkpoint_epoch_1.pt          
2024-04-10 22:10:18,467 - root - INFO - Saved checkpoint at epoch 1 to ./data/reversal/checkpoints/2024-04-10_22-10-12/model_checkpoint_epoch_1.pt
2024-04-10 22:10:18,469 - root - INFO - Total Samples:                   10000
2024-04-10 22:10:18,469 - root - INFO - Total Samples:                   10000
2024-04-10 22:10:18,469 - root - INFO - Total Batches:                   79
2024-04-10 22:10:18,469 - root - INFO - Total Batches:                   79
2024-04-10 22:10:18,470 - root - INFO - Average Epoch Valid Loss:        0.00084
2024-04-10 22:10:18,470 - root - INFO - Average Epoch Valid Loss:        0.00084
2024-04-10 22:10:18,470 - root - INFO - Average Epoch Valid Perplexity:  1.00084
2024-04-10 22:10:18,470 - root - INFO - Average Epoch Valid Perplexity:  1.00084
2024-04-10 22:10:18,470 - root - INFO - 

2024-04-10 22:10:18,470 - root - INFO - 

2024-04-10 22:10:18,697 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:10:18,697 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:10:18,698 - root - INFO - ====================================================== Starting Train Epoch: 2/10 ======================================================
2024-04-10 22:10:18,698 - root - INFO - ====================================================== Starting Train Epoch: 2/10 ======================================================
2024-04-10 22:10:18,698 - root - INFO - Learning rate:                   0.00000000000000000000
2024-04-10 22:10:18,698 - root - INFO - Learning rate:                   0.00000000000000000000
2024-04-10 22:10:23,571 - root - INFO - Total Samples:                   50000                                                                                                
2024-04-10 22:10:23,571 - root - INFO - Total Samples:                   50000
2024-04-10 22:10:23,571 - root - INFO - Total Batches:                   390
2024-04-10 22:10:23,571 - root - INFO - Total Batches:                   390
2024-04-10 22:10:23,571 - root - INFO - Average Epoch Train Loss:        0.00019
2024-04-10 22:10:23,571 - root - INFO - Average Epoch Train Loss:        0.00019
2024-04-10 22:10:23,572 - root - INFO - Average Epoch Train Perplexity:  1.00019
2024-04-10 22:10:23,572 - root - INFO - Average Epoch Train Perplexity:  1.00019
2024-04-10 22:10:23,572 - root - INFO - 

2024-04-10 22:10:23,572 - root - INFO - 

2024-04-10 22:10:23,572 - root - INFO - ====================================================== Starting Valid Epoch: 2/10 ======================================================
2024-04-10 22:10:23,572 - root - INFO - ====================================================== Starting Valid Epoch: 2/10 ======================================================
2024-04-10 22:10:24,196 - root - INFO - Saved checkpoint at epoch 2 to ./data/reversal/checkpoints/2024-04-10_22-10-12/model_checkpoint_epoch_2.pt          
2024-04-10 22:10:24,196 - root - INFO - Saved checkpoint at epoch 2 to ./data/reversal/checkpoints/2024-04-10_22-10-12/model_checkpoint_epoch_2.pt
2024-04-10 22:10:24,198 - root - INFO - Total Samples:                   10000
2024-04-10 22:10:24,198 - root - INFO - Total Samples:                   10000
2024-04-10 22:10:24,198 - root - INFO - Total Batches:                   79
2024-04-10 22:10:24,198 - root - INFO - Total Batches:                   79
2024-04-10 22:10:24,198 - root - INFO - Average Epoch Valid Loss:        0.00001
2024-04-10 22:10:24,198 - root - INFO - Average Epoch Valid Loss:        0.00001
2024-04-10 22:10:24,199 - root - INFO - Average Epoch Valid Perplexity:  1.00001
2024-04-10 22:10:24,199 - root - INFO - Average Epoch Valid Perplexity:  1.00001
2024-04-10 22:10:24,199 - root - INFO - 

2024-04-10 22:10:24,199 - root - INFO - 

2024-04-10 22:10:24,425 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:10:24,425 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:10:24,426 - root - INFO - ====================================================== Starting Train Epoch: 3/10 ======================================================
2024-04-10 22:10:24,426 - root - INFO - ====================================================== Starting Train Epoch: 3/10 ======================================================
2024-04-10 22:10:24,426 - root - INFO - Learning rate:                   0.00399999999999974681
2024-04-10 22:10:24,426 - root - INFO - Learning rate:                   0.00399999999999974681
2024-04-10 22:10:29,298 - root - INFO - Total Samples:                   50000                                                                                                
2024-04-10 22:10:29,298 - root - INFO - Total Samples:                   50000
2024-04-10 22:10:29,298 - root - INFO - Total Batches:                   390
2024-04-10 22:10:29,298 - root - INFO - Total Batches:                   390
2024-04-10 22:10:29,298 - root - INFO - Average Epoch Train Loss:        0.00000
2024-04-10 22:10:29,298 - root - INFO - Average Epoch Train Loss:        0.00000
2024-04-10 22:10:29,299 - root - INFO - Average Epoch Train Perplexity:  1.00000
2024-04-10 22:10:29,299 - root - INFO - Average Epoch Train Perplexity:  1.00000
2024-04-10 22:10:29,299 - root - INFO - 

2024-04-10 22:10:29,299 - root - INFO - 

2024-04-10 22:10:29,301 - root - INFO - ====================================================== Starting Valid Epoch: 3/10 ======================================================
2024-04-10 22:10:29,301 - root - INFO - ====================================================== Starting Valid Epoch: 3/10 ======================================================
2024-04-10 22:10:29,930 - root - INFO - Saved checkpoint at epoch 3 to ./data/reversal/checkpoints/2024-04-10_22-10-12/model_checkpoint_epoch_3.pt          
2024-04-10 22:10:29,930 - root - INFO - Saved checkpoint at epoch 3 to ./data/reversal/checkpoints/2024-04-10_22-10-12/model_checkpoint_epoch_3.pt
2024-04-10 22:10:29,931 - root - INFO - Total Samples:                   10000
2024-04-10 22:10:29,931 - root - INFO - Total Samples:                   10000
2024-04-10 22:10:29,931 - root - INFO - Total Batches:                   79
2024-04-10 22:10:29,931 - root - INFO - Total Batches:                   79
2024-04-10 22:10:29,932 - root - INFO - Average Epoch Valid Loss:        0.00000
2024-04-10 22:10:29,932 - root - INFO - Average Epoch Valid Loss:        0.00000
2024-04-10 22:10:29,932 - root - INFO - Average Epoch Valid Perplexity:  1.00000
2024-04-10 22:10:29,932 - root - INFO - Average Epoch Valid Perplexity:  1.00000
2024-04-10 22:10:29,933 - root - INFO - 

2024-04-10 22:10:29,933 - root - INFO - 

2024-04-10 22:10:30,153 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:10:30,153 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:10:30,154 - root - INFO - ====================================================== Starting Train Epoch: 4/10 ======================================================
2024-04-10 22:10:30,154 - root - INFO - ====================================================== Starting Train Epoch: 4/10 ======================================================
2024-04-10 22:10:30,155 - root - INFO - Learning rate:                   0.00000000000000000000
2024-04-10 22:10:30,155 - root - INFO - Learning rate:                   0.00000000000000000000
2024-04-10 22:10:34,799 - root - INFO - Total Samples:                   50000                                                                                                
2024-04-10 22:10:34,799 - root - INFO - Total Samples:                   50000
2024-04-10 22:10:34,800 - root - INFO - Total Batches:                   390
2024-04-10 22:10:34,800 - root - INFO - Total Batches:                   390
2024-04-10 22:10:34,800 - root - INFO - Average Epoch Train Loss:        0.00000
2024-04-10 22:10:34,800 - root - INFO - Average Epoch Train Loss:        0.00000
2024-04-10 22:10:34,801 - root - INFO - Average Epoch Train Perplexity:  1.00000
2024-04-10 22:10:34,801 - root - INFO - Average Epoch Train Perplexity:  1.00000
2024-04-10 22:10:34,801 - root - INFO - 

2024-04-10 22:10:34,801 - root - INFO - 

2024-04-10 22:10:34,801 - root - INFO - ====================================================== Starting Valid Epoch: 4/10 ======================================================
2024-04-10 22:10:34,801 - root - INFO - ====================================================== Starting Valid Epoch: 4/10 ======================================================
2024-04-10 22:10:35,392 - root - INFO - Saved checkpoint at epoch 4 to ./data/reversal/checkpoints/2024-04-10_22-10-12/model_checkpoint_epoch_4.pt          
2024-04-10 22:10:35,392 - root - INFO - Saved checkpoint at epoch 4 to ./data/reversal/checkpoints/2024-04-10_22-10-12/model_checkpoint_epoch_4.pt
2024-04-10 22:10:35,393 - root - INFO - Total Samples:                   10000
2024-04-10 22:10:35,393 - root - INFO - Total Samples:                   10000
2024-04-10 22:10:35,393 - root - INFO - Total Batches:                   79
2024-04-10 22:10:35,393 - root - INFO - Total Batches:                   79
2024-04-10 22:10:35,394 - root - INFO - Average Epoch Valid Loss:        0.00000
2024-04-10 22:10:35,394 - root - INFO - Average Epoch Valid Loss:        0.00000
2024-04-10 22:10:35,394 - root - INFO - Average Epoch Valid Perplexity:  1.00000
2024-04-10 22:10:35,394 - root - INFO - Average Epoch Valid Perplexity:  1.00000
2024-04-10 22:10:35,394 - root - INFO - 

2024-04-10 22:10:35,394 - root - INFO - 

2024-04-10 22:10:35,614 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:10:35,614 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:10:35,616 - root - INFO - ====================================================== Starting Train Epoch: 5/10 ======================================================
2024-04-10 22:10:35,616 - root - INFO - ====================================================== Starting Train Epoch: 5/10 ======================================================
2024-04-10 22:10:35,616 - root - INFO - Learning rate:                   0.00399999999999945538
2024-04-10 22:10:35,616 - root - INFO - Learning rate:                   0.00399999999999945538
2024-04-10 22:10:40,582 - root - INFO - Total Samples:                   50000                                                                                                
2024-04-10 22:10:40,582 - root - INFO - Total Samples:                   50000
2024-04-10 22:10:40,582 - root - INFO - Total Batches:                   390
2024-04-10 22:10:40,582 - root - INFO - Total Batches:                   390
2024-04-10 22:10:40,583 - root - INFO - Average Epoch Train Loss:        0.00000
2024-04-10 22:10:40,583 - root - INFO - Average Epoch Train Loss:        0.00000
2024-04-10 22:10:40,583 - root - INFO - Average Epoch Train Perplexity:  1.00000
2024-04-10 22:10:40,583 - root - INFO - Average Epoch Train Perplexity:  1.00000
2024-04-10 22:10:40,583 - root - INFO - 

2024-04-10 22:10:40,583 - root - INFO - 

2024-04-10 22:10:40,584 - root - INFO - ====================================================== Starting Valid Epoch: 5/10 ======================================================
2024-04-10 22:10:40,584 - root - INFO - ====================================================== Starting Valid Epoch: 5/10 ======================================================
2024-04-10 22:10:41,212 - root - INFO - Total Samples:                   10000                                                                              
2024-04-10 22:10:41,212 - root - INFO - Total Samples:                   10000
2024-04-10 22:10:41,216 - root - INFO - Total Batches:                   79
2024-04-10 22:10:41,216 - root - INFO - Total Batches:                   79
2024-04-10 22:10:41,221 - root - INFO - Average Epoch Valid Loss:        0.00000
2024-04-10 22:10:41,221 - root - INFO - Average Epoch Valid Loss:        0.00000
2024-04-10 22:10:41,223 - root - INFO - Average Epoch Valid Perplexity:  1.00000
2024-04-10 22:10:41,223 - root - INFO - Average Epoch Valid Perplexity:  1.00000
2024-04-10 22:10:41,223 - root - INFO - 

2024-04-10 22:10:41,223 - root - INFO - 

2024-04-10 22:10:41,452 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:10:41,452 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:10:41,453 - root - INFO - ====================================================== Starting Train Epoch: 6/10 ======================================================
2024-04-10 22:10:41,453 - root - INFO - ====================================================== Starting Train Epoch: 6/10 ======================================================
2024-04-10 22:10:41,453 - root - INFO - Learning rate:                   0.00000000000000000000
2024-04-10 22:10:41,453 - root - INFO - Learning rate:                   0.00000000000000000000
2024-04-10 22:10:46,230 - root - INFO - Total Samples:                   50000                                                                                                
2024-04-10 22:10:46,230 - root - INFO - Total Samples:                   50000
2024-04-10 22:10:46,231 - root - INFO - Total Batches:                   390
2024-04-10 22:10:46,231 - root - INFO - Total Batches:                   390
2024-04-10 22:10:46,231 - root - INFO - Average Epoch Train Loss:        0.00000
2024-04-10 22:10:46,231 - root - INFO - Average Epoch Train Loss:        0.00000
2024-04-10 22:10:46,231 - root - INFO - Average Epoch Train Perplexity:  1.00000
2024-04-10 22:10:46,231 - root - INFO - Average Epoch Train Perplexity:  1.00000
2024-04-10 22:10:46,232 - root - INFO - 

2024-04-10 22:10:46,232 - root - INFO - 

2024-04-10 22:10:46,232 - root - INFO - ====================================================== Starting Valid Epoch: 6/10 ======================================================
2024-04-10 22:10:46,232 - root - INFO - ====================================================== Starting Valid Epoch: 6/10 ======================================================
2024-04-10 22:10:46,838 - root - INFO - Total Samples:                   10000                                                                              
2024-04-10 22:10:46,838 - root - INFO - Total Samples:                   10000
2024-04-10 22:10:46,839 - root - INFO - Total Batches:                   79
2024-04-10 22:10:46,839 - root - INFO - Total Batches:                   79
2024-04-10 22:10:46,839 - root - INFO - Average Epoch Valid Loss:        0.00000
2024-04-10 22:10:46,839 - root - INFO - Average Epoch Valid Loss:        0.00000
2024-04-10 22:10:46,840 - root - INFO - Average Epoch Valid Perplexity:  1.00000
2024-04-10 22:10:46,840 - root - INFO - Average Epoch Valid Perplexity:  1.00000
2024-04-10 22:10:46,840 - root - INFO - 

2024-04-10 22:10:46,840 - root - INFO - 

2024-04-10 22:10:47,060 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:10:47,060 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:10:47,061 - root - INFO - ====================================================== Starting Train Epoch: 7/10 ======================================================
2024-04-10 22:10:47,061 - root - INFO - ====================================================== Starting Train Epoch: 7/10 ======================================================
2024-04-10 22:10:47,061 - root - INFO - Learning rate:                   0.00400000000000058989
2024-04-10 22:10:47,061 - root - INFO - Learning rate:                   0.00400000000000058989
2024-04-10 22:10:51,763 - root - INFO - Total Samples:                   50000                                                                                                 
2024-04-10 22:10:51,763 - root - INFO - Total Samples:                   50000
2024-04-10 22:10:51,763 - root - INFO - Total Batches:                   390
2024-04-10 22:10:51,763 - root - INFO - Total Batches:                   390
2024-04-10 22:10:51,764 - root - INFO - Average Epoch Train Loss:        0.00069
2024-04-10 22:10:51,764 - root - INFO - Average Epoch Train Loss:        0.00069
2024-04-10 22:10:51,764 - root - INFO - Average Epoch Train Perplexity:  1.00069
2024-04-10 22:10:51,764 - root - INFO - Average Epoch Train Perplexity:  1.00069
2024-04-10 22:10:51,764 - root - INFO - 

2024-04-10 22:10:51,764 - root - INFO - 

2024-04-10 22:10:51,765 - root - INFO - ====================================================== Starting Valid Epoch: 7/10 ======================================================
2024-04-10 22:10:51,765 - root - INFO - ====================================================== Starting Valid Epoch: 7/10 ======================================================
2024-04-10 22:10:52,401 - root - INFO - Total Samples:                   10000                                                                              
2024-04-10 22:10:52,401 - root - INFO - Total Samples:                   10000
2024-04-10 22:10:52,401 - root - INFO - Total Batches:                   79
2024-04-10 22:10:52,401 - root - INFO - Total Batches:                   79
2024-04-10 22:10:52,402 - root - INFO - Average Epoch Valid Loss:        0.00000
2024-04-10 22:10:52,402 - root - INFO - Average Epoch Valid Loss:        0.00000
2024-04-10 22:10:52,402 - root - INFO - Average Epoch Valid Perplexity:  1.00000
2024-04-10 22:10:52,402 - root - INFO - Average Epoch Valid Perplexity:  1.00000
2024-04-10 22:10:52,402 - root - INFO - 

2024-04-10 22:10:52,402 - root - INFO - 

2024-04-10 22:10:52,652 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:10:52,652 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:10:52,654 - root - INFO - ====================================================== Starting Train Epoch: 8/10 ======================================================
2024-04-10 22:10:52,654 - root - INFO - ====================================================== Starting Train Epoch: 8/10 ======================================================
2024-04-10 22:10:52,655 - root - INFO - Learning rate:                   0.00000000000000000000
2024-04-10 22:10:52,655 - root - INFO - Learning rate:                   0.00000000000000000000
2024-04-10 22:10:57,703 - root - INFO - Total Samples:                   50000                                                                                                
2024-04-10 22:10:57,703 - root - INFO - Total Samples:                   50000
2024-04-10 22:10:57,704 - root - INFO - Total Batches:                   390
2024-04-10 22:10:57,704 - root - INFO - Total Batches:                   390
2024-04-10 22:10:57,704 - root - INFO - Average Epoch Train Loss:        0.00000
2024-04-10 22:10:57,704 - root - INFO - Average Epoch Train Loss:        0.00000
2024-04-10 22:10:57,704 - root - INFO - Average Epoch Train Perplexity:  1.00000
2024-04-10 22:10:57,704 - root - INFO - Average Epoch Train Perplexity:  1.00000
2024-04-10 22:10:57,705 - root - INFO - 

2024-04-10 22:10:57,705 - root - INFO - 

2024-04-10 22:10:57,705 - root - INFO - ====================================================== Starting Valid Epoch: 8/10 ======================================================
2024-04-10 22:10:57,705 - root - INFO - ====================================================== Starting Valid Epoch: 8/10 ======================================================
2024-04-10 22:10:58,359 - root - INFO - Total Samples:                   10000                                                                              
2024-04-10 22:10:58,359 - root - INFO - Total Samples:                   10000
2024-04-10 22:10:58,359 - root - INFO - Total Batches:                   79
2024-04-10 22:10:58,359 - root - INFO - Total Batches:                   79
2024-04-10 22:10:58,360 - root - INFO - Average Epoch Valid Loss:        0.00000
2024-04-10 22:10:58,360 - root - INFO - Average Epoch Valid Loss:        0.00000
2024-04-10 22:10:58,360 - root - INFO - Average Epoch Valid Perplexity:  1.00000
2024-04-10 22:10:58,360 - root - INFO - Average Epoch Valid Perplexity:  1.00000
2024-04-10 22:10:58,360 - root - INFO - 

2024-04-10 22:10:58,360 - root - INFO - 

2024-04-10 22:10:58,632 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:10:58,632 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:10:58,633 - root - INFO - ====================================================== Starting Train Epoch: 9/10 ======================================================
2024-04-10 22:10:58,633 - root - INFO - ====================================================== Starting Train Epoch: 9/10 ======================================================
2024-04-10 22:10:58,634 - root - INFO - Learning rate:                   0.00400000000000172440
2024-04-10 22:10:58,634 - root - INFO - Learning rate:                   0.00400000000000172440
2024-04-10 22:11:03,454 - root - INFO - Total Samples:                   50000                                                                                                
2024-04-10 22:11:03,454 - root - INFO - Total Samples:                   50000
2024-04-10 22:11:03,455 - root - INFO - Total Batches:                   390
2024-04-10 22:11:03,455 - root - INFO - Total Batches:                   390
2024-04-10 22:11:03,455 - root - INFO - Average Epoch Train Loss:        0.00000
2024-04-10 22:11:03,455 - root - INFO - Average Epoch Train Loss:        0.00000
2024-04-10 22:11:03,456 - root - INFO - Average Epoch Train Perplexity:  1.00000
2024-04-10 22:11:03,456 - root - INFO - Average Epoch Train Perplexity:  1.00000
2024-04-10 22:11:03,456 - root - INFO - 

2024-04-10 22:11:03,456 - root - INFO - 

2024-04-10 22:11:03,456 - root - INFO - ====================================================== Starting Valid Epoch: 9/10 ======================================================
2024-04-10 22:11:03,456 - root - INFO - ====================================================== Starting Valid Epoch: 9/10 ======================================================
2024-04-10 22:11:04,113 - root - INFO - Total Samples:                   10000                                                                              
2024-04-10 22:11:04,113 - root - INFO - Total Samples:                   10000
2024-04-10 22:11:04,113 - root - INFO - Total Batches:                   79
2024-04-10 22:11:04,113 - root - INFO - Total Batches:                   79
2024-04-10 22:11:04,114 - root - INFO - Average Epoch Valid Loss:        0.00000
2024-04-10 22:11:04,114 - root - INFO - Average Epoch Valid Loss:        0.00000
2024-04-10 22:11:04,114 - root - INFO - Average Epoch Valid Perplexity:  1.00000
2024-04-10 22:11:04,114 - root - INFO - Average Epoch Valid Perplexity:  1.00000
2024-04-10 22:11:04,114 - root - INFO - 

2024-04-10 22:11:04,114 - root - INFO - 

2024-04-10 22:11:04,349 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:11:04,349 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:11:04,350 - root - INFO - ====================================================== Starting Train Epoch: 10/10 ======================================================
2024-04-10 22:11:04,350 - root - INFO - ====================================================== Starting Train Epoch: 10/10 ======================================================
2024-04-10 22:11:04,350 - root - INFO - Learning rate:                   0.00000000000000000000
2024-04-10 22:11:04,350 - root - INFO - Learning rate:                   0.00000000000000000000
2024-04-10 22:11:09,547 - root - INFO - Total Samples:                   50000                                                                                                 
2024-04-10 22:11:09,547 - root - INFO - Total Samples:                   50000
2024-04-10 22:11:09,547 - root - INFO - Total Batches:                   390
2024-04-10 22:11:09,547 - root - INFO - Total Batches:                   390
2024-04-10 22:11:09,548 - root - INFO - Average Epoch Train Loss:        0.00000
2024-04-10 22:11:09,548 - root - INFO - Average Epoch Train Loss:        0.00000
2024-04-10 22:11:09,548 - root - INFO - Average Epoch Train Perplexity:  1.00000
2024-04-10 22:11:09,548 - root - INFO - Average Epoch Train Perplexity:  1.00000
2024-04-10 22:11:09,549 - root - INFO - 

2024-04-10 22:11:09,549 - root - INFO - 

2024-04-10 22:11:09,549 - root - INFO - ====================================================== Starting Valid Epoch: 10/10 ======================================================
2024-04-10 22:11:09,549 - root - INFO - ====================================================== Starting Valid Epoch: 10/10 ======================================================
2024-04-10 22:11:10,214 - root - INFO - Total Samples:                   10000                                                                               
2024-04-10 22:11:10,214 - root - INFO - Total Samples:                   10000
2024-04-10 22:11:10,214 - root - INFO - Total Batches:                   79
2024-04-10 22:11:10,214 - root - INFO - Total Batches:                   79
2024-04-10 22:11:10,214 - root - INFO - Average Epoch Valid Loss:        0.00000
2024-04-10 22:11:10,214 - root - INFO - Average Epoch Valid Loss:        0.00000
2024-04-10 22:11:10,215 - root - INFO - Average Epoch Valid Perplexity:  1.00000
2024-04-10 22:11:10,215 - root - INFO - Average Epoch Valid Perplexity:  1.00000
2024-04-10 22:11:10,215 - root - INFO - 

2024-04-10 22:11:10,215 - root - INFO - 

2024-04-10 22:11:10,472 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
2024-04-10 22:11:10,472 - root - INFO - Overall Accuracy on Reverse Dataset: 1.0000
</pre></div>
</div>
</div>
</div>
<p>on importance of LR. lol.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">last_decoder_block</span> <span class="o">=</span> <span class="n">_trained_state</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">decoder_blocks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">last_decoder_block</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GPTDecoderBlock(
  (masked_self_attention_mha): MultiHeadedAttention(
    (W_Q): Linear(in_features=32, out_features=32, bias=False)
    (W_K): Linear(in_features=32, out_features=32, bias=False)
    (W_V): Linear(in_features=32, out_features=32, bias=False)
    (W_O): Linear(in_features=32, out_features=32, bias=False)
    (attention): ScaledDotProductAttention(
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (feed_forward): PositionwiseFeedForward(
    (ffn): ModuleDict(
      (context_fc): Linear(in_features=32, out_features=64, bias=True)
      (activation): GELU(approximate=&#39;tanh&#39;)
      (context_projection): Linear(in_features=64, out_features=32, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
  )
  (add_norm_1): AddNorm(
    (dropout): Dropout(p=0.0, inplace=False)
    (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
  )
  (add_norm_2): AddNorm(
    (dropout): Dropout(p=0.0, inplace=False)
    (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
  )
)
</pre></div>
</div>
</div>
</div>
</section>
<section id="attention-heatmap">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">Attention Heatmap</a><a class="headerlink" href="#attention-heatmap" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">masked_self_attention_mha</span> <span class="o">=</span> <span class="n">last_decoder_block</span><span class="o">.</span><span class="n">masked_self_attention_mha</span>
<span class="n">context_vector</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">masked_self_attention_mha</span><span class="o">.</span><span class="n">context_vector</span><span class="p">,</span> <span class="n">masked_self_attention_mha</span><span class="o">.</span><span class="n">attention_weights</span>

<span class="c1"># Number of heads</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="n">attention_weights</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Context vector shape:&quot;</span><span class="p">,</span> <span class="n">context_vector</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Attention weights shape:&quot;</span><span class="p">,</span> <span class="n">attention_weights</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of heads:&quot;</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Context vector shape: torch.Size([16, 1, 16, 32])
Attention weights shape: torch.Size([16, 1, 16, 16])
Number of heads: 1
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Input data: tensor([2, 0, 4, 0, 3, 0, 0, 1, 3, 9, 7, 4, 1, 8, 0, 8])</span>
<span class="c1"># Labels:     tensor([8, 0, 8, 1, 4, 7, 9, 3, 1, 0, 0, 3, 0, 4, 0, 2])</span>

<span class="nb">input</span> <span class="o">=</span> <span class="s2">&quot;2040300139741808&quot;</span>
<span class="n">label</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

    <span class="n">attention_matrix</span> <span class="o">=</span> <span class="n">attention_weights</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">head</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attention Weights Heatmap for &#39;</span><span class="si">{</span><span class="nb">input</span><span class="si">}</span><span class="s2">&#39; - Head </span><span class="si">{</span><span class="n">head</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Keys&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Queries&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1edf4c568b5ee3cf4108419d625be4947726a0489fac19dfa48043d4c0a744cf.png" src="../_images/1edf4c568b5ee3cf4108419d625be4947726a0489fac19dfa48043d4c0a744cf.png" />
</div>
</div>
<p>The heatmap diagonal is correct because in the above image, we can
treat it as a <span class="math notranslate nohighlight">\(T \times T\)</span>  matrix, where the first row is the first token from query,
interacting with every other token as keys. So if first token is 2 and last token is 8,
then it should be matching each other because of the “reversal”.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./playbook"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling</p>
      </div>
    </a>
    <a class="right-next"
       href="../deep_learning/training_chronicles/intro.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Training Chronicles</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dependencies">Dependencies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reproducibility">Reproducibility</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#constructing-a-reversal-dataset">Constructing A Reversal Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#from-causal-mask-to-non-causal-mask">From Causal Mask To Non-Causal Mask</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modifying-gpt-2-head-for-classification">Modifying GPT-2 Head For Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#callback">Callback</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention-heatmap">Attention Heatmap</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>