
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>How to Calculate the Number of FLOPs in GPT-2 &#8212; Omniverse</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=bb35926c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MYW5YKC2WF"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="application/vnd.jupyter.widget-state+json">{"state": {"b29d80fb065b417fa0d3d77ae63f1c7c": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a638f265d9024ad19e2cce45981abb6e": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "57c25066d46f420785c49af0f839bf5b": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_b29d80fb065b417fa0d3d77ae63f1c7c", "max": 665.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_a638f265d9024ad19e2cce45981abb6e", "value": 665.0}}, "80b5023c1556440ba9edcb4dda191694": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1c2dcd0946d04ca7bc39a71e45e14c4c": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "e13d02a4851a4d4aa51e155845dc831f": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_80b5023c1556440ba9edcb4dda191694", "placeholder": "\u200b", "style": "IPY_MODEL_1c2dcd0946d04ca7bc39a71e45e14c4c", "value": "config.json:\u2007100%"}}, "5769439cb6fe4fcab374b5257d2d0faf": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "03ab1f54420545d7a29693fdafaf8843": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "ae230490027449bf8af7a81a34aa3986": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_5769439cb6fe4fcab374b5257d2d0faf", "placeholder": "\u200b", "style": "IPY_MODEL_03ab1f54420545d7a29693fdafaf8843", "value": "\u2007665/665\u2007[00:00&lt;00:00,\u200769.3kB/s]"}}, "c60de75ab186414c81b79799fc008e70": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "a1a58858752649c79d2b01393bf517c9": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_e13d02a4851a4d4aa51e155845dc831f", "IPY_MODEL_57c25066d46f420785c49af0f839bf5b", "IPY_MODEL_ae230490027449bf8af7a81a34aa3986"], "layout": "IPY_MODEL_c60de75ab186414c81b79799fc008e70"}}, "033440ce90a6400fa7f43a4de5261dfc": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "15b1f84f4e094d079b5ae741dd443516": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "6c62a12fbe954ff49a94902e350b0ad0": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_033440ce90a6400fa7f43a4de5261dfc", "max": 548105171.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_15b1f84f4e094d079b5ae741dd443516", "value": 548105171.0}}, "01015b06a8594908b4903d7575c7c30f": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "4ab875540d554ff280e85893ad3b72ee": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "771c9198517540ccacbe3b400942397f": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_01015b06a8594908b4903d7575c7c30f", "placeholder": "\u200b", "style": "IPY_MODEL_4ab875540d554ff280e85893ad3b72ee", "value": "model.safetensors:\u2007100%"}}, "d3a71d4c100d49848773aa3f078d37fb": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "10e41c5e72a84726954d40c40859e20e": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "ad94b3ddfd5548d390afd3d225af6ce3": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_d3a71d4c100d49848773aa3f078d37fb", "placeholder": "\u200b", "style": "IPY_MODEL_10e41c5e72a84726954d40c40859e20e", "value": "\u2007548M/548M\u2007[00:02&lt;00:00,\u2007210MB/s]"}}, "b9bfc8d16f6843d3a97a1100dcd36c6e": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d027c0cce4ce4867a4626c770b46735d": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_771c9198517540ccacbe3b400942397f", "IPY_MODEL_6c62a12fbe954ff49a94902e350b0ad0", "IPY_MODEL_ad94b3ddfd5548d390afd3d225af6ce3"], "layout": "IPY_MODEL_b9bfc8d16f6843d3a97a1100dcd36c6e"}}, "735635e23c37439093a70b5bd0ce001b": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "48a3a6ba65be4e4abb34cce0f9835f8d": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "d94b7054624849898ec2cc34a8a07c12": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_735635e23c37439093a70b5bd0ce001b", "max": 124.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_48a3a6ba65be4e4abb34cce0f9835f8d", "value": 124.0}}, "b88b2a2ac09b4c5fa1fc1206cd9f316d": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7c94e03fe18c4f9e8a761176955107dc": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "329e72832e8048d7b7cd8bfb75b83105": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_b88b2a2ac09b4c5fa1fc1206cd9f316d", "placeholder": "\u200b", "style": "IPY_MODEL_7c94e03fe18c4f9e8a761176955107dc", "value": "generation_config.json:\u2007100%"}}, "d8b4c762e7f545d081685434496ddcdd": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "90730a484793433dbfc771d7b320e9dc": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "6ec0cef80e4a4361952dbaa15bf92402": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_d8b4c762e7f545d081685434496ddcdd", "placeholder": "\u200b", "style": "IPY_MODEL_90730a484793433dbfc771d7b320e9dc", "value": "\u2007124/124\u2007[00:00&lt;00:00,\u200712.1kB/s]"}}, "5123128f26e04cbd9c83477db5a2ce9b": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "02294e64ffb94a1e8a616cf375b8b86e": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_329e72832e8048d7b7cd8bfb75b83105", "IPY_MODEL_d94b7054624849898ec2cc34a8a07c12", "IPY_MODEL_6ec0cef80e4a4361952dbaa15bf92402"], "layout": "IPY_MODEL_5123128f26e04cbd9c83477db5a2ce9b"}}}, "version_major": 2, "version_minor": 0}</script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script crossorigin="anonymous" data-jupyter-widgets-cdn="https://cdn.jsdelivr.net/npm/" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@1.0.6/dist/embed-amd.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'playbook/how_to_calculate_flops_in_gpt2';</script>
    <link rel="canonical" href="https://www.gaohongnan.com/playbook/how_to_calculate_flops_in_gpt2.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="How to Inspect Function and Class Signatures in Python" href="how_to_inspect_function_and_class_signatures.html" />
    <link rel="prev" title="The Training Phase" href="../transformer/decoder/train_phase.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Omniverse - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Omniverse - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    🌌 Omniverse: A Journey Through Knowledge
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../notations/machine_learning.html">Machine Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative Pre-trained Transformer</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../transformer/decoder/intro.html">Generative Pre-trained Transformers</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../transformer/decoder/notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../transformer/decoder/concept.html">The Concept of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../transformer/decoder/implementation.html">The Implementation of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../transformer/decoder/adder.html">Training a Mini-GPT to Learn Two-Digit Addition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../transformer/decoder/train_phase.html">The Training Phase</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Playbook</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">How to Calculate the Number of FLOPs in GPT-2</a></li>
<li class="toctree-l1"><a class="reference internal" href="how_to_inspect_function_and_class_signatures.html">How to Inspect Function and Class Signatures in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="why_cosine_annealing_warmup_stabilize_training.html">Why Does Cosine Annealing With Warmup Stabilize Training?</a></li>
<li class="toctree-l1"><a class="reference internal" href="why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html">Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling</a></li>
<li class="toctree-l1"><a class="reference internal" href="how_to_finetune_gpt2.html">How To Fine-Tune GPT-2 To Classify Text</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../deep_learning/training_chronicles/intro.html">Training Chronicles</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../deep_learning/training_chronicles/loss.html">The Loss Landscape</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../software_engineering/devops/continuous-integration/concept.html">Continuous Integration (CI) Workflow</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/devops/continuous-integration/styling.html">Styling, Formatting, and Linting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/devops/continuous-integration/testing.html">Testing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../software_engineering/design_patterns/dependency-inversion-principle.html">Dependency Inversion Principle</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../software_engineering/concurrency_parallelism_asynchronous/intro.html">Concurrency, Parallelism and Asynchronous Programming</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/concurrency_parallelism_asynchronous/generator_yield.html">A Rudimentary Introduction to Generator and Yield in Python</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../software_engineering/serving/restful_api/intro.html">RESTful API</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/serving/restful_api/application_banking.html">Application: Designing a RESTful Banking API with FastAPI and SQLAlchemy</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../computer_science/type_theory/intro.html">Type Theory, A Very Rudimentary Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/01-subtypes.html">Subtypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/02-type-safety.html">Type Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/03-subsumption.html">Subsumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/04-generics.html">Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/05-typevar-bound-constraints.html">Bound and Constraint in Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/06-invariance-covariance-contravariance.html">Invariance, Covariance and Contravariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/07-pep-3124-overloading.html">Function Overloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/08-pep-661-sentinel-values.html">Sentinel Types</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Structures and Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/complexity_analysis/intro.html">Complexity Analysis</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/complexity_analysis/master_theorem.html">Master Theorem </a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/stack/intro.html">Stack</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/stack/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/searching_algorithms/linear_search/intro.html">Linear Search</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/searching_algorithms/linear_search/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/searching_algorithms/binary_search/intro.html">Binary Search</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/searching_algorithms/binary_search/concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dsa/searching_algorithms/binary_search/problems/875-koko-eating-bananas.html">Koko Eating Bananas</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../linear_algebra/01_preliminaries/intro.html">Preliminaries</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/01_preliminaries/01-fields.html">Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/01_preliminaries/02-systems-of-linear-equations.html">Systems of Linear Equations</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../linear_algebra/02_vectors/intro.html">Vectors</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/01-vector-definition.html">Vector and Its Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/02-vector-operation.html">Vector and Its Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/03-vector-norm.html">Vector Norm and Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/04-vector-products.html">A First Look at Vector Products</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../citations.html">IEEE (Style) Citations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../api/reproducibility.html">API Reference</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/gao-hongnan/omniverse/blob/main/omniverse/playbook/how_to_calculate_flops_in_gpt2.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse/issues/new?title=Issue%20on%20page%20%2Fplaybook/how_to_calculate_flops_in_gpt2.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/playbook/how_to_calculate_flops_in_gpt2.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>How to Calculate the Number of FLOPs in GPT-2</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#configurations-constants-and-enums">Configurations, Constants and Enums</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#total-trainable-parameters">Total Trainable Parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-checkpoint-size-and-fluff-ratio">Calculating Checkpoint Size and Fluff Ratio</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-memory-footprint-of-loading-model-and-optimizer">GPU Memory Footprint of Loading Model and Optimizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-flops-for-a-single-forward-pass">Estimating FLOPs for a Single Forward Pass</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basics-of-floating-point-numbers">Basics of Floating Point Numbers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#floating-point-operations-flops">Floating Point Operations (FLOPs)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#counting-flops-of-matrix-multiplications">Counting FLOPs of Matrix Multiplications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-flops-for-a-single-forward-pass-of-gpt-2">Estimating FLOPs for a Single Forward Pass of GPT-2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sanity-check-with-palm-paper-s-flops-calculation">Sanity Check with Palm Paper’s FLOPs Calculation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#floating-point-operations-per-second-flops">Floating Point Operations Per Second (FLOPS)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#some-practical-considerations-for-flops-in-deep-learning">Some Practical Considerations for FLOPs in Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flops-per-second-in-gpus">FLOPS Per Second in GPUs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-flops-utilization-mfu">Model FLOPs Utilization (MFU)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#theoretical-model-flops-utilization-mfu-indicates-a-rough-benchmark-of-efficiency">Theoretical Model FLOPs Utilization (MFU) Indicates a Rough Benchmark of Efficiency</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relation-of-mfu-and-tflops">Relation of MFU and TFLOPS</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#theoretical-flops-in-transformer-models">Theoretical FLOPs in Transformer Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="how-to-calculate-the-number-of-flops-in-gpt-2">
<h1>How to Calculate the Number of FLOPs in GPT-2<a class="headerlink" href="#how-to-calculate-the-number-of-flops-in-gpt-2" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://twitter.com/gaohongnan"><img alt="Twitter Handle" src="https://img.shields.io/badge/Twitter-&#64;gaohongnan-blue?style=social&amp;logo=twitter" /></a>
<a class="reference external" href="https://linkedin.com/in/gao-hongnan"><img alt="LinkedIn Profile" src="https://img.shields.io/badge/&#64;gaohongnan-blue?style=social&amp;logo=linkedin" /></a>
<a class="reference external" href="https://github.com/gao-hongnan"><img alt="GitHub Profile" src="https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&amp;logo=github" /></a>
<img alt="Tag" src="https://img.shields.io/badge/Tag-Brain_Dump-red" />
<a class="reference external" href="https://github.com/gao-hongnan/omniverse/blob/main/omnivault/utils/torch_utils/speed_monitor.py"><img alt="Code" src="https://img.shields.io/badge/View-Code-blue?style=flat-square&amp;logo=github" /></a></p>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#configurations-constants-and-enums" id="id1">Configurations, Constants and Enums</a></p></li>
<li><p><a class="reference internal" href="#total-trainable-parameters" id="id2">Total Trainable Parameters</a></p></li>
<li><p><a class="reference internal" href="#calculating-checkpoint-size-and-fluff-ratio" id="id3">Calculating Checkpoint Size and Fluff Ratio</a></p></li>
<li><p><a class="reference internal" href="#gpu-memory-footprint-of-loading-model-and-optimizer" id="id4">GPU Memory Footprint of Loading Model and Optimizer</a></p></li>
<li><p><a class="reference internal" href="#estimating-flops-for-a-single-forward-pass" id="id5">Estimating FLOPs for a Single Forward Pass</a></p>
<ul>
<li><p><a class="reference internal" href="#basics-of-floating-point-numbers" id="id6">Basics of Floating Point Numbers</a></p></li>
<li><p><a class="reference internal" href="#floating-point-operations-flops" id="id7">Floating Point Operations (FLOPs)</a></p></li>
<li><p><a class="reference internal" href="#counting-flops-of-matrix-multiplications" id="id8">Counting FLOPs of Matrix Multiplications</a></p></li>
<li><p><a class="reference internal" href="#estimating-flops-for-a-single-forward-pass-of-gpt-2" id="id9">Estimating FLOPs for a Single Forward Pass of GPT-2</a></p></li>
<li><p><a class="reference internal" href="#sanity-check-with-palm-paper-s-flops-calculation" id="id10">Sanity Check with Palm Paper’s FLOPs Calculation</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#floating-point-operations-per-second-flops" id="id11">Floating Point Operations Per Second (FLOPS)</a></p>
<ul>
<li><p><a class="reference internal" href="#some-practical-considerations-for-flops-in-deep-learning" id="id12">Some Practical Considerations for FLOPs in Deep Learning</a></p></li>
<li><p><a class="reference internal" href="#flops-per-second-in-gpus" id="id13">FLOPS Per Second in GPUs</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#model-flops-utilization-mfu" id="id14">Model FLOPs Utilization (MFU)</a></p>
<ul>
<li><p><a class="reference internal" href="#theoretical-model-flops-utilization-mfu-indicates-a-rough-benchmark-of-efficiency" id="id15">Theoretical Model FLOPs Utilization (MFU) Indicates a Rough Benchmark of Efficiency</a></p></li>
<li><p><a class="reference internal" href="#relation-of-mfu-and-tflops" id="id16">Relation of MFU and TFLOPS</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#theoretical-flops-in-transformer-models" id="id17">Theoretical FLOPs in Transformer Models</a></p></li>
<li><p><a class="reference internal" href="#references-and-further-readings" id="id18">References and Further Readings</a></p></li>
</ul>
</nav>
<p>This notebook references from
<a class="reference external" href="https://github.com/karpathy/nanoGPT/blob/master/transformer_sizing.ipynb">Andrej Karpathy’s NanoGPT</a>,
which originally stores a bunch of analysis about a Transformer, e.g. estimates
the number of FLOPs, parameters, peak memory footprint, checkpoint size, etc.</p>
<section id="configurations-constants-and-enums">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Configurations, Constants and Enums</a><a class="headerlink" href="#configurations-constants-and-enums" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">GPTConfig</span><span class="p">:</span>
    <span class="n">num_decoder_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span>
    <span class="n">context_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span>
    <span class="n">n_embd</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">768</span>
    <span class="n">ffw_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3072</span>  <span class="c1"># note, this is 4 * n_embd</span>
    <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50257</span>
    <span class="n">bias</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffw_size</span> <span class="o">==</span> <span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_embd</span><span class="p">,</span> <span class="s2">&quot;ffw_size must be 4 * n_embd&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;bias must be False in this experiment.&quot;</span>


<span class="k">class</span> <span class="nc">GPT2ModelType</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">GPT2</span> <span class="o">=</span> <span class="s2">&quot;gpt2&quot;</span>
    <span class="n">GPT2_MEDIUM</span> <span class="o">=</span> <span class="s2">&quot;gpt2-medium&quot;</span>
    <span class="n">GPT2_LARGE</span> <span class="o">=</span> <span class="s2">&quot;gpt2-large&quot;</span>
    <span class="n">GPT2_XL</span> <span class="o">=</span> <span class="s2">&quot;gpt2-xl&quot;</span>


<span class="k">class</span> <span class="nc">ByteUnits</span><span class="p">(</span><span class="n">IntEnum</span><span class="p">):</span>
    <span class="n">B</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># Byte = 1 byte</span>
    <span class="n">KB</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># Kilobyte = 10^3 bytes</span>
    <span class="n">MB</span> <span class="o">=</span> <span class="mi">1000</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># Megabyte = 10^6 bytes</span>
    <span class="n">GB</span> <span class="o">=</span> <span class="mi">1000</span><span class="o">**</span><span class="mi">3</span>  <span class="c1"># Gigabyte = 10^9 bytes</span>


<span class="k">class</span> <span class="nc">FloatingPointPrecision</span><span class="p">(</span><span class="n">IntEnum</span><span class="p">):</span>
    <span class="n">FP32</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># 32-bit floating-point, 4 bytes</span>
    <span class="n">FP16</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># 16-bit floating-point, 2 bytes</span>
    <span class="n">BFLOAT16</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># bfloat16, 16-bit, 2 bytes</span>


<span class="k">class</span> <span class="nc">GPUMemory</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">A100_40GB</span> <span class="o">=</span> <span class="mf">40e9</span>  <span class="c1"># 40 GB for NVIDIA A100</span>
    <span class="n">V100_16GB</span> <span class="o">=</span> <span class="mf">16e9</span>  <span class="c1"># 16 GB for NVIDIA V100</span>
    <span class="n">V100_32GB</span> <span class="o">=</span> <span class="mf">32e9</span>  <span class="c1"># 32 GB for NVIDIA V100</span>
    <span class="n">T4_16GB</span> <span class="o">=</span> <span class="mf">16e9</span>  <span class="c1"># 16 GB for NVIDIA T4</span>
    <span class="n">P100_16GB</span> <span class="o">=</span> <span class="mf">16e9</span>  <span class="c1"># 16 GB for NVIDIA P100</span>


<span class="k">class</span> <span class="nc">GPU</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">flops</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">FloatingPointPrecision</span><span class="p">,</span> <span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">flops</span> <span class="o">=</span> <span class="n">flops</span>

<span class="k">class</span> <span class="nc">A100</span><span class="p">(</span><span class="n">GPU</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="s2">&quot;A100&quot;</span><span class="p">,</span> <span class="p">{</span>
            <span class="n">FloatingPointPrecision</span><span class="o">.</span><span class="n">FP32</span><span class="p">:</span> <span class="mf">19.5e12</span><span class="p">,</span>
            <span class="n">FloatingPointPrecision</span><span class="o">.</span><span class="n">FP16</span><span class="p">:</span> <span class="mf">312e12</span><span class="p">,</span>
            <span class="n">FloatingPointPrecision</span><span class="o">.</span><span class="n">BFLOAT16</span><span class="p">:</span> <span class="mf">312e12</span>
        <span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gpt2_config</span> <span class="o">=</span> <span class="n">GPTConfig</span><span class="p">()</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">gpt2_config</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">GPTConfig</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">num_decoder_blocks</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">context_length</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1024</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">n_embd</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">768</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">ffw_size</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3072</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">n_head</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">12</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">vocab_size</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">50257</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">bias</span>=<span style="color: #ff0000; text-decoration-color: #ff0000; font-style: italic">False</span>
<span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
</section>
<section id="total-trainable-parameters">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Total Trainable Parameters</a><a class="headerlink" href="#total-trainable-parameters" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">total_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">include_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Returns the number of trainable parameters in the model.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">include_bias</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="s2">&quot;bias&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">name</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gpt2</span> <span class="o">=</span> <span class="n">GPT2LMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">GPT2ModelType</span><span class="o">.</span><span class="n">GPT2</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "a1a58858752649c79d2b01393bf517c9"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "d027c0cce4ce4867a4626c770b46735d"}</script><script type="application/vnd.jupyter.widget-view+json">{"version_major": 2, "version_minor": 0, "model_id": "02294e64ffb94a1e8a616cf375b8b86e"}</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gpt2_params_no_bias</span> <span class="o">=</span> <span class="n">total_trainable_parameters</span><span class="p">(</span><span class="n">gpt2</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">gpt2_params_with_bias</span> <span class="o">=</span> <span class="n">total_trainable_parameters</span><span class="p">(</span><span class="n">gpt2</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Number of trainable parameters in GPT2 model: </span><span class="si">{</span><span class="n">gpt2_params_no_bias</span><span class="si">}</span><span class="s2"> (excluding bias) and </span><span class="si">{</span><span class="n">gpt2_params_with_bias</span><span class="si">}</span><span class="s2"> (including bias).&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of trainable parameters in GPT2 model: 124337664 (excluding bias) and 124439808 (including bias).
</pre></div>
</div>
</div>
</div>
<p>Since Karpathy’s blog post assumed that there is no bias for simplicity, we will
also assume that there is no bias in the linear layers. We confirmed that the
number of params (<code class="docutils literal notranslate"><span class="pre">124337664</span></code>) for the smallest GPT-2 model indeed matches the
number of params given by Karpathy.</p>
<p>In what follows, we would assume the smallest GPT-2 model and work out the
theoretical model for the Transformer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># config_args = {</span>
<span class="c1">#     &#39;gpt2&#39;:         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params</span>
<span class="c1">#     &#39;gpt2-medium&#39;:  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params</span>
<span class="c1">#     &#39;gpt2-large&#39;:   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params</span>
<span class="c1">#     &#39;gpt2-xl&#39;:      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params</span>
<span class="c1"># }[model_type]</span>


<span class="k">def</span> <span class="nf">params</span><span class="p">(</span>
    <span class="n">num_decoder_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span>
    <span class="n">context_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
    <span class="n">n_embd</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">768</span><span class="p">,</span>
    <span class="n">ffw_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3072</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50257</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">OrderedDict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;estimates the number of parameters in the model&quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>

    <span class="c1"># token and position embeddings</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;embedding/position&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">*</span> <span class="n">context_length</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;embedding/token&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">*</span> <span class="n">vocab_size</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;embedding&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;embedding/position&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;embedding/token&quot;</span><span class="p">]</span>

    <span class="c1"># attention blocks</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;attention/ln&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_embd</span>  <span class="c1"># note, bias=False in our LN</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;attention/kqv&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">n_embd</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;attention/proj&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_embd</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;attention&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;attention/ln&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;attention/kqv&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;attention/proj&quot;</span><span class="p">]</span>

    <span class="c1"># MLP blocks</span>
    <span class="k">assert</span> <span class="n">ffw_size</span> <span class="o">==</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">n_embd</span><span class="p">,</span> <span class="s2">&quot;ffw_size must be 4 * n_embd&quot;</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;mlp/ln&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_embd</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;mlp/ffw&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">*</span> <span class="n">ffw_size</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;mlp/proj&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">ffw_size</span> <span class="o">*</span> <span class="n">n_embd</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;mlp&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;mlp/ln&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;mlp/ffw&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;mlp/proj&quot;</span><span class="p">]</span>

    <span class="c1"># the transformer and the rest of it</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;block&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;attention&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;mlp&quot;</span><span class="p">]</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_decoder_blocks</span> <span class="o">*</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;block&quot;</span><span class="p">]</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;ln_f&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_embd</span>  <span class="c1"># final layernorm</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;dense&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># 0 because of parameter sharing. This layer uses the weights from the embedding layer</span>

    <span class="c1"># total</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;total&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;embedding&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;ln_f&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;dense&quot;</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">params_dict</span> <span class="o">=</span> <span class="n">params</span><span class="p">()</span>
<span class="n">gpt2_params_no_bias_manual</span> <span class="o">=</span> <span class="n">params_dict</span><span class="p">[</span><span class="s2">&quot;total&quot;</span><span class="p">]</span>

<span class="c1"># Compare to expected PyTorch model parameter count</span>
<span class="n">expected_params</span> <span class="o">=</span> <span class="n">gpt2_params_no_bias</span>
<span class="n">comparison_result</span> <span class="o">=</span> <span class="n">gpt2_params_no_bias_manual</span> <span class="o">==</span> <span class="n">expected_params</span>
<span class="n">comparison_msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;We see: </span><span class="si">{</span><span class="n">gpt2_params_no_bias_manual</span><span class="si">}</span><span class="s2">, Expected: </span><span class="si">{</span><span class="n">expected_params</span><span class="si">}</span><span class="s2">, Match: </span><span class="si">{</span><span class="n">comparison_result</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;Name&quot;</span><span class="p">:</span> <span class="n">params_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span>
    <span class="s2">&quot;Parameters&quot;</span><span class="p">:</span> <span class="n">params_dict</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span>
    <span class="s2">&quot;Ratio (%)&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">value</span> <span class="o">/</span> <span class="n">gpt2_params_no_bias_manual</span> <span class="o">*</span> <span class="mi">100</span> <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">params_dict</span><span class="o">.</span><span class="n">values</span><span class="p">()],</span>
<span class="p">}</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Printing comparison result and parameter distribution table</span>
<span class="nb">print</span><span class="p">(</span><span class="n">comparison_msg</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tabulate</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="s2">&quot;keys&quot;</span><span class="p">,</span> <span class="n">tablefmt</span><span class="o">=</span><span class="s2">&quot;pretty&quot;</span><span class="p">,</span> <span class="n">showindex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">numalign</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">,</span> <span class="n">floatfmt</span><span class="o">=</span><span class="s2">&quot;.4f&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>We see: 124337664, Expected: 124337664, Match: True

+--------------------+------------+-----------------------+
|        Name        | Parameters |       Ratio (%)       |
+--------------------+------------+-----------------------+
| embedding/position |   786432   |  0.6324970042866496   |
|  embedding/token   |  38597376  |  31.042384711361475   |
|     embedding      |  39383808  |  31.674881715648123   |
|    attention/ln    |    768     | 0.0006176728557486812 |
|   attention/kqv    |  1769472   |  1.4231182596449616   |
|   attention/proj   |   589824   |  0.47437275321498723  |
|     attention      |  2360064   |  1.8981086857156975   |
|       mlp/ln       |    768     | 0.0006176728557486812 |
|      mlp/ffw       |  2359296   |   1.897491012859949   |
|      mlp/proj      |  2359296   |   1.897491012859949   |
|        mlp         |  4719360   |   3.795599698575646   |
|       block        |  7079424   |   5.693708384291344   |
|    transformer     |  84953088  |   68.32450061149613   |
|        ln_f        |    768     | 0.0006176728557486812 |
|       dense        |     0      |          0.0          |
|       total        | 124337664  |         100.0         |
+--------------------+------------+-----------------------+
</pre></div>
</div>
</div>
</div>
</section>
<section id="calculating-checkpoint-size-and-fluff-ratio">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Calculating Checkpoint Size and Fluff Ratio</a><a class="headerlink" href="#calculating-checkpoint-size-and-fluff-ratio" title="Link to this heading">#</a></h2>
<p>The functions below perform a series of calculations related to the size
of a GPT-2 model checkpoint, both measured and estimated, and computes the
“fluff ratio” to compare these sizes. The purpose of these calculations is to
evaluate how closely the estimated size of a GPT-2 model checkpoint matches the
actual measured size, and to quantify any overhead or additional data in the
checkpoint file as a percentage of the estimated size.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calculate_checkpoint_size</span><span class="p">(</span><span class="n">params_count</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">precision</span><span class="p">:</span> <span class="n">FloatingPointPrecision</span><span class="p">,</span> <span class="n">units</span><span class="p">:</span> <span class="n">ByteUnits</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the estimated checkpoint size in specified units.</span>

<span class="sd">    This function estimates the checkpoint size for a model given the number</span>
<span class="sd">    of parameters, the precision of these parameters, and</span>
<span class="sd">    the desired units for the result. It accounts for the AdamW optimizer&#39;s</span>
<span class="sd">    storage requirements by adding two times the parameter bytes to account</span>
<span class="sd">    for the optimizer&#39;s moment and velocity vectors.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    params_count : int</span>
<span class="sd">        The number of parameters excluding biases.</span>
<span class="sd">    precision : FloatingPointPrecision</span>
<span class="sd">        The floating point precision of the parameters.</span>
<span class="sd">    units : ByteUnits</span>
<span class="sd">        The units for the resulting checkpoint size.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    float</span>
<span class="sd">        The estimated checkpoint size in the specified units.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    The AdamW optimizer requires additional storage for each parameter</span>
<span class="sd">    for maintaining momentum and variance vectors, hence the calculation</span>
<span class="sd">    includes 2 * params_bytes to accommodate these.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">params_bytes</span> <span class="o">=</span> <span class="n">params_count</span> <span class="o">*</span> <span class="n">precision</span><span class="o">.</span><span class="n">value</span>
    <span class="n">params_and_buffers_bytes</span> <span class="o">=</span> <span class="n">params_bytes</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">params_bytes</span>  <span class="c1"># AdamW optimizer buffers</span>
    <span class="k">return</span> <span class="n">params_and_buffers_bytes</span> <span class="o">/</span> <span class="n">units</span><span class="o">.</span><span class="n">value</span>


<span class="k">def</span> <span class="nf">calculate_fluff_ratio</span><span class="p">(</span><span class="n">measured_bytes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">estimated_bytes</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">units</span><span class="p">:</span> <span class="n">ByteUnits</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the fluff ratio between measured and estimated checkpoint sizes.</span>

<span class="sd">    The fluff ratio is a measure of the overhead or additional data in the</span>
<span class="sd">    checkpoint file, expressed as a percentage of the estimated size. This</span>
<span class="sd">    function converts the estimated size from gigabytes (or specified units)</span>
<span class="sd">    to bytes before calculating the ratio to ensure consistency in units.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    measured_bytes : int</span>
<span class="sd">        The actual size of the checkpoint file, in bytes.</span>
<span class="sd">    estimated_bytes : float</span>
<span class="sd">        The estimated size of the checkpoint file, in the specified units.</span>
<span class="sd">    units : ByteUnits</span>
<span class="sd">        The units in which the estimated bytes are provided.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    float</span>
<span class="sd">        The fluff ratio, expressed as a percentage.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">estimated_bytes_in_bytes</span> <span class="o">=</span> <span class="n">estimated_bytes</span> <span class="o">*</span> <span class="n">units</span><span class="o">.</span><span class="n">value</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">measured_bytes</span> <span class="o">/</span> <span class="n">estimated_bytes_in_bytes</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
</pre></div>
</div>
</div>
</div>
<ol class="arabic simple">
<li><p><strong>Measured Checkpoint Size in Bytes</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">gpt2_checkpoint_size_measured_in_bytes</span></code> is assigned a numerical value
that represents the actual size of a GPT-2 model checkpoint file in bytes.
This value is obtained from the output of the Unix command
<code class="docutils literal notranslate"><span class="pre">wc</span> <span class="pre">-c</span> <span class="pre">ckpt.pt</span></code>, which counts the number of bytes in the file <code class="docutils literal notranslate"><span class="pre">ckpt.pt</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Estimated Checkpoint Size in Bytes</strong>:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">calculate_checkpoint_size</span></code> function is called with the number of
parameters excluding biases (<code class="docutils literal notranslate"><span class="pre">gpt2_params_no_bias</span></code>), the precision of the
model’s parameters (<code class="docutils literal notranslate"><span class="pre">FloatingPointPrecision.FP32</span></code>), and the unit of
measurement (<code class="docutils literal notranslate"><span class="pre">ByteUnits.B</span></code> for bytes). This function calculates the
estimated total size of the checkpoint in bytes, taking into account the
parameters and the additional storage required for the AdamW optimizer’s
buffers.</p></li>
<li><p>It is worth noting we are assuming floating-point precision of 32 bits (4
bytes) for the model’s parameters, and hence we are multiplying the number
of parameters by 4 to obtain the size in bytes.</p></li>
<li><p>The AdamW optimizer, which is commonly used in training deep learning
models for tasks like those involving GPT-2, maintains two additional
values (buffers) for each parameter: the first for the moment vector (<code class="docutils literal notranslate"><span class="pre">m</span></code>)
and the second for the squared moment vector (<code class="docutils literal notranslate"><span class="pre">v</span></code>). These buffers are used
to adapt the learning rates for each parameter during training. This is
why the storage requirement triples (<code class="docutils literal notranslate"><span class="pre">params_bytes</span> <span class="pre">+</span> <span class="pre">2</span> <span class="pre">*</span> <span class="pre">params_bytes</span></code>),
accounting for the original parameters plus the two buffers.</p></li>
</ul>
</li>
<li><p><strong>Fluff Ratio Calculation</strong>:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">calculate_fluff_ratio</span></code> function is called with the measured size in
bytes, the estimated size in bytes, and the unit of measurement for the
estimated size (bytes). This function calculates the fluff ratio, which
indicates the percentage of overhead or additional data in the measured
checkpoint file compared to the estimated size.</p></li>
</ul>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gpt2_checkpoint_size_measured_in_bytes</span> <span class="o">=</span> <span class="mi">1542470366</span>  <span class="c1"># from &#39;wc -c ckpt.pt&#39;</span>
<span class="n">gpt2_checkpoint_size_measured_in_gb</span> <span class="o">=</span> <span class="n">gpt2_checkpoint_size_measured_in_bytes</span> <span class="o">/</span> <span class="n">ByteUnits</span><span class="o">.</span><span class="n">GB</span>

<span class="n">gpt2_checkpoint_size_estimated_in_bytes</span> <span class="o">=</span> <span class="n">calculate_checkpoint_size</span><span class="p">(</span>
    <span class="n">params_count</span><span class="o">=</span><span class="n">gpt2_params_no_bias</span><span class="p">,</span>
    <span class="n">precision</span><span class="o">=</span><span class="n">FloatingPointPrecision</span><span class="o">.</span><span class="n">FP32</span><span class="p">,</span>
    <span class="n">units</span><span class="o">=</span><span class="n">ByteUnits</span><span class="o">.</span><span class="n">B</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">gpt2_checkpoint_size_estimated_in_gb</span> <span class="o">=</span> <span class="n">gpt2_checkpoint_size_estimated_in_bytes</span> <span class="o">/</span> <span class="n">ByteUnits</span><span class="o">.</span><span class="n">GB</span>


<span class="n">fluff_ratio</span> <span class="o">=</span> <span class="n">calculate_fluff_ratio</span><span class="p">(</span>
    <span class="n">measured_bytes</span><span class="o">=</span><span class="n">gpt2_checkpoint_size_measured_in_bytes</span><span class="p">,</span>
    <span class="n">estimated_bytes</span><span class="o">=</span><span class="n">gpt2_checkpoint_size_estimated_in_bytes</span><span class="p">,</span>
    <span class="n">units</span><span class="o">=</span><span class="n">ByteUnits</span><span class="o">.</span><span class="n">B</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="s2">&quot;Measured Checkpoint Size (bytes)&quot;</span><span class="p">,</span> <span class="n">gpt2_checkpoint_size_measured_in_bytes</span><span class="p">],</span>
    <span class="p">[</span><span class="s2">&quot;Measured Checkpoint Size (GB)&quot;</span><span class="p">,</span> <span class="n">gpt2_checkpoint_size_measured_in_gb</span><span class="p">],</span>
    <span class="p">[</span><span class="s2">&quot;Estimated Checkpoint Size (bytes)&quot;</span><span class="p">,</span> <span class="n">gpt2_checkpoint_size_estimated_in_bytes</span><span class="p">],</span>
    <span class="p">[</span><span class="s2">&quot;Estimated Checkpoint Size (GB)&quot;</span><span class="p">,</span> <span class="n">gpt2_checkpoint_size_estimated_in_gb</span><span class="p">],</span>
    <span class="p">[</span><span class="s2">&quot;Fluff Ratio&quot;</span><span class="p">,</span> <span class="n">fluff_ratio</span><span class="p">],</span>
<span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">tabulate</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Metric&quot;</span><span class="p">,</span> <span class="s2">&quot;Value&quot;</span><span class="p">],</span> <span class="n">tablefmt</span><span class="o">=</span><span class="s2">&quot;pretty&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+-----------------------------------+-------------------+
|              Metric               |       Value       |
+-----------------------------------+-------------------+
| Measured Checkpoint Size (bytes)  |    1542470366     |
|   Measured Checkpoint Size (GB)   |    1.542470366    |
| Estimated Checkpoint Size (bytes) |   1492051968.0    |
|  Estimated Checkpoint Size (GB)   |    1.492051968    |
|            Fluff Ratio            | 103.3791314968461 |
+-----------------------------------+-------------------+
</pre></div>
</div>
</div>
</div>
</section>
<section id="gpu-memory-footprint-of-loading-model-and-optimizer">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">GPU Memory Footprint of Loading Model and Optimizer</a><a class="headerlink" href="#gpu-memory-footprint-of-loading-model-and-optimizer" title="Link to this heading">#</a></h2>
<p>We can roughly understand that a checkpoint represents the amount of memory
needed to store not just the model itself (its weights) but also additional
information related to the optimizer state when you’re using GPUs for deep
learning tasks.</p>
<p>When loading a model from a checkpoint for further training or inference, the
GPU memory must accommodate the model weights and the optimizer state (if
continuing training).</p>
<p>Below, we estimate the ratio of our GPU memory that will be taken up by
the model and optimizer state when loading a GPT-2 model from a checkpoint.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calculate_memory_ratio</span><span class="p">(</span><span class="n">checkpoint_size</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">gpu_memory</span><span class="p">:</span> <span class="n">GPUMemory</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="n">memory_ratio</span> <span class="o">=</span> <span class="n">checkpoint_size</span> <span class="o">/</span> <span class="n">gpu_memory</span><span class="o">.</span><span class="n">value</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Memory ratio taken up just for parameters: </span><span class="si">{</span><span class="n">memory_ratio</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span>


<span class="nb">print</span><span class="p">(</span><span class="n">calculate_memory_ratio</span><span class="p">(</span><span class="n">checkpoint_size</span><span class="o">=</span><span class="n">gpt2_checkpoint_size_estimated_in_bytes</span><span class="p">,</span> <span class="n">gpu_memory</span><span class="o">=</span><span class="n">GPUMemory</span><span class="o">.</span><span class="n">A100_40GB</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Memory ratio taken up just for parameters: 3.73%
</pre></div>
</div>
</div>
</div>
<p>Assuming an A100 GPU with roughly 40GB memory, then the code calculates the
percentage of the GPU memory that the estimated checkpoint size (in bytes)
occupies. This calculation gives an insight into how much of the GPU’s memory is
dedicated to storing the model’s weights and the optimizer’s buffers, without
considering other memory usages such as activations during forward and backward
passes.</p>
<p>This percentage is relatively small, implying that most of the GPU memory is
actually used for activations. Activations are the intermediate outputs of
layers during the forward pass and their gradients during the backward pass,
which can consume significant amounts of memory, especially in deep models and
with large batch sizes.</p>
</section>
<section id="estimating-flops-for-a-single-forward-pass">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Estimating FLOPs for a Single Forward Pass</a><a class="headerlink" href="#estimating-flops-for-a-single-forward-pass" title="Link to this heading">#</a></h2>
<p>In order to estimate FLOPs for a single forward pass, we would first need to
define what is a FLOPS.</p>
<section id="basics-of-floating-point-numbers">
<h3><a class="toc-backref" href="#id6" role="doc-backlink">Basics of Floating Point Numbers</a><a class="headerlink" href="#basics-of-floating-point-numbers" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Floating Point Representation</strong>: In computers, numbers can be represented
in various formats, and one common format is floating point. This format is
used to represent real numbers (numbers with fractions) using a fixed amount
of memory, allowing for a wide range of values. A floating point number is
composed of a sign, an exponent, and a mantissa (or significand). This
representation can handle very large numbers, very small numbers, and
fractions.</p></li>
<li><p><strong>Operations on Floating Point Numbers</strong>: Operations on floating point
numbers include addition, subtraction, multiplication, and division. Each of
these operations takes one or more floating point numbers as input and
produces a floating point number as output.</p></li>
</ul>
</section>
<section id="floating-point-operations-flops">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">Floating Point Operations (FLOPs)</a><a class="headerlink" href="#floating-point-operations-flops" title="Link to this heading">#</a></h3>
<p>Floating Point Operations, or FLOPs, refer to individual mathematical operations
(additions, subtractions, multiplications, divisions) performed on
<a class="reference external" href="https://en.wikipedia.org/wiki/Floating-point_arithmetic">floating point numbers</a>.
Each operation counts as one FLOP.</p>
</section>
<section id="counting-flops-of-matrix-multiplications">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">Counting FLOPs of Matrix Multiplications</a><a class="headerlink" href="#counting-flops-of-matrix-multiplications" title="Link to this heading">#</a></h3>
<p>In the context of deep learning, many operations are done via matrix
multiplications, we will take a look at how to count FLOPs for matrix
multiplications next.</p>
<p>Deep learning, particularly in neural networks, relies heavily on matrix
multiplications. A single matrix multiplication operation involves multiple
floating point multiplications and additions.</p>
<p>Consider two matrices <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> of size <span class="math notranslate nohighlight">\(m \times n\)</span> and
<span class="math notranslate nohighlight">\(n \times p\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{A} = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{bmatrix}_{m \times n} \quad \mathbf{B} = \begin{bmatrix}
b_{11} &amp; b_{12} &amp; \cdots &amp; b_{1p} \\
b_{21} &amp; b_{22} &amp; \cdots &amp; b_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
b_{n1} &amp; b_{n2} &amp; \cdots &amp; b_{np}
\end{bmatrix}_{n \times p}
\end{split}\]</div>
<p>It is easy to see that if we want to compute the product
<span class="math notranslate nohighlight">\(\mathbf{C} = \mathbf{A} \mathbf{B}\)</span>, the element <span class="math notranslate nohighlight">\(c_{ij}\)</span> of <span class="math notranslate nohighlight">\(\mathbf{C}\)</span> is
given by:</p>
<div class="math notranslate nohighlight">
\[
c_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj}
\]</div>
<p>and therefore there are a total of <span class="math notranslate nohighlight">\(m \times n \times p\)</span> multiplications and
<span class="math notranslate nohighlight">\(m \times (n-1) \times p\)</span> additions. This amounts to roughly:</p>
<div class="math notranslate nohighlight" id="equation-playbook-transformer-flops-approximation">
<span class="eqno">(1)<a class="headerlink" href="#equation-playbook-transformer-flops-approximation" title="Link to this equation">#</a></span>\[
m \times n \times p + m \times (n-1) \times p \approx 2 \times m \times n \times p
\]</div>
<p>FLOPs. Note this is basically because matrix multiplication is a series of dot
products, and each dot product involves <span class="math notranslate nohighlight">\(n\)</span> multiplications and <span class="math notranslate nohighlight">\(n-1\)</span> additions.</p>
</section>
<section id="estimating-flops-for-a-single-forward-pass-of-gpt-2">
<h3><a class="toc-backref" href="#id9" role="doc-backlink">Estimating FLOPs for a Single Forward Pass of GPT-2</a><a class="headerlink" href="#estimating-flops-for-a-single-forward-pass-of-gpt-2" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">flops</span><span class="p">(</span>
    <span class="n">num_decoder_blocks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span>
    <span class="n">context_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
    <span class="n">n_embd</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">768</span><span class="p">,</span>
    <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span>
    <span class="n">ffw_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3072</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50257</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">OrderedDict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
    <span class="c1"># we only count Weight FLOPs, all other layers (LayerNorm, Softmax, etc) are effectively irrelevant</span>
    <span class="c1"># we count actual FLOPs, not MACs. Hence 2* all over the place</span>
    <span class="c1"># basically for any matrix multiply A (BxC) @ B (CxD) -&gt; (BxD) flops are 2*B*C*D</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
    <span class="n">head_size</span> <span class="o">=</span> <span class="n">n_embd</span> <span class="o">//</span> <span class="n">n_head</span>

    <span class="c1"># attention blocks</span>
    <span class="c1"># 1) the projection to key, query, values</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;attention/kqv&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">context_length</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_embd</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="c1"># 2) calculating the attention scores</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;attention/scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">context_length</span> <span class="o">*</span> <span class="n">context_length</span> <span class="o">*</span> <span class="n">n_embd</span>
    <span class="c1"># 3) the reduction of the values (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;attention/reduce&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">n_head</span> <span class="o">*</span> <span class="p">(</span><span class="n">context_length</span> <span class="o">*</span> <span class="n">context_length</span> <span class="o">*</span> <span class="n">head_size</span><span class="p">)</span>
    <span class="c1"># 4) the final linear projection</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;attention/proj&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">context_length</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_embd</span> <span class="o">*</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;attention&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="s2">&quot;attention/&quot;</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;kqv&quot;</span><span class="p">,</span> <span class="s2">&quot;scores&quot;</span><span class="p">,</span> <span class="s2">&quot;reduce&quot;</span><span class="p">,</span> <span class="s2">&quot;proj&quot;</span><span class="p">])</span>

    <span class="c1"># MLP blocks</span>
    <span class="n">ffw_size</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">n_embd</span>  <span class="c1"># feed forward size</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;mlp/ffw1&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">context_length</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_embd</span> <span class="o">*</span> <span class="n">ffw_size</span><span class="p">)</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;mlp/ffw2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">context_length</span> <span class="o">*</span> <span class="p">(</span><span class="n">ffw_size</span> <span class="o">*</span> <span class="n">n_embd</span><span class="p">)</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;mlp&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;mlp/ffw1&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;mlp/ffw2&quot;</span><span class="p">]</span>

    <span class="c1"># the transformer and the rest of it</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;block&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;attention&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;mlp&quot;</span><span class="p">]</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">num_decoder_blocks</span> <span class="o">*</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;block&quot;</span><span class="p">]</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;dense&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">context_length</span> <span class="o">*</span> <span class="p">(</span><span class="n">n_embd</span> <span class="o">*</span> <span class="n">vocab_size</span><span class="p">)</span>

    <span class="c1"># forward,backward,total</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;forward_total&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;transformer&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;dense&quot;</span><span class="p">]</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;backward_total&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;forward_total&quot;</span><span class="p">]</span>  <span class="c1"># use common estimate of bwd = 2*fwd</span>
    <span class="n">out</span><span class="p">[</span><span class="s2">&quot;total&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;forward_total&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">out</span><span class="p">[</span><span class="s2">&quot;backward_total&quot;</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</div>
<p>The flops function calculates the <strong>total</strong> number of <strong>floating point
operations</strong> required to process a <strong>single</strong> sample
<span class="math notranslate nohighlight">\(\mathbf{x} = \left(x_1, x_2, \ldots, x_{T}\right)\)</span> of length <span class="math notranslate nohighlight">\(T\)</span> through the
entire model for a single <strong>forward</strong> pass.</p>
<p>We take one sample snippet of code to explain how the flops are calculated:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 2) calculating the attention scores</span>
<span class="n">out</span><span class="p">[</span><span class="s2">&quot;attention/scores&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">context_length</span> <span class="o">*</span> <span class="n">context_length</span> <span class="o">*</span> <span class="n">n_embd</span>
</pre></div>
</div>
<p>This is not difficult to see if one recalls the attention mechanism:</p>
<div class="math notranslate nohighlight">
\[
\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d_k}}\right) \mathbf{V}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{K}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{V}\)</span> are the query, key, and value
matrices of size <span class="math notranslate nohighlight">\(T \times d_q\)</span>, <span class="math notranslate nohighlight">\(T \times d_k\)</span>, and <span class="math notranslate nohighlight">\(T \times d_v\)</span>,
respectively. For simplicity, we assume that <span class="math notranslate nohighlight">\(d_q = d_k = d_v = D\)</span> (which is
<code class="docutils literal notranslate"><span class="pre">n_embd</span></code> in the code).</p>
<p>In particular <code class="docutils literal notranslate"><span class="pre">attention_scores</span></code> is calculated as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">attention_scores</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">dim0</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">d_q</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>
</pre></div>
</div>
<p>which is the dot product of the query and key matrices, divided by the square
root of the dimension of the query matrix and is of shape <span class="math notranslate nohighlight">\(T \times T\)</span>. However,
recall that ultimately the matrix multiplication of the two matrices
<span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{K}^{\top}\)</span> is of shape <span class="math notranslate nohighlight">\(T \times D\)</span> and <span class="math notranslate nohighlight">\(D \times T\)</span>,
and by our earlier equation <a class="reference internal" href="#equation-playbook-transformer-flops-approximation">(1)</a>, this
would be a total of <span class="math notranslate nohighlight">\(2 \times T \times T \times D\)</span> FLOPs, coinciding with the
code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="n">flops</span><span class="p">()</span>
<span class="n">flops_total</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="s2">&quot;forward_total&quot;</span><span class="p">]</span>

<span class="n">table</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;flops&quot;</span><span class="p">,</span> <span class="s2">&quot;ratio (%)&quot;</span><span class="p">)]</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">table</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">v</span> <span class="o">/</span> <span class="n">flops_total</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">tabulate</span><span class="p">(</span><span class="n">table</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="s2">&quot;firstrow&quot;</span><span class="p">,</span> <span class="n">tablefmt</span><span class="o">=</span><span class="s2">&quot;pretty&quot;</span><span class="p">,</span> <span class="n">numalign</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>+------------------+--------------+---------------------+
|       name       |    flops     |      ratio (%)      |
+------------------+--------------+---------------------+
|  attention/kqv   |  3623878656  | 1.2425508965889174  |
| attention/scores |  1610612736  | 0.5522448429284077  |
| attention/reduce |  1610612736  | 0.5522448429284077  |
|  attention/proj  |  1207959552  | 0.41418363219630583 |
|    attention     |  8053063680  | 2.7612242146420387  |
|     mlp/ffw1     |  4831838208  | 1.6567345287852233  |
|     mlp/ffw2     |  4831838208  | 1.6567345287852233  |
|       mlp        |  9663676416  | 3.3134690575704466  |
|      block       | 17716740096  |  6.074693272212485  |
|   transformer    | 212600881152 |  72.89631926654981  |
|      dense       | 79047426048  |  27.10368073345018  |
|  forward_total   | 291648307200 |        100.0        |
|  backward_total  | 583296614400 |        200.0        |
|      total       | 874944921600 |        300.0        |
+------------------+--------------+---------------------+
</pre></div>
</div>
</div>
</div>
</section>
<section id="sanity-check-with-palm-paper-s-flops-calculation">
<h3><a class="toc-backref" href="#id10" role="doc-backlink">Sanity Check with Palm Paper’s FLOPs Calculation</a><a class="headerlink" href="#sanity-check-with-palm-paper-s-flops-calculation" title="Link to this heading">#</a></h3>
<p>NOTE: The notations below are purely based on the PaLM 2 paper, and is not
what I usually use.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">palm_flops</span></code> function, as inspired by the
<a class="reference external" href="https://ai.google/static/documents/palm2techreport.pdf">technical report of Google’s PaLM 2</a>,
calculates an estimate of the model’s floating point operations (FLOPs) but
introduces a specific formula for computing model FLOPs utilization (MFU) per
token and for the entire model.</p>
<ol class="arabic simple">
<li><p><strong>Non-Embedding Model Parameters (<code class="docutils literal notranslate"><span class="pre">N</span></code>)</strong>: This calculation starts by
estimating the number of non-embedding model parameters. In transformer
models like PaLM, a significant portion of the parameters resides in the
embedding layers. This formula adjusts for that by subtracting the
embedding/position parameters from the total, focusing on the parameters
actively involved in computations outside of embeddings.</p></li>
<li><p><strong>Model Dimensions (<code class="docutils literal notranslate"><span class="pre">L</span></code>, <code class="docutils literal notranslate"><span class="pre">H</span></code>, <code class="docutils literal notranslate"><span class="pre">Q</span></code>, <code class="docutils literal notranslate"><span class="pre">T</span></code>)</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">L</span></code> = Number of layers (<code class="docutils literal notranslate"><span class="pre">n_layer</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">H</span></code> = Number of attention heads (<code class="docutils literal notranslate"><span class="pre">n_head</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Q</span></code> = Size of each attention head (<code class="docutils literal notranslate"><span class="pre">n_embd</span> <span class="pre">//</span> <span class="pre">n_head</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">T</span></code> = Sequence length (<code class="docutils literal notranslate"><span class="pre">block_size</span></code>), also referred to as context length
in other discussions.</p></li>
</ul>
</li>
<li><p><strong>MF Per Token (<code class="docutils literal notranslate"><span class="pre">mf_per_token</span></code>)</strong>: This represents the estimated FLOPs for
processing a single token, calculated as <code class="docutils literal notranslate"><span class="pre">6*N</span> <span class="pre">+</span> <span class="pre">12*L*H*Q*T</span></code>.</p></li>
<li><p><strong>Total Model FLOPs for a sequence (<code class="docutils literal notranslate"><span class="pre">mf_per_sequence</span></code>)</strong>: This is calculated
by multiplying the per-token FLOPs estimate (<code class="docutils literal notranslate"><span class="pre">mf_per_token</span></code>) by the sequence
length (<code class="docutils literal notranslate"><span class="pre">block_size</span></code> or <code class="docutils literal notranslate"><span class="pre">T</span></code>). This gives the total estimated FLOPs for
processing a sequence of length <code class="docutils literal notranslate"><span class="pre">T</span></code>.</p></li>
</ol>
<p>This is more of a sanity check for Karpathy, and he confirms if using PaLM’s
<code class="docutils literal notranslate"><span class="pre">palm_flops</span></code> function to calculate FLOPs for our GPT-2 model yields similar
results to the ones he wrote himself (<code class="docutils literal notranslate"><span class="pre">flops</span></code> function).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># now here is an estimate copy pasted from the PaLM paper</span>
<span class="c1"># this formula is often used to calculate MFU (model flops utilization)</span>
<span class="k">def</span> <span class="nf">palm_flops</span><span class="p">(</span>
    <span class="n">params</span><span class="p">:</span> <span class="n">OrderedDict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">num_decoder_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_head</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">context_length</span><span class="p">:</span> <span class="nb">int</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Estimate of the model flops following PaLM paper formula.&quot;&quot;&quot;</span>
    <span class="c1"># non-embedding model parameters. note that we do not subtract the</span>
    <span class="c1"># embedding/token params because those are tied and get used in the last layer.</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">params</span><span class="p">()[</span><span class="s2">&quot;total&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">params</span><span class="p">()[</span><span class="s2">&quot;embedding/position&quot;</span><span class="p">]</span>
    <span class="n">L</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">num_decoder_blocks</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">n_embd</span> <span class="o">//</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">context_length</span>
    <span class="n">mf_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">12</span> <span class="o">*</span> <span class="n">L</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">Q</span> <span class="o">*</span> <span class="n">T</span>
    <span class="n">mf_per_sequence</span> <span class="o">=</span> <span class="n">mf_per_token</span> <span class="o">*</span> <span class="n">context_length</span>
    <span class="k">return</span> <span class="n">mf_per_sequence</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gpt2_flops_using_palm_flops_calculation</span> <span class="o">=</span> <span class="n">palm_flops</span><span class="p">(</span>
    <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">,</span>
    <span class="n">num_decoder_blocks</span><span class="o">=</span><span class="n">gpt2_config</span><span class="o">.</span><span class="n">num_decoder_blocks</span><span class="p">,</span>
    <span class="n">n_head</span><span class="o">=</span><span class="n">gpt2_config</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
    <span class="n">n_embd</span><span class="o">=</span><span class="n">gpt2_config</span><span class="o">.</span><span class="n">n_embd</span><span class="p">,</span>
    <span class="n">context_length</span><span class="o">=</span><span class="n">gpt2_config</span><span class="o">.</span><span class="n">context_length</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">gpt2_flops_using_own_flops_calculation</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="s2">&quot;total&quot;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;PaLM paper estimate of GPT2 flops: </span><span class="si">{</span><span class="n">gpt2_flops_using_palm_flops_calculation</span><span class="si">}</span><span class="s2">, Our estimate: </span><span class="si">{</span><span class="n">gpt2_flops_using_own_flops_calculation</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ratio: </span><span class="si">{</span><span class="n">gpt2_flops_using_palm_flops_calculation</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">gpt2_flops_using_own_flops_calculation</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.5f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>PaLM paper estimate of GPT2 flops: 875062886400, Our estimate: 874944921600
Ratio: 100.01348%
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="floating-point-operations-per-second-flops">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Floating Point Operations Per Second (FLOPS)</a><a class="headerlink" href="#floating-point-operations-per-second-flops" title="Link to this heading">#</a></h2>
<p>In computational tasks and processor performance assessment, we measure the
capacity for floating-point computation in terms of
<a class="reference external" href="https://en.wikipedia.org/wiki/FLOPS">FLOPS</a>, an acronym that stands for
<strong><em>Floating Point Operations Per Second</em></strong>. This metric indicates the quantity
of floating-point arithmetic operations—specifically, additions, subtractions,
multiplications, and divisions—that a computing system is capable of performing
<em>every second</em>. For example, a processor with the capability to execute one
trillion such operations within a second is said to have a computational
performance of 1 teraFLOP (TFLOP) or simply 1 TFLOPS.</p>
<section id="some-practical-considerations-for-flops-in-deep-learning">
<h3><a class="toc-backref" href="#id12" role="doc-backlink">Some Practical Considerations for FLOPs in Deep Learning</a><a class="headerlink" href="#some-practical-considerations-for-flops-in-deep-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>In deep learning models, the number of FLOPs required for a forward pass
through the network gives an indication of the model’s complexity and
efficiency. Models with higher FLOPs require more computational resources,
which can affect training and inference times, especially on large datasets.</p></li>
<li><p>Hardware wise we also can gauge what types of GPU is needed for the model,
especially when high FLOPS models are used, we would likely need a high-end
GPU that can operate at a higher FLOPS per second.</p></li>
</ul>
</section>
<section id="flops-per-second-in-gpus">
<h3><a class="toc-backref" href="#id13" role="doc-backlink">FLOPS Per Second in GPUs</a><a class="headerlink" href="#flops-per-second-in-gpus" title="Link to this heading">#</a></h3>
<p>Given the <strong>total number of FLOPs required for a forward pass through a deep
learning model</strong> and <strong>the theoretical FLOPS per second of a GPU</strong>, you can
estimate the time it takes to perform the forward pass on the GPU by dividing
the total FLOPs by the GPU’s FLOPS capacity. This calculation assumes ideal
conditions where the model fully utilizes the GPU’s computational capabilities.</p>
<div class="math notranslate nohighlight">
\[
\text{Time for Forward Pass (seconds)} = \frac{\text{Total FLOPs for Forward Pass}}{\text{Theoretical FLOPS of GPU}}
\]</div>
<p>This is assuming a single forward pass on a single sample.
However, in practice, we use Model FLOPs Utilization (MFU) to measure the
efficiency of the model, which is the ratio of the actual FLOPs to the
theoretical FLOPS of the GPU.</p>
<p>Note that different GPUs have different FLOPS, and the FLOPS of a GPU can be
affected by what floating point precision is used (e.g. FP16, FP32, FP64).</p>
</section>
</section>
<section id="model-flops-utilization-mfu">
<h2><a class="toc-backref" href="#id14" role="doc-backlink">Model FLOPs Utilization (MFU)</a><a class="headerlink" href="#model-flops-utilization-mfu" title="Link to this heading">#</a></h2>
<p>Given:</p>
<ul class="simple">
<li><p>Batch size with gradient accumulation factored in:
<span class="math notranslate nohighlight">\(\text{batch_size} = 20 \times 5\)</span> <strong>samples</strong>.</p></li>
<li><p>Measured time per iteration: <span class="math notranslate nohighlight">\(\text{measured_time} = 0.755\)</span> <strong>seconds</strong>
(note this is Karpathy’s own measured time for the forward and backward
pass - 1 iteration)</p></li>
<li><p>Total FLOPs required by the model for one forward and backward pass for a
single sample/sequence <span class="math notranslate nohighlight">\(\mathbf{x} = \left(x_1, x_2, \ldots, x_{T}\right)\)</span>:
<span class="math notranslate nohighlight">\(f[\text{total}]\)</span> having a unit of <strong>FLOPs/sample</strong>.</p></li>
<li><p>The calculation of total FLOPs for a model’s operations is independent of
the floating-point precision used (e.g., FP32, FP16, BF16). However when
training models, we may choose different floating-point precisions and by
extension, different hardware GPUs has different FLOPS for different
precisions.</p></li>
</ul>
<p>We will use FLOPS and TFLOPS (teraFLOPS) interchangeably where <span class="math notranslate nohighlight">\(1\)</span> TFLOPS
represents <span class="math notranslate nohighlight">\(10^{12}\)</span> FLOPS.</p>
<ol class="arabic">
<li><p><strong>Measured Throughput</strong>:</p>
<p>The throughput, in terms of samples processed per second, can be calculated
as follows:</p>
<div class="math notranslate nohighlight">
\[
    \text{measured_throughput} = \frac{\text{batch_size}}{\text{measured_time}} \text{ samples/second}
    \]</div>
<p>Substituting the given values:</p>
<div class="math notranslate nohighlight">
\[
    \text{measured_throughput} = \frac{20 \times 5}{0.755} = 132.4503311258278 \text{ samples/second}
    \]</div>
</li>
<li><p><strong>FLOPs Achieved</strong>:</p>
<p>The total floating point operations per second (FLOPs) achieved, based on
the measured throughput, is:</p>
<div class="math notranslate nohighlight">
\[
    \text{flops_achieved_per_second} = f[\text{total}] \times \text{measured_throughput}
    \]</div>
<p>Here, <span class="math notranslate nohighlight">\(f[\text{total}]\)</span> represents the total FLOPs required by the model for
one complete pass (forward and backward) of a single sample, <strong>with a unit
of FLOPs/sample</strong>. And multiplying it by the measured throughput gives the
effective FLOPs achieved per second.</p>
<div class="math notranslate nohighlight">
\[
    \text{flops_achieved_per_second} = \left( \frac{\text{FLOPs}}{\text{sample}} \right)
    \times \left( \frac{\text{sample}}{\text{second}} \right)
    \]</div>
<p>On a side note, there should be no confusion in the cancellation of the
<code class="docutils literal notranslate"><span class="pre">sample</span></code> term in the numerator and denominator even though I used <code class="docutils literal notranslate"><span class="pre">sample</span></code>
and <code class="docutils literal notranslate"><span class="pre">samples</span></code> earlier. It is like saying if I can process <span class="math notranslate nohighlight">\(100\)</span> samples per
second, and my model requires <span class="math notranslate nohighlight">\(1000\)</span> FLOPs per sample, then I can process
<span class="math notranslate nohighlight">\(100 \times 1000 = 100000\)</span> FLOPs per second because I processed <span class="math notranslate nohighlight">\(100\)</span>
samples in <span class="math notranslate nohighlight">\(1\)</span> second, and each sample required <span class="math notranslate nohighlight">\(1000\)</span> FLOPs.</p>
</li>
<li><p><strong>Fraction of A100 Utilization</strong>:</p>
<p>Given the
<a class="reference external" href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf">A100’s promised performance for <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> operations</a>:</p>
<div class="math notranslate nohighlight">
\[
    \text{a100_bfloat16_flops_promised} = 312 \times 10^{12} \text{ FLOPS}
    \]</div>
<p>The fraction of the A100 GPU utilized can be expressed as a percentage of
the promised FLOPs:</p>
<div class="math notranslate nohighlight">
\[
    \text{fraction of A100 used (\%)} = \left( \frac{\text{flops_achieved_per_second}}{\text{a100_bfloat16_flops_promised}} \right) \times 100
    \]</div>
<p>Substituting <span class="math notranslate nohighlight">\(\text{flops_achieved_per_second}\)</span> and
<span class="math notranslate nohighlight">\(\text{a100_bfloat16_flops_promised}\)</span> with their respective values provides
the percentage utilization of the A100 GPU’s computational capability.</p>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># here is what we currently roughly measure</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">20</span> <span class="o">*</span> <span class="mi">5</span>  <span class="c1"># 5 is grad_accum, so total batch size is 100</span>
<span class="n">measured_time</span> <span class="o">=</span> <span class="mf">0.755</span>  <span class="c1"># in seconds per iteration</span>
<span class="n">measured_throughput</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">/</span> <span class="n">measured_time</span> <span class="c1"># number of samples processed per second</span>
<span class="n">flops_achieved_per_second</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="s2">&quot;total&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">measured_throughput</span>

<span class="c1"># A100 is cited to be 312 TFLOPS of bfloat16 running on tensor cores</span>
<span class="n">a100_bfloat16_promised_flops</span> <span class="o">=</span> <span class="mf">312e12</span>

<span class="c1"># the fraction of the A100 that we are using:</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;fraction of A100 used: </span><span class="si">{</span><span class="n">flops_achieved_per_second</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">a100_bfloat16_promised_flops</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>fraction of A100 used: 37.14%
</pre></div>
</div>
</div>
</div>
<section id="theoretical-model-flops-utilization-mfu-indicates-a-rough-benchmark-of-efficiency">
<h3><a class="toc-backref" href="#id15" role="doc-backlink">Theoretical Model FLOPs Utilization (MFU) Indicates a Rough Benchmark of Efficiency</a><a class="headerlink" href="#theoretical-model-flops-utilization-mfu-indicates-a-rough-benchmark-of-efficiency" title="Link to this heading">#</a></h3>
<p>The Model FLOPs Utilization (MFU) of 37% in this context signifies that, for a
specific model (like GPT-2), on a particular GPU (e.g., A100), and using a given
floating point precision (e.g., bfloat16), the model’s training or inference
process is utilizing 37% of the GPU’s theoretical maximum FLOPS capability. This
percentage reflects the efficiency with which the model leverages the
computational power of the GPU under those specific conditions.</p>
<p>If we treat this metric of 37% as a benchmark reported by some paper, and if
your actual MFU is significantly lower than this 37%, it suggests that there
might be room for optimization in how the model is executed on the hardware. It
could indicate inefficiencies in data loading, model architecture not fully
leveraging the GPU’s capabilities, or potential bottlenecks in the computation
process that are preventing the GPU from being fully utilized. Consequently, MFU
is a very common metric to monitor in the context of training large language
models like GPT-2, as it can provide insights into the efficiency of the
training process and help identify areas for potential optimization. After all,
the GPUs are expensive to run, and we want to make sure we are getting the most
out of them!</p>
</section>
<section id="relation-of-mfu-and-tflops">
<h3><a class="toc-backref" href="#id16" role="doc-backlink">Relation of MFU and TFLOPS</a><a class="headerlink" href="#relation-of-mfu-and-tflops" title="Link to this heading">#</a></h3>
<p>As we have seen earlier, we can denote the MFU formula as:</p>
<div class="math notranslate nohighlight">
\[
\operatorname{MFU} = \frac{\operatorname{MODEL_FLOPS}}{\operatorname{GPU_FLOPS}},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\operatorname{MODEL_FLOPS}\)</span> is the number of floating point operations per
second and <span class="math notranslate nohighlight">\(\operatorname{GPU_FLOPS}\)</span> is the number of floating point operations
per second that the GPU can perform.</p>
<p>Then we can easily see that the <span class="math notranslate nohighlight">\(\operatorname{MODEL_FLOPS}\)</span> is given by a
re-arrangement of the MFU formula:</p>
<div class="math notranslate nohighlight">
\[
\operatorname{MODEL_FLOPS} = \operatorname{MFU} \times \operatorname{GPU_FLOPS}.
\]</div>
<p>This means if a report says they achieved <span class="math notranslate nohighlight">\(250\)</span> TFLOPS, and they are using
NVIDIA A100 GPU with <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> precision at a rate of <span class="math notranslate nohighlight">\(312\)</span> TFLOPS, then we can
calculate the MFU as:</p>
<div class="math notranslate nohighlight">
\[
\operatorname{MFU} = \frac{250}{312} \approx 0.8.
\]</div>
</section>
</section>
<section id="theoretical-flops-in-transformer-models">
<h2><a class="toc-backref" href="#id17" role="doc-backlink">Theoretical FLOPs in Transformer Models</a><a class="headerlink" href="#theoretical-flops-in-transformer-models" title="Link to this heading">#</a></h2>
<p>The excerpt from Appendix B, page 66 of the PaLM paper <em>PaLM: Scaling Language
Modeling with Pathways</em> provides a detailed explanation of how Model FLOPs
Utilization (MFU) is calculated for a dense Transformer language model, focusing
on the relationship between observed throughput (tokens-per-second) and the
theoretical maximum throughput based on the model’s computational demands and
the hardware’s peak FLOPs.</p>
<p>Given a corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> with <span class="math notranslate nohighlight">\(M\)</span> sequences:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{S} = \left\{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_M\right\},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x}_m\)</span> is a sequence of length <span class="math notranslate nohighlight">\(T\)</span> consisting of tokens:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}_m = \left\{x_{m,1}, x_{m,2}, \ldots, x_{m,T}\right\},
\]</div>
<p>Then we can easily see that there are a total of <span class="math notranslate nohighlight">\(D = M \times T\)</span> tokens. If we
further denote the model (in our case GPT-2) as <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>, and the model’s
learnable parameters as <span class="math notranslate nohighlight">\(\theta \in \Theta\)</span>, then the total number of learnable
parameters in the model can be denoted as <span class="math notranslate nohighlight">\(M = |\theta|\)</span>.</p>
<ol class="arabic">
<li><p><strong>Basic Computation per Token for Non-Attention Components</strong>: The paper notes
that, excluding self-attention, a decoder-only model with <span class="math notranslate nohighlight">\(N\)</span> parameters
requires <span class="math notranslate nohighlight">\(6N\)</span> matrix multiplication (matmul) FLOPs per token — <span class="math notranslate nohighlight">\(2N\)</span> FLOPs for
the forward pass and <span class="math notranslate nohighlight">\(4N\)</span> FLOPs for the backward pass. This doubling accounts
for the additional computations required during backpropagation (gradient
calculation and weight updates), where each matmul involves one
multiplication and one addition per pair of input values.</p></li>
<li><p><strong>Self-Attention Computation</strong>: For the self-attention mechanism, an
additional <span class="math notranslate nohighlight">\(6LHQ(2T)\)</span> FLOPs per token are needed, with <span class="math notranslate nohighlight">\(L\)</span>, <span class="math notranslate nohighlight">\(H\)</span>, <span class="math notranslate nohighlight">\(Q\)</span>, and <span class="math notranslate nohighlight">\(T\)</span>
representing the number of layers/blocks, heads, head dimension (embedding
size of 1 head’s query matrix) and sequence/context length, respectively.
This term accounts for the computations within the self-attention layers that
are more complex due to their dependency on the sequence length and the
architecture of the Transformer model.</p></li>
<li><p><strong>Total Computational Requirement for 1 Token</strong>: The total FLOPs per token,
considering both matmuls for the non-attention components and the
self-attention computations, are therefore summarized as <span class="math notranslate nohighlight">\(6N + 12LHQ(2T)\)</span>.
However, the self-attention part is noted to contribute a much smaller value
for large models, suggesting that the primary computational load comes from
the non-attention components.</p></li>
<li><p><strong>Total Computational Requirement for the Entire Corpus</strong>: The total
computational requirement for the entire corpus is then given by
<span class="math notranslate nohighlight">\(D(6N + 12LHQ(2T))\)</span>, where <span class="math notranslate nohighlight">\(D\)</span> is the total number of tokens in the corpus.</p>
<p>Sometimes people just use <span class="math notranslate nohighlight">\(6ND\)</span> as a rough estimate of the total FLOPs
required for the entire corpus, which is a reasonable approximation for
large models - in a sense <span class="math notranslate nohighlight">\(\mathcal{O}(ND)\)</span>.</p>
</li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Finally let&#39;s check out the 6ND approximation as total cost of training in FLOPs</span>
<span class="n">N</span> <span class="o">=</span> <span class="n">params</span><span class="p">()[</span><span class="s2">&quot;total&quot;</span><span class="p">]</span>  <span class="c1"># this is number of parameters, N</span>
<span class="n">D</span> <span class="o">=</span> <span class="mf">300e9</span>  <span class="c1"># 300B tokens, this is dataset size in tokens, D</span>
<span class="n">a100_bfloat16_promised_flops</span> <span class="o">=</span> <span class="mf">312e12</span>  <span class="c1"># 312 TFLOPS</span>
<span class="n">assumed_mfu</span> <span class="o">=</span> <span class="mf">0.3</span>  <span class="c1"># assume this model flops utilization (take the current 37% from above and add some DDP overhead)</span>
<span class="n">flops_throughput</span> <span class="o">=</span> <span class="n">a100_bfloat16_promised_flops</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">*</span> <span class="n">assumed_mfu</span>  <span class="c1"># assume an 8XA100 node at 30% utilization</span>
<span class="n">flops_needed</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">N</span> <span class="o">*</span> <span class="n">D</span>  <span class="c1"># 6ND</span>
<span class="n">time_needed_over_all_tokens_in_seconds</span> <span class="o">=</span> <span class="n">flops_needed</span> <span class="o">/</span> <span class="n">flops_throughput</span>  <span class="c1"># in seconds</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;time needed to train the model: </span><span class="si">{</span><span class="n">time_needed_over_all_tokens_in_seconds</span><span class="o">/</span><span class="mi">3600</span><span class="o">/</span><span class="mi">24</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> days&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>time needed to train the model: 3.46 days
</pre></div>
</div>
</div>
</div>
<p>Karpathy reported that this number 3.46 days is close to his training time ~4 days.
We see a modular function <code class="docutils literal notranslate"><span class="pre">estimate_mfu</span></code> below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MFUEstimationResult</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">flops_per_token_per_fwdbwd</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">flops_per_sequence_per_fwdbwd</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">flops_per_iter_per_fwdbwd</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">flops_achieved_per_second</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">mfu</span><span class="p">:</span> <span class="nb">float</span>


<span class="k">def</span> <span class="nf">estimate_mfu</span><span class="p">(</span>
    <span class="n">num_decoder_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">context_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">model_total_parameters</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">effective_batch_size_per_iter</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">time_taken_per_iter</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">gpu_promised_flops</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">312e12</span><span class="p">,</span>  <span class="c1"># A100 GPU bfloat16/float16 peak flops is 312 TFLOPS</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MFUEstimationResult</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimate Model FLOPs Utilization (MFU) as a ratio of achieved FLOPs to</span>
<span class="sd">    the A100 GPU&#39;s peak FLOPs capability.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    num_decoder_blocks : int</span>
<span class="sd">        Number of decoder blocks in the Transformer model.</span>
<span class="sd">    num_heads : int</span>
<span class="sd">        Number of attention heads in each Transformer block.</span>
<span class="sd">    d_model : int</span>
<span class="sd">        Dimension of the model&#39;s embeddings.</span>
<span class="sd">    context_length : int</span>
<span class="sd">        Number of tokens in each input sequence.</span>
<span class="sd">    model_total_parameters : int</span>
<span class="sd">        Total number of learnable parameters in the model.</span>
<span class="sd">    effective_batch_size_per_iter : int</span>
<span class="sd">        Effective batch size processed in one iteration, accounting for</span>
<span class="sd">        gradient accumulation.</span>
<span class="sd">    time_taken_per_iter : float</span>
<span class="sd">        Time taken per training iteration in seconds.</span>
<span class="sd">    gpu_promised_flops : float, optional</span>
<span class="sd">        Theoretical peak performance of the GPU in FLOPs (default is</span>
<span class="sd">        312e12 for A100 GPU bfloat16/float16 operations).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    MFUEstimationResult:</span>
<span class="sd">        Pydantic model with the following fields:</span>
<span class="sd">        - flops_per_token_per_fwdbwd: FLOPs required for forward and backward pass of a single token</span>
<span class="sd">        - flops_per_sequence_per_fwdbwd: FLOPs required for forward and backward pass of a single sequence</span>
<span class="sd">        - flops_per_iter_per_fwdbwd: FLOPs required for forward and backward pass of the effective batch size</span>
<span class="sd">        - flops_achieved_per_second: FLOPs achieved per second</span>
<span class="sd">        - mfu: Model FLOPs Utilization (MFU) as a ratio of achieved FLOPs to the A100 GPU&#39;s peak FLOPs capability</span>

<span class="sd">    Example</span>
<span class="sd">    -------</span>
<span class="sd">    &gt;&gt;&gt; estimate_mfu(</span>
<span class="sd">    ...     num_decoder_blocks=6,</span>
<span class="sd">    ...     num_heads=8,</span>
<span class="sd">    ...     d_model=512,</span>
<span class="sd">    ...     context_length=1024,</span>
<span class="sd">    ...     model_total_parameters=1_000_000,</span>
<span class="sd">    ...     effective_batch_size_per_iter=20 * 8,  # 20 sequences per GPU, 8 GPUs</span>
<span class="sd">    ...     time_taken_per_iter=0.1, # 0.1 seconds per iteration</span>
<span class="sd">    ...     gpu_promised_flops=312e12, # A100 GPU bfloat16/float16 peak flops</span>
<span class="sd">    ... )</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    This function utilizes the formula from the PaLM paper Appendix B</span>
<span class="sd">    (https://arxiv.org/abs/2204.02311) for estimating the FLOPs required</span>
<span class="sd">    for one forward and backward pass of a single token and scales it up to</span>
<span class="sd">    the effective batch size and the given model architecture to calculate MFU.</span>
<span class="sd">    You can likely use it as a callback in your `Trainer` to log the MFU during training.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># fmt: off</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">model_total_parameters</span><span class="p">,</span> <span class="n">num_decoder_blocks</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">context_length</span>
    <span class="n">flops_per_token_per_fwdbwd</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">12</span> <span class="o">*</span> <span class="n">L</span> <span class="o">*</span> <span class="n">H</span> <span class="o">*</span> <span class="n">Q</span> <span class="o">*</span> <span class="n">T</span> <span class="c1"># 1 token forward and backward flops</span>
    <span class="n">flops_per_sequence_per_fwdbwd</span> <span class="o">=</span> <span class="n">flops_per_token_per_fwdbwd</span> <span class="o">*</span> <span class="n">T</span> <span class="c1"># 1 sequence = T tokens</span>
    <span class="n">flops_per_iter_per_fwdbwd</span> <span class="o">=</span> <span class="n">flops_per_sequence_per_fwdbwd</span> <span class="o">*</span> <span class="n">effective_batch_size_per_iter</span> <span class="c1"># 1 iter means if batch size is 100, then 100 sequences are processed in 1 iter</span>
    <span class="c1"># express our flops throughput as ratio of A100 bfloat16 peak flops</span>
    <span class="n">flops_achieved_per_second</span> <span class="o">=</span> <span class="n">flops_per_iter_per_fwdbwd</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">time_taken_per_iter</span><span class="p">)</span>  <span class="c1"># per second</span>
    <span class="n">mfu</span> <span class="o">=</span> <span class="n">flops_achieved_per_second</span> <span class="o">/</span> <span class="n">gpu_promised_flops</span>
    <span class="c1"># fmt: on</span>
    <span class="k">return</span> <span class="n">MFUEstimationResult</span><span class="p">(</span>
        <span class="n">flops_per_token_per_fwdbwd</span><span class="o">=</span><span class="n">flops_per_token_per_fwdbwd</span><span class="p">,</span>
        <span class="n">flops_per_sequence_per_fwdbwd</span><span class="o">=</span><span class="n">flops_per_sequence_per_fwdbwd</span><span class="p">,</span>
        <span class="n">flops_per_iter_per_fwdbwd</span><span class="o">=</span><span class="n">flops_per_iter_per_fwdbwd</span><span class="p">,</span>
        <span class="n">flops_achieved_per_second</span><span class="o">=</span><span class="n">flops_achieved_per_second</span><span class="p">,</span>
        <span class="n">mfu</span><span class="o">=</span><span class="n">mfu</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gradient_accumulation</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">effective_batch_size</span> <span class="o">=</span> <span class="n">gradient_accumulation</span> <span class="o">*</span> <span class="n">batch_size</span>
<span class="n">measured_time</span> <span class="o">=</span> <span class="mf">0.755</span>  <span class="c1"># in seconds per iteration</span>
<span class="n">model_total_parameters</span> <span class="o">=</span> <span class="n">gpt2_params_no_bias</span> <span class="o">-</span> <span class="n">params</span><span class="p">()[</span><span class="s2">&quot;embedding/position&quot;</span><span class="p">]</span>

<span class="n">mfu_estimates</span> <span class="o">=</span> <span class="n">estimate_mfu</span><span class="p">(</span>
    <span class="n">num_decoder_blocks</span><span class="o">=</span><span class="n">gpt2_config</span><span class="o">.</span><span class="n">num_decoder_blocks</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="n">gpt2_config</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span>
    <span class="n">d_model</span><span class="o">=</span><span class="n">gpt2_config</span><span class="o">.</span><span class="n">n_embd</span><span class="p">,</span>
    <span class="n">context_length</span><span class="o">=</span><span class="n">gpt2_config</span><span class="o">.</span><span class="n">context_length</span><span class="p">,</span>
    <span class="n">model_total_parameters</span><span class="o">=</span><span class="n">model_total_parameters</span><span class="p">,</span>
    <span class="n">effective_batch_size_per_iter</span><span class="o">=</span><span class="n">effective_batch_size</span><span class="p">,</span>
    <span class="n">time_taken_per_iter</span><span class="o">=</span><span class="n">measured_time</span><span class="p">,</span>
    <span class="n">gpu_promised_flops</span><span class="o">=</span><span class="mf">312e12</span><span class="p">,</span>  <span class="c1"># A100 GPU bfloat16/float16 peak flops</span>
<span class="p">)</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">mfu_estimates</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">MFUEstimationResult</span><span style="font-weight: bold">(</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">flops_per_token_per_fwdbwd</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">854553600.0</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">flops_per_sequence_per_fwdbwd</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">875062886400.0</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">flops_per_iter_per_fwdbwd</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">87506288640000.0</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">flops_achieved_per_second</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">115902369059602.66</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #808000; text-decoration-color: #808000">mfu</span>=<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.37148195211411106</span>
<span style="font-weight: bold">)</span>
</pre>
</div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">estimate_training_days</span><span class="p">(</span>
    <span class="n">total_tokens_in_corpus</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">mfu_result</span><span class="p">:</span> <span class="n">MFUEstimationResult</span><span class="p">,</span>
    <span class="n">gpu_promised_flops</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">312e12</span><span class="p">,</span>  <span class="c1"># Default A100 GPU peak FLOPs</span>
    <span class="n">num_gpus</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>  <span class="c1"># Default number of GPUs</span>
    <span class="n">assumed_mfu</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># Optional manual MFU override</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Estimate the total training time in days based on the model FLOPs</span>
<span class="sd">    utilization and other training parameters.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    total_tokens_in_corpus : float</span>
<span class="sd">        Total number of tokens to be processed during training.</span>
<span class="sd">    mfu_result : MFUEstimationResult</span>
<span class="sd">        The result from the estimate_mfu function, containing FLOPs</span>
<span class="sd">        metrics and Model FLOPs Utilization.</span>
<span class="sd">    gpu_promised_flops : float, optional</span>
<span class="sd">        Theoretical peak FLOPs performance of a single GPU.</span>
<span class="sd">    num_gpus : int, optional</span>
<span class="sd">        Number of GPUs used in the training setup.</span>
<span class="sd">    assumed_mfu : float, optional</span>
<span class="sd">        If provided, overrides the MFU calculated in mfu_result to</span>
<span class="sd">        manually adjust the utilization rate.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    training_days : float</span>
<span class="sd">        Estimated total training time in days.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mfu</span> <span class="o">=</span> <span class="n">assumed_mfu</span> <span class="k">if</span> <span class="n">assumed_mfu</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">mfu_result</span><span class="o">.</span><span class="n">mfu</span>

    <span class="c1"># Total FLOPs needed for the entire dataset</span>
    <span class="n">total_flops_needed</span> <span class="o">=</span> <span class="n">total_tokens_in_corpus</span> <span class="o">*</span> <span class="n">mfu_result</span><span class="o">.</span><span class="n">flops_per_token_per_fwdbwd</span>

    <span class="c1"># Effective throughput considering MFU</span>
    <span class="n">effective_flops_per_second</span> <span class="o">=</span> <span class="n">gpu_promised_flops</span> <span class="o">*</span> <span class="n">num_gpus</span> <span class="o">*</span> <span class="n">mfu</span>

    <span class="c1"># Total training time in seconds</span>
    <span class="n">total_training_time_seconds</span> <span class="o">=</span> <span class="n">total_flops_needed</span> <span class="o">/</span> <span class="n">effective_flops_per_second</span>
    <span class="n">total_training_time_days</span> <span class="o">=</span> <span class="n">total_training_time_seconds</span> <span class="o">/</span> <span class="p">(</span><span class="mi">3600</span> <span class="o">*</span> <span class="mi">24</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">total_training_time_days</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">total_tokens_in_corpus</span> <span class="o">=</span> <span class="mf">300e9</span>  <span class="c1"># 300B tokens</span>
<span class="n">assumed_mfu</span> <span class="o">=</span> <span class="mf">0.3</span>  <span class="c1"># assume this model flops utilization</span>
<span class="n">num_gpus</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># 8 GPUs</span>
<span class="n">training_days</span> <span class="o">=</span> <span class="n">estimate_training_days</span><span class="p">(</span>
    <span class="n">total_tokens_in_corpus</span><span class="o">=</span><span class="n">total_tokens_in_corpus</span><span class="p">,</span>
    <span class="n">mfu_result</span><span class="o">=</span><span class="n">mfu_estimates</span><span class="p">,</span>
    <span class="n">gpu_promised_flops</span><span class="o">=</span><span class="n">A100</span><span class="p">()</span><span class="o">.</span><span class="n">flops</span><span class="p">[</span><span class="n">FloatingPointPrecision</span><span class="o">.</span><span class="n">BFLOAT16</span><span class="p">],</span>
    <span class="n">num_gpus</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">assumed_mfu</span><span class="o">=</span><span class="n">assumed_mfu</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">training_days</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.9626068376068373
</pre></div>
</div>
</div>
</div>
<p>For practical usage in training, see the following:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/karpathy/nanoGPT/blob/325be85d9be8c81b436728a420e85796c57dba7e/train.py#L326">Karpathy’s NanoGPT</a>
here.</p></li>
<li><p><a class="github reference external" href="https://github.com/mosaicml/composer/blob/dev/composer/callbacks/speed_monitor.py">mosaicml/composer</a></p></li>
</ul>
</section>
<section id="references-and-further-readings">
<h2><a class="toc-backref" href="#id18" role="doc-backlink">References and Further Readings</a><a class="headerlink" href="#references-and-further-readings" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>B. Shazeer, N. Parmar, Z. Lan, Y. Zhu, P. J. Liu, J. Kudugunta, E. Michel,
and A. Roberts,
<a class="reference external" href="https://arxiv.org/pdf/2204.02311.pdf">“PaLM: Scaling Language Modeling with Pathways”</a>,
arXiv preprint arXiv:2204.02311, 2022.</p></li>
<li><p>Google Research,
<a class="reference external" href="https://ai.google/static/documents/palm2techreport.pdf">“PaLM-2: Scaling Language Model Capacity and Customization with Pathways”</a>,
Google AI Blog, 2022.</p></li>
<li><p><a class="reference external" href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf">NVIDIA A100 Datasheet</a></p></li>
<li><p><a class="reference external" href="https://github.com/karpathy/nanoGPT/blob/master/transformer_sizing.ipynb">nanoGPT - Andrej Karpathy</a></p></li>
<li><p><a class="reference external" href="https://medium.com/&#64;dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4">The FLOPs Calculus of Language Model Training - Dzmitry Bahdanau on Medium</a></p></li>
<li><p><a class="reference external" href="https://github.com/keras-team/tf-keras/issues/6">TF-Keras Issue #6 on GitHub</a></p></li>
<li><p><a class="reference external" href="https://github.com/mosaicml/composer/blob/dev/composer/callbacks/speed_monitor.py">Composer Callbacks - Speed Monitor on GitHub by MosaicML</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./playbook"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../transformer/decoder/train_phase.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">The Training Phase</p>
      </div>
    </a>
    <a class="right-next"
       href="how_to_inspect_function_and_class_signatures.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">How to Inspect Function and Class Signatures in Python</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#configurations-constants-and-enums">Configurations, Constants and Enums</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#total-trainable-parameters">Total Trainable Parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-checkpoint-size-and-fluff-ratio">Calculating Checkpoint Size and Fluff Ratio</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-memory-footprint-of-loading-model-and-optimizer">GPU Memory Footprint of Loading Model and Optimizer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-flops-for-a-single-forward-pass">Estimating FLOPs for a Single Forward Pass</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basics-of-floating-point-numbers">Basics of Floating Point Numbers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#floating-point-operations-flops">Floating Point Operations (FLOPs)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#counting-flops-of-matrix-multiplications">Counting FLOPs of Matrix Multiplications</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-flops-for-a-single-forward-pass-of-gpt-2">Estimating FLOPs for a Single Forward Pass of GPT-2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sanity-check-with-palm-paper-s-flops-calculation">Sanity Check with Palm Paper’s FLOPs Calculation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#floating-point-operations-per-second-flops">Floating Point Operations Per Second (FLOPS)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#some-practical-considerations-for-flops-in-deep-learning">Some Practical Considerations for FLOPs in Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#flops-per-second-in-gpus">FLOPS Per Second in GPUs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-flops-utilization-mfu">Model FLOPs Utilization (MFU)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#theoretical-model-flops-utilization-mfu-indicates-a-rough-benchmark-of-efficiency">Theoretical Model FLOPs Utilization (MFU) Indicates a Rough Benchmark of Efficiency</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relation-of-mfu-and-tflops">Relation of MFU and TFLOPS</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#theoretical-flops-in-transformer-models">Theoretical FLOPs in Transformer Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>