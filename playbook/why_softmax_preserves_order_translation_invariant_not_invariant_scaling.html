
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling. &#8212; Omniverse</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=bb35926c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MYW5YKC2WF"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"defeq": "\\overset{\\text{def}}{=}", "defa": "\\overset{\\text{(a)}}{=}", "defb": "\\overset{\\text{(b)}}{=}", "defc": "\\overset{\\text{(c)}}{=}", "defd": "\\overset{\\text{(d)}}{=}", "st": "\\mid", "mod": "\\mid", "S": "\\Omega", "s": "\\omega", "e": "\\exp", "P": "\\mathbb{P}", "R": "\\mathbb{R}", "expectation": "\\mathbb{E}", "v": "\\mathbf{v}", "a": "\\mathbf{a}", "b": "\\mathbf{b}", "c": "\\mathbf{c}", "u": "\\mathbf{u}", "w": "\\mathbf{w}", "x": "\\mathbf{x}", "y": "\\mathbf{y}", "z": "\\mathbf{z}", "0": "\\mathbf{0}", "1": "\\mathbf{1}", "A": "\\mathbf{A}", "B": "\\mathbf{B}", "C": "\\mathbf{C}", "E": "\\mathcal{F}", "eventA": "\\mathcal{A}", "lset": "\\left\\{", "rset": "\\right\\}", "lsq": "\\left[", "rsq": "\\right]", "lpar": "\\left(", "rpar": "\\right)", "lcurl": "\\left\\{", "rcurl": "\\right\\}", "pmf": "p_X", "pdf": "f_X", "pdftwo": "f_{X,Y}", "pdfjoint": "f_{\\mathbf{X}}", "pmfjointxy": "p_{X, Y}", "pdfjointxy": "f_{X, Y}", "cdf": "F_X", "pspace": "(\\Omega, \\mathcal{F}, \\mathbb{P})", "var": "\\operatorname{Var}", "std": "\\operatorname{Std}", "bern": "\\operatorname{Bernoulli}", "binomial": "\\operatorname{Binomial}", "geometric": "\\operatorname{Geometric}", "poisson": "\\operatorname{Poisson}", "uniform": "\\operatorname{Uniform}", "normal": "\\operatorname{Normal}", "gaussian": "\\operatorname{Gaussian}", "gaussiansymbol": "\\mathcal{N}", "exponential": "\\operatorname{Exponential}", "iid": "\\textbf{i.i.d.}", "and": "\\text{and}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'playbook/why_softmax_preserves_order_translation_invariant_not_invariant_scaling';</script>
    <link rel="canonical" href="https://www.gaohongnan.com/playbook/why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="How to Inspect Function and Class Signatures in Python?" href="how_to_inspect_function_and_class_signatures.html" />
    <link rel="prev" title="How To Do Teacher-Student Knowledge Distillation?" href="training/how_to_teacher_student_knowledge_distillation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Omniverse - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Omniverse - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Omniverse
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../notations/machine_learning.html">Machine Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Influential Ideas and Papers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../influential/generative_pretrained_transformer/01_intro.html">Generative Pre-trained Transformers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../influential/generative_pretrained_transformer/02_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/generative_pretrained_transformer/03_concept.html">The Concept of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/generative_pretrained_transformer/04_implementation.html">The Implementation of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/generative_pretrained_transformer/05_adder.html">Training a Mini-GPT to Learn Two-Digit Addition</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../influential/low_rank_adaptation/01_intro.html">Low-Rank Adaptation Of Large Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../influential/low_rank_adaptation/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/low_rank_adaptation/03_implementation.html">Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../influential/empirical_risk_minimization/01_intro.html">Empirical Risk Minimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../influential/empirical_risk_minimization/02_concept.html">Concept: Empirical Risk Minimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/empirical_risk_minimization/03_bayes_optimal_classifier.html">Bayes Optimal Classifier</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../influential/learning_theory/01_intro.html">Is The Learning Problem Solvable?</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../influential/learning_theory/02_concept.html">Concept: Learning Theory</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../influential/kmeans_clustering/01_intro.html">Lloyd’s K-Means Clustering Algorithm</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../influential/kmeans_clustering/02_concept.html">Concept: K-Means Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/kmeans_clustering/03_implementation.html">Implementation: K-Means (Lloyd)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/kmeans_clustering/04_image_segmentation.html">Application: Image Compression and Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/kmeans_clustering/05_conceptual_questions.html">Conceptual Questions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../influential/naive_bayes/01_intro.html">Naive Bayes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../influential/naive_bayes/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/naive_bayes/03_implementation.html">Naives Bayes Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/naive_bayes/04_example_penguins.html">Naive Bayes Application: Penguins</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/naive_bayes/05_application_mnist.html">Naive Bayes Application (MNIST)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../influential/gaussian_mixture_models/01_intro.html">Mixture Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../influential/gaussian_mixture_models/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/gaussian_mixture_models/03_implementation.html">Gaussian Mixture Models Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../influential/linear_regression/01_intro.html">Linear Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../influential/linear_regression/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/linear_regression/03_implementation.html">Implementation</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Playbook</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="training/intro.html">Training Dynamics And Tricks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="training/how_to_calculate_flops_in_transformer_based_models.html">How to Calculate the Number of FLOPs in Transformer Based Models?</a></li>
<li class="toctree-l2"><a class="reference internal" href="training/why_cosine_annealing_warmup_stabilize_training.html">Why Does Cosine Annealing With Warmup Stabilize Training?</a></li>
<li class="toctree-l2"><a class="reference internal" href="training/how_to_finetune_decoder_with_last_token_pooling.html">How To Fine-Tune Decoder-Only Models For Sequence Classification Using Last Token Pooling?</a></li>
<li class="toctree-l2"><a class="reference internal" href="training/how_to_finetune_decoder_with_cross_attention.html">How To Fine-Tune Decoder-Only Models For Sequence Classification With Cross-Attention?</a></li>
<li class="toctree-l2"><a class="reference internal" href="training/how_to_teacher_student_knowledge_distillation.html">How To Do Teacher-Student Knowledge Distillation?</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling.</a></li>
<li class="toctree-l1"><a class="reference internal" href="how_to_inspect_function_and_class_signatures.html">How to Inspect Function and Class Signatures in Python?</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Probability Theory</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../probability_theory/01_mathematical_preliminaries/intro.html">Chapter 1. Mathematical Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/01_mathematical_preliminaries/01_combinatorics.html">Permutations and Combinations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/01_mathematical_preliminaries/02_calculus.html">Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/01_mathematical_preliminaries/03_contours.html">Contour Maps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/01_mathematical_preliminaries/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../probability_theory/02_probability/intro.html">Chapter 2. Probability</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/02_probability/0202_probability_space.html">Probability Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/02_probability/0203_probability_axioms.html">Probability Axioms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/02_probability/0204_conditional_probability.html">Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/02_probability/0205_independence.html">Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/02_probability/0206_bayes_theorem.html">Baye’s Theorem and the Law of Total Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/02_probability/summary.html">Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/intro.html">Chapter 3. Discrete Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/0301_random_variables.html">Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/0302_discrete_random_variables.html">Discrete Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/0303_probability_mass_function.html">Probability Mass Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/0304_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/0305_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/0306_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/uniform/intro.html">Discrete Uniform Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/bernoulli/intro.html">Bernoulli Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/iid.html">Independent and Identically Distributed (IID)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/binomial/intro.html">Binomial Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_implementation.html">Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_application.html">Real World Examples</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/geometric/intro.html">Geometric Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/geometric/0310_geometric_distribution_concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/poisson/intro.html">Poisson Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_implementation.html">Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/summary.html">Important</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/intro.html">Chapter 4. Continuous Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/from_discrete_to_continuous.html">From Discrete to Continuous</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0401_continuous_random_variables.html">Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0402_probability_density_function.html">Probability Density Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0403_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0404_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0405_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0406_mean_median_mode.html">Mean, Median and Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0407_continuous_uniform_distribution.html">Continuous Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0408_exponential_distribution.html">Exponential Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0409_gaussian_distribution.html">Gaussian Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0410_skewness_and_kurtosis.html">Skewness and Kurtosis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0411_convolve_and_sum_of_random_variables.html">Convolution and Sum of Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0412_functions_of_random_variables.html">Functions of Random Variables</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../probability_theory/05_joint_distributions/intro.html">Chapter 5. Joint Distributions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/05_joint_distributions/from_single_variable_to_joint_distributions.html">From Single Variable to Joint Distributions</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/intro.html">Joint PMF and PDF</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/intro.html">Joint Expectation and Correlation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/intro.html">Conditional PMF and PDF</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/intro.html">Conditional Expectation and Variance</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/05_joint_distributions/0505_sum_of_random_variables/intro.html">Sum of Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0505_sum_of_random_variables/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/05_joint_distributions/0506_random_vectors/intro.html">Random Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0506_random_vectors/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/05_joint_distributions/0507_multivariate_gaussian/intro.html">Multivariate Gaussian Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0507_multivariate_gaussian/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0507_multivariate_gaussian/application_transformation.html">Application: Plots and Transformations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0507_multivariate_gaussian/psd.html">Covariance Matrix is Positive Semi-Definite</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0507_multivariate_gaussian/eigendecomposition.html">Eigendecomposition and Covariance Matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0507_multivariate_gaussian/geometry_of_multivariate_gaussian.html">The Geometry of Multivariate Gaussians</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../probability_theory/06_sample_statistics/intro.html">Chapter 6. Sample Statistics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/intro.html">Moment Generating and Characteristic Functions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function.html">Moment Generating Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function_application_sum_of_rv.html">Application: Moment Generating Function and the Sum of Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/characteristic_function.html">Characteristic Function</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/06_sample_statistics/0602_probability_inequalities/intro.html">Probability Inequalities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/06_sample_statistics/0602_probability_inequalities/concept.html">Probability Inequalities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/06_sample_statistics/0602_probability_inequalities/application.html">Application: Learning Theory</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/06_sample_statistics/0603_law_of_large_numbers/intro.html">Law of Large Numbers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/06_sample_statistics/0603_law_of_large_numbers/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/06_sample_statistics/0603_law_of_large_numbers/convergence.html">Convergence of Sample Average</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/06_sample_statistics/0603_law_of_large_numbers/application.html">Application: Learning Theory</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../probability_theory/08_estimation_theory/intro.html">Chapter 8. Estimation Theory</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/08_estimation_theory/maximum_likelihood_estimation/intro.html">Maximum Likelihood Estimation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html">Concept</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Operations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../operations/distributed/intro.html">Distributed Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../operations/distributed/01_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/distributed/02_basics.html">Basics Of Distributed Data Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/distributed/03_how_to_setup_slurm_in_aws.html">How to Setup SLURM and ParallelCluster in AWS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/distributed/04_ablation.html">Ablations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../operations/profiling/intro.html">Profiling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../operations/profiling/01_synchronize.html">Synchronize CUDA To Time CUDA Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/profiling/02_timeit.html">Profiling Code With Timeit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/profiling/03_time_profiler.html">PyTorch’s Event And Profiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/profiling/04_small_gpt_profile.html">Profile GPT Small Time And Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/profiling/05_memory_leak.html">CUDA Memory Allocations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../operations/machine_learning_lifecycle/00_intro.html">The Lifecycle of an AIOps System</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../operations/machine_learning_lifecycle/01_problem_formulation.html">Stage 1. Problem Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/machine_learning_lifecycle/02_project_scoping.html">Stage 2. Project Scoping And Framing The Problem</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../operations/machine_learning_lifecycle/03_dataops_pipeline/03_dataops_pipeline.html">Stage 3. Data Pipeline (Data Engineering and DataOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../operations/machine_learning_lifecycle/03_dataops_pipeline/031_data_source_and_format.html">Stage 3.1. Data Source and Formats</a></li>
<li class="toctree-l3"><a class="reference internal" href="../operations/machine_learning_lifecycle/03_dataops_pipeline/032_data_model_and_storage.html">Stage 3.2. Data Model and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../operations/machine_learning_lifecycle/03_dataops_pipeline/033_etl.html">Stage 3.3. Extract, Transform, Load (ETL)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/machine_learning_lifecycle/04_mlops_data_pipeline.html">Stage 4. Data Extraction (MLOps), Data Analysis (Data Science), Data Preparation (Data Science)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../operations/machine_learning_lifecycle/05_model_development_selection_and_training/05_ml_training_pipeline.html">Stage 5. Model Development and Training (MLOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../operations/machine_learning_lifecycle/05_model_development_selection_and_training/051_model_selection.html">Stage 5.1. Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../operations/machine_learning_lifecycle/05_model_development_selection_and_training/052_metric_selection.html">Stage 5.2. Metric Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../operations/machine_learning_lifecycle/05_model_development_selection_and_training/053_experiment_tracking.html">Stage 5.3. Experiment Tracking And Versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../operations/machine_learning_lifecycle/05_model_development_selection_and_training/054_model_testing.html">Stage 5.4. Model Testing</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/machine_learning_lifecycle/06_model_evaluation.html">Stage 6. Model Evaluation (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/machine_learning_lifecycle/07_model_validation_registry_and_pushing_model_to_production.html">Stage 7. Model Validation, Registry and Pushing Model to Production (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/machine_learning_lifecycle/08_model_deployment_and_serving.html">Stage 8. Model Serving (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/machine_learning_lifecycle/09_model_monitoring.html">Stage 9. Model Monitoring (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/machine_learning_lifecycle/010_continuous_integration_deployment_learning_and_training.html">Stage 10. Continuous Integration, Deployment, Learning and Training (DevOps, DataOps, MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/machine_learning_lifecycle/011_infrastructure_and_tooling_for_mlops.html">Stage 11. Infrastructure and Tooling for MLOps</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../software_engineering/config_management/intro.html">Config, State, Metadata Management</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/config_management/concept.html">Configuration Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/config_management/01-pydra.html">Pydantic And Hydra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/config_management/02-state.html">State And Metadata Management</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../software_engineering/design_patterns/intro.html">Design Patterns</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/design_patterns/dependency_inversion_principle.html">Dependency Inversion Principle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/design_patterns/named_constructor.html">Named Constructor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/design_patterns/strategy.html">Strategy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/design_patterns/registry.html">Registry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/design_patterns/god_object_pattern.html">Context Object Pattern (God Object)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/design_patterns/factory_method.html">Factory Method</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../software_engineering/python/intro.html">Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/python/decorator.html">Decorator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/python/pydantic.html">Pydantic Is All You Need - Jason Liu</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../software_engineering/concurrency_parallelism_asynchronous/intro.html">Concurrency, Parallelism and Asynchronous Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/concurrency_parallelism_asynchronous/overview.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/concurrency_parallelism_asynchronous/generator_yield.html">A Rudimentary Introduction to Generator and Yield in Python</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../computer_science/type_theory/intro.html">Type Theory, A Very Rudimentary Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/01-subtypes.html">Subtypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/02-type-safety.html">Type Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/03-subsumption.html">Subsumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/04-generics.html">Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/05-typevar-bound-constraints.html">Bound and Constraint in Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/06-invariance-covariance-contravariance.html">Invariance, Covariance and Contravariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/07-pep-3124-overloading.html">Function Overloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/08-pep-661-sentinel-values.html">Sentinel Types</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Structures and Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/complexity_analysis/intro.html">Complexity Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/complexity_analysis/master_theorem.html">Master Theorem </a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/stack/intro.html">Stack</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/stack/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/searching_algorithms/linear_search/intro.html">Linear Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/searching_algorithms/linear_search/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/searching_algorithms/binary_search/intro.html">Binary Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/searching_algorithms/binary_search/concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dsa/searching_algorithms/binary_search/problems/875-koko-eating-bananas.html">Koko Eating Bananas</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../linear_algebra/01_preliminaries/intro.html">Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/01_preliminaries/01-fields.html">Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/01_preliminaries/02-systems-of-linear-equations.html">Systems of Linear Equations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../linear_algebra/02_vectors/intro.html">Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/01-vector-definition.html">Vector and Its Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/02-vector-operation.html">Vector and Its Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/03-vector-norm.html">Vector Norm and Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/04-vector-products.html">A First Look at Vector Products</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../citations.html">IEEE (Style) Citations</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/gao-hongnan/omniverse/blob/main/omniverse/playbook/why_softmax_preserves_order_translation_invariant_not_invariant_scaling.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse/issues/new?title=Issue%20on%20page%20%2Fplaybook/why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/playbook/why_softmax_preserves_order_translation_invariant_not_invariant_scaling.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../_sources/playbook/why_softmax_preserves_order_translation_invariant_not_invariant_scaling.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling.</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-formulation">Problem Formulation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#functional-form-of-f">Functional Form of <span class="math notranslate nohighlight">\(f\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-interpretation">Probabilistic Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unnormalized-logits">Unnormalized Logits</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-function">Softmax Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-obeys-the-three-axioms-of-probability-kolmogorov-axioms">Softmax Obeys The Three Axioms of Probability (Kolmogorov Axioms)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#non-negativity">Non-Negativity</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization">Normalization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#additivity">Additivity</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#calibration">Calibration</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-preserves-order-monotonicity">Softmax Preserves Order (Monotonicity)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-is-translation-invariance">Softmax Is Translation Invariance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-instability-of-the-softmax-function">Numerical Instability of the Softmax Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-is-not-invariant-under-scaling">Softmax Is Not Invariant Under Scaling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sharpening-and-dampening-the-softmax-distribution">Sharpening and Dampening the Softmax Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#temperature">Temperature</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-from-the-softmax-distribution">Sampling from the Softmax Distribution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-is-smooth-continuous-and-differentiable">Softmax Is Smooth, Continuous and Differentiable</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-function-via-exponential-family">Softmax Function via Exponential Family</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-jacobian-and-hessian-of-softmax">Gradient, Jacobian, and Hessian of Softmax</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-as-a-vector-function">Softmax as a Vector Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivatives-of-the-softmax-function">Derivatives of the Softmax Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-jacobian-matrix-of-softmax">The Jacobian Matrix of Softmax</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivative-of-the-softmax-function">Derivative of the Softmax Function</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#case-1-i-j">Case 1: <span class="math notranslate nohighlight">\(i = j\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#case-2-i-neq-j">Case 2: <span class="math notranslate nohighlight">\(i \neq j\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kronecker-delta">Kronecker Delta</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#representing-derivative-of-softmax-as-a-jacobian-matrix">Representing Derivative of Softmax as a Jacobian Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-of-the-jacobian-matrix">Implementation of the Jacobian Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivative-of-softmax-with-respect-to-weight-matrix">Derivative Of Softmax With Respect To Weight Matrix</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-derivative-of-mathcal-l-with-respect-to-mathbf-z">Step 1: Derivative of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> with respect to <span class="math notranslate nohighlight">\(\mathbf{z}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-derivative-of-mathbf-z-with-respect-to-boldsymbol-theta">Step 2: Derivative of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-derivative-of-mathcal-l-with-respect-to-boldsymbol-theta">Step 3: Derivative of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hessian-matrix">Hessian Matrix</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#citations">Citations</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="softmax-preserves-order-is-translation-invariant-but-not-invariant-under-scaling">
<h1>Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling.<a class="headerlink" href="#softmax-preserves-order-is-translation-invariant-but-not-invariant-under-scaling" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://twitter.com/gaohongnan"><img alt="Twitter Handle" src="https://img.shields.io/badge/Twitter-&#64;gaohongnan-blue?style=social&amp;logo=twitter" /></a>
<a class="reference external" href="https://linkedin.com/in/gao-hongnan"><img alt="LinkedIn Profile" src="https://img.shields.io/badge/&#64;gaohongnan-blue?style=social&amp;logo=linkedin" /></a>
<a class="reference external" href="https://github.com/gao-hongnan"><img alt="GitHub Profile" src="https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&amp;logo=github" /></a>
<img alt="Tag" src="https://img.shields.io/badge/Tag-Brain_Dump-red" />
<img alt="Tag" src="https://img.shields.io/badge/Level-Beginner-green" /></p>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#problem-formulation" id="id8">Problem Formulation</a></p>
<ul>
<li><p><a class="reference internal" href="#functional-form-of-f" id="id9">Functional Form of <span class="math notranslate nohighlight">\(f\)</span></a></p></li>
<li><p><a class="reference internal" href="#probabilistic-interpretation" id="id10">Probabilistic Interpretation</a></p></li>
<li><p><a class="reference internal" href="#unnormalized-logits" id="id11">Unnormalized Logits</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#softmax-function" id="id12">Softmax Function</a></p>
<ul>
<li><p><a class="reference internal" href="#softmax-obeys-the-three-axioms-of-probability-kolmogorov-axioms" id="id13">Softmax Obeys The Three Axioms of Probability (Kolmogorov Axioms)</a></p>
<ul>
<li><p><a class="reference internal" href="#non-negativity" id="id14">Non-Negativity</a></p></li>
<li><p><a class="reference internal" href="#normalization" id="id15">Normalization</a></p></li>
<li><p><a class="reference internal" href="#additivity" id="id16">Additivity</a></p></li>
<li><p><a class="reference internal" href="#calibration" id="id17">Calibration</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#implementation" id="id18">Implementation</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#softmax-preserves-order-monotonicity" id="id19">Softmax Preserves Order (Monotonicity)</a></p></li>
<li><p><a class="reference internal" href="#softmax-is-translation-invariance" id="id20">Softmax Is Translation Invariance</a></p>
<ul>
<li><p><a class="reference internal" href="#numerical-instability-of-the-softmax-function" id="id21">Numerical Instability of the Softmax Function</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#softmax-is-not-invariant-under-scaling" id="id22">Softmax Is Not Invariant Under Scaling</a></p></li>
<li><p><a class="reference internal" href="#sharpening-and-dampening-the-softmax-distribution" id="id23">Sharpening and Dampening the Softmax Distribution</a></p></li>
<li><p><a class="reference internal" href="#temperature" id="id24">Temperature</a></p>
<ul>
<li><p><a class="reference internal" href="#sampling-from-the-softmax-distribution" id="id25">Sampling from the Softmax Distribution</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#softmax-is-smooth-continuous-and-differentiable" id="id26">Softmax Is Smooth, Continuous and Differentiable</a></p></li>
<li><p><a class="reference internal" href="#softmax-function-via-exponential-family" id="id27">Softmax Function via Exponential Family</a></p></li>
<li><p><a class="reference internal" href="#gradient-jacobian-and-hessian-of-softmax" id="id28">Gradient, Jacobian, and Hessian of Softmax</a></p>
<ul>
<li><p><a class="reference internal" href="#softmax-as-a-vector-function" id="id29">Softmax as a Vector Function</a></p></li>
<li><p><a class="reference internal" href="#derivatives-of-the-softmax-function" id="id30">Derivatives of the Softmax Function</a></p></li>
<li><p><a class="reference internal" href="#the-jacobian-matrix-of-softmax" id="id31">The Jacobian Matrix of Softmax</a></p></li>
<li><p><a class="reference internal" href="#derivative-of-the-softmax-function" id="id32">Derivative of the Softmax Function</a></p>
<ul>
<li><p><a class="reference internal" href="#case-1-i-j" id="id33">Case 1: <span class="math notranslate nohighlight">\(i = j\)</span></a></p></li>
<li><p><a class="reference internal" href="#case-2-i-neq-j" id="id34">Case 2: <span class="math notranslate nohighlight">\(i \neq j\)</span></a></p></li>
<li><p><a class="reference internal" href="#kronecker-delta" id="id35">Kronecker Delta</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#representing-derivative-of-softmax-as-a-jacobian-matrix" id="id36">Representing Derivative of Softmax as a Jacobian Matrix</a></p></li>
<li><p><a class="reference internal" href="#implementation-of-the-jacobian-matrix" id="id37">Implementation of the Jacobian Matrix</a></p></li>
<li><p><a class="reference internal" href="#derivative-of-softmax-with-respect-to-weight-matrix" id="id38">Derivative Of Softmax With Respect To Weight Matrix</a></p>
<ul>
<li><p><a class="reference internal" href="#step-1-derivative-of-mathcal-l-with-respect-to-mathbf-z" id="id39">Step 1: Derivative of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> with respect to <span class="math notranslate nohighlight">\(\mathbf{z}\)</span></a></p></li>
<li><p><a class="reference internal" href="#step-2-derivative-of-mathbf-z-with-respect-to-boldsymbol-theta" id="id40">Step 2: Derivative of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span></a></p></li>
<li><p><a class="reference internal" href="#step-3-derivative-of-mathcal-l-with-respect-to-boldsymbol-theta" id="id41">Step 3: Derivative of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#hessian-matrix" id="id42">Hessian Matrix</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#references-and-further-readings" id="id43">References and Further Readings</a></p></li>
<li><p><a class="reference internal" href="#citations" id="id44">Citations</a></p></li>
</ul>
</nav>
<section id="problem-formulation">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">Problem Formulation</a><a class="headerlink" href="#problem-formulation" title="Link to this heading">#</a></h2>
<p>Consider a classification problem with a dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> with <span class="math notranslate nohighlight">\(N\)</span> samples,
defined formally below,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{S} \overset{\mathrm{iid}}{\sim} \mathcal{D} = \left \{ \left(\mathbf{x}^{(n)}, y^{(n)} \right) \right \}_{n=1}^N \in \left(\mathcal{X}, \mathcal{Y} \right)^N
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is the input space, <span class="math notranslate nohighlight">\(\mathcal{X} \subseteq \mathbb{R}^{D}\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> is the output space, <span class="math notranslate nohighlight">\(\mathcal{Y} \subseteq \mathbb{Z}\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}^{(n)} \in \mathcal{X}\)</span> is the input feature vector of the <span class="math notranslate nohighlight">\(n\)</span>-th
sample,</p></li>
<li><p><span class="math notranslate nohighlight">\(y^{(n)} \in \mathcal{Y}\)</span> is the output label of the <span class="math notranslate nohighlight">\(n\)</span>-th sample,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is the underlying, true but unknown, data distribution,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{S}\)</span> is the dataset, a finite sample drawn <span class="math notranslate nohighlight">\(\mathrm{iid}\)</span> (the
independent and identically distributed assumption) from <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</p></li>
</ul>
<p>Given the classification setuo, the goal is to define a functional form
<span class="math notranslate nohighlight">\(f(\cdot)\)</span> that maps an input vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to a predicted output label
<span class="math notranslate nohighlight">\(y\)</span>. This mapping is typically represented by a function
<span class="math notranslate nohighlight">\(f: \mathcal{X} \rightarrow \mathcal{Y}\)</span>, which takes an input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> from
the input space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and predicts an output label <span class="math notranslate nohighlight">\(y\)</span> from the output
space <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>. Consequently, for a classification problem with <span class="math notranslate nohighlight">\(K\)</span>
discrete classes <span class="math notranslate nohighlight">\(\mathcal{C}_{k}\)</span>, where <span class="math notranslate nohighlight">\(k=1 , \ldots, K\)</span>, the output space
<span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> corresponds to the set of these <span class="math notranslate nohighlight">\(K\)</span> classes, often represented as
<span class="math notranslate nohighlight">\(\{1, 2, \ldots, K\}\)</span> or <span class="math notranslate nohighlight">\(\{0, 1, \ldots, K-1\}\)</span>, depending on the indexing
convention. Therefore, <span class="math notranslate nohighlight">\(y \in \mathcal{Y}\)</span> is the class label assigned to the
input vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> <span id="id1">[<a class="reference internal" href="../bibliography.html#id4" title="Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer, 1 edition, 2007. ISBN 0387310738. URL: http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738%3FSubscriptionId%3D13CT5CVB80YFWJEPWS02%26tag%3Dws%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0387310738.">Bishop, 2007</a>]</span>.</p>
<section id="functional-form-of-f">
<h3><a class="toc-backref" href="#id9" role="doc-backlink">Functional Form of <span class="math notranslate nohighlight">\(f\)</span></a><a class="headerlink" href="#functional-form-of-f" title="Link to this heading">#</a></h3>
<p>The function <span class="math notranslate nohighlight">\(f\)</span> can be explicitly defined in terms of parameters that need to
be learned from the data <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>. A common approach in machine learning is
to use a parameterized model <span class="math notranslate nohighlight">\(f_{\boldsymbol{\theta}}\)</span>, where
<span class="math notranslate nohighlight">\(\boldsymbol{\theta} \in \boldsymbol{\Theta}\)</span> denotes the parameters of the
model. This model aims to approximate the true but unknown function that maps
inputs to outputs in the underlying data distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</p>
<p>For a given input vector <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span>, the functional form of the model
can be expressed as:</p>
<div class="math notranslate nohighlight">
\[
y^{(n)} = f_{\boldsymbol{\theta}}\left(\mathbf{x}^{(n)}\right)
\]</div>
<p>and the functional form of the estimated label <span class="math notranslate nohighlight">\(\hat{y}^{(n)}\)</span> can be expressed
as:</p>
<div class="math notranslate nohighlight">
\[
\hat{y}^{(n)} = f_{\hat{\boldsymbol{\theta}}}\left(\mathbf{x}^{(n)}\right)
\]</div>
<p>For simplicity, we would consider the family of <strong><em>linear models</em></strong>, where the
functional form can be expressed as:</p>
<div class="math notranslate nohighlight">
\[
f_{\boldsymbol{\theta}}\left(\mathbf{x}\right) = \boldsymbol{\theta}^{\top} \mathbf{x} + b
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\theta} \in \mathbb{R}^{D}\)</span> is the weight vector,
<span class="math notranslate nohighlight">\(b \in \mathbb{R}\)</span> is the bias term, and
<span class="math notranslate nohighlight">\(\boldsymbol{\theta} = \{\boldsymbol{\theta}, b\}\)</span> are the parameters of the
model. We then seek <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}\)</span> that minimizes the discrepancy
between the predicted label <span class="math notranslate nohighlight">\(\hat{y}^{(n)}\)</span> and the true label <span class="math notranslate nohighlight">\(y^{(n)}\)</span> for all
samples in the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> via an optimization procedure (learning
algorithm) <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> over some loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>.</p>
<p>It is also worth noting the functional form is usually extended to include an
activation (inverse is the link function in statistics theory) function
<span class="math notranslate nohighlight">\(\sigma(\cdot)\)</span>, which introduces non-linearity to the model, and more
importantly, ensures the output of the model is a valid probability distribution
over the classes (note ensuring the output is a valid probability distribution
does not equate to the model being well-calibrated). In our context, we would
treat the activation function as the <strong><em>softmax function</em></strong> and is only applied
to the output layer of the model.</p>
<div class="math notranslate nohighlight">
\[
f_{\boldsymbol{\theta}}\left(\mathbf{x}\right) = \sigma\left(\underbrace{\boldsymbol{\theta}^{\top} \mathbf{x} + b}_{\mathbf{z}}\right)
\]</div>
<p>In the setting of multiclass classification, it is common to represent the
predicted <span class="math notranslate nohighlight">\(\hat{y}\)</span> as a <span class="math notranslate nohighlight">\(K\)</span>-dimensional vector, where <span class="math notranslate nohighlight">\(K\)</span> is the number of
classes. And thus, the output of the model can be expressed as:</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{y}} := f_{\boldsymbol{\theta}}\left(\mathbf{x}\right) = \begin{bmatrix} \hat{y}_1 &amp; \hat{y}_2 &amp; \cdots &amp; \hat{y}_K \end{bmatrix}
\]</div>
<p>with <span class="math notranslate nohighlight">\(\hat{y}_k = p\left(y = \mathcal{C}_k \mid \mathbf{x}\right)\)</span>, the
probability of the input vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> belonging to class <span class="math notranslate nohighlight">\(\mathcal{C}_k\)</span>.</p>
<p>To this end, it would be incomplete not to mention the probabilistic
interpretation of the classification problem. We state very briefly in the next
section. For a more rigorous treatment, we refer the reader to chapter 4 of
<em>Pattern Recognition and Machine Learning</em> by Bishop <span id="id2">[<a class="reference internal" href="../bibliography.html#id4" title="Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer, 1 edition, 2007. ISBN 0387310738. URL: http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738%3FSubscriptionId%3D13CT5CVB80YFWJEPWS02%26tag%3Dws%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0387310738.">Bishop, 2007</a>]</span>.</p>
</section>
<section id="probabilistic-interpretation">
<h3><a class="toc-backref" href="#id10" role="doc-backlink">Probabilistic Interpretation</a><a class="headerlink" href="#probabilistic-interpretation" title="Link to this heading">#</a></h3>
<p>We can also divide the classification to either <strong><em>discriminative</em></strong> or
<strong><em>generative</em></strong> models. Discriminative models learn the decision boundary
directly from the data, while generative models learn the joint distribution of
the input and output, and then use Bayes’ rule to infer the decision boundary.</p>
<p>For example, if we look through the multiclass problem through the lens of
generative models, our goal is basically to find
<span class="math notranslate nohighlight">\(p\left(\mathcal{C}_k \mid \mathbf{x}\right)\)</span>. One approach to determine this
conditional probability is to adopt a generative approach in which we model the
class-conditional densities given by
<span class="math notranslate nohighlight">\(p\left(\mathbf{x} \mid \mathcal{C}_k\right)\)</span>, together with the prior
probabilities <span class="math notranslate nohighlight">\(p\left(\mathcal{C}_k\right)\)</span> for the classes, and then we compute
the required posterior probabilities using Bayes’ theorem <span id="id3">[<a class="reference internal" href="../bibliography.html#id4" title="Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer, 1 edition, 2007. ISBN 0387310738. URL: http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738%3FSubscriptionId%3D13CT5CVB80YFWJEPWS02%26tag%3Dws%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0387310738.">Bishop, 2007</a>]</span>,</p>
<div class="math notranslate nohighlight">
\[
p\left(\mathcal{C}_k \mid \mathbf{x} ; \boldsymbol{\theta} \right)=\frac{p\left(\mathbf{x} \mid \mathcal{C}_k\right) p\left(\mathcal{C}_k\right)}{p(\mathbf{x})}
\]</div>
</section>
<section id="unnormalized-logits">
<h3><a class="toc-backref" href="#id11" role="doc-backlink">Unnormalized Logits</a><a class="headerlink" href="#unnormalized-logits" title="Link to this heading">#</a></h3>
<p>At this juncture, we would take a look at what happens before the activation
function, in our case, the softmax function is applied. The linear projection
layer, often the head/output layer, of the model
<span class="math notranslate nohighlight">\(f_{\boldsymbol{\theta}}\left(\mathbf{x}\right)\)</span> (pre-softmax), yields what we
often referred to as the <strong><em>logits</em></strong> or <strong><em>unnormalized scores</em></strong>. We often
denote the logits as <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, and it is a <span class="math notranslate nohighlight">\(K\)</span>-dimensional vector, where <span class="math notranslate nohighlight">\(K\)</span>
is the number of classes.</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z} \in \mathbb{R}^{K} = \boldsymbol{\theta}^{\top} \mathbf{x} + b
\]</div>
<p>where each element <span class="math notranslate nohighlight">\(z_k\)</span> of the vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is the unnormalized score of
the <span class="math notranslate nohighlight">\(k\)</span>-th class.</p>
<p>Since the logits are unnormalized and unbounded, it follows that we need to
<em>induce</em> the model to produce a valid probability distribution over the classes.
This is where the softmax function comes into play.</p>
<div class="warning admonition">
<p class="admonition-title">Enough is Enough</p>
<p>I don’t even know enough to continue the theoretical blabbering. I would stop
here and transit to the highlight and regurgitate the key points of the softmax
function.</p>
</div>
</section>
</section>
<section id="softmax-function">
<h2><a class="toc-backref" href="#id12" role="doc-backlink">Softmax Function</a><a class="headerlink" href="#softmax-function" title="Link to this heading">#</a></h2>
<p>The softmax function (a <strong><em>vector function</em></strong>) takes as input a vector
<span class="math notranslate nohighlight">\(\mathbf{z} = \begin{bmatrix} z_1 &amp; z_2 &amp; \cdots &amp; z_K \end{bmatrix}^{\top} \in \mathbb{R}^{K}\)</span>
of <span class="math notranslate nohighlight">\(K\)</span> real numbers, and normalizes it into a probability distribution
consisting of <span class="math notranslate nohighlight">\(K\)</span> probabilities proportional to the exponentials of the input
numbers. That is, prior to applying softmax, some vector components could be
negative, or greater than one; and might not sum to 1 ; but after applying
softmax, each component will be in the interval <span class="math notranslate nohighlight">\((0,1)\)</span>, and the components will
add up to <span class="math notranslate nohighlight">\(1\)</span>, so that they can be interpreted as probabilities. Furthermore,
the larger input components will correspond to larger probabilities.</p>
<p>For a vector <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^{K}\)</span> of <span class="math notranslate nohighlight">\(K\)</span> real numbers, the standard
(unit) softmax function <span class="math notranslate nohighlight">\(\sigma: \mathbb{R}^K \mapsto(0,1)^K\)</span>, where <span class="math notranslate nohighlight">\(K \geq 1\)</span>,
is defined by the formula</p>
<div class="math notranslate nohighlight">
\[
\sigma(\mathbf{z})_j=\frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} \text { for } j=1, \ldots, K
\]</div>
<p>More concretely, we can write explicitly the mapping as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\sigma(\cdot): \mathbb{R}^{K} &amp; \rightarrow(0,1)^{K} \\
\mathbf{z} &amp; \mapsto \begin{bmatrix} \sigma(\mathbf{z})_1 &amp; \sigma(\mathbf{z})_2 &amp; \cdots &amp; \sigma(\mathbf{z})_K \end{bmatrix}^{\top} \\
\begin{bmatrix} z_1 &amp; z_2 &amp; \cdots &amp; z_K \end{bmatrix}^{\top} &amp; \mapsto \begin{bmatrix} \frac{e^{z_1}}{\sum_{k=1}^K e^{z_k}} &amp; \frac{e^{z_2}}{\sum_{k=1}^K e^{z_k}} &amp; \cdots &amp; \frac{e^{z_K}}{\sum_{k=1}^K e^{z_k}} \end{bmatrix}^{\top}
\end{aligned}
\end{split}\]</div>
<p>In particular, for any element <span class="math notranslate nohighlight">\(z_j\)</span> of the input vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, the
softmax function computes the exponential of the element and normalizes it by
the sum of the exponentials of all the elements in the input vector.</p>
<section id="softmax-obeys-the-three-axioms-of-probability-kolmogorov-axioms">
<h3><a class="toc-backref" href="#id13" role="doc-backlink">Softmax Obeys The Three Axioms of Probability (Kolmogorov Axioms)</a><a class="headerlink" href="#softmax-obeys-the-three-axioms-of-probability-kolmogorov-axioms" title="Link to this heading">#</a></h3>
<p>First of all, softmax induces a valid probability distribution over the classes.
Why so? We first need to understand the three axioms of probability, also known
as the Kolmogorov axioms.</p>
<p>Consider the probability space defined over the triplet
<span class="math notranslate nohighlight">\(\left(\Omega, \mathcal{F}, \mathbb{P}\right)\)</span>, where <span class="math notranslate nohighlight">\(\Omega\)</span> is the sample
space, <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is the sigma-algebra event space (collection of events),
and <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> is the probability function.</p>
<p>A probability function <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> defined over the probability space must
satisfy the three axioms below. Recall that the <strong>probability function</strong> in a
well defined <strong>experiment</strong> is a function <span class="math notranslate nohighlight">\(\mathbb{P}: \mathcal{F} \to [0, 1]\)</span>.
Informally, for any event <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(\mathbb{P}(A)\)</span> is defined as the probability of
event <span class="math notranslate nohighlight">\(A\)</span> happening.</p>
<p>This probability function/law <span class="math notranslate nohighlight">\(\mathbb{P}(A)\)</span> must satisfy the following three
axioms:</p>
<div class="proof axiom admonition" id="how-does-softmax-work-non-negativity">
<p class="admonition-title"><span class="caption-number">Axiom 1 </span> (Non-Negativity)</p>
<section class="axiom-content" id="proof-content">
<p><span class="math notranslate nohighlight">\(\mathbb{P}(A) \geq 0\)</span> for any event <span class="math notranslate nohighlight">\(A \subseteq \S\)</span>.</p>
</section>
</div><div class="proof axiom admonition" id="how-does-softmax-work-normalization">
<p class="admonition-title"><span class="caption-number">Axiom 2 </span> (Normalization)</p>
<section class="axiom-content" id="proof-content">
<p><span class="math notranslate nohighlight">\(\sum_{i=1}^{n}\mathbb{P}(A_i) = 1\)</span>
where <span class="math notranslate nohighlight">\(A_i\)</span> are all possible outcomes for <span class="math notranslate nohighlight">\(i = 1, 2,..., n\)</span>.</p>
</section>
</div><div class="proof axiom admonition" id="how-does-softmax-work-additivity">
<p class="admonition-title"><span class="caption-number">Axiom 3 </span> (Additivity)</p>
<section class="axiom-content" id="proof-content">
<p>Given a countable sequence of
<strong>disjoint events</strong> <span class="math notranslate nohighlight">\(A_1, A_2, ..., A_n,... \subset \S\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}\left(\bigsqcup_{i=1}^{\infty} A_i \right) = \sum_{i=1}^{\infty}\mathbb{P}[A_i]
\]</div>
</section>
</div><section id="non-negativity">
<h4><a class="toc-backref" href="#id14" role="doc-backlink">Non-Negativity</a><a class="headerlink" href="#non-negativity" title="Link to this heading">#</a></h4>
<p>To satisfy the first axiom, we need to show that <span class="math notranslate nohighlight">\(\sigma(\mathbf{z})_j \geq 0\)</span>
for all <span class="math notranslate nohighlight">\(j\)</span>.</p>
<p>It is easy to see that the exponential function <span class="math notranslate nohighlight">\(e^{z_j}\)</span> is always positive for
any real number <span class="math notranslate nohighlight">\(z_j\)</span>. Therefore, <span class="math notranslate nohighlight">\(e^{z_j} &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>. The denominator is
a sum of positive terms, thus also positive. A positive number divided by
another positive number is positive, hence <span class="math notranslate nohighlight">\(\sigma(\mathbf{z})_j &gt; 0\)</span> for all
<span class="math notranslate nohighlight">\(i\)</span>. This satisfies the non-negativity axiom.</p>
</section>
<section id="normalization">
<h4><a class="toc-backref" href="#id15" role="doc-backlink">Normalization</a><a class="headerlink" href="#normalization" title="Link to this heading">#</a></h4>
<p>To satisfy the second axiom, we need to prove that the sum of the softmax
function outputs equals 1, i.e., <span class="math notranslate nohighlight">\(\sum_{k=1}^{K} \sigma(\mathbf{z})_k = 1\)</span>.</p>
<p>By definition of softmax, for any element <span class="math notranslate nohighlight">\(z_j\)</span> of the input vector
<span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, we have
<span class="math notranslate nohighlight">\(\sigma(\mathbf{z})_j = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}}\)</span>, and summing
over all <span class="math notranslate nohighlight">\(K\)</span> classes, we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\sum_{j=1}^{K} \sigma(\mathbf{z})_j &amp; = \sum_{j=1}^{K} \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}} \\
&amp; = \frac{\sum_{j=1}^{K} e^{z_j}}{\sum_{k=1}^{K} e^{z_k}} \\
&amp; = 1
\end{aligned}
\end{split}\]</div>
<p>This shows that the sum of the outputs of the softmax function equals 1,
satisfying the normalization axiom.</p>
</section>
<section id="additivity">
<h4><a class="toc-backref" href="#id16" role="doc-backlink">Additivity</a><a class="headerlink" href="#additivity" title="Link to this heading">#</a></h4>
<p>This is evident because if we treat each instance <span class="math notranslate nohighlight">\(z_j\)</span> as a disjoint event
since each <span class="math notranslate nohighlight">\(z_j\)</span> can only belong to one class, then the events (or classes) form
a countable sequence of disjoint events.</p>
</section>
<section id="calibration">
<h4><a class="toc-backref" href="#id17" role="doc-backlink">Calibration</a><a class="headerlink" href="#calibration" title="Link to this heading">#</a></h4>
<p>While softmax ensures that the output of the model is a valid probability
distribution over the classes, it does not guarantee that the model is
well-calibrated. A well-calibrated model is one that produces predicted
probabilities that are close to the true probabilities.</p>
</section>
</section>
<section id="implementation">
<h3><a class="toc-backref" href="#id18" role="doc-backlink">Implementation</a><a class="headerlink" href="#implementation" title="Link to this heading">#</a></h3>
<p>Here we show a simple implementation of the softmax function in PyTorch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="k">class</span> <span class="nc">Softmax</span><span class="p">:</span>
<span class="linenos"> 2</span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="linenos"> 3</span><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos"> 4</span><span class="sd">        Initialize the softmax function.</span>
<span class="linenos"> 5</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos"> 6</span>        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
<span class="linenos"> 7</span>
<span class="linenos"> 8</span>    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="linenos"> 9</span><span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">10</span><span class="sd">        Compute the softmax function for a given input.</span>
<span class="linenos">11</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos">12</span>        <span class="n">numerator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="linenos">13</span>        <span class="n">denominator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">numerator</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos">14</span>        <span class="n">g</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>
<span class="linenos">15</span>        <span class="k">return</span> <span class="n">g</span>
</pre></div>
</div>
</div>
</div>
<p>We do a rough comparison of the softmax function implemented in PyTorch with the
readily available implementation in PyTorch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="linenos"> 2</span><span class="n">pytorch_softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos"> 3</span><span class="n">pytorch_softmax_probs</span> <span class="o">=</span> <span class="n">pytorch_softmax</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="linenos"> 4</span><span class="n">pprint</span><span class="p">(</span><span class="n">pytorch_softmax_probs</span><span class="p">)</span>
<span class="linenos"> 5</span>
<span class="linenos"> 6</span><span class="n">my_softmax</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos"> 7</span><span class="n">my_softmax_probs</span> <span class="o">=</span> <span class="n">my_softmax</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="linenos"> 8</span><span class="n">pprint</span><span class="p">(</span><span class="n">my_softmax_probs</span><span class="p">)</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span>
<span class="linenos">11</span>    <span class="n">pytorch_softmax_probs</span><span class="p">,</span> <span class="n">my_softmax_probs</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1.3e-6</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">msg</span><span class="o">=</span><span class="s2">&quot;Softmax function outputs do not match.&quot;</span>
<span class="linenos">12</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0835</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5885</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0744</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1418</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1117</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2984</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2299</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2533</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1253</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0932</span><span style="font-weight: bold">]]</span>, <span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">SoftmaxBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0835</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5885</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0744</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1418</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1117</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2984</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2299</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2533</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1253</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0932</span><span style="font-weight: bold">]]</span>, <span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">DivBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div></div>
</div>
</section>
</section>
<section id="softmax-preserves-order-monotonicity">
<h2><a class="toc-backref" href="#id19" role="doc-backlink">Softmax Preserves Order (Monotonicity)</a><a class="headerlink" href="#softmax-preserves-order-monotonicity" title="Link to this heading">#</a></h2>
<p>Order preservation, in the context of mathematical functions, refers to a
property where the function maintains the relative ordering of its input
elements in the output. Specifically, for a function <span class="math notranslate nohighlight">\(f\)</span> to be considered
order-preserving, the following condition must hold:</p>
<p>Given any two elements <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> in the domain of <span class="math notranslate nohighlight">\(f\)</span>, if <span class="math notranslate nohighlight">\(a &gt; b\)</span>, then
<span class="math notranslate nohighlight">\(f(a) &gt; f(b)\)</span>.</p>
<p>For the softmax function, which is defined for a vector
<span class="math notranslate nohighlight">\(\mathbf{z} = \begin{bmatrix} z_1 &amp; z_2 &amp; \cdots &amp; z_K \end{bmatrix}^{\top}\)</span> of
<span class="math notranslate nohighlight">\(K\)</span> real numbers as:</p>
<div class="math notranslate nohighlight">
\[
\sigma(\mathbf{z})_j=\frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} \text { for } j=1, \ldots, K
\]</div>
<p>and for it to be order-preserving, it must satisfy the condition that for any
two elements <span class="math notranslate nohighlight">\(z_i\)</span> and <span class="math notranslate nohighlight">\(z_j\)</span> in the input vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, if <span class="math notranslate nohighlight">\(z_i &gt; z_j\)</span>,
then <span class="math notranslate nohighlight">\(\sigma(\mathbf{z})_i &gt; \sigma(\mathbf{z})_j\)</span>. The proof is relatively
simple, since the exponential function is monotonically increasing.</p>
<p>We can show one example in code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="linenos"> 2</span><span class="n">my_softmax</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos"> 3</span><span class="n">my_softmax_probs</span> <span class="o">=</span> <span class="n">my_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="n">_</span><span class="p">,</span> <span class="n">indices_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="linenos"> 6</span><span class="n">_</span><span class="p">,</span> <span class="n">indices_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">my_softmax_probs</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="linenos"> 7</span>
<span class="linenos"> 8</span><span class="n">pprint</span><span class="p">(</span><span class="n">indices_logits</span><span class="p">)</span>
<span class="linenos"> 9</span><span class="n">pprint</span><span class="p">(</span><span class="n">indices_probs</span><span class="p">)</span>
<span class="linenos">10</span>
<span class="linenos">11</span><span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span>
<span class="linenos">12</span>    <span class="n">indices_logits</span><span class="p">,</span> <span class="n">indices_probs</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1.3e-6</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">msg</span><span class="o">=</span><span class="s2">&quot;Softmax Ordering is not preserved?!&quot;</span>
<span class="linenos">13</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span><span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">2</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">4</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">3</span><span style="font-weight: bold">]])</span>
</pre>
</div></div>
</div>
<p>Indeed, if the logits is
<span class="math notranslate nohighlight">\(\mathbf{z} = \begin{bmatrix} 2.0 &amp; 1.0 &amp; 3.0 &amp; 5.0 &amp; 4.0 \end{bmatrix}\)</span>, with
ordering of the elements <span class="math notranslate nohighlight">\(z_2 &lt; z_1 &lt; z_3 &lt; z_5 &lt; z_4\)</span> (indices 1, 0, 2, 4, 3),
we show that the softmax function preserves the ordering of the elements in the
output probability distribution.</p>
</section>
<section id="softmax-is-translation-invariance">
<h2><a class="toc-backref" href="#id20" role="doc-backlink">Softmax Is Translation Invariance</a><a class="headerlink" href="#softmax-is-translation-invariance" title="Link to this heading">#</a></h2>
<p>The softmax function showcases an important characteristic known as translation
invariance. This means that if we translate each coordinate of the input vector
<span class="math notranslate nohighlight">\(\mathbf{z}\)</span> by the same scalar value <span class="math notranslate nohighlight">\(c\)</span>, the output of the softmax function
remains unchanged. Mathematically, adding a constant vector
<span class="math notranslate nohighlight">\(\mathbf{c} = (c, \ldots, c)\)</span> to the inputs <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> results in
<span class="math notranslate nohighlight">\(\sigma(\mathbf{z} + \mathbf{c}) = \sigma(\mathbf{z})\)</span>, because this operation
multiplies each exponent in the softmax function by the same factor <span class="math notranslate nohighlight">\(e^c\)</span>, due
to the property <span class="math notranslate nohighlight">\(e^{z_i + c} = e^{z_i} \cdot e^c\)</span>. Consequently, the ratios of
the exponents do not alter.</p>
<p>Given the input vector
<span class="math notranslate nohighlight">\(\mathbf{z} = \begin{bmatrix} z_1 &amp; z_2 &amp; \ldots &amp; z_K \end{bmatrix}\)</span> and a
constant scalar <span class="math notranslate nohighlight">\(c\)</span>, the translation invariance of the softmax function can be
expressed as:</p>
<div class="math notranslate nohighlight">
\[
\sigma(\mathbf{z} + \mathbf{c})_j = \frac{e^{z_j + c}}{\sum_{k=1}^{K}
e^{z_k + c}} = \frac{e^{z_j} \cdot e^c}{\sum_{k=1}^{K} e^{z_k} \cdot e^c} =
\sigma(\mathbf{z})_j
\]</div>
<p>for <span class="math notranslate nohighlight">\(j = 1, \ldots, K\)</span>, where <span class="math notranslate nohighlight">\(K\)</span> is the number of elements in vector
<span class="math notranslate nohighlight">\(\mathbf{z}\)</span>.</p>
<p>In code, we can demonstrate this property as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="n">constant</span> <span class="o">=</span> <span class="mi">10</span>
<span class="linenos">2</span><span class="n">translated_logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">+</span> <span class="n">constant</span>
<span class="linenos">3</span><span class="n">translated_probs</span> <span class="o">=</span> <span class="n">my_softmax</span><span class="p">(</span><span class="n">translated_logits</span><span class="p">)</span>
<span class="linenos">4</span>
<span class="linenos">5</span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original softmax  :&quot;</span><span class="p">,</span> <span class="n">my_softmax_probs</span><span class="p">)</span>
<span class="linenos">6</span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Translated softmax:&quot;</span><span class="p">,</span> <span class="n">translated_probs</span><span class="p">)</span>
<span class="linenos">7</span>
<span class="linenos">8</span><span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span><span class="n">my_softmax_probs</span><span class="p">,</span> <span class="n">translated_probs</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1.3e-6</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">msg</span><span class="o">=</span><span class="s2">&quot;Translation Invariance Violated!&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original softmax  : tensor([[0.0317, 0.0117, 0.0861, 0.6364, 0.2341]])
Translated softmax: tensor([[0.0317, 0.0117, 0.0861, 0.6364, 0.2341]])
</pre></div>
</div>
</div>
</div>
<section id="numerical-instability-of-the-softmax-function">
<h3><a class="toc-backref" href="#id21" role="doc-backlink">Numerical Instability of the Softmax Function</a><a class="headerlink" href="#numerical-instability-of-the-softmax-function" title="Link to this heading">#</a></h3>
<p>Recall the softmax function as follows:</p>
<div class="math notranslate nohighlight">
\[
\sigma(\mathbf{z})_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} \quad \text{for }
j = 1, \ldots, K
\]</div>
<p>When elements of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> are very large (positive or negative), computing
<span class="math notranslate nohighlight">\(e^{z_j}\)</span> can lead to numerical instability:</p>
<ul class="simple">
<li><p><strong>Overflow</strong>: If <span class="math notranslate nohighlight">\(z_j\)</span> is very large, <span class="math notranslate nohighlight">\(e^{z_j}\)</span> can exceed the range of
floating-point numbers, leading to infinity (<span class="math notranslate nohighlight">\(\infty\)</span>).</p></li>
<li><p><strong>Underflow</strong>: If <span class="math notranslate nohighlight">\(z_j\)</span> is very negative, <span class="math notranslate nohighlight">\(e^{z_j}\)</span> can become so small that
it’s considered as zero in floating-point arithmetic.</p></li>
</ul>
<p>Both situations can cause issues when computing the softmax, leading to
inaccurate results or failures in computation due to divisions by infinity or
zero.</p>
<p>To mitigate these issues, we leverage the invariance property of softmax to
shifts in the input vector <span id="id4">[<a class="reference internal" href="../bibliography.html#id6" title="Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.">Goodfellow <em>et al.</em>, 2016</a>]</span>. Specifically, adding
or subtracting the same scalar from every element of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> doesn’t change
the output of the softmax function:</p>
<div class="math notranslate nohighlight">
\[\sigma(\mathbf{z}) = \sigma(\mathbf{z} + c)\]</div>
<p>By choosing <span class="math notranslate nohighlight">\(c = -\max_i z_i\)</span>, we subtract the maximum element of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>
from every element. This operation ensures that the maximum value in the shifted
vector is <span class="math notranslate nohighlight">\(0\)</span>, which helps in avoiding overflow (since <span class="math notranslate nohighlight">\(e^0 = 1\)</span> is within
range) and reduces the risk of underflow for other elements.</p>
<p>The numerically stable softmax function can thus be reformulated as:</p>
<div class="math notranslate nohighlight">
\[
\sigma(\mathbf{z})_j = \frac{e^{z_j - \max_i z_i}}{\sum_{k=1}^K e^{z_k -
\max_i z_i}} \quad \text{for } j = 1, \ldots, K
\]</div>
<p>This reformulation achieves the following:</p>
<ul class="simple">
<li><p><strong>Maximal Value Normalization</strong>: By ensuring that the maximum exponentiated
value is <span class="math notranslate nohighlight">\(e^0 = 1\)</span>, we prevent overflow.</p></li>
<li><p><strong>Relative Differences Preserved</strong>: The operation preserves the relative
differences between the elements of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, ensuring the softmax
output remains unchanged in terms of its distribution properties.</p></li>
<li><p><strong>Improved Numerical Stability</strong>: The risk of underflow is also mitigated
since subtracting a large value (<span class="math notranslate nohighlight">\(\max_i z_i\)</span>) from each <span class="math notranslate nohighlight">\(z_j\)</span> makes the
values more manageable for computation without affecting the ratios that
softmax aims to model.</p></li>
</ul>
<p>To this end, a rough implementation of the numerically stable softmax function
is shown below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">super_large_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1000.0</span><span class="p">,</span> <span class="mf">2000.0</span><span class="p">,</span> <span class="mf">3000.0</span><span class="p">,</span> <span class="mf">4000.0</span><span class="p">,</span> <span class="mf">5000.0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="linenos"> 2</span><span class="n">unstable_softmax</span> <span class="o">=</span> <span class="n">my_softmax</span><span class="p">(</span><span class="n">super_large_logits</span><span class="p">)</span>
<span class="linenos"> 3</span><span class="n">pprint</span><span class="p">(</span><span class="n">unstable_softmax</span><span class="p">)</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="k">def</span> <span class="nf">stable_softmax</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="linenos"> 6</span><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos"> 7</span><span class="sd">    Compute the numerically stable softmax function for a given input.</span>
<span class="linenos"> 8</span><span class="sd">    &quot;&quot;&quot;</span>
<span class="linenos"> 9</span>    <span class="n">max_z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
<span class="linenos">10</span>    <span class="n">numerator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">max_z</span><span class="p">)</span>
<span class="linenos">11</span>    <span class="n">denominator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">numerator</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos">12</span>    <span class="n">g</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>
<span class="linenos">13</span>    <span class="k">return</span> <span class="n">g</span>
<span class="linenos">14</span>
<span class="linenos">15</span><span class="n">stable_softmax_probs</span> <span class="o">=</span> <span class="n">stable_softmax</span><span class="p">(</span><span class="n">super_large_logits</span><span class="p">)</span>
<span class="linenos">16</span><span class="n">pprint</span><span class="p">(</span><span class="n">stable_softmax_probs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span>nan, nan, nan, nan, nan<span style="font-weight: bold">]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>., <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>., <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>., <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0</span>., <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>.<span style="font-weight: bold">]])</span>
</pre>
</div></div>
</div>
</section>
</section>
<section id="softmax-is-not-invariant-under-scaling">
<h2><a class="toc-backref" href="#id22" role="doc-backlink">Softmax Is Not Invariant Under Scaling</a><a class="headerlink" href="#softmax-is-not-invariant-under-scaling" title="Link to this heading">#</a></h2>
<p>A function <span class="math notranslate nohighlight">\(f: \mathbb{R}^D \rightarrow \mathbb{R}^D\)</span> is said to be scale
invariant if for any positive scalar <span class="math notranslate nohighlight">\(c &gt; 0\)</span> and any vector
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^D\)</span>, the following condition holds:</p>
<div class="math notranslate nohighlight">
\[
f(c\mathbf{x}) = f(\mathbf{x})
\]</div>
<p>This means that scaling the input vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> by any positive factor <span class="math notranslate nohighlight">\(c\)</span>
does not change the output of the function <span class="math notranslate nohighlight">\(f\)</span>. Scale invariance implies that
the function’s output depends only on the direction of the vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>
and not its magnitude.</p>
<p>In contrast, a function is not invariant under scaling if there exists at least
one vector <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^D\)</span> and one scalar <span class="math notranslate nohighlight">\(c &gt; 0\)</span> such that
<span class="math notranslate nohighlight">\(f(c\mathbf{x}) \neq f(\mathbf{x})\)</span>.</p>
<p>The softmax function is <strong><em>not</em></strong> invariant under scaling. This is because the
softmax output changes when the input vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is scaled by a
positive scalar <span class="math notranslate nohighlight">\(c\)</span>, as the exponential function magnifies differences in the
scaled inputs.</p>
<p>The difference in softmax’s response to scaling can be attributed to the
exponential operation applied to each element of the input vector before
normalization. Since the exponential function is not linear, scaling the input
vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> by a scalar <span class="math notranslate nohighlight">\(c\)</span> does not merely scale the output by the same
factor, but rather changes the relative weights of the output probabilities.</p>
<p>Consider the input vector
<span class="math notranslate nohighlight">\(\mathbf{z} = \begin{bmatrix} 10.0 &amp; 20.0 &amp; 30.0 \end{bmatrix}\)</span>, and its scaled
version
<span class="math notranslate nohighlight">\(\tilde{\mathbf{z}} = c \cdot \mathbf{z} = \begin{bmatrix} 0.1 &amp; 0.2 &amp; 0.3 \end{bmatrix}\)</span>
where <span class="math notranslate nohighlight">\(c = 0.01\)</span>. The softmax output for the larger input vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is
approximately
<span class="math notranslate nohighlight">\(\begin{bmatrix} 2.0611e-09 &amp; 4.5398e-05 &amp; 9.9995e-01 \end{bmatrix}\)</span>, while the
softmax output for the scaled input vector <span class="math notranslate nohighlight">\(\tilde{\mathbf{z}}\)</span> is approximately
<span class="math notranslate nohighlight">\(\begin{bmatrix} 0.3006 &amp; 0.3322 &amp; 0.3672 \end{bmatrix}\)</span> which show that the
softmax function is not invariant under scaling by comparing the softmax outputs
for the original and scaled input vectors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">c</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="linenos"> 2</span><span class="n">input_large</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="linenos"> 3</span><span class="n">input_small</span> <span class="o">=</span> <span class="n">c</span> <span class="o">*</span> <span class="n">input_large</span> <span class="c1"># torch.tensor([0.1, 0.2, 0.3], dtype=torch.float32)</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="n">softmax_large</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)(</span><span class="n">input_large</span><span class="p">)</span>
<span class="linenos"> 6</span><span class="n">softmax_small</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)(</span><span class="n">input_small</span><span class="p">)</span>
<span class="linenos"> 7</span>
<span class="linenos"> 8</span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Softmax of the larger input :&quot;</span><span class="p">,</span> <span class="n">softmax_large</span><span class="p">)</span>
<span class="linenos"> 9</span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Softmax of the smaller input:&quot;</span><span class="p">,</span> <span class="n">softmax_small</span><span class="p">)</span>
<span class="linenos">10</span>
<span class="linenos">11</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="linenos">12</span>
<span class="linenos">13</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="linenos">14</span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">softmax_large</span><span class="p">)),</span> <span class="n">softmax_large</span><span class="p">)</span>
<span class="linenos">15</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Softmax probabilities (original scale)&#39;</span><span class="p">)</span>
<span class="linenos">16</span>
<span class="linenos">17</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="linenos">18</span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">softmax_small</span><span class="p">)),</span> <span class="n">softmax_small</span><span class="p">)</span>
<span class="linenos">19</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Softmax probabilities (scale factor: </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="linenos">20</span>
<span class="linenos">21</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Softmax of the larger input : tensor([2.0611e-09, 4.5398e-05, 9.9995e-01])
Softmax of the smaller input: tensor([0.3006, 0.3322, 0.3672])
</pre></div>
</div>
<img alt="../_images/46b977181e0b7de8f3a65106349acce6a7643be136c542b6727daf5b281ab627.svg" src="../_images/46b977181e0b7de8f3a65106349acce6a7643be136c542b6727daf5b281ab627.svg" />
</div>
</div>
<p>Another example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="n">c</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="linenos"> 2</span><span class="n">input_large</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="linenos"> 3</span><span class="n">input_small</span> <span class="o">=</span> <span class="n">c</span> <span class="o">*</span> <span class="n">input_large</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="n">softmax_large</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)(</span><span class="n">input_large</span><span class="p">)</span>
<span class="linenos"> 6</span><span class="n">softmax_small</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)(</span><span class="n">input_small</span><span class="p">)</span>
<span class="linenos"> 7</span>
<span class="linenos"> 8</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="linenos">11</span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">softmax_large</span><span class="p">)),</span> <span class="n">softmax_large</span><span class="p">)</span>
<span class="linenos">12</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Softmax probabilities (original scale)&#39;</span><span class="p">)</span>
<span class="linenos">13</span>
<span class="linenos">14</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="linenos">15</span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">softmax_small</span><span class="p">)),</span> <span class="n">softmax_small</span><span class="p">)</span>
<span class="linenos">16</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Softmax probabilities (scale factor: </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="linenos">17</span>
<span class="linenos">18</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/bb1682505e723080cec479b6e14e97fd79b541e3b892215f912ab5b0ace5f5f8.svg" src="../_images/bb1682505e723080cec479b6e14e97fd79b541e3b892215f912ab5b0ace5f5f8.svg" />
</div>
</div>
</section>
<section id="sharpening-and-dampening-the-softmax-distribution">
<h2><a class="toc-backref" href="#id23" role="doc-backlink">Sharpening and Dampening the Softmax Distribution</a><a class="headerlink" href="#sharpening-and-dampening-the-softmax-distribution" title="Link to this heading">#</a></h2>
<p>Continuing from our first example, the 3rd element of the softmax output has the
largest weight (<code class="docutils literal notranslate"><span class="pre">9.9995e-01</span></code>), thereby suppressing the other elements. The sharp
readers would have noticed that even though the 3rd element has the largest
weight, it was at a whooping <span class="math notranslate nohighlight">\(0.99995\)</span>, which is almost <span class="math notranslate nohighlight">\(1\)</span> - which means the
rest of the elements are almost zero. As we shall briefly touch upon later,
sampling from a
<a class="reference external" href="https://en.wikipedia.org/wiki/Multinomial_distribution">multinomial distribution</a>
(number of trials is 1 because we are sampling one token at each time step) with
the parameter <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> is the softmax
output, almost surely will select the 3rd element because of the high
probability. This is what we call a <strong><em>sharp</em></strong> softmax distribution.</p>
<p>On the other hand, if we scale the input vector by a factor <span class="math notranslate nohighlight">\(c=0.01\)</span>, even
though the ranking (order) is preserved, the softmax output is more uniformly
distributed, with the weights of the elements more evenly spread out. We can see
the 3rd element has a weight of <span class="math notranslate nohighlight">\(0.3672\)</span>, which is still the maximum in the
array, but the relative weights of the other elements are higher compared to the
original softmax output. Similarly, sampling from a multinomial distribution
with the parameter <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> this time will be more diverse, because
our <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span> is more uniform (at
<span class="math notranslate nohighlight">\(\begin{bmatrix} 0.3006 &amp; 0.3322 &amp; 0.3672 \end{bmatrix}\)</span> and therefore the other
two elements are likely to get selected as they are quite close to the max
weight). This is what we call a <strong><em>dampened</em></strong> softmax distribution.</p>
<p>For people who has toyed around with the temperature parameter in the language
model, we were told that when the temperature is high, the model is more random,
and when the temperature is low, the model is more deterministic. This is
because the temperature parameter effectively scales the logits by <span class="math notranslate nohighlight">\(T\)</span> (the
temperature), where we can think of it as <code class="docutils literal notranslate"><span class="pre">logits</span> <span class="pre">=</span> <span class="pre">logits</span> <span class="pre">/</span> <span class="pre">(T</span> <span class="pre">+</span> <span class="pre">epsilon)</span></code>.
This is the same as scaling the logits by a factor <span class="math notranslate nohighlight">\(c = 1/T\)</span>. When <span class="math notranslate nohighlight">\(T\)</span> is high,
the softmax distribution is more uniform, and when <span class="math notranslate nohighlight">\(T\)</span> is low, the softmax
distribution is more sharp and the highest weight displays an one-hot manner,
where the rest of the weights are almost zero.</p>
</section>
<section id="temperature">
<h2><a class="toc-backref" href="#id24" role="doc-backlink">Temperature</a><a class="headerlink" href="#temperature" title="Link to this heading">#</a></h2>
<p>We start off this section with an extracted portion from the Wikipedia page that
is relevant to our discussion on the temperature parameter in the softmax
function.</p>
<blockquote class="epigraph">
<div><p>The term “softmax” derives from the amplifying effects of the exponential on any
maxima in the input vector. For example, the standard softmax of <span class="math notranslate nohighlight">\((1,2,8)\)</span> is
approximately <span class="math notranslate nohighlight">\((0.001,0.002,0.997)\)</span>, which amounts to assigning almost all of
the total unit weight in the result to the position of the vector’s maximal
element (of 8).</p>
<p>In general, instead of <span class="math notranslate nohighlight">\(e\)</span> a different base <span class="math notranslate nohighlight">\(b&gt;0\)</span> can be used. If <span class="math notranslate nohighlight">\(0&lt;b&lt;1\)</span>,
smaller input components will result in larger output probabilities, and
decreasing the value of <span class="math notranslate nohighlight">\(b\)</span> will create probability distributions that are more
concentrated around the positions of the smallest input values. Conversely, as
above, if <span class="math notranslate nohighlight">\(b&gt;1\)</span> larger input components will result in larger output
probabilities, and increasing the value of <span class="math notranslate nohighlight">\(b\)</span> will create probability
distributions that are more concentrated around the positions of the largest
input values.</p>
<p>Writing <span class="math notranslate nohighlight">\(b=e^\beta\)</span> or <span class="math notranslate nohighlight">\(b=e^{-\beta[\mathbf{a}]}\)</span> (for real <span class="math notranslate nohighlight">\(\beta\)</span>) yields the
expressions:</p>
<div class="math notranslate nohighlight">
\[
\sigma(\mathbf{z})_i=\frac{e^{\beta z_i}}{\sum_{j=1}^K e^{\beta z_j}} \text { or } \sigma(\mathbf{z})_i=\frac{e^{-\beta z_i}}{\sum_{j=1}^K e^{-\beta z_j}} \text { for } i=1, \ldots, K
\]</div>
<p>The reciprocal of <span class="math notranslate nohighlight">\(\beta\)</span> is sometimes referred to as the temperature,
<span class="math notranslate nohighlight">\(T=1 / \beta\)</span>, with <span class="math notranslate nohighlight">\(b=e^{1 / T}\)</span>. A higher temperature results in a more
uniform output distribution (i.e. with higher entropy, and “more random”), while
a lower temperature results in a sharper output distribution, with one value
dominating.</p>
<p class="attribution">—<a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_function">Softmax - Wikipedia</a></p>
</div></blockquote>
<p>More concretely, how temperature is implemented in large language models like
GPT is by scaling the logits by the temperature parameter <span class="math notranslate nohighlight">\(T\)</span> before applying
the softmax function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">temperature</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">temperature</span><span class="p">:</span>
    <span class="n">scaled_logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">/</span> <span class="n">t</span>
    <span class="n">scaled_probs</span> <span class="o">=</span> <span class="n">my_softmax</span><span class="p">(</span><span class="n">scaled_logits</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Temperature: </span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s2">, Scaled Probs: </span><span class="si">{</span><span class="n">scaled_probs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>So now we can define the input of the softmax function as <span class="math notranslate nohighlight">\(\frac{z}{T}\)</span> where
<span class="math notranslate nohighlight">\(T\)</span> is the temperature parameter. Now our softmax would be like the below:</p>
<div class="math notranslate nohighlight">
\[
\sigma\left(\frac{\mathbf{z}}{T}\right)_j = \frac{e^{z_j / T}}{\sum_{k=1}^K e^{z_k / T}}
\]</div>
<p>which coincides with the definition from the Wikipedia page.</p>
<p>To conclude, the temperature modifies the “sharpness” of the probability
distribution without altering the order of the probabilities, a desirable
property:</p>
<ul class="simple">
<li><p>At high temperatures (<span class="math notranslate nohighlight">\(T &gt; 1\)</span>), the distribution becomes more uniform, but
if <span class="math notranslate nohighlight">\(z_a &gt; z_b\)</span>, then <span class="math notranslate nohighlight">\(\text{softmax}_T(z_a) &gt; \text{softmax}_T(z_b)\)</span> still
holds. The probabilities become closer to each other, making the choice more
“random” or “equally likely” among options, but the ranking remains the
same.</p></li>
<li><p>At low temperatures (<span class="math notranslate nohighlight">\(T &lt; 1\)</span>), the distribution becomes sharper, with a more
pronounced difference between the higher and lower probabilities, amplifying
the differences in likelihood as determined by the logits. The probability
of the largest logit increases towards 1, while others decrease towards 0.
Yet, the order of logits is preserved—higher logits translate to higher
probabilities.</p></li>
<li><p>At <span class="math notranslate nohighlight">\(T = 1\)</span>, the softmax function is the standard softmax function.</p></li>
</ul>
<section id="sampling-from-the-softmax-distribution">
<h3><a class="toc-backref" href="#id25" role="doc-backlink">Sampling from the Softmax Distribution</a><a class="headerlink" href="#sampling-from-the-softmax-distribution" title="Link to this heading">#</a></h3>
<p>Below we show the effect of temperature on multinomial sampling from the softmax
distribution. The experiements are repeated 1000 times for each temperature, and
the distribution of sampled outcomes is visualized for each temperature.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="kn">import</span> <span class="nn">torch</span>
<span class="linenos"> 2</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="linenos"> 3</span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="linenos"> 4</span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="linenos"> 5</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="linenos"> 6</span>
<span class="linenos"> 7</span>
<span class="linenos"> 8</span><span class="k">def</span> <span class="nf">demonstrate_multinomial_sampling_effect</span><span class="p">(</span>
<span class="linenos"> 9</span>    <span class="n">logits</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">temperatures</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">num_experiments</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span>
<span class="linenos">10</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]:</span>
<span class="linenos">11</span><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">12</span><span class="sd">    Demonstrates the effect of temperature on multinomial sampling from the softmax distribution.</span>
<span class="linenos">13</span>
<span class="linenos">14</span><span class="sd">    Parameters</span>
<span class="linenos">15</span><span class="sd">    ----------</span>
<span class="linenos">16</span><span class="sd">    logits : torch.Tensor</span>
<span class="linenos">17</span><span class="sd">        The input logits vector.</span>
<span class="linenos">18</span><span class="sd">    temperatures : List[float]</span>
<span class="linenos">19</span><span class="sd">        A list of temperatures to apply to the softmax function.</span>
<span class="linenos">20</span><span class="sd">    num_experiments : int</span>
<span class="linenos">21</span><span class="sd">        The number of sampling experiments to run for each temperature.</span>
<span class="linenos">22</span><span class="sd">    epsilon : float, optional</span>
<span class="linenos">23</span><span class="sd">        A small value added to the temperature to prevent division by zero, by default 1e-8.</span>
<span class="linenos">24</span>
<span class="linenos">25</span><span class="sd">    Returns</span>
<span class="linenos">26</span><span class="sd">    -------</span>
<span class="linenos">27</span><span class="sd">    Tuple[Dict[float, np.ndarray], Dict[float, List[int]]]</span>
<span class="linenos">28</span><span class="sd">        A tuple containing two dictionaries:</span>
<span class="linenos">29</span><span class="sd">        - The first maps each temperature to its corresponding softmax probabilities as a numpy array.</span>
<span class="linenos">30</span><span class="sd">        - The second maps each temperature to a list of sampled outcomes (indices of logits).</span>
<span class="linenos">31</span>
<span class="linenos">32</span><span class="sd">    Examples</span>
<span class="linenos">33</span><span class="sd">    --------</span>
<span class="linenos">34</span><span class="sd">    &gt;&gt;&gt; logits = torch.tensor([2.0, 1.0, 3.0, 5.0, 4.0], dtype=torch.float32)</span>
<span class="linenos">35</span><span class="sd">    &gt;&gt;&gt; temperatures = [0.1, 1.0, 10.0]</span>
<span class="linenos">36</span><span class="sd">    &gt;&gt;&gt; softmax_results, sampling_results = demonstrate_multinomial_sampling_effect(</span>
<span class="linenos">37</span><span class="sd">    ...     logits, temperatures, num_experiments=1000</span>
<span class="linenos">38</span><span class="sd">    ... )</span>
<span class="linenos">39</span><span class="sd">    &quot;&quot;&quot;</span>
<span class="linenos">40</span>    <span class="n">softmax_results</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
<span class="linenos">41</span>    <span class="n">sampling_results</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<span class="linenos">42</span>
<span class="linenos">43</span>    <span class="k">for</span> <span class="n">temperature</span> <span class="ow">in</span> <span class="n">temperatures</span><span class="p">:</span>
<span class="linenos">44</span>        <span class="c1"># Scale logits by temperature</span>
<span class="linenos">45</span>        <span class="n">scaled_logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">/</span> <span class="p">(</span><span class="n">temperature</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
<span class="linenos">46</span>        <span class="c1"># Apply softmax to scaled logits</span>
<span class="linenos">47</span>        <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scaled_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos">48</span>        <span class="n">softmax_results</span><span class="p">[</span><span class="n">temperature</span><span class="p">]</span> <span class="o">=</span> <span class="n">probs</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="linenos">49</span>
<span class="linenos">50</span>        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_experiments</span><span class="p">):</span>
<span class="linenos">51</span>            <span class="n">sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos">52</span>            <span class="n">sampling_results</span><span class="p">[</span><span class="n">temperature</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="linenos">53</span>
<span class="linenos">54</span>    <span class="k">return</span> <span class="n">softmax_results</span><span class="p">,</span> <span class="nb">dict</span><span class="p">(</span><span class="n">sampling_results</span><span class="p">)</span>
<span class="linenos">55</span>
<span class="linenos">56</span>
<span class="linenos">57</span><span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="linenos">58</span><span class="n">temperatures</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">]</span>
<span class="linenos">59</span>
<span class="linenos">60</span><span class="n">num_experiments</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="linenos">61</span><span class="n">softmax_results</span><span class="p">,</span> <span class="n">sampling_results</span> <span class="o">=</span> <span class="n">demonstrate_multinomial_sampling_effect</span><span class="p">(</span>
<span class="linenos">62</span>    <span class="n">logits</span><span class="p">,</span> <span class="n">temperatures</span><span class="p">,</span> <span class="n">num_experiments</span><span class="o">=</span><span class="n">num_experiments</span>
<span class="linenos">63</span><span class="p">)</span>
<span class="linenos">64</span>
<span class="linenos">65</span><span class="n">outcome_counts</span> <span class="o">=</span> <span class="p">{</span>
<span class="linenos">66</span>    <span class="n">temp</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">outcomes</span><span class="p">,</span> <span class="n">minlength</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span> <span class="o">/</span> <span class="n">num_experiments</span> <span class="o">*</span> <span class="mi">100</span>
<span class="linenos">67</span>    <span class="k">for</span> <span class="n">temp</span><span class="p">,</span> <span class="n">outcomes</span> <span class="ow">in</span> <span class="n">sampling_results</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
<span class="linenos">68</span><span class="p">}</span>
<span class="linenos">69</span>
<span class="linenos">70</span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">temperatures</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">temperatures</span><span class="p">)))</span>
<span class="linenos">71</span><span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">temperature</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axs</span><span class="p">,</span> <span class="n">temperatures</span><span class="p">):</span>
<span class="linenos">72</span>    <span class="n">bars</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">logits</span><span class="p">)),</span> <span class="n">outcome_counts</span><span class="p">[</span><span class="n">temperature</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Temperature: </span><span class="si">{</span><span class="n">temperature</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">73</span>    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="linenos">74</span>    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Distribution of sampled outcomes at temperature </span><span class="si">{</span><span class="n">temperature</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">75</span>    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Outcome&quot;</span><span class="p">)</span>
<span class="linenos">76</span>    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Percentage (%)&quot;</span><span class="p">)</span>
<span class="linenos">77</span>
<span class="linenos">78</span>    <span class="k">for</span> <span class="n">bar</span> <span class="ow">in</span> <span class="n">bars</span><span class="p">:</span>
<span class="linenos">79</span>        <span class="n">yval</span> <span class="o">=</span> <span class="n">bar</span><span class="o">.</span><span class="n">get_height</span><span class="p">()</span>
<span class="linenos">80</span>        <span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">bar</span><span class="o">.</span><span class="n">get_x</span><span class="p">()</span> <span class="o">+</span> <span class="n">bar</span><span class="o">.</span><span class="n">get_width</span><span class="p">()</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="n">yval</span> <span class="o">+</span> <span class="mf">0.01</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">yval</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;bottom&quot;</span><span class="p">)</span>
<span class="linenos">81</span>
<span class="linenos">82</span><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="linenos">83</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/665c6c92877ffd3f0bb249a28a23aa8e2c4e53718b40b909a83028c8caacf05c.svg" src="../_images/665c6c92877ffd3f0bb249a28a23aa8e2c4e53718b40b909a83028c8caacf05c.svg" />
</div>
</div>
</section>
</section>
<section id="softmax-is-smooth-continuous-and-differentiable">
<h2><a class="toc-backref" href="#id26" role="doc-backlink">Softmax Is Smooth, Continuous and Differentiable</a><a class="headerlink" href="#softmax-is-smooth-continuous-and-differentiable" title="Link to this heading">#</a></h2>
<p>The term “softmax” is a misnomer, as one would have thought it is a
<a class="reference external" href="https://en.wikipedia.org/wiki/Smooth_maximum">smooth max</a> - a
<a class="reference external" href="https://en.wikipedia.org/wiki/Smoothness">smooth approximation</a> of the
<a class="reference external" href="https://en.wikipedia.org/wiki/Maximum_function">maximum function</a>. However, the
softmax function is not a smooth approximation of the maximum function, but
rather a smooth approximation of the
<a class="reference external" href="https://en.wikipedia.org/wiki/Arg_max">argmax</a> function<a class="footnote-reference brackets" href="#softmax-wikipedia" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>,
where argmax is the function that outputs the <strong>index</strong> of the maximum element
in a vector.</p>
<p>The argmax function is <em>not</em> continuous or differentiable, because it is a
discrete function that jumps from one index to another as the input vector
changes (i.e. not smooth). The softmax function, on the other hand, is a smooth
approximation of the argmax function, as it outputs a probability distribution
over the elements of the input vector, with the highest probability assigned to
the maximum element - essentially a continuous and differentiable version of the
argmax function (we call it a “softened” version of the argmax function)
<span id="id6">[<a class="reference internal" href="../bibliography.html#id6" title="Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.">Goodfellow <em>et al.</em>, 2016</a>]</span>. Consequently, it is more appropriate to name the
function “softargmax” instead of “softmax”.</p>
<p>To this end, it is worth mentioning that softmax gains its popularity in the
deep learning space because it is smooth and normalizer over the input vector.</p>
<p>The smoothness assumption is a common one in the context of deep learning,
because for when we say that the estimator function
<span class="math notranslate nohighlight">\(f_{\hat{\boldsymbol{\Theta}}}(\cdot)\)</span> is <em>smooth</em> with respect to the parameter
space <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Theta}}\)</span>, it means that the estimator function
<span class="math notranslate nohighlight">\(f_{\hat{\boldsymbol{\Theta}}}(\cdot)\)</span> is <em>smooth</em> with respect to the parameter
space <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Theta}}\)</span> if the function is continuous and
differentiable with respect to the parameter space <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Theta}}\)</span>
up to a certain order (usually the first for SGD variants and second order for
Newton).</p>
<p>What this implies is that the derivative of the function with respect to the
parameter space <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Theta}}\)</span>, denoted as
<span class="math notranslate nohighlight">\(\nabla_{\hat{\boldsymbol{\Theta}}} f_{\hat{\boldsymbol{\Theta}}}(\cdot)\)</span> is
continuous. Loosely, you can think of that a small perturbation in the parameter
space <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Theta}}\)</span> will result in a small change in the output of
the function <span class="math notranslate nohighlight">\(f_{\hat{\boldsymbol{\Theta}}}(\cdot)\)</span> - enabling gradient-based
optimization algorithms to work effectively as if not, then taking a step in the
direction of the gradient would not guarantee a decrease in the loss function,
slowing down convergence.</p>
</section>
<section id="softmax-function-via-exponential-family">
<h2><a class="toc-backref" href="#id27" role="doc-backlink">Softmax Function via Exponential Family</a><a class="headerlink" href="#softmax-function-via-exponential-family" title="Link to this heading">#</a></h2>
<p>For a rigorous derivation of the softmax function - which will give rise to
intuition on how sigmoid and softmax is derived from the exponential family of
distributions - we refer the reader to chapter 2.4. The Exponential Family of
Christopher Bishop’s <em>Pattern Recognition and Machine Learning</em>
<span id="id7">[<a class="reference internal" href="../bibliography.html#id4" title="Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer, 1 edition, 2007. ISBN 0387310738. URL: http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738%3FSubscriptionId%3D13CT5CVB80YFWJEPWS02%26tag%3Dws%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0387310738.">Bishop, 2007</a>]</span> with reference
<a class="reference external" href="https://math.stackexchange.com/questions/328115/derivation-of-softmax-function">here</a>.</p>
</section>
<section id="gradient-jacobian-and-hessian-of-softmax">
<h2><a class="toc-backref" href="#id28" role="doc-backlink">Gradient, Jacobian, and Hessian of Softmax</a><a class="headerlink" href="#gradient-jacobian-and-hessian-of-softmax" title="Link to this heading">#</a></h2>
<section id="softmax-as-a-vector-function">
<h3><a class="toc-backref" href="#id29" role="doc-backlink">Softmax as a Vector Function</a><a class="headerlink" href="#softmax-as-a-vector-function" title="Link to this heading">#</a></h3>
<p>The softmax function <span class="math notranslate nohighlight">\(\sigma\)</span> represents a mapping from <span class="math notranslate nohighlight">\(\mathbb{R}^K\)</span> to
<span class="math notranslate nohighlight">\((0,1)^K\)</span>, where each output component <span class="math notranslate nohighlight">\(\sigma(\mathbf{z})_j\)</span> is a function of
all input components <span class="math notranslate nohighlight">\(z_1, z_2, \ldots, z_K\)</span>. Given its vector-to-vector nature,
it does not make sense to talk about the derivative of softmax as a
scalar-valued function. Instead, the analysis of its derivatives involves the
following:</p>
<ol class="arabic simple">
<li><p>Which component of the output vector we are interested in, and</p></li>
<li><p>Which and how the input components affect that output component.</p></li>
</ol>
</section>
<section id="derivatives-of-the-softmax-function">
<h3><a class="toc-backref" href="#id30" role="doc-backlink">Derivatives of the Softmax Function</a><a class="headerlink" href="#derivatives-of-the-softmax-function" title="Link to this heading">#</a></h3>
<p>As a result, the above considerations lands ourselves naturally into the realm
of multivariable calculus, where we consider the partial derivatives of the
softmax function - which measure the change in a specific output component in
response to a small change in a specific input component. For the <span class="math notranslate nohighlight">\(i\)</span>-th output
of softmax with respect to the <span class="math notranslate nohighlight">\(j\)</span>-th input, we denote the partial derivative
as:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \sigma(\mathbf{z})_i}{\partial z_j}\]</div>
<p>This notation precisely indicates the partial derivative of the <span class="math notranslate nohighlight">\(i\)</span>-th output
with respect to the <span class="math notranslate nohighlight">\(j\)</span>-th input.</p>
<p>To this end, given the softmax function <span class="math notranslate nohighlight">\(\sigma\)</span> defined on a vector
<span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^K\)</span>, the goal is to compute the partial derivative of
the <span class="math notranslate nohighlight">\(i\)</span>-th output of the softmax function with respect to the <span class="math notranslate nohighlight">\(j\)</span> th component
of the input vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \sigma(\mathbf{z})_i}{\partial z_j}=\frac{\partial \frac{e^{z_i}}{\sum_{k=1}^K e^{z_k}}}{\partial z_j}
\]</div>
</section>
<section id="the-jacobian-matrix-of-softmax">
<h3><a class="toc-backref" href="#id31" role="doc-backlink">The Jacobian Matrix of Softmax</a><a class="headerlink" href="#the-jacobian-matrix-of-softmax" title="Link to this heading">#</a></h3>
<p>The formal representation of the derivatives for a vector function like softmax
is the Jacobian matrix, <span class="math notranslate nohighlight">\(\mathbf{J}_{\sigma}\)</span>, which contains all the
first-order partial derivatives. For the softmax function, the Jacobian matrix
is defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{J}_{\sigma}(\mathbf{z}) = \begin{bmatrix} \frac{\partial
\sigma(\mathbf{z})_1}{\partial z_1} &amp; \cdots &amp; \frac{\partial
\sigma(\mathbf{z})_1}{\partial z_K} \\ \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial \sigma(\mathbf{z})_K}{\partial z_1} &amp; \cdots &amp; \frac{\partial
\sigma(\mathbf{z})_K}{\partial z_K} \end{bmatrix}
\end{split}\]</div>
<p>This matrix provides a full description of how the softmax function responds to
changes in its inputs, indicating the sensitivity of each output component to
each input component.</p>
<p>While “gradient” is a term often used in machine learning to describe the
derivative of a function, it strictly applies to scalar-valued functions. For
vector-valued functions like softmax, describing the comprehensive derivative
structure requires the Jacobian matrix, <span class="math notranslate nohighlight">\(\mathbf{J}_{\sigma}\)</span>, not a gradient.
Therefore, in discussions involving softmax and its impact on learning in neural
networks, it is more accurate to refer to the Jacobian matrix when considering
its derivatives.</p>
</section>
<section id="derivative-of-the-softmax-function">
<h3><a class="toc-backref" href="#id32" role="doc-backlink">Derivative of the Softmax Function</a><a class="headerlink" href="#derivative-of-the-softmax-function" title="Link to this heading">#</a></h3>
<p>To compute the partial derivative of the <span class="math notranslate nohighlight">\(i\)</span>-th component of the softmax output
with respect to the <span class="math notranslate nohighlight">\(j\)</span>-th component of the input vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, we
express this as:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \sigma(\mathbf{z})_i}{\partial z_j} = \frac{\partial
\frac{e^{z_i}}{\sum_{k=1}^K e^{z_k}}}{\partial z_j}
\]</div>
<p>Since there is division involved, we’ll apply the quotient rule of derivatives,
which for a function <span class="math notranslate nohighlight">\(f(x) = \frac{g(x)}{h(x)}\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[f'(x) = \frac{g'(x)h(x) - h'(x)g(x)}{[h(x)]^2}\]</div>
<p>For our specific case, the functions <span class="math notranslate nohighlight">\(g\)</span> and <span class="math notranslate nohighlight">\(h\)</span> with respect to the <span class="math notranslate nohighlight">\(i\)</span>-th
component are defined as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned} g_i &amp;= e^{z_i} \\ h &amp;= \sum_{k=1}^K e^{z_k} \end{aligned}
\end{split}\]</div>
<div class="note admonition">
<p class="admonition-title">Some Derivatives Tips So Far…</p>
<p>Firstly, the derivative of <span class="math notranslate nohighlight">\(h\)</span> with respect to any <span class="math notranslate nohighlight">\(z_j\)</span> is always <span class="math notranslate nohighlight">\(e^{z_j}\)</span>.
Why? The function <span class="math notranslate nohighlight">\(h\)</span> represents the sum of the exponentials of all components
of the input vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>. Mathematically, <span class="math notranslate nohighlight">\(h\)</span> is independent of the
index <span class="math notranslate nohighlight">\(i\)</span> and is a function of all <span class="math notranslate nohighlight">\(z_k\)</span> in the vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>. When we
differentiate <span class="math notranslate nohighlight">\(h\)</span> with respect to <span class="math notranslate nohighlight">\(z_j\)</span>, we treat <span class="math notranslate nohighlight">\(z_j\)</span> as the variable and
all other <span class="math notranslate nohighlight">\(z_k\)</span> (<span class="math notranslate nohighlight">\(k \neq j\)</span>)$ as constants.</p>
<p>The derivative of <span class="math notranslate nohighlight">\(e^{z_k}\)</span> with respect to <span class="math notranslate nohighlight">\(z_j\)</span> is <span class="math notranslate nohighlight">\(e^{z_j}\)</span> when <span class="math notranslate nohighlight">\(k=j\)</span>
because of the basic derivative rule <span class="math notranslate nohighlight">\(d\left(e^x\right) / d x=e^x\)</span>, and it’s 0
for all <span class="math notranslate nohighlight">\(k \neq j\)</span> because the derivative of a constant is 0 . Therefore, when
you sum up all these derivatives <span class="math notranslate nohighlight">\(\left(e^{z_j}\right.\)</span> for the term where <span class="math notranslate nohighlight">\(k=j\)</span>
and 0 for all others), the derivative of the sum <span class="math notranslate nohighlight">\(h_k\)</span> with respect to <span class="math notranslate nohighlight">\(z_j\)</span> is
simply <span class="math notranslate nohighlight">\(e^{z_j}\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial h}{\partial z_j}=\frac{\partial}{\partial z_j}\left(\sum_{k=1}^K e^{z_k}\right)=e^{z_j}
\]</div>
<p>Secondly, the derivative of <span class="math notranslate nohighlight">\(g_i\)</span> with respect to <span class="math notranslate nohighlight">\(z_j\)</span> is <span class="math notranslate nohighlight">\(e^{z_j}\)</span> if <span class="math notranslate nohighlight">\(i=j\)</span>,
and 0 otherwise. Why? The function <span class="math notranslate nohighlight">\(g_i\)</span> is defined as <span class="math notranslate nohighlight">\(e^{z_i}\)</span>, which only
involves a single component <span class="math notranslate nohighlight">\(z_i\)</span> of the input vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>. The
derivative of <span class="math notranslate nohighlight">\(g_i\)</span> with respect to <span class="math notranslate nohighlight">\(z_j\)</span> depends on whether <span class="math notranslate nohighlight">\(i\)</span> equals <span class="math notranslate nohighlight">\(j\)</span></p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(i=j\)</span>, then <span class="math notranslate nohighlight">\(g_i=e^{z_j}\)</span>, and the derivative of <span class="math notranslate nohighlight">\(e^{z_j}\)</span> with respect
to <span class="math notranslate nohighlight">\(z_j\)</span> is <span class="math notranslate nohighlight">\(e^{z_j}\)</span> itself, following the basic exponential derivative
rule.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(i \neq j, z_i\)</span> and <span class="math notranslate nohighlight">\(z_j\)</span> are considered independent variables. Since
<span class="math notranslate nohighlight">\(g_i\)</span> does not involve <span class="math notranslate nohighlight">\(z_j\)</span> in this case, the derivative is 0, reflecting
the principle that the derivative of a constant (or a term that does not
involve the variable of differentiation) is 0.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial g_i}{\partial z_j}=\frac{\partial e^{z_i}}{\partial z_j}= \begin{cases}e^{z_j} &amp; \text { if } i=j \\ 0 &amp; \text { if } i \neq j\end{cases}
\end{split}\]</div>
</div>
<section id="case-1-i-j">
<h4><a class="toc-backref" href="#id33" role="doc-backlink">Case 1: <span class="math notranslate nohighlight">\(i = j\)</span></a><a class="headerlink" href="#case-1-i-j" title="Link to this heading">#</a></h4>
<p>We seek to find the derivative of <span class="math notranslate nohighlight">\(\sigma(\mathbf{z})_i\)</span> with respect to <span class="math notranslate nohighlight">\(z_j\)</span>
and since <span class="math notranslate nohighlight">\(i=j\)</span>, we seek to find the derivative of <span class="math notranslate nohighlight">\(\sigma(\mathbf{z})_i\)</span> with
respect to <span class="math notranslate nohighlight">\(z_i\)</span>. Note it doesn’t matter if we use <span class="math notranslate nohighlight">\(i\)</span> or <span class="math notranslate nohighlight">\(j\)</span> as the index
because it applies to <em>any</em> index.</p>
<p>Given the softmax function’s <span class="math notranslate nohighlight">\(i\)</span>-th component:</p>
<div class="math notranslate nohighlight">
\[
\sigma(\mathbf{z})_i=\frac{e^{z_i}}{\sum_{k=1}^K e^{z_k}}
\]</div>
<p>To find the derivative of <span class="math notranslate nohighlight">\(\sigma(\mathbf{z})_i\)</span> with respect to <span class="math notranslate nohighlight">\(z_i\)</span>, we apply
the quotient rule:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \sigma(\mathbf{z})_i}{\partial z_i}=\frac{\left(e^{z_i} \cdot \sum_{k=1}^K e^{z_k}\right)-\left(e^{z_i} \cdot e^{z_i}\right)}{\left(\sum_{k=1}^K e^{z_k}\right)^2}
\]</div>
<p>Simplifying this expression, where the denominator is the square of the sum of
all exponentiated components of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, and recognizing that the numerator
simplifies to the difference between the product of <span class="math notranslate nohighlight">\(e^{z_i}\)</span> and the sum of all
exponentiated components minus the square of <span class="math notranslate nohighlight">\(e^{z_i}\)</span> :</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \sigma(\mathbf{z})_i}{\partial z_i}=\frac{e^{z_i} \cdot \sum_{k=1}^K e^{z_k}-e^{2 z_i}}{\left(\sum_{k=1}^K e^{z_k}\right)^2}
\]</div>
<p>Upon substituting the definition of <span class="math notranslate nohighlight">\(\sigma(\mathbf{z})_i\)</span> into this equation,
we observe:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \sigma(\mathbf{z})_i}{\partial z_i}=\frac{e^{z_i}}{\sum_{k=1}^K e^{z_k}}-\left(\frac{e^{z_i}}{\sum_{k=1}^K e^{z_k}}\right)^2
\]</div>
<p>Given that <span class="math notranslate nohighlight">\(\sigma(\mathbf{z})_i=\frac{e^{z_i}}{\sum_{k=1}^K e^{z_k}}\)</span>, it
follows that the derivative simplifies to:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \sigma(\mathbf{z})_i}{\partial z_i}=\sigma(\mathbf{z})_i-\sigma(\mathbf{z})_i^2
\]</div>
<p>Thus, expressing it in a more compact form:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \sigma(\mathbf{z})_i}{\partial z_i} = \sigma(\mathbf{z})_i (1 - \sigma(\mathbf{z})_i)
\]</div>
</section>
<section id="case-2-i-neq-j">
<h4><a class="toc-backref" href="#id34" role="doc-backlink">Case 2: <span class="math notranslate nohighlight">\(i \neq j\)</span></a><a class="headerlink" href="#case-2-i-neq-j" title="Link to this heading">#</a></h4>
<p>We then consider the scenario where we aim to compute the derivative of the
<span class="math notranslate nohighlight">\(i\)</span>-th component of the softmax output with respect to a different component
<span class="math notranslate nohighlight">\(z_j\)</span>, where <span class="math notranslate nohighlight">\(i \neq j\)</span>. This examines how changes in one input logit affect the
probability associated with a different class.</p>
<p>Given the <span class="math notranslate nohighlight">\(i\)</span>-th component of the softmax function:</p>
<div class="math notranslate nohighlight">
\[
\sigma(\mathbf{z})_i=\frac{e^{z_i}}{\sum_{k=1}^K e^{z_k}}
\]</div>
<p>To compute <span class="math notranslate nohighlight">\(\frac{\partial \sigma(\mathbf{z})_i}{\partial z_j}\)</span> for <span class="math notranslate nohighlight">\(i \neq j\)</span>,
we again apply the quotient rule, but with an understanding that the derivative
of the numerator <span class="math notranslate nohighlight">\(e^{z_i}\)</span> with respect to <span class="math notranslate nohighlight">\(z_j\)</span> is 0 , since <span class="math notranslate nohighlight">\(z_i\)</span> does not
change when <span class="math notranslate nohighlight">\(z_j\)</span> changes:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \sigma(\mathbf{z})_i}{\partial z_j}=\frac{0-e^{z_i} \cdot e^{z_j}}{\left(\sum_{k=1}^K e^{z_k}\right)^2}
\]</div>
<p>This expression can be simplified as:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \sigma(\mathbf{z})_i}{\partial z_j}=-\frac{e^{z_i} \cdot e^{z_j}}{\left(\sum_{k=1}^K e^{z_k}\right)^2}
\]</div>
<p>Substituting the definition of <span class="math notranslate nohighlight">\(\sigma(\mathbf{z})_i\)</span> into this equation, we
get:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \sigma(\mathbf{z})_i}{\partial z_j}=-\frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} \cdot \frac{e^{z_i}}{\sum_{k=1}^K e^{z_k}}
\]</div>
<p>Given <span class="math notranslate nohighlight">\(\sigma(\mathbf{z})_i=\frac{e^{z_i}}{\sum_{k=1}^K e^{z_k}}\)</span> and similarly,
the term <span class="math notranslate nohighlight">\(\frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}\)</span> can be recognized as
<span class="math notranslate nohighlight">\(\sigma(\mathbf{z})_j\)</span> , the expression simplifies to:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \sigma(\mathbf{z})_i}{\partial z_j}=-\sigma(\mathbf{z})_i \sigma(\mathbf{z})_j
\]</div>
<p>This formulation shows that the rate of change of the probability of one class
<span class="math notranslate nohighlight">\((i)\)</span> with respect to the logit of a different class <span class="math notranslate nohighlight">\((j)\)</span> is negative,
indicating that as the logit <span class="math notranslate nohighlight">\(z_j\)</span> increases, the probability of class <span class="math notranslate nohighlight">\(i\)</span>
decreases, assuming <span class="math notranslate nohighlight">\(i \neq j\)</span>.</p>
<p>Thus, for <span class="math notranslate nohighlight">\(i \neq j\)</span>, the derivative of the <span class="math notranslate nohighlight">\(i\)</span>-th softmax component with
respect to <span class="math notranslate nohighlight">\(z_j\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \sigma(\mathbf{z})_i}{\partial z_j}=-\sigma(\mathbf{z})_i \sigma(\mathbf{z})_j
\]</div>
</section>
<section id="kronecker-delta">
<h4><a class="toc-backref" href="#id35" role="doc-backlink">Kronecker Delta</a><a class="headerlink" href="#kronecker-delta" title="Link to this heading">#</a></h4>
<p>The partial derivative of the softmax output with respect to its input,
considering both cases, can be written in cases:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial \sigma(\mathbf{z})_i}{\partial z_j} = \begin{cases} \sigma(\mathbf{z})_i (1 - \sigma(\mathbf{z})_i) &amp; \text{if } i=j \\ -\sigma(\mathbf{z})_i \sigma(\mathbf{z})_j &amp; \text{if } i \neq j \end{cases}
\end{split}\]</div>
<p>And it is common to represent the above case either as indicator functions or
the <a class="reference external" href="https://en.wikipedia.org/wiki/Kronecker_delta">Kronecker delta</a>
<span class="math notranslate nohighlight">\(\delta_{ij}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \sigma(\mathbf{z})_i}{\partial z_j} = \sigma(\mathbf{z})_i
(\delta_{ij} - \sigma(\mathbf{z})_j)
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\delta_{ij} = \begin{cases} 1 &amp; \text{if } i=j \\ 0 &amp; \text{if } i \neq j \end{cases}
\end{split}\]</div>
<p>And if we were to use indicator functions, the above equation can be written as:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \sigma(\mathbf{z})_i}{\partial z_j} = \sigma(\mathbf{z})_i
\left(\mathbb{1}_{i=j} - \sigma(\mathbf{z})_j\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{1}_{i=j}\)</span> is the indicator function that evaluates to 1 if <span class="math notranslate nohighlight">\(i=j\)</span>
and 0 otherwise.</p>
</section>
</section>
<section id="representing-derivative-of-softmax-as-a-jacobian-matrix">
<h3><a class="toc-backref" href="#id36" role="doc-backlink">Representing Derivative of Softmax as a Jacobian Matrix</a><a class="headerlink" href="#representing-derivative-of-softmax-as-a-jacobian-matrix" title="Link to this heading">#</a></h3>
<p>Consider the softmax output vector <span class="math notranslate nohighlight">\(S\)</span> for an input vector
<span class="math notranslate nohighlight">\(\mathbf{z} = \begin{bmatrix} z_1 &amp; z_2 &amp; \ldots &amp; z_K \end{bmatrix}^\top\)</span>,
where <span class="math notranslate nohighlight">\(K\)</span> is the number of classes or dimensions of the softmax output. The
softmax function is defined as:</p>
<div class="math notranslate nohighlight">
\[
S_i = \sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{k=1}^K e^{z_k}} \quad
\text{for } i=1, \ldots, K
\]</div>
<div class="proof definition admonition" id="softmax-output-vector">
<p class="admonition-title"><span class="caption-number">Definition 59 </span> (Softmax Output as Vector)</p>
<section class="definition-content" id="proof-content">
<p>The softmax output vector <span class="math notranslate nohighlight">\(S\)</span> can be explicitly represented as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
S = \begin{bmatrix} S_1 \\ S_2 \\ \vdots \\ S_K \end{bmatrix} =
\begin{bmatrix} \frac{e^{z_1}}{\sum_{k=1}^K e^{z_k}} \\
\frac{e^{z_2}}{\sum_{k=1}^K e^{z_k}} \\ \vdots \\ \frac{e^{z_K}}{\sum_{k=1}^K
e^{z_k}} \end{bmatrix}
\end{split}\]</div>
</section>
</div><div class="proof definition admonition" id="jacobian-softmax">
<p class="admonition-title"><span class="caption-number">Definition 60 </span> (Jacobian Matrix of Softmax Function)</p>
<section class="definition-content" id="proof-content">
<p>The Jacobian matrix <span class="math notranslate nohighlight">\(J_S\)</span> of the softmax function captures the partial
derivatives of each component of <span class="math notranslate nohighlight">\(S\)</span> with respect to each component of
<span class="math notranslate nohighlight">\(\mathbf{z}\)</span>. Specifically, the element at the <span class="math notranslate nohighlight">\(i\)</span>-th row and <span class="math notranslate nohighlight">\(j\)</span>-th column of
<span class="math notranslate nohighlight">\(J_S\)</span>, denoted as <span class="math notranslate nohighlight">\((J_S)_{ij}\)</span>, is the partial derivative of <span class="math notranslate nohighlight">\(S_i\)</span> with respect
to <span class="math notranslate nohighlight">\(z_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[(J_S)_{ij} = \frac{\partial S_i}{\partial z_j}\]</div>
<p>For <span class="math notranslate nohighlight">\(i = j\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial S_i}{\partial z_i} = S_i (1 - S_i)\]</div>
<p>For <span class="math notranslate nohighlight">\(i \neq j\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial S_i}{\partial z_j} = -S_i S_j\]</div>
<p>Putting it all together, the Jacobian matrix <span class="math notranslate nohighlight">\(J_S\)</span> can be written as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_S = \begin{bmatrix} S_1 (1 - S_1) &amp; -S_1 S_2 &amp; \cdots &amp; -S_1 S_K \\ -S_2
S_1 &amp; S_2 (1 - S_2) &amp; \cdots &amp; -S_2 S_K \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
-S_K S_1 &amp; -S_K S_2 &amp; \cdots &amp; S_K (1 - S_K) \end{bmatrix}
\end{split}\]</div>
</section>
</div><div class="proof definition admonition" id="matrix-formulation-softmax">
<p class="admonition-title"><span class="caption-number">Definition 61 </span> (Matrix Formulation)</p>
<section class="definition-content" id="proof-content">
<p>We can express <span class="math notranslate nohighlight">\(J_S\)</span> using matrix operations for compactness:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{diag}(S)\)</span> is a diagonal matrix where each diagonal element is <span class="math notranslate nohighlight">\(S_i\)</span>,
the softmax output for class <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(S S^T\)</span> is the outer product of <span class="math notranslate nohighlight">\(S\)</span> with itself, creating a matrix where
each element <span class="math notranslate nohighlight">\((i, j)\)</span> is <span class="math notranslate nohighlight">\(S_i S_j\)</span>.</p></li>
</ul>
<p>Thus, the Jacobian matrix <span class="math notranslate nohighlight">\(J_S\)</span> can be expressed as:</p>
<div class="math notranslate nohighlight">
\[
J_S = \text{diag}(S) - S S^T
\]</div>
</section>
</div></section>
<section id="implementation-of-the-jacobian-matrix">
<h3><a class="toc-backref" href="#id37" role="doc-backlink">Implementation of the Jacobian Matrix</a><a class="headerlink" href="#implementation-of-the-jacobian-matrix" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="kn">import</span> <span class="nn">torch</span>
<span class="linenos"> 2</span><span class="kn">from</span> <span class="nn">rich.pretty</span> <span class="kn">import</span> <span class="n">pprint</span>
<span class="linenos"> 3</span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="k">class</span> <span class="nc">Softmax</span><span class="p">:</span>
<span class="linenos"> 6</span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="linenos"> 7</span>        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span>    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="linenos">10</span>        <span class="n">max_z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
<span class="linenos">11</span>        <span class="n">numerator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">max_z</span><span class="p">)</span>
<span class="linenos">12</span>        <span class="n">denominator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">numerator</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos">13</span>        <span class="n">g</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>
<span class="linenos">14</span>        <span class="k">return</span> <span class="n">g</span>
<span class="linenos">15</span>
<span class="linenos">16</span>    <span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="linenos">17</span>        <span class="n">S</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="fm">__call__</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="linenos">18</span>        <span class="n">diag_S</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag_embed</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
<span class="linenos">19</span>        <span class="n">outer_S</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">S</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">S</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="linenos">20</span>        <span class="n">gradient</span> <span class="o">=</span> <span class="n">diag_S</span> <span class="o">-</span> <span class="n">outer_S</span>
<span class="linenos">21</span>        <span class="k">return</span> <span class="n">gradient</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
<span class="linenos">22</span>
<span class="linenos">23</span>
<span class="linenos">24</span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="linenos">25</span><span class="n">pprint</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="linenos">26</span>
<span class="linenos">27</span><span class="n">pytorch_softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos">28</span><span class="n">pytorch_softmax_outputs</span> <span class="o">=</span> <span class="n">pytorch_softmax</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="linenos">29</span><span class="n">pprint</span><span class="p">(</span><span class="n">pytorch_softmax_outputs</span><span class="p">)</span>
<span class="linenos">30</span>
<span class="linenos">31</span><span class="n">my_softmax</span> <span class="o">=</span> <span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos">32</span><span class="n">my_softmax_outputs</span> <span class="o">=</span> <span class="n">my_softmax</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="linenos">33</span><span class="n">pprint</span><span class="p">(</span><span class="n">my_softmax_outputs</span><span class="p">)</span>
<span class="linenos">34</span>
<span class="linenos">35</span><span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span>
<span class="linenos">36</span>    <span class="n">pytorch_softmax_outputs</span><span class="p">,</span> <span class="n">my_softmax_outputs</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1.3e-6</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">msg</span><span class="o">=</span><span class="s2">&quot;Softmax function outputs do not match.&quot;</span>
<span class="linenos">37</span><span class="p">)</span>
<span class="linenos">38</span>
<span class="linenos">39</span><span class="n">B</span><span class="p">,</span> <span class="n">K</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="linenos">40</span>
<span class="linenos">41</span><span class="n">pytorch_jacobian</span> <span class="o">=</span> <span class="p">[]</span>
<span class="linenos">42</span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
<span class="linenos">43</span>    <span class="n">grad</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">pytorch_softmax_outputs</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">z</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="linenos">44</span>    <span class="n">pytorch_jacobian</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
<span class="linenos">45</span>
<span class="linenos">46</span><span class="n">pytorch_jacobian</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">pytorch_jacobian</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos">47</span><span class="n">pprint</span><span class="p">(</span><span class="n">pytorch_jacobian</span><span class="p">)</span>
<span class="linenos">48</span>
<span class="linenos">49</span><span class="n">my_jacobian</span> <span class="o">=</span> <span class="n">my_softmax</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="linenos">50</span><span class="n">pprint</span><span class="p">(</span><span class="n">my_jacobian</span><span class="p">)</span>
<span class="linenos">51</span>
<span class="linenos">52</span><span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span>
<span class="linenos">53</span>    <span class="n">pytorch_jacobian</span><span class="p">,</span> <span class="n">my_jacobian</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1.3e-6</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">msg</span><span class="o">=</span><span class="s2">&quot;Jacobian matrices do not match.&quot;</span>
<span class="linenos">54</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.7717</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.2647</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.1533</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.5957</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4134</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.3994</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.9086</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.4960</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.1280</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-1.4486</span><span style="font-weight: bold">]]</span>, <span style="color: #808000; text-decoration-color: #808000">requires_grad</span>=<span style="color: #00ff00; text-decoration-color: #00ff00; font-style: italic">True</span><span style="font-weight: bold">)</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5354</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1899</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0781</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1364</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0602</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3118</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5189</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1263</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0249</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0181</span><span style="font-weight: bold">]]</span>, <span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">SoftmaxBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5354</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1899</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0781</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1364</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0602</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.3118</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.5189</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1263</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0249</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0181</span><span style="font-weight: bold">]]</span>, <span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">DivBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2487</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1017</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0418</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0730</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0322</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1017</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1538</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0148</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0259</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0114</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0418</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0148</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0720</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0107</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0047</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0730</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0259</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0107</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1178</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0082</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0322</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0114</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0047</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0082</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0566</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2146</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1618</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0394</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0078</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0056</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1618</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2496</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0656</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0129</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0094</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0394</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0656</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1104</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0031</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0023</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0078</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0129</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0031</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0243</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0005</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0056</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0094</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0023</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0005</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0177</span><span style="font-weight: bold">]]])</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">tensor</span><span style="font-weight: bold">([[[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2487</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1017</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0418</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0730</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0322</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1017</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1538</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0148</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0259</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0114</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0418</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0148</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0720</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0107</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0047</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0730</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0259</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0107</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1178</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0082</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0322</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0114</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0047</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0082</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0566</span><span style="font-weight: bold">]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │   </span><span style="font-weight: bold">[[</span> <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2146</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1618</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0394</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0078</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0056</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.1618</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.2496</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0656</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0129</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0094</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0394</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0656</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.1104</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0031</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0023</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0078</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0129</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0031</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0243</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0005</span><span style="font-weight: bold">]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   │    </span><span style="font-weight: bold">[</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0056</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0094</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0023</span>, <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">-0.0005</span>,  <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0177</span><span style="font-weight: bold">]]]</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│      </span><span style="color: #808000; text-decoration-color: #808000">grad_fn</span>=<span style="font-weight: bold">&lt;</span><span style="color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold">SqueezeBackward0</span><span style="font-weight: bold">&gt;)</span>
</pre>
</div></div>
</div>
</section>
<section id="derivative-of-softmax-with-respect-to-weight-matrix">
<h3><a class="toc-backref" href="#id38" role="doc-backlink">Derivative Of Softmax With Respect To Weight Matrix</a><a class="headerlink" href="#derivative-of-softmax-with-respect-to-weight-matrix" title="Link to this heading">#</a></h3>
<p>In the context of neural networks, the softmax function is often used in the
output layer to compute the probabilities of each class. The output of the
linear transformation can be represented as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z} = \boldsymbol{\theta}^{\top} \mathbf{x} + \mathbf{b}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is the weight matrix, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is the input
vector, and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> is the bias vector.</p>
<p>Our predicted softmax output is expressed as:</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathbf{y}} = \sigma(\mathbf{z}) \quad \textrm{where}\quad \hat{y}_k = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}}
\]</div>
<p>For a classification task, a common choice is the Cross-Entropy Loss, defined
between the predicted probability distribution <span class="math notranslate nohighlight">\(\hat{\mathbf{y}}\)</span> and the true
distribution <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>. If <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is one-hot encoded, where <span class="math notranslate nohighlight">\(y_i = 1\)</span>
for the correct class and <span class="math notranslate nohighlight">\(y_i = 0\)</span> for all others, the Cross-Entropy Loss
<span class="math notranslate nohighlight">\(\mathcal{L}\)</span> over <span class="math notranslate nohighlight">\(K\)</span> classes can be defined as:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}\left(\mathbf{y}, \hat{\mathbf{y}}, \boldsymbol{\hat{\theta}}\right) = -\sum_{j=1}^{K} y_j \log(\hat{y}_j)
\]</div>
<p>where we make explicit the dependence on the parameters
<span class="math notranslate nohighlight">\(\boldsymbol{\hat{\theta}}\)</span> of the model because ultimately, the loss function
is a function of the model’s parameters.</p>
<section id="step-1-derivative-of-mathcal-l-with-respect-to-mathbf-z">
<h4><a class="toc-backref" href="#id39" role="doc-backlink">Step 1: Derivative of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> with respect to <span class="math notranslate nohighlight">\(\mathbf{z}\)</span></a><a class="headerlink" href="#step-1-derivative-of-mathcal-l-with-respect-to-mathbf-z" title="Link to this heading">#</a></h4>
<p>Given:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L} = -\sum_{j=1}^{K} y_j \log(\hat{y}_j)\]</div>
<p>And the softmax output:</p>
<div class="math notranslate nohighlight">
\[\hat{y}_j = \sigma(\mathbf{z})_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}\]</div>
<p>The derivative of the Cross-Entropy Loss with respect to each logit <span class="math notranslate nohighlight">\(z_j\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial \mathcal{L}}{\partial z_j} = \hat{y}_j - y_j\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\frac{\partial \mathcal{L}}{\partial z_j}\)</span> can be expressed as chain
rule:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial z_j} &amp;= \frac{\partial \mathcal{L}}{\partial \hat{y}_j} \cdot \frac{\partial \hat{y}_j}{\partial z_j} \\
&amp;= \frac{\partial \mathcal{L}}{\partial \sigma(\mathbf{z})_j} \cdot \frac{\partial \sigma(\mathbf{z})_j}{\partial z_j} \\
\end{aligned}
\end{split}\]</div>
</section>
<section id="step-2-derivative-of-mathbf-z-with-respect-to-boldsymbol-theta">
<h4><a class="toc-backref" href="#id40" role="doc-backlink">Step 2: Derivative of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span></a><a class="headerlink" href="#step-2-derivative-of-mathbf-z-with-respect-to-boldsymbol-theta" title="Link to this heading">#</a></h4>
<p>The linear transformation that produces the logits is:</p>
<div class="math notranslate nohighlight">
\[\mathbf{z} = \boldsymbol{\theta}^{\top} \mathbf{x} + \mathbf{b}\]</div>
<p>For a specific element <span class="math notranslate nohighlight">\(z_j\)</span> and a weight <span class="math notranslate nohighlight">\(\theta_{ij}\)</span> (the weight connecting
input <span class="math notranslate nohighlight">\(i\)</span> to output class <span class="math notranslate nohighlight">\(j\)</span>), the derivative is:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial z_j}{\partial \theta_{ij}} = x_i\]</div>
</section>
<section id="step-3-derivative-of-mathcal-l-with-respect-to-boldsymbol-theta">
<h4><a class="toc-backref" href="#id41" role="doc-backlink">Step 3: Derivative of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span></a><a class="headerlink" href="#step-3-derivative-of-mathcal-l-with-respect-to-boldsymbol-theta" title="Link to this heading">#</a></h4>
<p>Applying the chain rule to connect the derivative of the loss with respect to
<span class="math notranslate nohighlight">\(\theta_{ij}\)</span> through the logits:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}}{\partial \theta_{ij}} = \frac{\partial
\mathcal{L}}{\partial z_j} \cdot \frac{\partial z_j}{\partial \theta_{ij}} =
(\hat{y}_j - y_j) \cdot x_i
\]</div>
<p>The full gradient with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is constructed by
aggregating these individual derivatives:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \boldsymbol{\theta}} &amp;= \left(\hat{\mathbf{y}} - \mathbf{y}\right) \mathbf{x}^{\top} \\
\end{aligned}
\end{split}\]</div>
</section>
</section>
<section id="hessian-matrix">
<h3><a class="toc-backref" href="#id42" role="doc-backlink">Hessian Matrix</a><a class="headerlink" href="#hessian-matrix" title="Link to this heading">#</a></h3>
<p>The Hessian matrix represents the second-order partial derivatives of the
softmax function’s components with respect to the components of the input vector
<span class="math notranslate nohighlight">\(\mathbf{z}\)</span>. The Hessian gives us insight into the curvature of the softmax
function, which is particularly useful for understanding the optimization
landscape. In fact, the Hessian matrix is a key component in monitoring the loss
function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> where the first order partial derivatives measures the
rate of change of the loss function with respect to the parameters, and the
second order partial derivatives measures the curvature of the loss function -
which says about convexity/concavity, saddle points, minima and maxima etc. See
<a class="reference external" href="https://d2l.ai/chapter_optimization/optimization-intro.html">12.1. Optimization and Deep Learning - Dive Into Deep Learning</a>
for more insights.</p>
<p>Given the softmax function <span class="math notranslate nohighlight">\(\sigma(\mathbf{z})_i\)</span> for component <span class="math notranslate nohighlight">\(i\)</span>, defined as:</p>
<div class="math notranslate nohighlight">
\[
\sigma(\mathbf{z})_i=\frac{e^{z_i}}{\sum_{k=1}^K e^{z_k}}
\]</div>
<p>Hessian Matrix for Softmax The Hessian matrix <span class="math notranslate nohighlight">\(\left(H_\sigma\right)\)</span> of the
softmax function with respect to the input vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is a <span class="math notranslate nohighlight">\(K \times K\)</span>
matrix where each element <span class="math notranslate nohighlight">\((i, j)\)</span> is the second-order partial derivative of the
softmax output for class <span class="math notranslate nohighlight">\(i\)</span> with respect to the inputs <span class="math notranslate nohighlight">\(z_i\)</span> and <span class="math notranslate nohighlight">\(z_j\)</span>, denoted
as:</p>
<div class="math notranslate nohighlight">
\[
\left(H_\sigma\right)_{i j}=\frac{\partial^2 \sigma(\mathbf{z})}{\partial z_i \partial z_j}
\]</div>
</section>
</section>
<section id="references-and-further-readings">
<h2><a class="toc-backref" href="#id43" role="doc-backlink">References and Further Readings</a><a class="headerlink" href="#references-and-further-readings" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_function">Softmax - Wikipedia</a></p></li>
<li><p><a class="reference external" href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/">The Softmax function and its derivative - Eli Bendersky</a></p></li>
<li><p><a class="reference external" href="https://stats.stackexchange.com/questions/527080/what-is-the-role-of-temperature-in-softmax">What is the role of temperature in Softmax?</a></p></li>
<li><p><a class="reference external" href="https://stats.stackexchange.com/questions/235528/backpropagation-with-softmax-cross-entropy">Backpropagation with Softmax / Cross Entropy</a></p></li>
<li><p><a class="reference external" href="https://stats.stackexchange.com/questions/189331/why-is-the-softmax-used-to-represent-a-probability-distribution">Why is the softmax used to represent a probability distribution?</a></p></li>
<li><p><a class="reference external" href="https://jmlb.github.io/ml/2017/12/26/Calculate_Gradient_Softmax/">Derivation of the Gradient of the cross-entropy Loss</a></p></li>
</ul>
</section>
<section id="citations">
<h2><a class="toc-backref" href="#id44" role="doc-backlink">Citations</a><a class="headerlink" href="#citations" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>[1] D. A. Roberts, S. Yaida, B. Hanin, “Chapter 6.2.1 Bayesian Model
Fitting” in
<a class="reference external" href="https://arxiv.org/abs/2106.10165">“The Principles of Deep Learning Theory”</a>,
arXiv preprint arXiv:2106.10165, [Submitted on 18 Jun 2021 (v1), last
revised 24 Aug 2021 (this version, v2)].</p></li>
<li><p>[2] I. Goodfellow, Y. Bengio, A. Courville, Chapter 6.2.2.3 Softmax Units
for Multinoulli Output Distributions in
<a class="reference external" href="http://www.deeplearningbook.org/">“Deep Learning”</a>, MIT Press, 2016. pp.
180-184</p></li>
<li><p>[3] C. M. Bishop, Chapter 4. Linear Models for Classification in Pattern
Recognition and Machine Learning. New York: Springer, 2006.</p></li>
<li><p>[4] A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola,
<a class="reference external" href="https://d2l.ai/chapter_linear-classification/softmax-regression.html">“Chapter 3.4. Softmax Regression”</a>
in Dive into Deep Learning, Cambridge University Press, 2023.</p></li>
<li><p>[5] A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola,
<a class="reference external" href="https://d2l.ai/chapter_optimization/optimization-intro.html">“Chapter 12.1. Optimization and Deep Learning”</a>
in Dive into Deep Learning, Cambridge University Press, 2023.</p></li>
</ul>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="softmax-wikipedia" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_function">Softmax - Wikipedia</a></p>
</aside>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./playbook"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="training/how_to_teacher_student_knowledge_distillation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">How To Do Teacher-Student Knowledge Distillation?</p>
      </div>
    </a>
    <a class="right-next"
       href="how_to_inspect_function_and_class_signatures.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">How to Inspect Function and Class Signatures in Python?</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-formulation">Problem Formulation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#functional-form-of-f">Functional Form of <span class="math notranslate nohighlight">\(f\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-interpretation">Probabilistic Interpretation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unnormalized-logits">Unnormalized Logits</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-function">Softmax Function</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-obeys-the-three-axioms-of-probability-kolmogorov-axioms">Softmax Obeys The Three Axioms of Probability (Kolmogorov Axioms)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#non-negativity">Non-Negativity</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization">Normalization</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#additivity">Additivity</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#calibration">Calibration</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-preserves-order-monotonicity">Softmax Preserves Order (Monotonicity)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-is-translation-invariance">Softmax Is Translation Invariance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-instability-of-the-softmax-function">Numerical Instability of the Softmax Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-is-not-invariant-under-scaling">Softmax Is Not Invariant Under Scaling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sharpening-and-dampening-the-softmax-distribution">Sharpening and Dampening the Softmax Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#temperature">Temperature</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-from-the-softmax-distribution">Sampling from the Softmax Distribution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-is-smooth-continuous-and-differentiable">Softmax Is Smooth, Continuous and Differentiable</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-function-via-exponential-family">Softmax Function via Exponential Family</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-jacobian-and-hessian-of-softmax">Gradient, Jacobian, and Hessian of Softmax</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-as-a-vector-function">Softmax as a Vector Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivatives-of-the-softmax-function">Derivatives of the Softmax Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-jacobian-matrix-of-softmax">The Jacobian Matrix of Softmax</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivative-of-the-softmax-function">Derivative of the Softmax Function</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#case-1-i-j">Case 1: <span class="math notranslate nohighlight">\(i = j\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#case-2-i-neq-j">Case 2: <span class="math notranslate nohighlight">\(i \neq j\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kronecker-delta">Kronecker Delta</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#representing-derivative-of-softmax-as-a-jacobian-matrix">Representing Derivative of Softmax as a Jacobian Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-of-the-jacobian-matrix">Implementation of the Jacobian Matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivative-of-softmax-with-respect-to-weight-matrix">Derivative Of Softmax With Respect To Weight Matrix</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-derivative-of-mathcal-l-with-respect-to-mathbf-z">Step 1: Derivative of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> with respect to <span class="math notranslate nohighlight">\(\mathbf{z}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-derivative-of-mathbf-z-with-respect-to-boldsymbol-theta">Step 2: Derivative of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-derivative-of-mathcal-l-with-respect-to-boldsymbol-theta">Step 3: Derivative of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hessian-matrix">Hessian Matrix</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#citations">Citations</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>