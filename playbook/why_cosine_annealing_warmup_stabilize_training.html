
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Why Does Cosine Annealing With Warmup Stabilize Training? &#8212; Omniverse</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=bb35926c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MYW5YKC2WF"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"defeq": "\\overset{\\text{def}}{=}", "defa": "\\overset{\\text{(a)}}{=}", "defb": "\\overset{\\text{(b)}}{=}", "defc": "\\overset{\\text{(c)}}{=}", "defd": "\\overset{\\text{(d)}}{=}", "st": "\\mid", "mod": "\\mid", "S": "\\Omega", "s": "\\omega", "e": "\\exp", "P": "\\mathbb{P}", "R": "\\mathbb{R}", "expectation": "\\mathbb{E}", "v": "\\mathbf{v}", "a": "\\mathbf{a}", "b": "\\mathbf{b}", "c": "\\mathbf{c}", "u": "\\mathbf{u}", "w": "\\mathbf{w}", "x": "\\mathbf{x}", "y": "\\mathbf{y}", "z": "\\mathbf{z}", "0": "\\mathbf{0}", "1": "\\mathbf{1}", "A": "\\mathbf{A}", "B": "\\mathbf{B}", "C": "\\mathbf{C}", "E": "\\mathcal{F}", "eventA": "\\mathcal{A}", "lset": "\\left\\{", "rset": "\\right\\}", "lsq": "\\left[", "rsq": "\\right]", "lpar": "\\left(", "rpar": "\\right)", "lcurl": "\\left\\{", "rcurl": "\\right\\}", "pmf": "p_X", "pdf": "f_X", "pdftwo": "f_{X,Y}", "pdfjoint": "f_{\\mathbf{X}}", "pmfjointxy": "p_{X, Y}", "pdfjointxy": "f_{X, Y}", "cdf": "F_X", "pspace": "(\\Omega, \\mathcal{F}, \\mathbb{P})", "var": "\\operatorname{Var}", "std": "\\operatorname{Std}", "bern": "\\operatorname{Bernoulli}", "binomial": "\\operatorname{Binomial}", "geometric": "\\operatorname{Geometric}", "poisson": "\\operatorname{Poisson}", "uniform": "\\operatorname{Uniform}", "normal": "\\operatorname{Normal}", "gaussian": "\\operatorname{Gaussian}", "gaussiansymbol": "\\mathcal{N}", "exponential": "\\operatorname{Exponential}", "iid": "\\textbf{i.i.d.}", "and": "\\text{and}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'playbook/why_cosine_annealing_warmup_stabilize_training';</script>
    <link rel="canonical" href="https://www.gaohongnan.com/playbook/why_cosine_annealing_warmup_stabilize_training.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling." href="why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html" />
    <link rel="prev" title="How to Inspect Function and Class Signatures in Python?" href="how_to_inspect_function_and_class_signatures.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Omniverse - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Omniverse - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    🌌 Omniverse: A Journey Through Knowledge
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../notations/machine_learning.html">Machine Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Influential Ideas and Papers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../influential/generative_pretrained_transformer/01_intro.html">Generative Pre-trained Transformers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../influential/generative_pretrained_transformer/02_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/generative_pretrained_transformer/03_concept.html">The Concept of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/generative_pretrained_transformer/04_implementation.html">The Implementation of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/generative_pretrained_transformer/05_adder.html">Training a Mini-GPT to Learn Two-Digit Addition</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../influential/low_rank_adaptation/01_intro.html">Low-Rank Adaptation Of Large Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../influential/low_rank_adaptation/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/low_rank_adaptation/03_implementation.html">Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../influential/empirical_risk_minimization/01_intro.html">Empirical Risk Minimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../influential/empirical_risk_minimization/02_concept.html">Concept: Empirical Risk Minimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/empirical_risk_minimization/03_bayes_optimal_classifier.html">Bayes Optimal Classifier</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../influential/learning_theory/01_intro.html">Is The Learning Problem Solvable?</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../influential/learning_theory/02_concept.html">Concept: Learning Theory</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../influential/kmeans_clustering/01_intro.html">K-Means</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../influential/kmeans_clustering/02_concept.html">Concept: K-Means Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/kmeans_clustering/03_implementation.html">Implementation: K-Means (Lloyd)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/kmeans_clustering/04_image_segmentation.html">Application: Image Compression and Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/kmeans_clustering/05_conceptual_questions.html">Conceptual Questions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../influential/naive_bayes/01_intro.html">Naive Bayes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../influential/naive_bayes/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/naive_bayes/03_implementation.html">Naives Bayes Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/naive_bayes/04_example_penguins.html">Naive Bayes Application: Penguins</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/naive_bayes/05_application_mnist.html">Naive Bayes Application (MNIST)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../influential/gaussian_mixture_models/01_intro.html">Mixture Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../influential/gaussian_mixture_models/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../influential/gaussian_mixture_models/03_implementation.html">Gaussian Mixture Models Implementation</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Playbook</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="how_to_calculate_flops_in_transformer_based_models.html">How to Calculate the Number of FLOPs in Transformer Based Models?</a></li>
<li class="toctree-l1"><a class="reference internal" href="how_to_inspect_function_and_class_signatures.html">How to Inspect Function and Class Signatures in Python?</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Why Does Cosine Annealing With Warmup Stabilize Training?</a></li>
<li class="toctree-l1"><a class="reference internal" href="why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html">Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling.</a></li>
<li class="toctree-l1"><a class="reference internal" href="how_to_finetune_decoder_with_last_token_pooling.html">How To Fine-Tune Decoder-Only Models For Sequence Classification Using Last Token Pooling?</a></li>
<li class="toctree-l1"><a class="reference internal" href="how_to_finetune_decoder_with_cross_attention.html">How To Fine-Tune Decoder-Only Models For Sequence Classification With Cross-Attention?</a></li>
<li class="toctree-l1"><a class="reference internal" href="how_to_teacher_student_knowledge_distillation.html">How To Do Teacher-Student Knowledge Distillation?</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Probability Theory</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../probability_theory/01_mathematical_preliminaries/intro.html">Chapter 1. Mathematical Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/01_mathematical_preliminaries/01_combinatorics.html">Permutations and Combinations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/01_mathematical_preliminaries/02_calculus.html">Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/01_mathematical_preliminaries/03_contours.html">Contour Maps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/01_mathematical_preliminaries/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../probability_theory/02_probability/intro.html">Chapter 2. Probability</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/02_probability/0202_probability_space.html">Probability Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/02_probability/0203_probability_axioms.html">Probability Axioms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/02_probability/0204_conditional_probability.html">Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/02_probability/0205_independence.html">Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/02_probability/0206_bayes_theorem.html">Baye’s Theorem and the Law of Total Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/02_probability/summary.html">Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/intro.html">Chapter 3. Discrete Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/0301_random_variables.html">Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/0302_discrete_random_variables.html">Discrete Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/0303_probability_mass_function.html">Probability Mass Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/0304_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/0305_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/0306_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/uniform/intro.html">Discrete Uniform Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/bernoulli/intro.html">Bernoulli Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/iid.html">Independent and Identically Distributed (IID)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/binomial/intro.html">Binomial Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_implementation.html">Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/binomial/0309_binomial_distribution_application.html">Real World Examples</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/geometric/intro.html">Geometric Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/geometric/0310_geometric_distribution_concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/poisson/intro.html">Poisson Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/poisson/0311_poisson_distribution_implementation.html">Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/summary.html">Important</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/03_discrete_random_variables/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/intro.html">Chapter 4. Continuous Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/from_discrete_to_continuous.html">From Discrete to Continuous</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0401_continuous_random_variables.html">Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0402_probability_density_function.html">Probability Density Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0403_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0404_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0405_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0406_mean_median_mode.html">Mean, Median and Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0407_continuous_uniform_distribution.html">Continuous Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0408_exponential_distribution.html">Exponential Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0409_gaussian_distribution.html">Gaussian Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0410_skewness_and_kurtosis.html">Skewness and Kurtosis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0411_convolve_and_sum_of_random_variables.html">Convolution and Sum of Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/04_continuous_random_variables/0412_functions_of_random_variables.html">Functions of Random Variables</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../probability_theory/05_joint_distributions/intro.html">Chapter 5. Joint Distributions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../probability_theory/05_joint_distributions/from_single_variable_to_joint_distributions.html">From Single Variable to Joint Distributions</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/intro.html">Joint PMF and PDF</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0501_joint_pmf_pdf/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/intro.html">Joint Expectation and Correlation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0502_joint_expectation_and_correlation/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/intro.html">Conditional PMF and PDF</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0503_conditional_pmf_pdf/application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/intro.html">Conditional Expectation and Variance</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0504_conditional_expectation_variance/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/05_joint_distributions/0505_sum_of_random_variables/intro.html">Sum of Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0505_sum_of_random_variables/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/05_joint_distributions/0506_random_vectors/intro.html">Random Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0506_random_vectors/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/05_joint_distributions/0507_multivariate_gaussian/intro.html">Multivariate Gaussian Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0507_multivariate_gaussian/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0507_multivariate_gaussian/application_transformation.html">Application: Plots and Transformations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0507_multivariate_gaussian/psd.html">Covariance Matrix is Positive Semi-Definite</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0507_multivariate_gaussian/eigendecomposition.html">Eigendecomposition and Covariance Matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/05_joint_distributions/0507_multivariate_gaussian/geometry_of_multivariate_gaussian.html">The Geometry of Multivariate Gaussians</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../probability_theory/06_sample_statistics/intro.html">Chapter 6. Sample Statistics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/intro.html">Moment Generating and Characteristic Functions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function.html">Moment Generating Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function_application_sum_of_rv.html">Application: Moment Generating Function and the Sum of Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/06_sample_statistics/0601_moment_generating_and_characteristic_functions/characteristic_function.html">Characteristic Function</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/06_sample_statistics/0602_probability_inequalities/intro.html">Probability Inequalities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/06_sample_statistics/0602_probability_inequalities/concept.html">Probability Inequalities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/06_sample_statistics/0602_probability_inequalities/application.html">Application: Learning Theory</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/06_sample_statistics/0603_law_of_large_numbers/intro.html">Law of Large Numbers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/06_sample_statistics/0603_law_of_large_numbers/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/06_sample_statistics/0603_law_of_large_numbers/convergence.html">Convergence of Sample Average</a></li>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/06_sample_statistics/0603_law_of_large_numbers/application.html">Application: Learning Theory</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../probability_theory/08_estimation_theory/intro.html">Chapter 8. Estimation Theory</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../probability_theory/08_estimation_theory/maximum_likelihood_estimation/intro.html">Maximum Likelihood Estimation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html">Concept</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Operations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../operations/distributed/intro.html">Distributed Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../operations/distributed/01_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/distributed/02_basics.html">Basics Of Distributed Data Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/distributed/03_how_to_setup_slurm_in_aws.html">How to Setup SLURM and ParallelCluster in AWS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/distributed/04_ablation.html">Ablations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../operations/profiling/intro.html">Profiling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../operations/profiling/01_synchronize.html">Synchronize CUDA To Time CUDA Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/profiling/02_timeit.html">Profiling Code With Timeit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/profiling/03_time_profiler.html">PyTorch’s Event And Profiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/profiling/04_small_gpt_profile.html">Profile GPT Small Time And Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/profiling/05_memory_leak.html">CUDA Memory Allocations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../operations/machine_learning_lifecycle/00_intro.html">The Lifecycle of an AIOps System</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../operations/machine_learning_lifecycle/01_problem_formulation.html">Stage 1. Problem Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/machine_learning_lifecycle/02_project_scoping.html">Stage 2. Project Scoping And Framing The Problem</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../operations/machine_learning_lifecycle/03_dataops_pipeline/03_dataops_pipeline.html">Stage 3. Data Pipeline (Data Engineering and DataOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../operations/machine_learning_lifecycle/03_dataops_pipeline/031_data_source_and_format.html">Stage 3.1. Data Source and Formats</a></li>
<li class="toctree-l3"><a class="reference internal" href="../operations/machine_learning_lifecycle/03_dataops_pipeline/032_data_model_and_storage.html">Stage 3.2. Data Model and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../operations/machine_learning_lifecycle/03_dataops_pipeline/033_etl.html">Stage 3.3. Extract, Transform, Load (ETL)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/machine_learning_lifecycle/04_mlops_data_pipeline.html">Stage 4. Data Extraction (MLOps), Data Analysis (Data Science), Data Preparation (Data Science)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../operations/machine_learning_lifecycle/05_model_development_selection_and_training/05_ml_training_pipeline.html">Stage 5. Model Development and Training (MLOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../operations/machine_learning_lifecycle/05_model_development_selection_and_training/051_model_selection.html">Stage 5.1. Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../operations/machine_learning_lifecycle/05_model_development_selection_and_training/052_metric_selection.html">Stage 5.2. Metric Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../operations/machine_learning_lifecycle/05_model_development_selection_and_training/053_experiment_tracking.html">Stage 5.3. Experiment Tracking And Versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../operations/machine_learning_lifecycle/05_model_development_selection_and_training/054_model_testing.html">Stage 5.4. Model Testing</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/machine_learning_lifecycle/06_model_evaluation.html">Stage 6. Model Evaluation (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/machine_learning_lifecycle/07_model_validation_registry_and_pushing_model_to_production.html">Stage 7. Model Validation, Registry and Pushing Model to Production (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/machine_learning_lifecycle/08_model_deployment_and_serving.html">Stage 8. Model Serving (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/machine_learning_lifecycle/09_model_monitoring.html">Stage 9. Model Monitoring (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/machine_learning_lifecycle/010_continuous_integration_deployment_learning_and_training.html">Stage 10. Continuous Integration, Deployment, Learning and Training (DevOps, DataOps, MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../operations/machine_learning_lifecycle/011_infrastructure_and_tooling_for_mlops.html">Stage 11. Infrastructure and Tooling for MLOps</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../software_engineering/config_management/concept.html">Configuration Management</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/config_management/01-pydra.html">Pydantic And Hydra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/config_management/02-state.html">State And Metadata Management</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../software_engineering/design_patterns/intro.html">Design Patterns</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/design_patterns/dependency-inversion-principle.html">Dependency Inversion Principle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/design_patterns/strategy.html">Strategy Pattern</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/design_patterns/registry.html">Registry Design Pattern</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../software_engineering/python/intro.html">Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/python/decorator.html">Decorator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/python/pydantic.html">Pydantic Is All You Need - Jason Liu</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../software_engineering/concurrency_parallelism_asynchronous/intro.html">Concurrency, Parallelism and Asynchronous Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../software_engineering/concurrency_parallelism_asynchronous/generator_yield.html">A Rudimentary Introduction to Generator and Yield in Python</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../computer_science/type_theory/intro.html">Type Theory, A Very Rudimentary Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/01-subtypes.html">Subtypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/02-type-safety.html">Type Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/03-subsumption.html">Subsumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/04-generics.html">Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/05-typevar-bound-constraints.html">Bound and Constraint in Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/06-invariance-covariance-contravariance.html">Invariance, Covariance and Contravariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/07-pep-3124-overloading.html">Function Overloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../computer_science/type_theory/08-pep-661-sentinel-values.html">Sentinel Types</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Structures and Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/complexity_analysis/intro.html">Complexity Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/complexity_analysis/master_theorem.html">Master Theorem </a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/stack/intro.html">Stack</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/stack/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/searching_algorithms/linear_search/intro.html">Linear Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/searching_algorithms/linear_search/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../dsa/searching_algorithms/binary_search/intro.html">Binary Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../dsa/searching_algorithms/binary_search/concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dsa/searching_algorithms/binary_search/problems/875-koko-eating-bananas.html">Koko Eating Bananas</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../linear_algebra/01_preliminaries/intro.html">Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/01_preliminaries/01-fields.html">Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/01_preliminaries/02-systems-of-linear-equations.html">Systems of Linear Equations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../linear_algebra/02_vectors/intro.html">Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/01-vector-definition.html">Vector and Its Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/02-vector-operation.html">Vector and Its Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/03-vector-norm.html">Vector Norm and Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../linear_algebra/02_vectors/04-vector-products.html">A First Look at Vector Products</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../citations.html">IEEE (Style) Citations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../api/reproducibility.html">API Reference</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/gao-hongnan/omniverse/blob/main/omniverse/playbook/why_cosine_annealing_warmup_stabilize_training.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse/issues/new?title=Issue%20on%20page%20%2Fplaybook/why_cosine_annealing_warmup_stabilize_training.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/playbook/why_cosine_annealing_warmup_stabilize_training.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../_sources/playbook/why_cosine_annealing_warmup_stabilize_training.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Why Does Cosine Annealing With Warmup Stabilize Training?</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#warmup">Warmup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-walkthrough">An Example Walkthrough</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warmup-phase">1. Warmup Phase</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cosine-decay-phase">2. Cosine Decay Phase</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tau-fraction">2.1. Tau Fraction</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-multiplier">2.2. Learning Rate Multiplier</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate">2.3. Learning Rate</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-intuition">Mathematical Intuition</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-s-cosineannealinglr-vs-composer-s-cosineannealingscheduler">PyTorch’s CosineAnnealingLR vs. Composer’s CosineAnnealingScheduler</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#citations">Citations</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="why-does-cosine-annealing-with-warmup-stabilize-training">
<h1>Why Does Cosine Annealing With Warmup Stabilize Training?<a class="headerlink" href="#why-does-cosine-annealing-with-warmup-stabilize-training" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://twitter.com/gaohongnan"><img alt="Twitter Handle" src="https://img.shields.io/badge/Twitter-&#64;gaohongnan-blue?style=social&amp;logo=twitter" /></a>
<a class="reference external" href="https://linkedin.com/in/gao-hongnan"><img alt="LinkedIn Profile" src="https://img.shields.io/badge/&#64;gaohongnan-blue?style=social&amp;logo=linkedin" /></a>
<a class="reference external" href="https://github.com/gao-hongnan"><img alt="GitHub Profile" src="https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&amp;logo=github" /></a>
<img alt="Tag" src="https://img.shields.io/badge/Tag-Brain_Dump-red" />
<img alt="Tag" src="https://img.shields.io/badge/Level-Beginner-green" />
<a class="reference external" href="https://github.com/gao-hongnan/omniverse/blob/main/omnivault/schedulers/cosine_annealing_warmup.py"><img alt="Code" src="https://img.shields.io/badge/View-Code-blue?style=flat-square&amp;logo=github" /></a></p>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#motivation" id="id8">Motivation</a></p></li>
<li><p><a class="reference internal" href="#warmup" id="id9">Warmup</a></p></li>
<li><p><a class="reference internal" href="#definition" id="id10">Definition</a></p></li>
<li><p><a class="reference internal" href="#implementation" id="id11">Implementation</a></p></li>
<li><p><a class="reference internal" href="#an-example-walkthrough" id="id12">An Example Walkthrough</a></p>
<ul>
<li><p><a class="reference internal" href="#warmup-phase" id="id13">1. Warmup Phase</a></p></li>
<li><p><a class="reference internal" href="#cosine-decay-phase" id="id14">2. Cosine Decay Phase</a></p>
<ul>
<li><p><a class="reference internal" href="#tau-fraction" id="id15">2.1. Tau Fraction</a></p></li>
<li><p><a class="reference internal" href="#learning-rate-multiplier" id="id16">2.2. Learning Rate Multiplier</a></p></li>
<li><p><a class="reference internal" href="#learning-rate" id="id17">2.3. Learning Rate</a></p></li>
<li><p><a class="reference internal" href="#example" id="id18">Example</a></p></li>
<li><p><a class="reference internal" href="#mathematical-intuition" id="id19">Mathematical Intuition</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#pytorch-s-cosineannealinglr-vs-composer-s-cosineannealingscheduler" id="id20">PyTorch’s CosineAnnealingLR vs. Composer’s CosineAnnealingScheduler</a></p></li>
<li><p><a class="reference internal" href="#references-and-further-readings" id="id21">References and Further Readings</a></p></li>
<li><p><a class="reference internal" href="#citations" id="id22">Citations</a></p></li>
</ul>
</nav>
<section id="motivation">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">Motivation</a><a class="headerlink" href="#motivation" title="Link to this heading">#</a></h2>
<p>In training deep neural networks, learning rate is definitely one of the most
important parameter to tune. Optimization algorithms like
<a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam</a> and
<a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">SGD</a> tell us how the
weights <span class="math notranslate nohighlight">\(\boldsymbol{\theta} \in \boldsymbol{\Theta}\)</span> should be updated, but the
learning rate <span class="math notranslate nohighlight">\(\eta\)</span> tells us the <strong><em>rate</em></strong> at which the weights are being
updated.</p>
<p>Theoretically and empircally, the <strong><em>magnitude</em></strong> of the learning rate <span class="math notranslate nohighlight">\(\eta\)</span>
can have a significant impact on the training process. If the learning rate is
too <em>large</em>, we might experience
<a class="reference external" href="https://en.wikipedia.org/wiki/Divergence_(mathematics)">divergence</a>, on the
other hand, if the learning rate is too <em>small</em>, the model might take longer to
converge or might get stuck in a local
<a class="reference external" href="https://en.wikipedia.org/wiki/Maxima_and_minima">minima</a>. The condition number
of the problem also impacts optimization efficiency, as discussed in
<a class="reference external" href="https://d2l.ai/chapter_optimization/momentum.html#sec-momentum">the momentum section</a>,
where the concept can be understood as the ratio between the smallest and
largest changes possible in response to adjustments in different directions of
the parameter space, reflecting the variance in sensitivity across these
directions<a class="footnote-reference brackets" href="#id7" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> <span id="id2">[<a class="reference internal" href="../bibliography.html#id5" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into Deep Learning. Cambridge University Press, 2023. URL: https://D2L.ai.">Zhang <em>et al.</em>, 2023</a>]</span>. As we progress through the training steps,
it is also equally important to apply a learning rate scheduler to adjust (may
<em>not</em> be monotonous decay) the learning rate <em>discriminatively</em>.</p>
<p>In the paper
<a class="reference external" href="https://arxiv.org/abs/1608.03983"><em>SGDR: Stochastic Gradient Descent with Restarts</em></a>
by Loshchilov and Hutter, they introduced an heuristic that relies on the
empirical observation that we can improve the convergence of the model (usually
in ill-conditioned situations) if we want follow an <strong><em>annealing</em></strong> process over
the learning rate. This means that at the beginning of training, we do not want
to decrease the learning too drastically. My (potentially wrong) intuition is
that this may allow the model to consider exploring a larger parameter space
without too much constraints than if we were to rapidly decrease the learning
rate. The authors further claim that as we progress towards the end of the
training, we would want to “fine-tune” the model parameters with a very small
learning rate, as it could potentially help “refine” the solution space to find
a “more optimal” set of parameters <span id="id3">[<a class="reference internal" href="../bibliography.html#id30" title="Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with restarts. CoRR, 2016. URL: http://arxiv.org/abs/1608.03983, arXiv:1608.03983.">Loshchilov and Hutter, 2016</a>]</span>.
This idea <em>naturally lands</em> us to using <em>cosine function</em> because the cosine
curve starts with a <em>gentle slope</em>, which coincides with the idea of <em>gradual
decrease</em> in learning rate in the beginning, and the cosine curve naturally
flattens and approaches zero towards the end as it reaches the end of its cycle,
which again coincides with the idea of <em>fine-tuning</em> the model parameters with a
very small learning rate.</p>
<p>Consequently, a cosine decaying scheduler has the below function form for
learning rates in the range <span class="math notranslate nohighlight">\(t \in [0, T]\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\eta_t=\eta_T+\frac{\eta_0-\eta_T}{2}\left(1+\cos \left(\frac{\pi t}{T}\right)\right)
\]</div>
<p>Here <span class="math notranslate nohighlight">\(\eta_0\)</span> is the initial learning rate, <span class="math notranslate nohighlight">\(\eta_T\)</span> is the target rate at time
<span class="math notranslate nohighlight">\(T\)</span>. Furthermore, for <span class="math notranslate nohighlight">\(t&gt;T\)</span> we simply pin the value to <span class="math notranslate nohighlight">\(\eta_T\)</span> without
increasing it again. <span class="math notranslate nohighlight">\(T\)</span> represents the end of the learning rate annealing phase
rather than the absolute end of training. It’s the point in time when the
learning rate reaches <span class="math notranslate nohighlight">\(\eta_T\)</span>, the target rate, and beyond which the learning
rate is maintained constant at <span class="math notranslate nohighlight">\(\eta_T\)</span>.</p>
<ul class="simple">
<li><p>During <span class="math notranslate nohighlight">\(0 \leq t &lt; T\)</span>: The learning rate <span class="math notranslate nohighlight">\(\eta_t\)</span> is actively adjusted
according to the cosine annealing formula. It transitions from the initial
learning rate <span class="math notranslate nohighlight">\(\eta_0\)</span> towards the target rate <span class="math notranslate nohighlight">\(\eta_T\)</span>, following a
half-cosine wave.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(t \geq T\)</span>: The learning rate is set to <span class="math notranslate nohighlight">\(\eta_T\)</span> and no longer changes.
This doesn’t necessarily mean that training must stop at <span class="math notranslate nohighlight">\(t = T\)</span>. Training
can continue beyond <span class="math notranslate nohighlight">\(T\)</span> with the learning rate fixed at <span class="math notranslate nohighlight">\(\eta_T\)</span>.</p></li>
</ul>
<p>In code, we can observe the behavior of the cosine annealing scheduler as
follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">List</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="linenos"> 6</span><span class="kn">import</span> <span class="nn">torch</span>
<span class="linenos"> 7</span><span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span>
<span class="linenos"> 8</span><span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">CosineAnnealingLR</span><span class="p">,</span> <span class="n">_LRScheduler</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="k">def</span> <span class="nf">get_learning_rates</span><span class="p">(</span><span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">:</span> <span class="n">_LRScheduler</span><span class="p">,</span> <span class="n">steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="linenos">11</span>    <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="linenos">12</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
<span class="linenos">13</span>        <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">])</span>
<span class="linenos">14</span>        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="linenos">15</span>        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
<span class="linenos">16</span>    <span class="k">return</span> <span class="n">lrs</span>
<span class="linenos">17</span>
<span class="linenos">18</span><span class="k">def</span> <span class="nf">plot_learning_rates</span><span class="p">(</span>
<span class="linenos">19</span>    <span class="n">lrs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">title</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">marker</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="p">:</span> <span class="n">plt</span><span class="o">.</span><span class="n">Axes</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
<span class="linenos">20</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="linenos">21</span>    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="linenos">22</span>
<span class="linenos">23</span>    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lrs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">title</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="linenos">24</span>    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
<span class="linenos">25</span>    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Step&quot;</span><span class="p">)</span>
<span class="linenos">26</span>    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Learning Rate&quot;</span><span class="p">)</span>
<span class="linenos">27</span>    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="linenos">28</span>
<span class="linenos">29</span><span class="k">def</span> <span class="nf">main</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="linenos">30</span>    <span class="n">initial_lr</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="linenos">31</span>    <span class="n">eta_min</span> <span class="o">=</span> <span class="mi">0</span>
<span class="linenos">32</span>    <span class="n">steps</span> <span class="o">=</span> <span class="mi">100</span>
<span class="linenos">33</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="linenos">34</span>
<span class="linenos">35</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">initial_lr</span><span class="p">)</span>
<span class="linenos">36</span>    <span class="n">scheduler_non_cyclic</span> <span class="o">=</span> <span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">steps</span><span class="p">,</span> <span class="n">eta_min</span><span class="o">=</span><span class="n">eta_min</span><span class="p">)</span>
<span class="linenos">37</span>    <span class="n">lrs_non_cyclic</span> <span class="o">=</span> <span class="n">get_learning_rates</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler_non_cyclic</span><span class="p">,</span> <span class="n">steps</span><span class="p">)</span>
<span class="linenos">38</span>
<span class="linenos">39</span>    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">initial_lr</span><span class="p">)</span>
<span class="linenos">40</span>    <span class="n">scheduler_cyclic</span> <span class="o">=</span> <span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">steps</span> <span class="o">//</span> <span class="mi">8</span><span class="p">,</span> <span class="n">eta_min</span><span class="o">=</span><span class="n">eta_min</span><span class="p">)</span>
<span class="linenos">41</span>    <span class="n">lrs_cyclic</span> <span class="o">=</span> <span class="n">get_learning_rates</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler_cyclic</span><span class="p">,</span> <span class="n">steps</span><span class="p">)</span>
<span class="linenos">42</span>
<span class="linenos">43</span>    <span class="c1"># Plotting</span>
<span class="linenos">44</span>    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="linenos">45</span>    <span class="n">plot_learning_rates</span><span class="p">(</span><span class="n">lrs_non_cyclic</span><span class="p">,</span> <span class="s1">&#39;Non-Cyclic Cosine Annealing&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="linenos">46</span>    <span class="n">plot_learning_rates</span><span class="p">(</span><span class="n">lrs_cyclic</span><span class="p">,</span> <span class="s1">&#39;Cyclic Cosine Annealing&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="linenos">47</span>
<span class="linenos">48</span>    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="linenos">49</span>    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="linenos">50</span>
<span class="linenos">51</span><span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d999c12511ae6da68ba48dd9ef45b7eee141f08f6ee9483a822fa0063a3fc051.svg" src="../_images/d999c12511ae6da68ba48dd9ef45b7eee141f08f6ee9483a822fa0063a3fc051.svg" />
</div>
</div>
</section>
<section id="warmup">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Warmup</a><a class="headerlink" href="#warmup" title="Link to this heading">#</a></h2>
<p>Our motivation would have ended here, but in practice, we often see that the
cosine annealing scheduler is often combined with a warmup phase. In
<a class="reference internal" href="#why-cosine-warmup-loss-plot"><span class="std std-numref">Fig. 18</span></a>, we can see that the loss curve is
relatively smooth and converges way better than the ones without warmup.</p>
<figure class="align-default" id="why-cosine-warmup-loss-plot">
<img alt="../_images/warmup_loss_plot_uvadlc.svg" src="../_images/warmup_loss_plot_uvadlc.svg" />
<figcaption>
<p><span class="caption-number">Fig. 18 </span><span class="caption-text">Training loss v.s. # of iterations of Transformers on the De-En IWSLT’14 dataset.</span><a class="headerlink" href="#why-cosine-warmup-loss-plot" title="Link to this image">#</a></p>
<div class="legend">
<p><strong>Image Credit:</strong>
<a class="reference external" href="https://arxiv.org/pdf/1908.03265.pdf">ON THE VARIANCE OF THE ADAPTIVE LEARNING RATE AND BEYOND</a></p>
</div>
</figcaption>
</figure>
<p>It might be worth having some intuition on why warmup works so well in practice,
and in particular, in language models like
<a class="reference external" href="https://arxiv.org/abs/1706.03762">Transformers</a>.</p>
<p>Firstly, the <a class="reference external" href="https://arxiv.org/pdf/1908.03265.pdf">RAdam</a> paper suggests warmup
works as a variance reduction technique, which overcomes the problem of
<a class="reference external" href="https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for">bias correction factors</a>
in optimizers like Adam, where having these bias correction factors would lead
to larger variance in the adaptive learning rate during the <strong>initial</strong> training
iterations <span id="id4">[<a class="reference internal" href="../bibliography.html#id21" title="Phillip Lippe. UvA Deep Learning Tutorials. https://uvadlc-notebooks.readthedocs.io/en/latest/, 2023.">Lippe, 2023</a>]</span>. More concretely, Adam estimates the first
and second moments of the gradient to change the learning rate of each
individual parameter (hence adaptive) and having high variance between adaptive
learning rates may de-stablize the training. If we don’t want to swap out Adam,
then this calls for a warmup phase to stabilize the learning rate and reduce the
variance in the early stages of training.</p>
<p>Secondly, language models like Transformers use iteratively applied Layer
Normalization across layers can lead to very high gradients during the first
iterations, which can be solved by using
<a class="reference external" href="https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf">Pre-Layer Normalization</a>
(similar to Pre-Activation ResNet), which applies normalization before the
layer’s main operations, contributing to gradient stabilization and reducing the
necessity for a warm-up phase, or replacing Layer Normalization by other
techniques
(<a class="reference external" href="https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf">Adaptive Normalization</a>,
<a class="reference external" href="https://arxiv.org/abs/2003.07845">Power Normalization</a>)
<span id="id5">[<a class="reference internal" href="../bibliography.html#id21" title="Phillip Lippe. UvA Deep Learning Tutorials. https://uvadlc-notebooks.readthedocs.io/en/latest/, 2023.">Lippe, 2023</a>]</span>.</p>
<p>However, even though there are solutions to the problem, certain setups still
use the Adam optimizer, and therefore warmup is still a simple and effective
technique to stabilize the learning rate in the early stages of training -
solving the afforementioned problems (i.e. stabilize the bias correction
factors, moving averages of gradients and squared gradients).</p>
<p>To this end, we end our discussion on the motivation behind 1) using cosine
annealing schedulers and 2) using warmup phases, often coupled with cosine
annealing schedulers. In what follows, we will provide a more formal definition
of the cosine annealing scheduler with warmup, and provide a running example to
illustrate the behavior of the scheduler.</p>
</section>
<section id="definition">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">Definition</a><a class="headerlink" href="#definition" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">CosineAnnealingWithWarmupScheduler</span></code> decays the learning rate <span class="math notranslate nohighlight">\(\eta\)</span>
according to the decreasing part of a cosine curve, with an initial warmup
<span class="math notranslate nohighlight">\(t_{\text{warmup}}\)</span>.</p>
<p>This scheduler modulates <span class="math notranslate nohighlight">\(\eta\)</span> within defined upper and lower bounds over a
predetermined interval, employing a cosine function. The formula for cosine
annealing reflects the shape of a half-cosine wave, which decreases from a
maximum value to a minimum and then increases back to the maximum. This cycle
can repeat multiple times over the training process, depending on how the
scheduler is configured. Although this approach suggests cyclic adjustments
(oscillations) within the training duration, for simplicity’s sake, our specific
implementation, inspired by
<a class="reference external" href="https://docs.mosaicml.com/projects/composer/en/latest/api_reference/generated/composer.optim.CosineAnnealingWithWarmupScheduler.html"><strong>MosaicML’s Composer’s CosineAnnealingWithWarmupScheduler</strong></a>,
explicitly excludes considerations for such cycles/oscillations.</p>
<div class="proof definition admonition" id="why-do-we-use-warmup-cosine-scheduler-definition">
<p class="admonition-title"><span class="caption-number">Definition 55 </span> (Cosine Annealing With Warmup)</p>
<section class="definition-content" id="proof-content">
<p>The <code class="docutils literal notranslate"><span class="pre">CosineAnnealingWithWarmupScheduler</span></code> modulates the <strong>learning rate</strong> <span class="math notranslate nohighlight">\(\eta\)</span>
according to a <strong>two-phase</strong> process: a <strong><em>warmup</em></strong> phase followed by a
<strong>cosine annealing</strong> phase. The learning rate <em>multiplier</em><a class="footnote-reference brackets" href="#lr-multiplier" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>
<span class="math notranslate nohighlight">\(\alpha_{t}\)</span> at any given time (step) <span class="math notranslate nohighlight">\(t\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
\alpha_{t} = \begin{cases}
    \frac{t}{t_{\text{warmup}}}, &amp; \text{if } t &lt; t_{\text{warmup}} \\
    \alpha_f + (1 - \alpha_f) \times \frac{1}{2} \left[1 + \cos(\pi \times \tau_w) \right], &amp; \text{otherwise}
\end{cases}
\end{equation}
\end{split}\]</div>
<p>where we denote:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(t\)</span> represents the <strong>current</strong> training step or epoch.</p></li>
<li><p><span class="math notranslate nohighlight">\(\eta_{\max}\)</span> as the <strong>maximum</strong> learning rate reached during training, and
often is the <strong>initial</strong> learning rate given into an optimizer.</p></li>
<li><p><span class="math notranslate nohighlight">\(t_{\text{warmup}}\)</span> denotes the duration of the warmup period, in terms of
the number of steps or epochs, during which the learning rate <strong>linearly</strong>
increases to the maximum learning rate <span class="math notranslate nohighlight">\(\eta_{\max}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(t_{\max}\)</span> as the <strong>maximum</strong> number of training steps, or maximum number of
iterations in an epoch (see
<a class="reference external" href="https://github.com/skorch-dev/skorch/issues/610">here</a>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\tau_w = \frac{t - t_{\text{warmup}}}{t_{\max}}\)</span>, the fraction of
post-warmup time elapsed,</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha_f\)</span> is a <em>scaling</em> factor that determines the <strong>final</strong> learning rate
multiplier to decay to (a value between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>), and this is a <em>fixed</em>
value. For example, if <span class="math notranslate nohighlight">\(\alpha_f = 0.1\)</span> and the initial learning rate is
<span class="math notranslate nohighlight">\(\eta_{\max} = 3e-4\)</span>, then the final learning rate will be
<span class="math notranslate nohighlight">\(\eta_{\min} = 3e-4 \times 0.1 = 3e-5\)</span>.</p></li>
</ul>
<p>The actual learning rate <span class="math notranslate nohighlight">\(\eta_{t}\)</span> at time (step) <span class="math notranslate nohighlight">\(t\)</span> is then computed as:</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
    \eta_{t} = \alpha_{t} \times \eta_{\max}
\end{equation}
\]</div>
<p>where we emphasize again that <span class="math notranslate nohighlight">\(\eta_{\max}\)</span> is the <strong>maximum</strong> learning rate
reached during training.</p>
</section>
</div><div class="note admonition">
<p class="admonition-title">A Word on Oscillations</p>
<p>Note that if you set <span class="math notranslate nohighlight">\(t_{\max}\)</span> to the total number of training steps that is
needed for the entire dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, the scheduler <em>will only decay</em> the
learning rate after the warmup phase and not oscillate further. This
configuration means that after completing the linear increase during the warmup,
the learning rate will decrease following a cosine curve until it reaches the
final learning rate determined by <span class="math notranslate nohighlight">\(\alpha_f\)</span>.</p>
<ul class="simple">
<li><p><strong>Single Cycle (No Oscillation)</strong>: If <span class="math notranslate nohighlight">\(t_{\max}\)</span> is set to cover exactly one
half-cycle of the cosine function from the end of the warmup phase to the
conclusion of training, the learning rate will monotonically decrease from
its maximum value (at the end of warmup) to its minimum value (as determined
by <span class="math notranslate nohighlight">\(\alpha_f\)</span>) without oscillating. This is because the scheduler’s active
period only spans a single descent phase of the cosine wave.</p></li>
<li><p><strong>Multiple Cycles (Oscillation)</strong>: If <span class="math notranslate nohighlight">\(t_{\max}\)</span> is set to allow for a
longer duration than what is needed for a single half-cycle descent, the
cosine annealing function can complete its initial descent and then begin to
ascend as part of a new cycle. This leads to oscillations in the learning
rate—after decreasing, it will start to increase again, potentially multiple
times, depending on the total number of cycles fitted within <span class="math notranslate nohighlight">\(t_{\max}\)</span>.
This is where the term “oscillation” comes into play; it describes the
periodic increase and decrease in the learning rate according to the cosine
function over multiple cycles.</p></li>
</ul>
<p>True oscillation, where the learning rate decreases and then increases within a
training regime, typically requires either a restart mechanism (as seen in
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html">Cosine Annealing with Warm Restarts</a>)
or an explicit multi-cycle configuration. A standard cosine annealing scheduler,
especially with a warmup phase, generally only supports a monotonic decrease
within a single cycle, unless it is specifically designed to handle restarts or
multiple cycles.</p>
</div>
</section>
<section id="implementation">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Implementation</a><a class="headerlink" href="#implementation" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="kn">import</span> <span class="nn">math</span>
<span class="linenos"> 4</span><span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="linenos"> 5</span>
<span class="linenos"> 6</span><span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">LambdaLR</span>
<span class="linenos"> 7</span><span class="kn">from</span> <span class="nn">torch.optim.optimizer</span> <span class="kn">import</span> <span class="n">Optimizer</span>
<span class="linenos"> 8</span><span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="linenos"> 9</span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="linenos">10</span>
<span class="linenos">11</span><span class="k">def</span> <span class="nf">_get_cosine_schedule_with_warmup_lr_lambda</span><span class="p">(</span>
<span class="linenos">12</span>    <span class="n">current_step</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">alpha_f</span><span class="p">:</span> <span class="nb">float</span>
<span class="linenos">13</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="linenos">14</span>    <span class="k">if</span> <span class="n">current_step</span> <span class="o">&lt;</span> <span class="n">num_warmup_steps</span><span class="p">:</span>
<span class="linenos">15</span>        <span class="n">alpha</span> <span class="o">=</span> <span class="n">current_step</span> <span class="o">/</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="p">)</span>
<span class="linenos">16</span>    <span class="k">else</span><span class="p">:</span>
<span class="linenos">17</span>        <span class="n">tau_w</span> <span class="o">=</span> <span class="p">(</span><span class="n">current_step</span> <span class="o">-</span> <span class="n">num_warmup_steps</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_training_steps</span>
<span class="linenos">18</span>        <span class="n">tau_w</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">tau_w</span><span class="p">)</span>
<span class="linenos">19</span>        <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha_f</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_f</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">tau_w</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
<span class="linenos">20</span>    <span class="k">return</span> <span class="n">alpha</span>
<span class="linenos">21</span>
<span class="linenos">22</span>
<span class="linenos">23</span><span class="k">def</span> <span class="nf">get_cosine_annealing_with_warmup</span><span class="p">(</span>
<span class="linenos">24</span>    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Optimizer</span><span class="p">,</span>
<span class="linenos">25</span>    <span class="n">num_warmup_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="linenos">26</span>    <span class="n">num_training_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="linenos">27</span>    <span class="n">alpha_f</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
<span class="linenos">28</span>    <span class="n">last_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
<span class="linenos">29</span>    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="linenos">30</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LambdaLR</span><span class="p">:</span>
<span class="linenos">31</span>    <span class="n">lr_lambda</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
<span class="linenos">32</span>        <span class="n">_get_cosine_schedule_with_warmup_lr_lambda</span><span class="p">,</span>
<span class="linenos">33</span>        <span class="n">num_warmup_steps</span><span class="o">=</span><span class="n">num_warmup_steps</span><span class="p">,</span>
<span class="linenos">34</span>        <span class="n">num_training_steps</span><span class="o">=</span><span class="n">num_training_steps</span><span class="p">,</span>
<span class="linenos">35</span>        <span class="n">alpha_f</span><span class="o">=</span><span class="n">alpha_f</span><span class="p">,</span>
<span class="linenos">36</span>    <span class="p">)</span>
<span class="linenos">37</span>    <span class="k">return</span> <span class="n">LambdaLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">lr_lambda</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">,</span> <span class="n">verbose</span><span class="p">)</span>
<span class="linenos">38</span>
<span class="linenos">39</span><span class="c1"># Experiment 1</span>
<span class="linenos">40</span><span class="n">num_warmup_steps</span> <span class="o">=</span> <span class="mi">5</span>
<span class="linenos">41</span><span class="n">num_training_steps</span> <span class="o">=</span> <span class="mi">10</span>
<span class="linenos">42</span><span class="n">alpha_f</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="linenos">43</span><span class="n">initial_lr</span> <span class="o">=</span> <span class="mf">3e-4</span>
<span class="linenos">44</span>
<span class="linenos">45</span><span class="n">dummy_model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="linenos">46</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">dummy_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">initial_lr</span><span class="p">)</span>
<span class="linenos">47</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_cosine_annealing_with_warmup</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="p">,</span> <span class="n">alpha_f</span><span class="p">)</span>
<span class="linenos">48</span><span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scheduler</span><span class="p">,</span> <span class="n">LambdaLR</span><span class="p">)</span>
<span class="linenos">49</span><span class="n">lrs1</span> <span class="o">=</span> <span class="n">get_learning_rates</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="n">num_training_steps</span><span class="p">)</span>
<span class="linenos">50</span>
<span class="linenos">51</span><span class="c1"># Experiment 2</span>
<span class="linenos">52</span><span class="n">num_warmup_steps</span> <span class="o">=</span> <span class="mi">200</span>
<span class="linenos">53</span><span class="n">num_training_steps</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="linenos">54</span>
<span class="linenos">55</span><span class="n">dummy_model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="linenos">56</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">dummy_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">initial_lr</span><span class="p">)</span>
<span class="linenos">57</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_cosine_annealing_with_warmup</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="p">,</span> <span class="n">alpha_f</span><span class="p">)</span>
<span class="linenos">58</span><span class="n">lrs2</span> <span class="o">=</span> <span class="n">get_learning_rates</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="n">num_training_steps</span><span class="p">)</span>
<span class="linenos">59</span>
<span class="linenos">60</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="linenos">61</span><span class="n">plot_learning_rates</span><span class="p">(</span><span class="n">lrs1</span><span class="p">,</span> <span class="s1">&#39;Cosine Annealing With Warmup (Short)&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="linenos">62</span><span class="n">plot_learning_rates</span><span class="p">(</span><span class="n">lrs2</span><span class="p">,</span> <span class="s1">&#39;Cosine Annealing With Warmup (Long)&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(
</pre></div>
</div>
<img alt="../_images/3731a398b48e8528e03c90eed620cd0d81ca88b1545fdb6fc3614683ac1815b9.svg" src="../_images/3731a398b48e8528e03c90eed620cd0d81ca88b1545fdb6fc3614683ac1815b9.svg" />
</div>
</div>
</section>
<section id="an-example-walkthrough">
<h2><a class="toc-backref" href="#id12" role="doc-backlink">An Example Walkthrough</a><a class="headerlink" href="#an-example-walkthrough" title="Link to this heading">#</a></h2>
<p>For simplicity, we assume that there are a total of <span class="math notranslate nohighlight">\(10\)</span> training steps (or
epoches) depending on how you define it. We will use the following
hyperparameters:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\eta_{\max} = 3 \times 10^{-4}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(t_{\text{warmup}} = 5\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(t_{\max} = 10\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha_f = 0.5\)</span></p></li>
</ul>
<p>We can use the code to verify the learning rate at each step with our manual
computation.</p>
<div class="cell tag_remove-input docutils container">
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">[</span>
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">5.9999999999999995e-05</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.00011999999999999999</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.00017999999999999998</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.00023999999999999998</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0003</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0002963292387221365</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.00028567627457812104</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.0002690838939219355</span>,
<span style="color: #7fbf7f; text-decoration-color: #7fbf7f">│   </span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.00024817627457812105</span>
<span style="font-weight: bold">]</span>
</pre>
</div></div>
</div>
<section id="warmup-phase">
<h3><a class="toc-backref" href="#id13" role="doc-backlink">1. Warmup Phase</a><a class="headerlink" href="#warmup-phase" title="Link to this heading">#</a></h3>
<p>During the warmup phase, when the <strong>current</strong> training step <span class="math notranslate nohighlight">\(t\)</span> is less than the
warmup time <span class="math notranslate nohighlight">\(t_{\text{warmup}}\)</span>, the learning rate multiplier is <strong>linearly</strong>
increased from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>Mathematically, the learning rate multiplier <span class="math notranslate nohighlight">\(\alpha_{t}\)</span> at time (step) <span class="math notranslate nohighlight">\(t\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\alpha_{t} = \frac{t}{t_{\text{warmup}}}
\]</div>
<p>The learning rate at this phase is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\eta_{t} &amp;=  \alpha_{t} \times \eta_{\max} \\
&amp;= \frac{t}{t_{\text{warmup}}} \times \eta_{\max}
\end{align*}
\end{split}\]</div>
<p>During the warmup phase, the learning rate will linearly increase from <span class="math notranslate nohighlight">\(0\)</span> to
<span class="math notranslate nohighlight">\(\eta_{\max}\)</span> in the first <span class="math notranslate nohighlight">\(t_{\text{warmup}}\)</span> steps. Since
<span class="math notranslate nohighlight">\(\eta_{\max} = 3 \times 10^{-4}\)</span> and <span class="math notranslate nohighlight">\(t_{\text{warmup}} = 5\)</span>, the learning rate
will be increased as follows:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(t = 1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    \alpha_1 &amp;= \frac{t}{t_{\text{warmup}}} = \frac{1}{5} = 0.2 \\
    \eta_1 &amp;= \alpha_1 \times \eta_{\max} = 0.2 \times 3 \times 10^{-4} = 6 \times 10^{-5}
    \end{align*}
    \end{split}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(t = 2\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    \alpha_2 &amp;= \frac{t}{t_{\text{warmup}}} = \frac{2}{5} = 0.4 \\
    \eta_2 &amp;= \alpha_2 \times \eta_{\max} = 0.4 \times 3 \times 10^{-4} = 1.2 \times 10^{-4}
    \end{align*}
    \end{split}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(t = 3\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    \alpha_3 &amp;= \frac{t}{t_{\text{warmup}}} = \frac{3}{5} = 0.6 \\
    \eta_3 &amp;= \alpha_3 \times \eta_{\max} = 0.6 \times 3 \times 10^{-4} = 1.8 \times 10^{-4}
    \end{align*}
    \end{split}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(t = 4\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    \alpha_4 &amp;= \frac{t}{t_{\text{warmup}}} = \frac{4}{5} = 0.8 \\
    \eta_4 &amp;= \alpha_4 \times \eta_{\max} = 0.8 \times 3 \times 10^{-4} = 2.4 \times 10^{-4}
    \end{align*}
    \end{split}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(t = 5\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    \alpha_5 &amp;= \frac{t}{t_{\text{warmup}}} = \frac{5}{5} = 1 \\
    \eta_5 &amp;= \alpha_5 \times \eta_{\max} = 3 \times 10^{-4} \times 1 = 3 \times 10^{-4}
    \end{align*}
    \end{split}\]</div>
</li>
</ul>
<p>The linear relationship for the warmup phase can be represented as a function of
the current training step <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight">
\[
f(t) = \frac{t}{t_{\text{warmup}}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(t_{\text{warmup}}\)</span>is the total number of steps in the warmup phase. This
function describes how the learning rate multiplier <span class="math notranslate nohighlight">\(\alpha_t\)</span> grows linearly
from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(1\)</span> as <span class="math notranslate nohighlight">\(t\)</span> progresses from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(t_{\text{warmup}}\)</span>.</p>
</section>
<section id="cosine-decay-phase">
<h3><a class="toc-backref" href="#id14" role="doc-backlink">2. Cosine Decay Phase</a><a class="headerlink" href="#cosine-decay-phase" title="Link to this heading">#</a></h3>
<p>After the warmup phase, the learning rate multiplier follows a cosine decay
pattern. This phase commences once the current training step <span class="math notranslate nohighlight">\(t\)</span> is greater than
or equal to the warmup time <span class="math notranslate nohighlight">\(t_{\text{warmup}}\)</span>, and it continues until the
maximum training step <span class="math notranslate nohighlight">\(t_{\text{max}}\)</span>.</p>
<section id="tau-fraction">
<h4><a class="toc-backref" href="#id15" role="doc-backlink">2.1. Tau Fraction</a><a class="headerlink" href="#tau-fraction" title="Link to this heading">#</a></h4>
<p>We first define a variable <span class="math notranslate nohighlight">\(\tau_w\)</span> to represent the fraction of post-warmup
time elapsed. Mathematically, it is defined as:</p>
<div class="math notranslate nohighlight">
\[
\tau_w = \frac{t - t_{\text{warmup}}}{t_{\text{max}}}
\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(t\)</span>: Current training step.</p></li>
<li><p><span class="math notranslate nohighlight">\(t_{\text{warmup}}\)</span>: Warmup time in training steps.</p></li>
<li><p><span class="math notranslate nohighlight">\(t_{\text{max}}\)</span>: Total duration of the scheduler in training steps.</p></li>
</ul>
</section>
<section id="learning-rate-multiplier">
<h4><a class="toc-backref" href="#id16" role="doc-backlink">2.2. Learning Rate Multiplier</a><a class="headerlink" href="#learning-rate-multiplier" title="Link to this heading">#</a></h4>
<p>The learning rate multiplier <span class="math notranslate nohighlight">\(\alpha_t\)</span> during the cosine decay phase is given
by:</p>
<div class="math notranslate nohighlight">
\[
\alpha_{t} = \alpha_f + \frac{1}{2}(1 - \alpha_f)  \left(1 + \cos \left(\pi \times \tau_w\right)\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha_f\)</span> is the scaling factor that determines the final learning rate
multiplier to decay to.</p>
</section>
<section id="learning-rate">
<h4><a class="toc-backref" href="#id17" role="doc-backlink">2.3. Learning Rate</a><a class="headerlink" href="#learning-rate" title="Link to this heading">#</a></h4>
<p>The actual learning rate <span class="math notranslate nohighlight">\(\eta_t\)</span> during this phase is then computed as:</p>
<div class="math notranslate nohighlight">
\[
\eta_{t} = \alpha_{t} \times \eta_{\text{max}}
\]</div>
</section>
<section id="example">
<h4><a class="toc-backref" href="#id18" role="doc-backlink">Example</a><a class="headerlink" href="#example" title="Link to this heading">#</a></h4>
<p>Using the running example with:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\eta_{\text{max}} = 3 \times 10^{-4}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(t_{\text{warmup}} = 5\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(t_{\text{max}} = 10\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha_f = 0.5\)</span></p></li>
</ul>
<p>The learning rate will be computed as follows:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(t = 6\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    \tau_w &amp;= \frac{6 - 5}{10} = 0.1 \\
    \alpha_6 &amp;= 0.5 + \frac{1}{2}(1 - 0.5) \left(1 + \cos \left(0.1\pi\right)\right) = 0.975445 \\
    \eta_6 &amp;= 3 \times 10^{-4} \times 0.975445 = 0.0002963292387221365
    \end{align*}
    \end{split}\]</div>
</li>
<li><p><span class="math notranslate nohighlight">\(t = 7\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    \tau_w &amp;= \frac{7 - 5}{10} = 0.2 \\
    \alpha_7 &amp;= 0.5 + \frac{1}{2}(1 - 0.5) \left(1 + \cos \left(0.2\pi\right)\right) = 0.904508 \\
    \eta_7 &amp;= 3 \times 10^{-4} \times 0.904508 = 0.00028567627457812104
    \end{align*}
    \end{split}\]</div>
</li>
<li><p>… (and so on for the remaining steps)</p></li>
</ul>
</section>
<section id="mathematical-intuition">
<h4><a class="toc-backref" href="#id19" role="doc-backlink">Mathematical Intuition</a><a class="headerlink" href="#mathematical-intuition" title="Link to this heading">#</a></h4>
<p>This version of implementation is slightly confusing because there is an
<span class="math notranslate nohighlight">\(\alpha_f\)</span> term in the cosine decay formula.</p>
<ol class="arabic">
<li><p><strong>Cosine Function</strong>: The cosine function oscillates between -1 and 1. By
taking a scaled and shifted version of the cosine function, we can create a
curve that starts at its highest point and gradually descends to its lowest
point over the interval <span class="math notranslate nohighlight">\([0, t_{\text{max}}]\)</span>.</p></li>
<li><p><strong>Decay Formula</strong>: The formula for the learning rate multiplier during the
decay phase is:</p>
<div class="math notranslate nohighlight">
\[
    \alpha_{t} = \alpha_f + \frac{1}{2}(1 - \alpha_f)\left(1 + \cos \left(\pi \times \tau_w\right)\right)
    \]</div>
<p>Here, <span class="math notranslate nohighlight">\(\tau_w\)</span> is the fraction of time elapsed since the warmup phase, and
it ranges from 0 to 1. The <span class="math notranslate nohighlight">\(\cos(\pi \times \tau_w)\)</span> term creates a curve
that starts at 1 (when <span class="math notranslate nohighlight">\(\tau_w = 0\)</span>) and ends at -1 (when <span class="math notranslate nohighlight">\(\tau_w = 1\)</span>). The
scaling and shifting ensure that <span class="math notranslate nohighlight">\(\alpha_t\)</span> starts at 1 and decays to
<span class="math notranslate nohighlight">\(\alpha_f\)</span>.</p>
</li>
</ol>
<p>More concretely, the expression</p>
<div class="math notranslate nohighlight">
\[
\alpha_{t} = \alpha_f + \frac{1}{2}(1 - \alpha_f)\left(1 + \cos \left(\pi \times \tau_w\right)\right)
\]</div>
<p>describes the learning rate multiplier during the decay phase, where <span class="math notranslate nohighlight">\(\tau_w\)</span> is
the fraction of time elapsed since the warmup phase where <span class="math notranslate nohighlight">\(\tau_w\)</span> is the
fraction of time elapsed since the warmup phase, and it ranges from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>Let’s zoom into the cosine decay part in more details:</p>
<ul class="simple">
<li><p>The term <span class="math notranslate nohighlight">\(\cos(\pi \times \tau_w)\)</span> oscillates between <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(-1\)</span> as
<span class="math notranslate nohighlight">\(\tau_w\)</span> varies from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(1\)</span>.</p></li>
<li><p>When you add <span class="math notranslate nohighlight">\(1\)</span> to this term, the expression <span class="math notranslate nohighlight">\(1 + \cos(\pi \times \tau_w)\)</span>
oscillates between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(2\)</span>.</p></li>
<li><p>Multiplying this by <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span> scales it down, so the expression
<span class="math notranslate nohighlight">\(\frac{1}{2}\left(1 + \cos(\pi \times \tau_w)\right)\)</span> oscillates between 0
and 1.</p></li>
<li><p>The term <span class="math notranslate nohighlight">\(\frac{1}{2}(1 - \alpha_f)\)</span> scales this oscillation so that the
amplitude is adjusted based on the desired final learning rate multiplier
<span class="math notranslate nohighlight">\(\alpha_f\)</span>. This means if <span class="math notranslate nohighlight">\(\alpha_f = 0.5\)</span>, the expression
<span class="math notranslate nohighlight">\(\frac{1}{2}(1 - \alpha_f)\left(1 + \cos(\pi \times \tau_w)\right)\)</span>
oscillates between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(0.5\)</span>.</p></li>
<li><p>Adding <span class="math notranslate nohighlight">\(\alpha_f\)</span> shifts the entire expression so that it starts at 1 when
<span class="math notranslate nohighlight">\(\tau_w = 0\)</span> and decays to <span class="math notranslate nohighlight">\(\alpha_f\)</span> when <span class="math notranslate nohighlight">\(\tau_w = 1\)</span>.</p></li>
</ul>
<p>The addition of <span class="math notranslate nohighlight">\(\alpha_f\)</span> in the decay formula serves the purpose of setting
the final value of the learning rate multiplier <span class="math notranslate nohighlight">\(\alpha_t\)</span> to <span class="math notranslate nohighlight">\(\alpha_f\)</span> at the
end of training. Let’s break down the equation step by step to understand why
<span class="math notranslate nohighlight">\(\alpha_f\)</span> is added back.</p>
<p>Given the formula:</p>
<div class="math notranslate nohighlight">
\[
\alpha_{t} = \alpha_f + \frac{1}{2}(1 - \alpha_f)\left(1 + \cos \left(\pi \times \tau_w\right)\right)
\]</div>
<p>First, consider the case where <span class="math notranslate nohighlight">\(\tau_w = 0\)</span> (i.e., the beginning of the decay
phase):</p>
<ul class="simple">
<li><p>The cosine term becomes <span class="math notranslate nohighlight">\(\cos(0) = 1\)</span>.</p></li>
<li><p>The entire expression inside the parentheses becomes <span class="math notranslate nohighlight">\(1 + 1 = 2\)</span>.</p></li>
<li><p>The scaling factor <span class="math notranslate nohighlight">\(\frac{1}{2}(1 - \alpha_f)\)</span> then multiplies this by
<span class="math notranslate nohighlight">\(\frac{1 - \alpha_f}{2}\)</span>.</p></li>
<li><p>So the expression becomes
<span class="math notranslate nohighlight">\(\alpha_f + \frac{1 - \alpha_f}{2} \times 2 = \alpha_f + (1 - \alpha_f) = 1\)</span>.</p></li>
</ul>
<p>So at <span class="math notranslate nohighlight">\(\tau_w = 0\)</span>, <span class="math notranslate nohighlight">\(\alpha_t\)</span> starts at 1.</p>
<p>Now consider the case where <span class="math notranslate nohighlight">\(\tau_w = 1\)</span> (i.e., the end of training):</p>
<ul class="simple">
<li><p>The cosine term becomes <span class="math notranslate nohighlight">\(\cos(\pi) = -1\)</span>.</p></li>
<li><p>The entire expression inside the parentheses becomes <span class="math notranslate nohighlight">\(1 - 1 = 0\)</span>.</p></li>
<li><p>The scaling factor then multiplies this by
<span class="math notranslate nohighlight">\(\frac{1 - \alpha_f}{2} \times 0 = 0\)</span>.</p></li>
<li><p>So the expression becomes <span class="math notranslate nohighlight">\(\alpha_f + 0 = \alpha_f\)</span>.</p></li>
</ul>
<p>So at <span class="math notranslate nohighlight">\(\tau_w = 1\)</span>, <span class="math notranslate nohighlight">\(\alpha_t\)</span> decays to <span class="math notranslate nohighlight">\(\alpha_f\)</span>.</p>
<p>By adding <span class="math notranslate nohighlight">\(\alpha_f\)</span>, we ensure that the learning rate multiplier starts at <span class="math notranslate nohighlight">\(1\)</span>
and smoothly decays to the desired final value <span class="math notranslate nohighlight">\(\alpha_f\)</span>. Without adding
<span class="math notranslate nohighlight">\(\alpha_f\)</span>, the expression would start at <span class="math notranslate nohighlight">\(1\)</span> but decay to <span class="math notranslate nohighlight">\(0\)</span>, rather than the
intended final value. The addition of <span class="math notranslate nohighlight">\(\alpha_f\)</span> shifts the entire decay curve
so that it aligns with the desired starting and ending values.</p>
</section>
</section>
</section>
<section id="pytorch-s-cosineannealinglr-vs-composer-s-cosineannealingscheduler">
<h2><a class="toc-backref" href="#id20" role="doc-backlink">PyTorch’s CosineAnnealingLR vs. Composer’s CosineAnnealingScheduler</a><a class="headerlink" href="#pytorch-s-cosineannealinglr-vs-composer-s-cosineannealingscheduler" title="Link to this heading">#</a></h2>
<p>We know that Composers’ <code class="docutils literal notranslate"><span class="pre">CosineAnnealingWithWarmupScheduler</span></code> is basically its
<code class="docutils literal notranslate"><span class="pre">CosineAnnealingScheduler</span></code> with warmup, and the latter is also basically a copy
of PyTorch’s <code class="docutils literal notranslate"><span class="pre">CosineAnnealingLR</span></code>. However, there is a slight difference in their
formulas. Let’s compare the two to see if they are equivalent.</p>
<p>In PyTorch’s
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR">CosineAnnealingLR</a>,
they implemented the cosine annealing scheduler without warmup, but the base
formula should be similar. Let’s take a look to see how they coincide.</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR">PyTorch’s <code class="docutils literal notranslate"><span class="pre">CosineAnnealingLR</span></code> without warmup</a></p></li>
<li><p><a class="reference external" href="https://docs.mosaicml.com/projects/composer/en/stable/api_reference/generated/composer.optim.CosineAnnealingScheduler.html#composer.optim.CosineAnnealingScheduler">Composers <code class="docutils literal notranslate"><span class="pre">CosineAnnealingScheduler</span></code> without warmup</a></p></li>
</ol>
<p>The formula looks a bit different at first glance (without loss of generality,
we can ignore warmup here), after digging a bit deeper, I tried to establish the
equivalence by setting <code class="docutils literal notranslate"><span class="pre">eta_min</span></code> of pytorch to <code class="docutils literal notranslate"><span class="pre">alpha_f</span> <span class="pre">x</span> <span class="pre">eta_max</span></code>,</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\eta_{t} = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{t}{t_{\max}}\pi\right)\right)
\end{equation}
\]</div>
<p>where there is a new notation <span class="math notranslate nohighlight">\(\eta_{\min}\)</span>, which is the minimum learning rate.
Furthermore, they do not have the <span class="math notranslate nohighlight">\(\alpha_f\)</span> term, which is the scaling factor
that determines the final learning rate multiplier to decay to.</p>
<p>To prove their equivalence, set <span class="math notranslate nohighlight">\(\eta_{\min} = \alpha_f \times \eta_{\max}\)</span>, and</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\eta_{t} &amp; = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{t}{t_{\max}}\pi\right)\right) \\
         &amp; = \underbrace{\alpha_f \times \eta_{\max}}_{\eta_{\min}} + \frac{1}{2}\left(\eta_{\max} - \underbrace{\alpha_f \times \eta_{\max}}_{\eta_{\min}}\right)\left(1 + \cos\left(\frac{t}{t_{\max}}\pi\right)\right) \\
         &amp; = \alpha_f \times \eta_{\max} + \frac{1}{2}\left(1 - \alpha_f\right) \eta_{\max}\left(1 + \cos\left(\frac{t}{t_{\max}}\pi\right)\right) \\
         &amp; = \eta_{\max} \left(\alpha_f + \frac{1}{2}\left(1 - \alpha_f\right) \left(1 + \cos\left(\frac{t}{t_{\max}}\pi\right)\right)\right) \\
         &amp; = \eta_{\max} \underbrace{\left(\alpha_f + \frac{1}{2}(1 - \alpha_f)  \left(1 + \cos \left(\pi \times \tau_w\right)\right)\right)}_{\alpha_t} \\
         &amp; = \alpha_t \times \eta_{\max}
\end{align*}
\end{split}\]</div>
<p>Setting <span class="math notranslate nohighlight">\(\eta_{\min} = \alpha_f \times \eta_{\max}\)</span> is also not an arbitrary
choice. If we interpret <span class="math notranslate nohighlight">\(\eta_{\min}\)</span> as the minimum learning rate, then it
makes sense to set it to <span class="math notranslate nohighlight">\(\alpha_f \times \eta_{\max}\)</span>, since <span class="math notranslate nohighlight">\(\alpha_f\)</span> is the
scaling factor that determines the final learning rate multiplier to decay to
from <span class="math notranslate nohighlight">\(\eta_{\max}\)</span>. More concretely, if the initial learning rate
<span class="math notranslate nohighlight">\(\eta_{\max} = 3e-4\)</span> and <span class="math notranslate nohighlight">\(\alpha_f = 0.1\)</span>, then the final learning rate will be
<span class="math notranslate nohighlight">\(\eta_{\min} = 3e-4 \times 0.1 = 3e-5\)</span>.</p>
</section>
<section id="references-and-further-readings">
<h2><a class="toc-backref" href="#id21" role="doc-backlink">References and Further Readings</a><a class="headerlink" href="#references-and-further-readings" title="Link to this heading">#</a></h2>
<div class="seealso admonition">
<p class="admonition-title">References</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.mosaicml.com/projects/composer/en/latest/api_reference/generated/composer.optim.CosineAnnealingWithWarmupScheduler.html">MosaicML’s Composer’s CosineAnnealingWithWarmupScheduler</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html">PyTorch’s CosineAnnealingLR</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/docs/transformers/en/main_classes/optimizer_schedules#transformers.get_cosine_schedule_with_warmup">HuggingFace’s CosineDecayWithWarmupScheduler</a></p></li>
<li><p><a class="reference external" href="https://d2l.ai/chapter_optimization/lr-scheduler.html">Learning Rate Scheduler - Dive Into Deep Learning</a></p></li>
</ul>
</div>
</section>
<section id="citations">
<h2><a class="toc-backref" href="#id22" role="doc-backlink">Citations</a><a class="headerlink" href="#citations" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>[1] I. Loshchilov and F. Hutter,
<a class="reference external" href="http://arxiv.org/abs/1608.03983">“SGDR: Stochastic Gradient Descent with Restarts”</a>,
<em>CoRR</em>, vol. abs/1608.03983, 2016.</p></li>
<li><p>[2] A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola,
<a class="reference external" href="https://d2l.ai/chapter_optimization/lr-scheduler.html">“Chapter 12.11. Learning Rate Scheduling”</a>
in Dive into Deep Learning, Cambridge University Press, 2023.</p></li>
<li><p>[3] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han,
<a class="reference external" href="https://arxiv.org/abs/1908.03265">“On the Variance of the Adaptive Learning Rate and Beyond”</a>,
arXiv preprint arXiv:1908.03265, [Submitted on 8 Aug 2019 (v1), last revised
26 Oct 2021 (this version, v4)].</p></li>
</ul>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id7" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola,
<a class="reference external" href="https://d2l.ai/chapter_optimization/lr-scheduler.html">“Chapter 12.11. Learning Rate Scheduling”</a>
in Dive into Deep Learning, Cambridge University Press, 2023.</p>
</aside>
<aside class="footnote brackets" id="lr-multiplier" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">2</a><span class="fn-bracket">]</span></span>
<p>this is a multiplier and not the real learning rate</p>
</aside>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./playbook"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="how_to_inspect_function_and_class_signatures.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">How to Inspect Function and Class Signatures in Python?</p>
      </div>
    </a>
    <a class="right-next"
       href="why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling.</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#warmup">Warmup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#an-example-walkthrough">An Example Walkthrough</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warmup-phase">1. Warmup Phase</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cosine-decay-phase">2. Cosine Decay Phase</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tau-fraction">2.1. Tau Fraction</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-multiplier">2.2. Learning Rate Multiplier</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate">2.3. Learning Rate</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-intuition">Mathematical Intuition</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-s-cosineannealinglr-vs-composer-s-cosineannealingscheduler">PyTorch’s CosineAnnealingLR vs. Composer’s CosineAnnealingScheduler</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#citations">Citations</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>