{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from typing import Any, Dict, List, Literal\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "from rich.pretty import pprint\n",
    "from torchinfo import summary\n",
    "import psutil\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    DistilBertTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    PreTrainedTokenizer, PreTrainedModel\n",
    ")\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "from tqdm.notebook import tqdm  # Use notebook version for better UI in notebooks\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from omnivault.utils.reproducibility.seed import seed_all\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(42, seed_torch=True, set_torch_deterministic=False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "LOGGER.addHandler(handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'label'],\n",
       "    num_rows: 2264\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('financial_phrasebank', 'sentences_allagree', trust_remote_code=True)[\"train\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">303</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1391</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">570</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[1;36m0\u001b[0m: \u001b[1;36m303\u001b[0m, \u001b[1;36m1\u001b[0m: \u001b[1;36m1391\u001b[0m, \u001b[1;36m2\u001b[0m: \u001b[1;36m570\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def count_labels(labels: List[int]) -> Dict[int, int]:\n",
    "    label_counts = Counter(labels)\n",
    "    ordered_label_counts = OrderedDict(sorted(label_counts.items()))\n",
    "    return dict(ordered_label_counts)\n",
    "\n",
    "\n",
    "sentences_allagree = dataset['sentence']\n",
    "labels_allagree = dataset['label']\n",
    "\n",
    "label_counts = count_labels(labels_allagree)\n",
    "pprint(label_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_split = dataset.train_test_split(test_size=0.1, shuffle=True, stratify_by_column='label')\n",
    "train_dataset = train_valid_split['train']\n",
    "valid_dataset = train_valid_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'bos_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'eos_token'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'unk_token'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'bos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'eos_token'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'<|endoftext|>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'unk_token'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'<|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from transformers.models.deberta_v2.modeling_deberta_v2 import DebertaV2Config, StableDropout\n",
    "\n",
    "\n",
    "class MeanPooler(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Qwen/Qwen1.5-0.5B\n",
    "    padding side = right\n",
    "    B=2, T=3, D=4\n",
    "    attention_mask: [B, T] -> [[1, 1, 0], [1, 0, 0]]\n",
    "    last_hidden_state: [B, T, D] -> [\n",
    "                                        [[1, 2, 3, 4],    [5, 6, 7, 8],     [1, 1, 5, 2]],\n",
    "                                        [[9, 10, 11, 12], [13, 14, 15, 16], [1, 3, 2, 2]]\n",
    "                                    ]\n",
    "    input_mask_expanded: [B, T, D] ->   [\n",
    "                                            [[1, 1, 1, 1], [1, 1, 1, 1], [0, 0, 0, 0]],\n",
    "                                            [[1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
    "                                        ]\n",
    "\n",
    "    sum_embeddings: [B, D] -> the idea is simple, you want the sequence position\n",
    "    for which the attention mask is 1, and sum the embeddings for that position.\n",
    "    In other words, if the attention mask is 0, you want to nullify the embedding\n",
    "    for that position. This is achieved by multiplying the embeddings with the\n",
    "    attention mask. This is done for all the positions in the sequence. This\n",
    "    effectively make [1,1,5,2] * [0,0,0,0] = [0,0,0,0] in the example above.\n",
    "    We just want:\n",
    "\n",
    "    1st sequence in the batch to become shape [D] by:\n",
    "        - do a multiplication of the last hidden state with the attention mask\n",
    "            [1, 2, 3, 4] * [1, 1, 1, 1] = [1, 2, 3, 4]\n",
    "            [5, 6, 7, 8] * [1, 1, 1, 1] = [5, 6, 7, 8]\n",
    "            [1, 1, 5, 2] * [0, 0, 0, 0] = [0, 0, 0, 0]\n",
    "\n",
    "            leads to stacked shape of [T, D] for the first sequence\n",
    "\n",
    "        - sum the embeddings for each position in the sequence\n",
    "            [1, 2, 3, 4] + [5, 6, 7, 8] + [0, 0, 0, 0] = [6, 8, 10, 12]\n",
    "\n",
    "                leads to shape [D] for the first sequence\n",
    "        - divide the sum by the sum of the attention mask, in this example\n",
    "            our sum of the attention mask is [1, 1, 1, 1] + [1, 1, 1, 1] + [0, 0, 0, 0] = [2, 2, 2, 2]\n",
    "            in other words we have 2 valid tokens in the sequence to be divided\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        backbone_outputs: BaseModelOutput,\n",
    "        _input_ids: torch.Tensor | None = None,\n",
    "        _attention_mask: torch.Tensor | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        if _attention_mask is None:\n",
    "            raise ValueError(\"Attention mask is required for mean pooling.\")\n",
    "\n",
    "        last_hidden_state = backbone_outputs.last_hidden_state\n",
    "        input_mask_expanded = _attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        return self._output_dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = torch.tensor([\n",
    "    [1, 1, 0],  # Sequence 1\n",
    "    [1, 0, 0]   # Sequence 2\n",
    "])\n",
    "last_hidden_state = torch.tensor([\n",
    "    [[1, 2, 3, 4], [5, 6, 7, 8], [1, 1, 5, 2]],  # Sequence 1\n",
    "    [[9, 10, 11, 12], [13, 14, 15, 16], [1, 3, 2, 2]]  # Sequence 2\n",
    "])\n",
    "\n",
    "attention_mask.shape, last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first, if there is no padding and all sequences in the batch are truncated to the\n",
    "same context length of $T$, then first sequence say is 3 by 4 would simply mean\n",
    "I have a stack of 3 token level hidden embeddings. But as we know pooling is an aggregation\n",
    "to make \"token\" level to \"sequence\" level and a simple mean pooling would be\n",
    "$\\frac{1}{3} \\sum_{i=1}^{3} h_i$ where $h_i$ is the hidden embedding of token $i$.\n",
    "This idea is similar to computer vision's mean pooling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "input_mask_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  8., 10., 12.],\n",
       "        [ 9., 10., 11., 12.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, dim=1)\n",
    "sum_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2., 2.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_mask = input_mask_expanded.sum(dim=1)\n",
    "sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  4.,  5.,  6.],\n",
       "        [ 9., 10., 11., 12.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_embeddings = sum_embeddings / sum_mask\n",
    "mean_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omniverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
