{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training a Mini-GPT to Learn Two-Digit Addition\n",
        "\n",
        "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
        "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
        "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
        "![Tag](https://img.shields.io/badge/Tag-Organized_Chaos-orange)\n",
        "[![Code](https://img.shields.io/badge/View-Code-blue?style=flat-square&logo=github)](https://github.com/gao-hongnan/omniverse/tree/5221d5d8b9bd845568b2e323d908be282c6e8434/omnivault/transformer/projects/adder)\n",
        "\n",
        "\n",
        "```{contents}\n",
        ":local:\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Motivation\n",
        "\n",
        "Generative Pre-trained Transformer (GPT) are well known to perform bad on\n",
        "arithmetic tasks such as addition. This should not come as a surprise since GPT\n",
        "is a _language_ model and not a _math_ model. It is designed to train on a large\n",
        "corpus of text and learn the patterns and structure of natural language. While\n",
        "we do encounter many arithmetic operations in corpus, the encoding of these\n",
        "operations are often in a form that is in the text sense, not in the\n",
        "mathematical sense. After all, what GPT does best is to predict the next token\n",
        "over the entire **vocabulary** distribution.\n",
        "\n",
        "In one of the examples provided from the repository\n",
        "[minGPT](https://github.com/karpathy/minGPT/tree/master), Karpathy demonstrates\n",
        "training a GPT model to learn the addition of two numbers presented as strings.\n",
        "This is a simple task designed to illustrate how a decoder-only model can be\n",
        "trained to learn \"addition\". Thus, the input is a sequence of characters\n",
        "representing an addition operation (like \"12 + 35\") and the output is the\n",
        "sequence of characters representing the result of the addition (like \"47\").\n",
        "\n",
        "To this end, we replicate his example, which serves as a proof-of-concept to\n",
        "show that decoder only models, which are often used for language-related tasks,\n",
        "can learn other patterns or \"languages,\" such as the \"language\" of arithmetic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cvk1SmuCrdzK"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import rich\n",
        "import torch\n",
        "from rich.pretty import pprint\n",
        "from torch import nn\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import LRScheduler\n",
        "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
        "from omegaconf import OmegaConf as om\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def find_root_dir(current_path: Path | None = None, marker: str = '.git') -> Path | None:\n",
        "    \"\"\"\n",
        "    Find the root directory by searching for a directory or file that serves as a\n",
        "    marker.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    current_path : Path | None\n",
        "        The starting path to search from. If None, the current working directory\n",
        "        `Path.cwd()` is used.\n",
        "    marker : str\n",
        "        The name of the file or directory that signifies the root.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Path | None\n",
        "        The path to the root directory. Returns None if the marker is not found.\n",
        "    \"\"\"\n",
        "    if not current_path:\n",
        "        current_path = Path.cwd()\n",
        "    current_path = current_path.resolve()\n",
        "    for parent in [current_path, *current_path.parents]:\n",
        "        if (parent / marker).exists():\n",
        "            return parent\n",
        "    return None\n",
        "\n",
        "current_file_path = Path(os.getcwd())\n",
        "root_dir          = find_root_dir(current_file_path, marker='omnivault')\n",
        "\n",
        "if root_dir is not None:\n",
        "    sys.path.append(str(root_dir))\n",
        "    from omnivault._types._alias import Accuracy, Loss\n",
        "    from omnivault.transformer.config.composer import Composer, DataConfig\n",
        "    from omnivault.transformer.config.constants import MaybeConstant\n",
        "    from omnivault.transformer.config.decoder import (\n",
        "        AddNormConfig,\n",
        "        DecoderBlockConfig,\n",
        "        DecoderConfig,\n",
        "        MultiHeadedAttentionConfig,\n",
        "        PositionwiseFeedForwardConfig,\n",
        "    )\n",
        "    from omnivault.transformer.config.global_ import MaybeGlobal\n",
        "    from omnivault.transformer.config.trainer import TrainerConfig\n",
        "    from omnivault.transformer.config.optim import AdamConfig, OptimizerConfig\n",
        "    from omnivault.transformer.config.generator import GeneratorConfig\n",
        "    from omnivault.transformer.core.dataset import AdderDataset, create_loader, split_dataset, construct_dummy_batch_future_masks, construct_dummy_batch_target_padding_masks\n",
        "    from omnivault.transformer.core.trainer import Trainer\n",
        "    from omnivault.transformer.core.vocabulary import AdderVocabulary\n",
        "    from omnivault.transformer.decoder.core import GPTDecoder\n",
        "    from omnivault.transformer.modules.attention.core import ScaledDotProductAttention\n",
        "    from omnivault.utils.reproducibility.seed import seed_all\n",
        "    from omnivault.transformer.core.tokenizer import AdderTokenizer\n",
        "    from omnivault.transformer.config.optim import OPTIMIZER_REGISTRY\n",
        "    from omnivault.transformer.config.scheduler import SCHEDULER_REGISTRY, LambdaLRConfig\n",
        "    from omnivault.transformer.utils.general_utils import create_directory, download_file\n",
        "    from omnivault.transformer.core.optim import apply_weight_decay_to_different_param_groups\n",
        "    from omnivault.utils.config_management.omegaconf import load_yaml_config, merge_configs\n",
        "    from omnivault.core.logger import RichLogger\n",
        "    from omnivault.utils.inspector.core import get_field_annotations\n",
        "    import inspect\n",
        "    from omnivault.transformer.core.trainer import Trainer, TrainerEvent\n",
        "    from omnivault.transformer.core.callbacks import save_state\n",
        "    from omnivault.transformer.projects.adder.main import evaluate_and_generate_on_valid_epoch_end\n",
        "else:\n",
        "    raise ImportError(\"Root directory not found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yaml_cfg = load_yaml_config(yaml_path=root_dir / \"omnivault/transformer/projects/adder/config.yaml\")\n",
        "cfg = merge_configs(yaml_cfg, args_list=[])\n",
        "om.resolve(cfg)  # inplace ops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "constants: MaybeConstant = MaybeConstant(\n",
        "    NUM_DIGITS=2,\n",
        "    TOKENS=[\n",
        "        \"0\",\n",
        "        \"1\",\n",
        "        \"2\",\n",
        "        \"3\",\n",
        "        \"4\",\n",
        "        \"5\",\n",
        "        \"6\",\n",
        "        \"7\",\n",
        "        \"8\",\n",
        "        \"9\",\n",
        "        \"+\",\n",
        "        \"*\",\n",
        "        \"-\",\n",
        "        \"=\",\n",
        "        \"<BOS>\",\n",
        "        \"<EOS>\",\n",
        "        \"<PAD>\",\n",
        "        \"<UNK>\",\n",
        "    ],\n",
        ")\n",
        "global_config: MaybeGlobal = MaybeGlobal(seed=42, debug=True, debug_samples=100)\n",
        "data_config: DataConfig = DataConfig(**cfg.data)\n",
        "optimizer_config = AdamConfig(name=\"torch.optim.Adam\", lr=0.2, betas=(0.9, 0.98), eps=1e-9)\n",
        "cfg.trainer.device = \"cpu\"\n",
        "\n",
        "trainer_config = TrainerConfig(**cfg.trainer)\n",
        "generate_config = GeneratorConfig(**cfg.generator)\n",
        "composer = Composer(\n",
        "    constants=constants,\n",
        "    global_=global_config,\n",
        "    data=data_config,\n",
        "    optimizer=optimizer_config,\n",
        "    trainer=trainer_config,\n",
        "    generator=generate_config,\n",
        ")\n",
        "pprint(composer)\n",
        "\n",
        "LOGGER = RichLogger(**composer.logger.model_dump(mode=\"python\")).logger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reproducibility\n",
        "\n",
        "Reproducibility in deep learning ensures that experiments can be repeated with\n",
        "identical results, critical for verifying research findings and deploying\n",
        "reliable models. Distributed training introduces complexity because it involves\n",
        "multiple computation units which may not synchronize their random states\n",
        "perfectly. If training is paused and resumed, ensuring each unit starts with the\n",
        "correct seed to reproduce the exact computational path becomes challenging. To\n",
        "address this, one can find more sophisticated examples in libraries like\n",
        "Composer, where the whole library's core is built around training deep neural\n",
        "nets in any environment (distributed or not) with reproducibility in mind.\n",
        "\n",
        "```{admonition} References\n",
        ":class: seealso\n",
        "\n",
        "-   [Composer](https://github.com/mosaicml/composer/blob/dev/composer/utils/reproducibility.py)\n",
        "-   [PyTorch Reproducibility](https://pytorch.org/docs/stable/notes/randomness.html)\n",
        "-   [PyTorch Worker](https://pytorch.org/docs/stable/notes/randomness.html#dataloader)\n",
        "-   [PyTorch deterministic algorithms](https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html)\n",
        "-   [CUBLAS reproducibility](https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWV1lAHkrdzN"
      },
      "outputs": [],
      "source": [
        "print(get_field_annotations(func_or_method = seed_all)[0])\n",
        "print(\"\\n\")\n",
        "print(inspect.getdoc(seed_all))\n",
        "\n",
        "seed_all(composer.global_.seed, seed_torch=True, set_torch_deterministic=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocabulary = AdderVocabulary.from_tokens(tokens=constants.TOKENS, num_digits=constants.NUM_DIGITS)  # type: ignore[attr-defined]\n",
        "token_to_index = vocabulary.token_to_index\n",
        "index_to_token = vocabulary.index_to_token\n",
        "vocab_size = vocabulary.vocab_size\n",
        "pprint(token_to_index)\n",
        "pprint(index_to_token)\n",
        "pprint(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assign `vocab_size` to `composer.model` because we don't want to hardcode\n",
        "`vocab_size` beforehand, and want to derive concrete values from the\n",
        "`Vocabulary` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    composer.model.vocab_size = vocab_size\n",
        "except AttributeError as err:\n",
        "    LOGGER.error(err)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ah okay haha, this is the price of writing overly complex and useless code to\n",
        "look fancy and you end up a mess. Anyways, we will handle this later on where\n",
        "we can explicitly instantiate the model config class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AdderTokenizer(vocabulary=vocabulary)\n",
        "assert tokenizer.vocabulary.token_to_index == token_to_index\n",
        "assert tokenizer.vocabulary.index_to_token == index_to_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pprint(tokenizer.encode(\"1\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sequence = \"15+57=072\"\n",
        "sequences = [\"15+57=072\", \"01+02=003\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoded_sentence = tokenizer.encode(sequence)\n",
        "print(f\"Encoded sentence: {encoded_sentence}\")\n",
        "\n",
        "decoded_sentence = tokenizer.decode(encoded_sentence)\n",
        "print(f\"Decoded sentence: {decoded_sentence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoded_sentences = tokenizer.encode_batch(sequences)  # type: ignore[attr-defined]\n",
        "print(f\"Encoded sentences: {encoded_sentences}\")\n",
        "decoded_sentences = tokenizer.decode_batch(encoded_sentences)  # type: ignore[attr-defined]\n",
        "print(f\"Decoded sentences: {decoded_sentences}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PAD = vocabulary.token_to_index[vocabulary.PAD]\n",
        "# UNK = vocabulary.token_to_index[vocabulary.UNK]\n",
        "# ADD = vocabulary.token_to_index[vocabulary.ADD]\n",
        "# EQUAL = vocabulary.token_to_index[vocabulary.EQUAL]\n",
        "# BOS = vocabulary.token_to_index[vocabulary.BOS]\n",
        "# EOS = vocabulary.token_to_index[vocabulary.EOS]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZX9ORv5IrdzR"
      },
      "outputs": [],
      "source": [
        "def pad_number(num: int, length: int) -> str:\n",
        "    \"\"\"\n",
        "    Pad numbers with zeros in front so that they have uniform length.\n",
        "\n",
        "    Note, if a + b = c and num digits allowed to add is 2, then for\n",
        "    a and b we always pad to length 2, but for c we always pad to length 3.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    6 + 90 = 96 -> 06 + 90 = 096\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num : int\n",
        "        Number to be padded.\n",
        "    num_digits : int\n",
        "        Length of the resulting padded number string.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Padded number string.\n",
        "    \"\"\"\n",
        "    return str(num).zfill(length)\n",
        "\n",
        "\n",
        "def equation_to_string(a: int, b: int, c: int, num_digits: int) -> str:\n",
        "    \"\"\"\n",
        "    Formats the addition equation as a string.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    a : int\n",
        "        First addend.\n",
        "    b : int\n",
        "        Second addend.\n",
        "    c : int\n",
        "        Sum of a and b.\n",
        "    num_digits : int\n",
        "        Number of digits each number in the equation should have.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        Formatted equation string.\n",
        "    \"\"\"\n",
        "    padded_a = pad_number(a, num_digits)\n",
        "    padded_b = pad_number(b, num_digits)\n",
        "    padded_c = pad_number(c, num_digits + 1) # note the padding here!\n",
        "    return f\"{padded_a}+{padded_b}={padded_c}\"\n",
        "\n",
        "def decode_equation(vocab: AdderVocabulary, equation: torch.Tensor | List[int]) -> str:\n",
        "    \"\"\"\n",
        "    Convert an equation in list format to string format.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    equation : List[int]\n",
        "        The equation in list format.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The equation in string format.\n",
        "    \"\"\"\n",
        "    if isinstance(equation, torch.Tensor):\n",
        "        equation = equation.tolist()\n",
        "\n",
        "    UNK = vocab.token_to_index[vocab.UNK]\n",
        "    decoded_equation = \"\".join([str(index_to_token.get(x, UNK)) for x in equation])\n",
        "    return decoded_equation.replace(\"<BOS>\", \"\").replace(\"<EOS>\", \"\")\n",
        "\n",
        "def batch_decode_equation(vocab: AdderVocabulary, equations: torch.Tensor | List[List[int]]) -> List[str]:\n",
        "    decoded_equations = []\n",
        "    for equation in equations:\n",
        "        decoded_equation = decode_equation(vocab, equation)\n",
        "        decoded_equations.append(decoded_equation)\n",
        "    return decoded_equations\n",
        "\n",
        "def encode_equation(vocab: AdderVocabulary, equation: str, num_digits: int, device: torch.device) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Convert an equation (up to the equal sign in it) in string format to a list.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    equation : str\n",
        "        The equation in string format.\n",
        "    num_digits : int\n",
        "        Number of digits each number in the equation should have.\n",
        "    device : torch.device\n",
        "        The device to which the tensor should be sent.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        The equation in list format as a tensor.\n",
        "    \"\"\"\n",
        "    plus_idx = equation.index(vocab.ADD)\n",
        "    equal_idx = equation.index(vocab.EQUAL)\n",
        "\n",
        "    BOS = vocab.token_to_index[vocab.BOS]\n",
        "    UNK = vocab.token_to_index[vocab.UNK]\n",
        "\n",
        "    a = pad_number(int(equation[:plus_idx]), num_digits)\n",
        "    b = pad_number(int(equation[plus_idx + 1:equal_idx]), num_digits)\n",
        "\n",
        "    new_equation = f\"{a}+{b}=\"\n",
        "\n",
        "    return torch.tensor(\n",
        "        [BOS] + [token_to_index.get(n, UNK) for n in new_equation],\n",
        "        dtype=torch.int\n",
        "    ).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SovDUghLrdzR"
      },
      "outputs": [],
      "source": [
        "def create_add_dataset(\n",
        "    vocab: AdderVocabulary, num_digits: int, dataset_size: int, rng_seed: int = 1337\n",
        ") -> Tuple[List[torch.Tensor], List[str]]:\n",
        "    BOS = vocab.token_to_index[vocab.BOS]\n",
        "    EOS = vocab.token_to_index[vocab.EOS]\n",
        "    UNK = vocab.token_to_index[vocab.UNK]\n",
        "\n",
        "    rng = torch.Generator()\n",
        "    rng.manual_seed(rng_seed)\n",
        "\n",
        "    max_num = 10**num_digits - 1\n",
        "\n",
        "    dataset_str = []\n",
        "    for _ in range(dataset_size):\n",
        "        a = torch.randint(low=0, high=max_num + 1, size=(1,), generator=rng).item()\n",
        "        b = torch.randint(low=0, high=max_num + 1, size=(1,), generator=rng).item()\n",
        "        c = a + b\n",
        "\n",
        "        equation = equation_to_string(a, b, c, num_digits)\n",
        "\n",
        "        dataset_str.append(equation)\n",
        "\n",
        "    dataset_tensor = [\n",
        "        torch.tensor([BOS] + [token_to_index.get(n, UNK) for n in x] + [EOS])\n",
        "        for x in dataset_str\n",
        "    ]\n",
        "    return dataset_tensor, dataset_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEfK777ErdzR",
        "outputId": "20b75567-38b1-4e96-9892-08f60d18195d"
      },
      "outputs": [],
      "source": [
        "dataset_tensor, dataset_str = create_add_dataset(vocab=vocabulary, num_digits=2, dataset_size=4)\n",
        "pprint(dataset_tensor)\n",
        "pprint(dataset_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Decoded equation: {decode_equation(vocabulary, dataset_tensor[0])}\")\n",
        "assert (\n",
        "    decode_equation(vocabulary, dataset_tensor[0])\n",
        "    == dataset_str[0]\n",
        "    == decode_equation(vocabulary, [15, 1, 5, 10, 5, 7, 13, 0, 7, 2, 14])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "if we encode equation, we can encode up to equal sign like below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Encoded equation: {encode_equation(vocabulary, dataset_str[0], num_digits=2, device=composer.trainer.device)}\")\n",
        "\n",
        "torch.testing.assert_close(\n",
        "    encode_equation(vocabulary, dataset_str[0], num_digits=2, device=composer.trainer.device),\n",
        "    torch.tensor([14, 1, 5, 10, 5, 7, 13], dtype=torch.int32),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uncomment the below code to generate the dataset into a text file and yes, I am\n",
        "lazy to add a config variable for whether to generate the dataset or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataset, dataset_str = create_add_dataset(vocab, self.num_digits, self.dataset_size)\n",
        "\n",
        "# write dataset_str to a file\n",
        "# with open(\"dataset_str.txt\", \"w\") as f:\n",
        "#     for item in dataset_str:\n",
        "#         f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding Strategy Overview\n",
        "\n",
        "Our strategy for encoding arithmetic expressions is pretty self-explanatory,\n",
        "where given a string `D1 + D2 = D3`, we encode it as `<BOS>D1+D2=0D3<EOS>`.\n",
        "However, this is verbose for clarity sake. In fact, Karpathy's encoding strategy\n",
        "simplifies arithmetic expressions by concatenating the digits of operands and\n",
        "the result into a single string without explicit symbols for operations or\n",
        "equality. This method relies on a fixed number of digits (`num_digits`) for\n",
        "operands, which streamlines the model's interpretation of the sequence. For\n",
        "example, if `num_digits` is set to 2, every encoded expression is structured to\n",
        "follow a predictable pattern: the first two digits represent the first operand,\n",
        "the next two digits represent the second operand, and the final digits are\n",
        "encoded as 3 digits because the max sum of two 2-digit numbers is 199, which is\n",
        "3 digits. The digits of the result are encoded in reverse order. This\n",
        "counterintuitive approach is designed to align with the GPT model's learning\n",
        "algorithm, facilitating easier learning of the addition operation by mimicking\n",
        "the traditional right-to-left calculation process in addition.\n",
        "\n",
        "To illustrate, let's examine the encoding of arithmetic expressions with\n",
        "`num_digits=2`:\n",
        "\n",
        "For the expression `6 + 39 = 45`, we have the following:\n",
        "\n",
        "-   The first two digits `06` represent the number 6, zero-padded to adhere to\n",
        "    the `num_digits=2` requirement.\n",
        "-   The next two digits `39` represent the number 39, already fitting the digit\n",
        "    requirement.\n",
        "-   The final part `054` represents the result 45, reversed to `54` and preceded\n",
        "    by a zero to maintain the total length of $2n + (n + 1) = 7 $ digits for\n",
        "    `num_digits=2`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHs3grM1rdzS"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_directory(composer.data.dataset_dir)\n",
        "download_file(url=composer.data.dataset_url, output_path=composer.data.dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(composer.data.dataset_path, \"r\") as file:\n",
        "    sequences = [line.strip() for line in file]\n",
        "\n",
        "dataset = AdderDataset(data=sequences, tokenizer=tokenizer)\n",
        "\n",
        "pprint(next(iter(dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construct Batches, Collate Function and DataLoader\n",
        "\n",
        "We first reverse engineer what our dataset is returning. The disclaimer here is\n",
        "that for decoder only models like GPT, many people often omit the padding mask\n",
        "since all the samples $\\mathbf{x}$ are chunked to sequence/context length of\n",
        "window size $T$, and future masks are usually handled within the `Attention`\n",
        "class since we will never attend to the future tokens. However, for the sake of\n",
        "clarity, we will include the padding and future mask in the dataset (i.e.\n",
        "actually it is for the sake of my own understanding when I started to implement\n",
        "decoder from scratch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input, target, target_padding_mask, future_mask = next(iter(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Input and Target\n",
        "\n",
        "I think if you've read my\n",
        "[section here](https://www.gaohongnan.com/transformer/decoder/implementation.html#construction-of-input-and-target-sequences),\n",
        "then we would easily see that given an input sequence $\\mathbf{x}$, the target\n",
        "sequence $\\mathbf{y}$ is simply the input sequence $\\mathbf{x}$ shifted by one\n",
        "time step to the left. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Input : {input}\")\n",
        "print(f\"Target: {target}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Target Padding Mask\n",
        "\n",
        "When you're dealing with sequences of different lengths, you pad the shorter\n",
        "sequences with a special token `PAD` (usually $0$ or $-100$) to make them the\n",
        "same length as the longest one in the batch. These paddings should not\n",
        "contribute to the model's learning, so you need to mask them out. In practice,\n",
        "you'll often see a mask argument in `Attention` layers in PyTorch where if\n",
        "`True`, the attention scores are set to `-inf` for the padded positions so that\n",
        "these positions become zero after the softmax operation, thereby not\n",
        "contributing to the weighted sum of the input sequence.\n",
        "\n",
        "In a decoder-only model like GPT, the input sequence is essentially the target.\n",
        "The model aims to generate tokens that come after the given input, treating it\n",
        "as the \"history\" or \"context\" for the task of text generation. Unlike\n",
        "encoder-decoder models like the original Transformer, where the encoder\n",
        "processes a source sequence and the decoder generates a target sequence, a\n",
        "decoder-only model works solely with what would traditionally be considered the\n",
        "target sequence.\n",
        "\n",
        "Consequently, although the terminology \"target padding mask\" might seem more\n",
        "intuitive in the context of encoder-decoder models, where the distinction\n",
        "between source (input) and target (output) sequences is clear. The distinction\n",
        "is blurred in decoder-only models like GPT as the model processes input to\n",
        "predict the next token in a sequence. Here, the source is essentially the target\n",
        "at different stages of processing: the model uses previous tokens (source) to\n",
        "predict the next token (target). However, during my implementation, I was mainly\n",
        "referring to transformer models that use encoder-decoder architecture, and the\n",
        "terminology therefore stemmed from that context.\n",
        "\n",
        "The definition of a target padding mask is a binary mark that ignores pad-tokens\n",
        "in the source input (in decoder only model, the source is the target). And the\n",
        "shape is $(\\mathcal{B}, T)$.\n",
        "\n",
        "Let's illustrate the target padding mask with an example. Suppose we have a\n",
        "batch of sequences with different lengths:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_batch = [\n",
        "    [5, 7, 9],\n",
        "    [8, 6],\n",
        "    [3, 12, 4, 11, 17],\n",
        "    [2, 1, 4, 5],\n",
        "]\n",
        "pprint(target_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we try to \"batch\" these sequences, PyTorch would throw an error indicating\n",
        "that you need all sequences to have the same length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    target_batch = torch.tensor(target_batch, dtype=torch.int64)\n",
        "except ValueError as err:\n",
        "    LOGGER.error(err)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To address this issue, we could pad the sequences to the same length and create a mask to indicate\n",
        "which positions are padded.  We pad the shorter sequences with a special token `PAD`\n",
        "to make them the same length as the longest one in the batch. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PAD = vocabulary.token_to_index[vocabulary.PAD]\n",
        "\n",
        "max_len = max(len(seq) for seq in target_batch)\n",
        "target_batch = [seq + [PAD] * (max_len - len(seq)) for seq in target_batch]\n",
        "pprint(target_batch)\n",
        "\n",
        "target_batch = torch.tensor(target_batch, dtype=torch.int64)\n",
        "pprint(target_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size, seq_len = target_batch.size()\n",
        "\n",
        "target_padding_mask = target_batch != PAD\n",
        "\n",
        "pprint(target_padding_mask)\n",
        "\n",
        "assert target_padding_mask.size() == (batch_size, seq_len) == (4, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Of course, we would need a _batch_ of these masks, so we would have a shape of\n",
        "$(\\mathcal{B}, T)$ like mentioned above. As we will see later, we will still\n",
        "need to broadcast the shape to $(\\mathcal{B}, 1, T, T)$ to match the shape of\n",
        "the attention scores.\n",
        "\n",
        "Theoretically speaking, it is possible for the sequence length $T$ to vary\n",
        "across samples $\\mathbf{x}$. However, we usually have the same length for all\n",
        "samples in GPT, and in this particular case, we do know that each sample\n",
        "necessarily have the same length by _design_. However, for the sake of\n",
        "explanation, we note that in our `Dataset`, it will only generate 1 single\n",
        "sample data point and do not worry about different sequence length across other\n",
        "samples in the dataset $\\mathcal{S}$, but in deep learning we train in\n",
        "mini-batches $\\mathcal{B}$, and with different batch sizes we may encounter\n",
        "issues (i.e. matrix multiplication may not work)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Future Mask\n",
        "\n",
        "In the decoder, each position can only attend to positions that come before it\n",
        "in the sequence to maintain the auto-regressive property. This is different from\n",
        "the encoder, where all positions can attend to all other positions.\n",
        "\n",
        "The definition of future mask is basically a look-ahead mask to ensure that each\n",
        "position only attends to positions before it in the sequence where we mask out\n",
        "future positions (i.e., positions that come after the current position) so that\n",
        "they don't contribute to the current attention scores. Before the softmax\n",
        "operation, we'll mark these positions as `-inf` so that they become zero after\n",
        "the softmax operation - effectively zeroing out the attention scores for future\n",
        "positions. What does zeroing out these masked logits actually does? Basically,\n",
        "the attention mechanism can be thought of as a weighted average of all the\n",
        "tokens in the input sequence. Each token is assigned a weight, with higher\n",
        "weights indicating more relevance to the token under consideration. If a certain\n",
        "token should not be considered at all (e.g., it's a future token that should not\n",
        "be visible to the current decoder step, or it's a padding token), its weight\n",
        "should be zero.\n",
        "\n",
        "The shape of the future mask is $(T, T)$ for a target sequence/sample\n",
        "$\\mathbf{x}$ of length $T$. Let's see a concrete example to illustrate the\n",
        "future mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seq_len = 5\n",
        "future_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
        "future_mask = future_mask == 0\n",
        "\n",
        "pprint(future_mask)\n",
        "assert future_mask.size() == (seq_len, seq_len) == (5, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merge Padding and Future Masks\n",
        "\n",
        "We see from our `decoder` implementation below, that one of the method is \n",
        "creating the target masks. In other words, we are creating the target padding\n",
        "masks and future masks, and merging them together. \n",
        "\n",
        "\n",
        "```{code-block} md\n",
        "---\n",
        "linenos: true\n",
        "emphasize-lines: 27\n",
        "---\n",
        "\n",
        "def create_target_masks(\n",
        "    self,\n",
        "    batch_size: int,\n",
        "    seq_len: int,\n",
        "    target_padding_masks: torch.BoolTensor | NotGiven = NOT_GIVEN,\n",
        "    future_masks: torch.BoolTensor | NotGiven = NOT_GIVEN,\n",
        ") -> torch.BoolTensor:\n",
        "    target_masks_shape = (batch_size, 1, seq_len, seq_len)\n",
        "    if target_padding_masks is NOT_GIVEN and future_masks is NOT_GIVEN:\n",
        "        target_padding_masks = cast(\n",
        "            torch.BoolTensor, construct_dummy_batch_target_padding_masks(batch_size, seq_len)\n",
        "        )\n",
        "        future_masks = cast(torch.BoolTensor, construct_dummy_batch_future_masks(batch_size, seq_len))\n",
        "\n",
        "    if target_padding_masks is NOT_GIVEN:\n",
        "        target_padding_masks = cast(\n",
        "            torch.BoolTensor, construct_dummy_batch_target_padding_masks(batch_size, seq_len)\n",
        "        )\n",
        "\n",
        "    if future_masks is NOT_GIVEN:\n",
        "        future_masks = cast(torch.BoolTensor, construct_dummy_batch_future_masks(batch_size, seq_len))\n",
        "\n",
        "    assert target_padding_masks.shape == future_masks.shape == target_masks_shape  # type: ignore[union-attr]\n",
        "\n",
        "    return cast(\n",
        "        torch.BoolTensor,\n",
        "        torch.logical_and(cast(torch.Tensor, target_padding_masks), cast(torch.Tensor, future_masks)).bool(),\n",
        "    )\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The purpose of applying `logical_and` between `target_padding_mask` and\n",
        "`future_mask` is to combine the constraints from both masks when calculating\n",
        "self-attention scores in the transformer's decoder. The `target_padding_mask` is\n",
        "designed to mask out the padding tokens in the input sequence, while the\n",
        "`future_mask` ensures that a given position cannot attend to future positions in\n",
        "the sequence. By combining these masks, you can perform the necessary masking\n",
        "for both padding and future tokens in a single step.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "1. `target_padding_mask`: Masks out the padding tokens so that they don't\n",
        "   contribute to the attention calculations. True values mean \"attend to this\n",
        "   token,\" and False values mean \"ignore this token.\"\n",
        "\n",
        "2. `future_mask`: The future mask is created as a lower triangular matrix, where\n",
        "   the lower triangle, including the diagonal, is filled with ones, and the\n",
        "   upper triangle is filled with zeros. Masks out future tokens in a sequence so\n",
        "   that a token at a given position can only attend to positions that come\n",
        "   before it (and itself). True values mean \"attend to this token,\" and False\n",
        "   values mean \"ignore this token.\"\n",
        "\n",
        "3. `logical_and(target_padding_mask, future_mask)`: Combines the two masks. A\n",
        "   True in the resulting mask means that the condition for both padding and\n",
        "   future attention is satisfied.\n",
        "\n",
        "By combining these two masks, the decoder obeys the autoregressive property,\n",
        "ensuring it doesn't see future tokens, while also ignoring padding tokens in the\n",
        "input sequence. We may term it the `target_mask`.\n",
        "\n",
        "#### First Sample First Token\n",
        "\n",
        "-   `target_padding_mask` has size of `[4, 5]`.\n",
        "    -   We zoom in to the first row (sample) which is of length 5.\n",
        "    -   This length 5 is the sequence length, which is `T, T, T, F, F`\n",
        "        indicating the last 2 tokens being padded.\n",
        "-   `future_mask` has size of `[5, 5]`.\n",
        "    -   We note that this is indepedent of batch size. Each sample should have\n",
        "        the same future mask shape of `[L, L]`.\n",
        "    -   This `L=5` should necessary be same for the sequence length in\n",
        "        `target_padding_mask`.\n",
        "-   First, let's consider one batch of 4 samples. What we do first is to\n",
        "    broadcast `future_mask` to `[4, 5, 5]` because we want each sample/row in\n",
        "    the batch to have the same future mask. As shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pprint(future_mask)\n",
        "future_mask = future_mask.view(1, seq_len, seq_len).expand(size=(batch_size, -1, -1))\n",
        "pprint(future_mask)\n",
        "pprint(future_mask.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Now, we can zoom in to one particular sample since both\n",
        "    `target_padding_mask` and `future_mask` have the same first dimension of\n",
        "    batch size.\n",
        "-   What is incomplete is that we need to broadcast `target_padding_mask`'s last\n",
        "    dimension to have the same dimensions as `future_mask`. This means we\n",
        "    broadcast `[4, 5]` to `[4, 5, 5]`. But why?\n",
        "-   For simplicity, we slice the first same of both below.\n",
        "-   The first row of the `future_mask` of the first sample is `T, F, F, F, F`.\n",
        "    This corresponds to what? This is the future mask of the first token in the\n",
        "    sequence. Well, that is confusing, because it apparently have 5 elements,\n",
        "    and has \"information\" of the other 4 tokens in the sequence. Let's explain\n",
        "    in details below:\n",
        "    -   Regarding the first row of the `future_mask` in the first sample, which\n",
        "        is `[T, F, F, F, F]`, it might initially seem confusing why there are 5\n",
        "        elements. Each of these elements, in fact, corresponds to whether the\n",
        "        first token can attend to other tokens at each respective position in\n",
        "        the sequence. Here's how to interpret it:\n",
        "        -   The first element (`True`) indicates that the first token can attend\n",
        "            to itself.\n",
        "        -   The next four elements (`False`) specify that the first token should\n",
        "            not attend to any of the future tokens in the sequence.\n",
        "-   Consequently, what is the first token in the sequence of the\n",
        "    `target_padding_mask`? Recall earlier we mentioned that the first sample's\n",
        "    `target_padding_mask` is `T, T, T, F, F` and therefore the first token in\n",
        "    the sequence is `T`.\n",
        "-   What do we want to achieve here? We want to make sure that the model does\n",
        "    not **attend** to tokens in the sequence that are masked with `False`.\n",
        "-   In other words, the first token in the sequence of the first sample has\n",
        "    `target_padding_mask` of `T` and `future_masks` of `T, F, F, F, F`.\n",
        "-   We need to broadcast this `T` to `T, T, T, T, T` to align with\n",
        "    `T, F, F, F, F` because? Because we need ensure that this first token in the\n",
        "    sequence is also able to considered in relation to every other token in the\n",
        "    sequence.\n",
        "-   So the first token is not a padded token, which is `T`, similarly, the first\n",
        "    token needs to attend to itself at the first position, hence `T` and `T`\n",
        "    give `T`. But for the second `T` in the now broadcasted\n",
        "    `target_padding_mask`, it is still representing the first token or?\n",
        "-   Broadcasting the first token's `target_padding_mask` value of `T` to\n",
        "    `[T, T, T, T, T]` ensures that when this first token is being considered for\n",
        "    attention computations, it is free to attend to any position, barring any\n",
        "    restrictions set by `future_mask`.\n",
        "-   Tricky: after broadcasting, each `T` in `[T, T, T, T, T]` is still\n",
        "    representing the first token. They indicate that when the first token is\n",
        "    compared with _any_ token in the sequence (including itself), it is not a\n",
        "    padding token. The element-wise `AND` with the `future_mask` then further\n",
        "    refines this by restricting it from attending to future tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pprint(target_padding_mask)\n",
        "pprint(target_padding_mask[0])\n",
        "\n",
        "target_padding_mask = target_padding_mask.view(batch_size, 1, seq_len).expand(size=(batch_size, seq_len, seq_len))\n",
        "pprint(target_padding_mask)\n",
        "pprint(target_padding_mask.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pprint(target_padding_mask[0])\n",
        "pprint(future_mask[0])\n",
        "pprint(target_padding_mask[0] & future_mask[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### First Sample Fourth Token\n",
        "\n",
        "Now let's look at another exampleâ€”the 4th token in the sequence, where\n",
        "`target_padding_mask = [T, T, T, F, F]` and `future_mask` is a lower triangular\n",
        "matrix with `True`s.\n",
        "\n",
        "1. **4th Token's target_padding_mask**: The 4th token has a value of `F` in\n",
        "   `target_padding_mask`, indicating it's a padding token.\n",
        "\n",
        "2. **4th Row of future_mask**: The 4th row in `future_mask` is\n",
        "   `[True, True, True, True, False]`. This means that if this token were not a\n",
        "   padding token, it would be allowed to attend to all the previous tokens in\n",
        "   the sequence and itself, but not to any future token.\n",
        "\n",
        "3. **Broadcast target_padding_mask**: To align `target_padding_mask` with\n",
        "   `future_mask`, we'd broadcast `F` from the `target_padding_mask` to\n",
        "   `[F, F, F, F, F]`. This way, when we consider the 4th token in relation to\n",
        "   any other token in the sequence, it's still marked as a padding token.\n",
        "\n",
        "4. **Element-wise AND with future_mask**: After broadcasting, you'd perform an\n",
        "   element-wise AND between `[F, F, F, F, F]` and\n",
        "   `[True, True, True, True, False]`, resulting in `[F, F, F, F, F]`.\n",
        "\n",
        "5. **Interpretation**: This effectively means that the 4th token won't attend to\n",
        "   any other token in the sequence, and no token will attend to it either, as it\n",
        "   is a padding token.\n",
        "\n",
        "So, the masks are doing their jobs correctly: the `target_padding_mask`\n",
        "indicates whether each token is a padding token or not, and `future_mask`\n",
        "dictates the \"rules\" of attention regarding what each token can attend to.\n",
        "Combining them ensures that both conditions are met.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Further Add a Singleton Dimension in Target Masks\n",
        "\n",
        "Now both masks are of shape: `(B, L, L)` but we need to add a singleton\n",
        "dimension to the last dimension to make it `(B, 1, L, L)`.\n",
        "\n",
        "In deep learning frameworks like PyTorch, the dimensions of the tensors involved\n",
        "in operations like matrix multiplication or attention mechanisms often have\n",
        "specific semantic meanings. In the context of attention mechanisms, especially\n",
        "in the transformer architecture, the attention mask usually has a shape that is\n",
        "compatible with the attention logits for element-wise multiplication.\n",
        "\n",
        "In the transformer model, the attention logits are often computed as a dot\n",
        "product between query and key vectors, resulting in a tensor of shape\n",
        "`(Batch size, Num heads, Sequence length, Sequence length)` or `(B, H, L, L)`.\n",
        "Here, `B` is the batch size, `H` is the number of attention heads, and `L` is\n",
        "the sequence length.\n",
        "\n",
        "To make the mask tensor compatible for element-wise operations with this 4D\n",
        "tensor, it needs to have a shape that can be broadcasted to `(B, H, L, L)`. A\n",
        "mask of shape `(B, 1, L, L)` fulfills this requirement.\n",
        "\n",
        "The singleton dimension is added so that the mask can be easily broadcast to the\n",
        "shape of the attention logits tensor during the computation. When a tensor with\n",
        "shape `(B, 1, L, L)` is element-wise multiplied with a tensor of shape\n",
        "`(B, H, L, L)`, the singleton dimension (the `1`) allows the mask to be used for\n",
        "each attention head without explicitly replicating the mask `H` times. This is\n",
        "more memory-efficient and often faster.\n",
        "\n",
        "Thus, adding a singleton dimension in masks is a preparatory step that allows\n",
        "for efficient element-wise operations later in the model's forward pass.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_padding_mask = target_padding_mask.unsqueeze(1)\n",
        "pprint(target_padding_mask.shape)\n",
        "\n",
        "future_mask = future_mask.unsqueeze(1)\n",
        "pprint(future_mask.shape)\n",
        "\n",
        "target_mask = target_padding_mask & future_mask\n",
        "pprint(target_mask.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why mask our target in Adder?\n",
        "\n",
        "If you see the source code of how the `AdderDataset` is constructed, you will\n",
        "see that we masked out all the tokens before (and including) the equal sign.\n",
        "\n",
        "For example, if our sequence is `12+97=109`, the input sequence will be\n",
        "tokenized to the following:\n",
        "\n",
        "```python\n",
        "input = [BOS, 1, 2, +, 9, 7, =, 1, 0, 9]\n",
        "target = [1, 2, +, 9, 7, =, 1, 0, 9, EOS]\n",
        "```\n",
        "\n",
        "What our code below does is to mask out the tokens before the equal sign for the\n",
        "target sequence.\n",
        "\n",
        "```python\n",
        "target = [MASK, MASK, MASK, MASK, MASK, MASK, 1, 0, 9, EOS]\n",
        "```\n",
        "\n",
        "```python\n",
        "def construct_target_tensor(self, input_sequence: torch.Tensor) -> torch.LongTensor:\n",
        "    target = input_sequence.clone()\n",
        "    where_equal_index = torch.where(input_sequence == self.equal_token_id)[0].item()\n",
        "    where_equal_index = int(where_equal_index)  # to appease mypy lol\n",
        "    target[: where_equal_index + 1] = self.pad_token_id\n",
        "    return torch.LongTensor(target[1:])\n",
        "```\n",
        "\n",
        "Simply put, we do not care what the model predict for anything before the equal\n",
        "sign. By masking out (or ignoring) the tokens before the =, we are asking the\n",
        "model to \"focus\" on generating the correct answer after the equal sign.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split to Train-Valid-Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size   = 256\n",
        "\n",
        "composer.data.train_loader[\"batch_size\"] = batch_size\n",
        "composer.data.valid_loader[\"batch_size\"] = batch_size\n",
        "composer.data.test_loader[\"batch_size\"] = batch_size\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = split_dataset(\n",
        "    dataset=dataset, split=composer.data.split, seed=composer.global_.seed\n",
        ")\n",
        "\n",
        "train_size, valid_size, test_size = len(train_dataset), len(valid_dataset), len(test_dataset)\n",
        "train_size, valid_size, test_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# max_seq_len is determined by 1+ num_digits + 1 + num_digits + 1 + num_digits + 1 + 1\n",
        "# where the 1s represent BOS, Plus sign, Equal sign, the extra digit in the sum, EOS, respectively.\n",
        "max_seq_len = 1 + 1 + 1 + 1 + 2 * composer.constants.NUM_DIGITS + (composer.constants.NUM_DIGITS + 1)\n",
        "assert max_seq_len == composer.data.context_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = create_loader(\n",
        "    dataset=train_dataset,\n",
        "    loader_config=composer.data.train_loader,\n",
        "    collate_fn_config=composer.data.collate_fn,\n",
        ")\n",
        "\n",
        "valid_loader = create_loader(\n",
        "    dataset=valid_dataset,\n",
        "    loader_config=composer.data.valid_loader,\n",
        "    collate_fn_config=composer.data.collate_fn,\n",
        ")\n",
        "\n",
        "test_loader = create_loader(\n",
        "    dataset=test_dataset,\n",
        "    loader_config=composer.data.test_loader,\n",
        "    collate_fn_config=composer.data.collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `collate_fn` defines how to combine these variable-length samples into a\n",
        "batch. This usually involves padding the sequences in the batch to a common\n",
        "length, which is typically the length of the longest sequence in the batch. Note\n",
        "here the padding in collate is \"redundant\" since in our earlier code we ensured\n",
        "that all sample has same number of characters by way of padding zeros in front.\n",
        "For example, `23 + 3 =26` will become `23 + 03 = 026`. Consequently, all samples\n",
        "in the mini-batch will have same length by definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(composer.global_.seed)\n",
        "\n",
        "batch_index = 0\n",
        "for batch in train_loader:\n",
        "    # Each batch is a tuple containing all elements for the batch\n",
        "    inputs_padded, targets_padded, padding_masks_padded_and_expanded, future_masks_expanded = batch\n",
        "\n",
        "    # Print the length of each component in the batch\n",
        "    print(\"Batch Size:\", len(inputs_padded))\n",
        "\n",
        "    # Now you can print shapes or other properties of each batch element\n",
        "    print(\"Inputs Shape:\", inputs_padded.shape)\n",
        "    print(\"Targets Shape:\", targets_padded.shape)\n",
        "\n",
        "    # Decoding and other processing can be done here\n",
        "    # For example, decoding the first sequence in the batch\n",
        "    print(\"Decoded First Equation/Sample of the Batch:\", decode_equation(vocabulary, inputs_padded[0].tolist()))\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    batch_index += 1\n",
        "    if batch_index == 4: break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model\n",
        "\n",
        "We have went into extensive details on the implementation of the decoder in the\n",
        "[implementation section](implementation.ipynb). We will not repeat the concepts\n",
        "here, instead we will just compile the model with the configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTROZtD4rdzc",
        "outputId": "253c0d26-f2d1-49ad-bd5c-b623d6c79339"
      },
      "outputs": [],
      "source": [
        "# Create individual component configurations\n",
        "masked_self_attention_mha_config = MultiHeadedAttentionConfig(\n",
        "     attention=ScaledDotProductAttention(),\n",
        "    d_model=128, H=4, dropout=0.1\n",
        ")\n",
        "\n",
        "feed_forward_config = PositionwiseFeedForwardConfig(\n",
        "    d_model=128, d_ff=256, activation=nn.GELU(approximate=\"tanh\"), dropout=0.1, bias=True\n",
        ")\n",
        "\n",
        "add_norm_config_1 = AddNormConfig(feature_dim=128, dropout=0.1)\n",
        "add_norm_config_2 = AddNormConfig(feature_dim=128, dropout=0.1)\n",
        "\n",
        "# Create DecoderBlockConfig\n",
        "decoder_block_config = DecoderBlockConfig(\n",
        "    masked_self_attention_mha=masked_self_attention_mha_config,\n",
        "    feed_forward=feed_forward_config,\n",
        "    add_norm_1=add_norm_config_1,\n",
        "    add_norm_2=add_norm_config_2,\n",
        ")\n",
        "\n",
        "# Create the overall DecoderConfig\n",
        "model_config = DecoderConfig(\n",
        "    d_model=128,\n",
        "    vocab_size=vocab_size,\n",
        "    context_length=max_seq_len,\n",
        "    num_decoder_blocks=2,\n",
        "    dropout=0.1,\n",
        "    decoder_block=decoder_block_config,\n",
        ")\n",
        "\n",
        "model = GPTDecoder(model_config).to(composer.trainer.device)\n",
        "\n",
        "model_size = model.total_trainable_parameters\n",
        "print(f'model_size: {model_size}, train_set_size: {train_size}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Paradigm\n",
        "\n",
        "Here, we would list some of the training paradigms that we would be using in\n",
        "this project.\n",
        "\n",
        "### Optimizer\n",
        "\n",
        "We start off by defining the optimizer for GPT-2. A common choice used is the\n",
        "[Adam](https://arxiv.org/abs/1412.6980) {cite}`kingma2014adam` or\n",
        "[AdamW](https://arxiv.org/abs/1711.05101) {cite}`loshchilov2017decoupled`. We\n",
        "conveniently take the configuration provided in Karpathy's\n",
        "[nanoGPT](https://github.com/karpathy/nanoGPT).\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\eta_{\\max} &= 6 \\times 10^{-4} \\\\\n",
        "\\beta_1 &= 0.9 \\\\\n",
        "\\beta_2 &= 0.95 \\\\\n",
        "\\epsilon &= 10^{-8} \\\\\n",
        "\\lambda &= 10^{-1}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Furthermore, we briefly mention that Karpathy applies weight decay to different\n",
        "parameter groups - which is quite a common practice. As we can see from the code\n",
        "below, we define whitelisted and blacklisted modules that we want to apply\n",
        "weight decay to. The whitelist modules are `nn.Linear` and the blacklist modules\n",
        "are `nn.LayerNorm`, `nn.Embedding`.\n",
        "\n",
        "Weight decary, which is basically L2 regularization penalizes the square of the\n",
        "weights, encouraging smaller weight values. This can lead to a \"spreading out\"\n",
        "effect, as it discourages the model from relying too heavily on a small number\n",
        "of input features, promoting a more even distribution of weight values and, by\n",
        "extension, a more balanced consideration of input dimensions. This\n",
        "regularization technique is particularly beneficial for layers that perform\n",
        "matrix multiplication, as it helps in ensuring that the model utilizes a broader\n",
        "range of input features rather than becoming overly dependent on a few dominant\n",
        "ones. We can find more intuition in the discussion\n",
        "[Why not perform weight decay on layernorm/embedding?](https://stats.stackexchange.com/questions/576463/why-not-perform-weight-decay-on-layernorm-embedding),\n",
        "[Weight decay in the optimizers is a bad idea (especially with BatchNorm)](https://discuss.pytorch.org/t/weight-decay-in-the-optimizers-is-a-bad-idea-especially-with-batchnorm/16994)\n",
        "and\n",
        "[Weight decay exclusions (Karpathy)](https://github.com/karpathy/minGPT/pull/24#issuecomment-679316025).\n",
        "\n",
        "```{code-block} md\n",
        "---\n",
        "linenos: true\n",
        "emphasize-lines: 6-7\n",
        "---\n",
        "\n",
        "def apply_weight_decay_to_different_param_groups(\n",
        "    model: nn.Module, weight_decay: float\n",
        ") -> List[Dict[Literal[\"params\", \"weight_decay\"], List[torch.nn.Parameter] | float]]:\n",
        "    decay: Set[str] = set()\n",
        "    no_decay: Set[str] = set()\n",
        "    whitelist_weight_modules: Tuple[Type[nn.Module], ...] = (nn.Linear,)\n",
        "    blacklist_weight_modules: Tuple[Type[nn.Module], ...] = (nn.LayerNorm, nn.Embedding, LayerNorm)\n",
        "\n",
        "    for module_name, module in model.named_modules():\n",
        "        for parameter_name, _parameter in module.named_parameters():\n",
        "            full_parameter_name = f\"{module_name}.{parameter_name}\" if module_name else parameter_name\n",
        "            if parameter_name.endswith(\"bias\"):\n",
        "                # biases of all modules are not decayed\n",
        "                no_decay.add(full_parameter_name)\n",
        "            elif parameter_name.endswith(\"weight\") and isinstance(module, whitelist_weight_modules):\n",
        "                # weights of whitelisted modules are decayed\n",
        "                decay.add(full_parameter_name)\n",
        "            elif parameter_name.endswith(\"in_proj_weight\"):\n",
        "                # MHA projection layer, does not exist in my implementation\n",
        "                decay.add(full_parameter_name)\n",
        "            elif parameter_name.endswith(\"weight\") and isinstance(module, blacklist_weight_modules):\n",
        "                # weights of blacklisted modules are not decayed\n",
        "                no_decay.add(full_parameter_name)\n",
        "            elif (parameter_name.endswith(\"gamma\") or parameter_name.endswith(\"beta\")) and isinstance(\n",
        "                module, LayerNorm\n",
        "            ):\n",
        "                # weights of LayerNorm modules are not decayed\n",
        "                # TODO: why do I need to do this is because my custom LayerNorm has gamma and beta\n",
        "                # as their \"weight\" and \"bias\" attributes, respectively.\n",
        "                no_decay.add(full_parameter_name)\n",
        "            elif parameter_name.endswith(\"pos_embed\"):\n",
        "                no_decay.add(full_parameter_name)\n",
        "\n",
        "    param_dict = {parameter_name: parameter for parameter_name, parameter in model.named_parameters()}  # noqa: C416\n",
        "    inter_params = decay & no_decay\n",
        "    union_params = decay | no_decay\n",
        "    assert not inter_params, f\"Parameters {inter_params} are in both decay and no_decay sets.\"\n",
        "    assert not (\n",
        "        param_dict.keys() - union_params\n",
        "    ), f\"Parameters {param_dict.keys() - union_params} were not categorized.\"\n",
        "\n",
        "    optim_groups: List[Dict[Literal[\"params\", \"weight_decay\"], List[torch.nn.Parameter] | float]] = [\n",
        "        {\"params\": [param_dict[parameter_name] for parameter_name in sorted(decay)], \"weight_decay\": weight_decay},\n",
        "        {\"params\": [param_dict[parameter_name] for parameter_name in sorted(no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "\n",
        "    return optim_groups\n",
        "```\n",
        "\n",
        "We won't go into too much technical rigour on the optimizer, but note that more\n",
        "modern variations exist, for instance\n",
        "[DecoupledAdamW](https://docs.mosaicml.com/projects/composer/en/stable/api_reference/generated/composer.optim.DecoupledAdamW.html),\n",
        "which furthers decouple the weight decay term $\\lambda$ from the learning rate,\n",
        "as well [RAdam](https://arxiv.org/abs/1908.03265) {cite}`liu2019variance`, which\n",
        "is intended to address bias correction factors leading to higher variance in the\n",
        "adaptive learning rate for the initial training iterations.\n",
        "\n",
        "To this end, we create the optimizer in code as follows, noting that we would\n",
        "not use the exact same configuration as Karpathy, but rather use what is\n",
        "deemed fit for the case at hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pprint(composer.optimizer)\n",
        "optimizer_config_cls = OPTIMIZER_REGISTRY[composer.optimizer.name]\n",
        "optimizer_pydantic_config = optimizer_config_cls(**composer.optimizer.model_dump(mode=\"python\"))\n",
        "pprint(optimizer_pydantic_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "assert hasattr(composer.optimizer, \"weight_decay\")\n",
        "\n",
        "optimizer = optimizer_pydantic_config.build(\n",
        "    params=apply_weight_decay_to_different_param_groups(\n",
        "        model=model, weight_decay=composer.optimizer.weight_decay\n",
        "    )\n",
        ")\n",
        "pprint(optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learning Rate Scheduler\n",
        "\n",
        "#### Motivation\n",
        "\n",
        "In training deep neural networks, learning rate is definitely one of the most\n",
        "important parameter to tune. Optimization algorithms like\n",
        "[Adam](https://arxiv.org/abs/1412.6980) and\n",
        "[SGD](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) tell us how the\n",
        "weights $\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}$ should be updated, but the\n",
        "learning rate $\\eta$ tells us the **_rate_** at which the weights are being\n",
        "updated.\n",
        "\n",
        "Theoretically and empircally, the **_magnitude_** of the learning rate $\\eta$\n",
        "can have a significant impact on the training process. If the learning rate is\n",
        "too _large_, we might experience\n",
        "[divergence](<https://en.wikipedia.org/wiki/Divergence_(mathematics)>), on the\n",
        "other hand, if the learning rate is too _small_, the model might take longer to\n",
        "converge or might get stuck in a local\n",
        "[minima](https://en.wikipedia.org/wiki/Maxima_and_minima). The condition number\n",
        "of the problem also impacts optimization efficiency, as discussed in\n",
        "[the momentum section](https://d2l.ai/chapter_optimization/momentum.html#sec-momentum),\n",
        "where the concept can be understood as the ratio between the smallest and\n",
        "largest changes possible in response to adjustments in different directions of\n",
        "the parameter space, reflecting the variance in sensitivity across these\n",
        "directions[^1] {cite}`zhang2023dive`. As we progress through the training steps,\n",
        "it is also equally important to apply a learning rate scheduler to adjust (may\n",
        "_not_ be monotonous decay) the learning rate _discriminatively_.\n",
        "\n",
        "In the paper\n",
        "[_SGDR: Stochastic Gradient Descent with Restarts_](https://arxiv.org/abs/1608.03983)\n",
        "by Loshchilov and Hutter, they introduced an heuristic that relies on the\n",
        "empirical observation that we can improve the convergence of the model (usually\n",
        "in ill-conditioned situations) if we want follow an **_annealing_** process over\n",
        "the learning rate. This means that at the beginning of training, we do not want\n",
        "to decrease the learning too drastically. My (potentially wrong) intuition is\n",
        "that this may allow the model to consider exploring a larger parameter space\n",
        "without too much constraints than if we were to rapidly decrease the learning\n",
        "rate. The authors further claim that as we progress towards the end of the\n",
        "training, we would want to \"fine-tune\" the model parameters with a very small\n",
        "learning rate, as it could potentially help \"refine\" the solution space to find\n",
        "a \"more optimal\" set of parameters {cite}`DBLP:journals/corr/LoshchilovH16a`.\n",
        "This idea _naturally lands_ us to using _cosine function_ because the cosine\n",
        "curve starts with a _gentle slope_, which coincides with the idea of _gradual\n",
        "decrease_ in learning rate in the beginning, and the cosine curve naturally\n",
        "flattens and approaches zero towards the end as it reaches the end of its cycle,\n",
        "which again coincides with the idea of _fine-tuning_ the model parameters with a\n",
        "very small learning rate.\n",
        "\n",
        "Consequently, a cosine decaying scheduler has the below function form for\n",
        "learning rates in the range $t \\in [0, T]$:\n",
        "\n",
        "$$\n",
        "\\eta_t=\\eta_T+\\frac{\\eta_0-\\eta_T}{2}\\left(1+\\cos \\left(\\frac{\\pi t}{T}\\right)\\right)\n",
        "$$\n",
        "\n",
        "Here $\\eta_0$ is the initial learning rate, $\\eta_T$ is the target rate at time\n",
        "$T$. Furthermore, for $t>T$ we simply pin the value to $\\eta_T$ without\n",
        "increasing it again. $T$ represents the end of the learning rate annealing phase\n",
        "rather than the absolute end of training. It's the point in time when the\n",
        "learning rate reaches $\\eta_T$, the target rate, and beyond which the learning\n",
        "rate is maintained constant at $\\eta_T$.\n",
        "\n",
        "-   During $0 \\leq t < T$: The learning rate $\\eta_t$ is actively adjusted\n",
        "    according to the cosine annealing formula. It transitions from the initial\n",
        "    learning rate $\\eta_0$ towards the target rate $\\eta_T$, following a\n",
        "    half-cosine wave.\n",
        "-   For $t \\geq T$: The learning rate is set to $\\eta_T$ and no longer changes.\n",
        "    This doesn't necessarily mean that training must stop at $t = T$. Training\n",
        "    can continue beyond $T$ with the learning rate fixed at $\\eta_T$.\n",
        "\n",
        "In code, we can observe the behavior of the cosine annealing scheduler as\n",
        "follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from typing import Any, List\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, _LRScheduler\n",
        "\n",
        "def get_learning_rates(optimizer: Optimizer, scheduler: _LRScheduler, steps: int) -> List[float]:\n",
        "    lrs = []\n",
        "    for _ in range(steps):\n",
        "        lrs.append(optimizer.param_groups[0][\"lr\"])\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    return lrs\n",
        "\n",
        "def plot_learning_rates(\n",
        "    lrs: List[float], title: str, marker: str = \"o\", ax: plt.Axes | None = None, **kwargs: Any\n",
        ") -> None:\n",
        "    ax = ax or plt.gca()\n",
        "\n",
        "    ax.plot(lrs, label=title, marker=marker, **kwargs)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"Step\")\n",
        "    ax.set_ylabel(\"Learning Rate\")\n",
        "    ax.legend()\n",
        "\n",
        "def main() -> None:\n",
        "    initial_lr = 0.1\n",
        "    eta_min = 0\n",
        "    steps = 100\n",
        "    model = torch.nn.Linear(2, 1)\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr)\n",
        "    scheduler_non_cyclic = CosineAnnealingLR(optimizer, T_max=steps, eta_min=eta_min)\n",
        "    lrs_non_cyclic = get_learning_rates(optimizer, scheduler_non_cyclic, steps)\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr)\n",
        "    scheduler_cyclic = CosineAnnealingLR(optimizer, T_max=steps // 8, eta_min=eta_min)\n",
        "    lrs_cyclic = get_learning_rates(optimizer, scheduler_cyclic, steps)\n",
        "\n",
        "    # Plotting\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    plot_learning_rates(lrs_non_cyclic, 'Non-Cyclic Cosine Annealing', ax=axes[0])\n",
        "    plot_learning_rates(lrs_cyclic, 'Cyclic Cosine Annealing', ax=axes[1])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Warmup\n",
        "\n",
        "Our motivation would have ended here, but in practice, we often see that the\n",
        "cosine annealing scheduler is often combined with a warmup phase. In\n",
        "{numref}`why-cosine-warmup-loss-plot`, we can see that the loss curve is\n",
        "relatively smooth and converges way better than the ones without warmup.\n",
        "\n",
        "```{figure} ../../playbook/assets/warmup_loss_plot_uvadlc.svg\n",
        "---\n",
        "name: why-cosine-warmup-loss-plot-duplicate\n",
        "---\n",
        "\n",
        "Training loss v.s. # of iterations of Transformers on the De-En IWSLTâ€™14 dataset.\n",
        "\n",
        "**Image Credit:**\n",
        "[ON THE VARIANCE OF THE ADAPTIVE LEARNING RATE AND BEYOND](https://arxiv.org/pdf/1908.03265.pdf)\n",
        "```\n",
        "\n",
        "It might be worth having some intuition on why warmup works so well in practice,\n",
        "and in particular, in language models like\n",
        "[Transformers](https://arxiv.org/abs/1706.03762).\n",
        "\n",
        "Firstly, the [RAdam](https://arxiv.org/pdf/1908.03265.pdf) paper suggests warmup\n",
        "works as a variance reduction technique, which overcomes the problem of\n",
        "[bias correction factors](https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for)\n",
        "in optimizers like Adam, where having these bias correction factors would lead\n",
        "to larger variance in the adaptive learning rate during the **initial** training\n",
        "iterations {cite}`lippe2023uvadlc`. More concretely, Adam estimates the first\n",
        "and second moments of the gradient to change the learning rate of each\n",
        "individual parameter (hence adaptive) and having high variance between adaptive\n",
        "learning rates may de-stablize the training. If we don't want to swap out Adam,\n",
        "then this calls for a warmup phase to stabilize the learning rate and reduce the\n",
        "variance in the early stages of training.\n",
        "\n",
        "Secondly, language models like Transformers use iteratively applied Layer\n",
        "Normalization across layers can lead to very high gradients during the first\n",
        "iterations, which can be solved by using\n",
        "[Pre-Layer Normalization](https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf)\n",
        "(similar to Pre-Activation ResNet), which applies normalization before the\n",
        "layer's main operations, contributing to gradient stabilization and reducing the\n",
        "necessity for a warm-up phase, or replacing Layer Normalization by other\n",
        "techniques\n",
        "([Adaptive Normalization](https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf),\n",
        "[Power Normalization](https://arxiv.org/abs/2003.07845))\n",
        "{cite}`lippe2023uvadlc`.\n",
        "\n",
        "However, even though there are solutions to the problem, certain setups still\n",
        "use the Adam optimizer, and therefore warmup is still a simple and effective\n",
        "technique to stabilize the learning rate in the early stages of training -\n",
        "solving the afforementioned problems (i.e. stabilize the bias correction\n",
        "factors, moving averages of gradients and squared gradients).\n",
        "\n",
        "To this end, we end our discussion on the motivation behind 1) using cosine\n",
        "annealing schedulers and 2) using warmup phases, often coupled with cosine\n",
        "annealing schedulers. In what follows, we will provide a more formal definition\n",
        "of the cosine annealing scheduler with warmup, and provide a running example to\n",
        "illustrate the behavior of the scheduler.\n",
        "\n",
        "#### Definition\n",
        "\n",
        "The `CosineAnnealingWithWarmupScheduler` decays the learning rate $\\eta$\n",
        "according to the decreasing part of a cosine curve, with an initial warmup\n",
        "$t_{\\text{warmup}}$.\n",
        "\n",
        "This scheduler modulates $\\eta$ within defined upper and lower bounds over a\n",
        "predetermined interval, employing a cosine function. The formula for cosine\n",
        "annealing reflects the shape of a half-cosine wave, which decreases from a\n",
        "maximum value to a minimum and then increases back to the maximum. This cycle\n",
        "can repeat multiple times over the training process, depending on how the\n",
        "scheduler is configured. Although this approach suggests cyclic adjustments\n",
        "(oscillations) within the training duration, for simplicity's sake, our specific\n",
        "implementation, inspired by\n",
        "[**MosaicML's Composer's CosineAnnealingWithWarmupScheduler**](https://docs.mosaicml.com/projects/composer/en/latest/api_reference/generated/composer.optim.CosineAnnealingWithWarmupScheduler.html),\n",
        "explicitly excludes considerations for such cycles/oscillations.\n",
        "\n",
        "```{prf:definition} Cosine Annealing With Warmup\n",
        ":label: why-do-we-use-warmup-cosine-scheduler-definition-duplicate\n",
        "\n",
        "The `CosineAnnealingWithWarmupScheduler` modulates the **learning rate** $\\eta$\n",
        "according to a **two-phase** process: a **_warmup_** phase followed by a\n",
        "**cosine annealing** phase. The learning rate _multiplier_[^lr-multiplier]\n",
        "$\\alpha_{t}$ at any given time (step) $t$ is given by:\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\alpha_{t} = \\begin{cases}\n",
        "    \\frac{t}{t_{\\text{warmup}}}, & \\text{if } t < t_{\\text{warmup}} \\\\\n",
        "    \\alpha_f + (1 - \\alpha_f) \\times \\frac{1}{2} \\left[1 + \\cos(\\pi \\times \\tau_w) \\right], & \\text{otherwise}\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "where we denote:\n",
        "\n",
        "-   $t$ represents the **current** training step or epoch.\n",
        "-   $\\eta_{\\max}$ as the **maximum** learning rate reached during training, and\n",
        "    often is the **initial** learning rate given into an optimizer.\n",
        "-   $t_{\\text{warmup}}$ denotes the duration of the warmup period, in terms of\n",
        "    the number of steps or epochs, during which the learning rate **linearly**\n",
        "    increases to the maximum learning rate $\\eta_{\\max}$.\n",
        "-   $t_{\\max}$ as the **maximum** number of training steps, or maximum number of\n",
        "    iterations in an epoch (see\n",
        "    [here](https://github.com/skorch-dev/skorch/issues/610)).\n",
        "-   $\\tau_w = \\frac{t - t_{\\text{warmup}}}{t_{\\max}}$, the fraction of\n",
        "    post-warmup time elapsed,\n",
        "-   $\\alpha_f$ is a _scaling_ factor that determines the **final** learning rate\n",
        "    multiplier to decay to (a value between $0$ and $1$), and this is a _fixed_\n",
        "    value. For example, if $\\alpha_f = 0.1$ and the initial learning rate is\n",
        "    $\\eta_{\\max} = 3e-4$, then the final learning rate will be\n",
        "    $\\eta_{\\min} = 3e-4 \\times 0.1 = 3e-5$.\n",
        "\n",
        "The actual learning rate $\\eta_{t}$ at time (step) $t$ is then computed as:\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "    \\eta_{t} = \\alpha_{t} \\times \\eta_{\\max}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "where we emphasize again that $\\eta_{\\max}$ is the **maximum** learning rate\n",
        "reached during training.\n",
        "```\n",
        "\n",
        "```{admonition} A Word on Oscillations\n",
        ":class: note\n",
        "\n",
        "Note that if you set $t_{\\max}$ to the total number of training steps that is\n",
        "needed for the entire dataset $\\mathcal{S}$, the scheduler _will only decay_ the\n",
        "learning rate after the warmup phase and not oscillate further. This\n",
        "configuration means that after completing the linear increase during the warmup,\n",
        "the learning rate will decrease following a cosine curve until it reaches the\n",
        "final learning rate determined by $\\alpha_f$.\n",
        "\n",
        "-   **Single Cycle (No Oscillation)**: If $t_{\\max}$ is set to cover exactly one\n",
        "    half-cycle of the cosine function from the end of the warmup phase to the\n",
        "    conclusion of training, the learning rate will monotonically decrease from\n",
        "    its maximum value (at the end of warmup) to its minimum value (as determined\n",
        "    by $\\alpha_f$) without oscillating. This is because the scheduler's active\n",
        "    period only spans a single descent phase of the cosine wave.\n",
        "-   **Multiple Cycles (Oscillation)**: If $t_{\\max}$ is set to allow for a\n",
        "    longer duration than what is needed for a single half-cycle descent, the\n",
        "    cosine annealing function can complete its initial descent and then begin to\n",
        "    ascend as part of a new cycle. This leads to oscillations in the learning\n",
        "    rateâ€”after decreasing, it will start to increase again, potentially multiple\n",
        "    times, depending on the total number of cycles fitted within $t_{\\max}$.\n",
        "    This is where the term \"oscillation\" comes into play; it describes the\n",
        "    periodic increase and decrease in the learning rate according to the cosine\n",
        "    function over multiple cycles.\n",
        "\n",
        "True oscillation, where the learning rate decreases and then increases within a\n",
        "training regime, typically requires either a restart mechanism (as seen in\n",
        "[Cosine Annealing with Warm Restarts](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html))\n",
        "or an explicit multi-cycle configuration. A standard cosine annealing scheduler,\n",
        "especially with a warmup phase, generally only supports a monotonic decrease\n",
        "within a single cycle, unless it is specifically designed to handle restarts or\n",
        "multiple cycles.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import math\n",
        "from functools import partial\n",
        "\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "def _get_cosine_schedule_with_warmup_lr_lambda(\n",
        "    current_step: int, *, num_warmup_steps: int, num_training_steps: int, alpha_f: float\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Helper function for calculating the learning rate using cosine annealing\n",
        "    with warmup.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    current_step: int\n",
        "        The current step in the training process.\n",
        "    num_warmup_steps: int\n",
        "        The number of steps for the warmup phase.\n",
        "    num_training_steps: int\n",
        "        The total number of training steps.\n",
        "    alpha_f: float\n",
        "        The minimum learning rate at the end of the schedule.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The calculated learning rate.\n",
        "    \"\"\"\n",
        "\n",
        "    if current_step < num_warmup_steps:\n",
        "        alpha = current_step / max(1, num_warmup_steps)\n",
        "    else:\n",
        "        tau_w = (current_step - num_warmup_steps) / num_training_steps\n",
        "        tau_w = min(1.0, tau_w)\n",
        "        alpha = alpha_f + (1 - alpha_f) * (1 + math.cos(math.pi * tau_w)) / 2\n",
        "    return alpha\n",
        "\n",
        "\n",
        "def get_cosine_annealing_with_warmup(\n",
        "    optimizer: Optimizer,\n",
        "    num_warmup_steps: int,\n",
        "    num_training_steps: int,\n",
        "    alpha_f: float = 0.1,\n",
        "    last_epoch: int = -1,\n",
        "    verbose: bool = False,\n",
        ") -> LambdaLR:\n",
        "    \"\"\"\n",
        "    Create a schedule with a learning rate that decreases following the values\n",
        "    of the cosine function between the initial lr set in the optimizer to 0,\n",
        "    after a warmup period during which it increases linearly between 0 and the\n",
        "    initial lr set in the optimizer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    optimizer: `~torch.optim.Optimizer`\n",
        "        The optimizer for which to schedule the learning rate.\n",
        "    num_warmup_steps: int\n",
        "        The number of steps for the warmup phase.\n",
        "    num_training_steps: int\n",
        "        The total number of training steps.\n",
        "    alpha_f: float\n",
        "        The minimum learning rate at the end of the schedule, by default 0.1.\n",
        "    last_epoch: int\n",
        "        The index of the last epoch when resuming training, by default -1.\n",
        "    verbose: bool\n",
        "        Whether to print the learning rate at every update, by default False.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    `torch.optim.lr_scheduler.LambdaLR`\n",
        "        The scheduler with the appropriate schedule.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from torch import nn\n",
        "    >>> from torch.optim import Adam\n",
        "    >>> dummy_model = nn.Linear(1, 1)\n",
        "    >>> optimizer = Adam(dummy_model.parameters(), lr=3e-4)\n",
        "    >>> scheduler = get_cosine_annealing_with_warmup(optimizer, num_warmup_steps=5, num_training_steps=10, alpha_f=0.5)\n",
        "    >>> assert isinstance(scheduler, LambdaLR)\n",
        "    \"\"\"\n",
        "\n",
        "    lr_lambda = partial(\n",
        "        _get_cosine_schedule_with_warmup_lr_lambda,\n",
        "        num_warmup_steps=num_warmup_steps,\n",
        "        num_training_steps=num_training_steps,\n",
        "        alpha_f=alpha_f,\n",
        "    )\n",
        "    return LambdaLR(optimizer, lr_lambda, last_epoch, verbose)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_warmup_steps = 3 * len(train_loader)\n",
        "num_training_steps = composer.trainer.max_epochs * (len(train_dataset) // composer.data.train_loader[\"batch_size\"])\n",
        "alpha_f = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scheduler = get_cosine_annealing_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps, alpha_f=alpha_f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Criterion\n",
        "\n",
        "The Cross Entropy Loss function calculates the difference between two\n",
        "probability distributions - the predicted probability distribution output by the\n",
        "model (logits) and the actual distribution (target labels). It's primarily used\n",
        "in classification tasks involving $C$ classes.\n",
        "\n",
        "-   $\\mathcal{B}$ : Denotes batch size,\n",
        "-   $K$ : The number of additional dimensions beyond batch and class,\n",
        "    representing spatial or other feature dimensions in the input tensor,\n",
        "-   $N=\\mathcal{B} \\times d_1 \\times \\ldots \\times d_K$ : Total count of\n",
        "    individual elements across all dimensions, including batch and spatial\n",
        "    dimensions. This value adjusts as per the dimensional complexity:\n",
        "-   For $K=0, N=\\mathcal{B}$,\n",
        "-   For $K=1, N=\\mathcal{B} \\times d_1$,\n",
        "-   For $K>1, N$ scales accordingly.\n",
        "-   $C$ : The total number of classification categories,\n",
        "-   $x$ : Represents the input logits tensor,\n",
        "-   $y$ : Denotes the target tensor,\n",
        "-   $w$ : An optional tensor assigning weights to each class,\n",
        "-   $\\mathcal{L}$ : Symbolizes the aggregate loss prior to any reduction,\n",
        "-   $l_b$ : The loss corresponding to the $b$ th element within the batch,\n",
        "    ranging over $b=1$ to $\\mathcal{B}$.\n",
        "\n",
        "#### Inputs and Targets\n",
        "\n",
        "-   **Inputs (Logits)**: The function expects unnormalized logits for each class\n",
        "    per input. These logits do not necessarily need to be positive values nor\n",
        "    sum to 1. The shape of the input tensor can be:\n",
        "\n",
        "    -   For unbatched input: $(C)$,\n",
        "    -   For batched input: $(\\mathcal{B}, C)$,\n",
        "    -   For $K$-dimensional input: $(\\mathcal{B}, C, d_1, d_2, \\ldots, d_K)$,\n",
        "        suitable for tasks like pixel-wise classification in images where\n",
        "        $K \\geq 1$.\n",
        "\n",
        "-   **Targets**: When configuring the targets for the Cross Entropy Loss\n",
        "    function, their expected shapes vary based on the nature of the targets\n",
        "    (class indices vs. probabilities) and the dimensionality of the input:\n",
        "\n",
        "    -   **For Class Indices as Targets**:\n",
        "\n",
        "        -   Unbatched input: The shape should be a scalar representing a single\n",
        "            class index in $[0, C)$.\n",
        "        -   Batched input: The shape should be $(\\mathcal{B},)$, where each\n",
        "            element is a class index for the corresponding input in the batch.\n",
        "        -   $K$-dimensional input: The shape should be\n",
        "            $(\\mathcal{B}, d_1, d_2, \\ldots, d_K)$ for the $K$-dimensional case,\n",
        "            with each element representing a class index for the corresponding\n",
        "            spatial location.\n",
        "\n",
        "    -   **For Probabilities as Targets** (applicable in advanced scenarios like\n",
        "        label smoothing or multi-label classification):\n",
        "        -   The shape of the targets must match the shape of the input logits\n",
        "            tensor: $(\\mathcal{B}, C)$ for batched input or\n",
        "            $(\\mathcal{B}, C,\n",
        "            d_1, d_2, \\ldots, d_K)$ for\n",
        "            $K$-dimensional input. Each element in this tensor should be a\n",
        "            probability corresponding to the likelihood of the class, with\n",
        "            values in $[0, 1]$.\n",
        "\n",
        "#### Loss Computation\n",
        "\n",
        "1. **For Class Indices as Targets**:\n",
        "\n",
        "The loss for each element $n$, accurately spanning across all considered\n",
        "dimensions, is calculated as:\n",
        "\n",
        "$$\n",
        "\\ell(x, y) = \\mathcal{L} = \\{l_1, \\ldots, l_{N}\\}^{\\top}, \\quad l_n = -w_{y_n} \\cdot \\log \\left( \\frac{\\exp(x_{n, y_n})}{\\sum_{c=1}^{C} \\exp(x_{n, c})} \\right) \\cdot \\mathbb{1}\\{y_n \\neq \\text{ignore_index}\\}\n",
        "$$\n",
        "\n",
        "Here, $N$ correctly reflects the aggregate count of elements when considering\n",
        "$\\mathcal{B}$ and the $K$-dimensional context. Consequently, if $K=0$, $N$\n",
        "reduces to $\\mathcal{B}$.\n",
        "\n",
        "2. **For Probabilities as Targets**:\n",
        "\n",
        "In cases where the targets are probabilities, the calculation over each element\n",
        "$n$, aligning with $N$'s definition, should be:\n",
        "\n",
        "$$\n",
        "\\ell(x, y) = \\mathcal{L} = \\{l_1, \\ldots, l_{N}\\}^{\\top}, \\quad l_n = -\\sum_{c=1}^{C} w_c \\cdot y_{n, c} \\cdot \\log \\left( \\frac{\\exp(x_{n, c})}{\\sum_{i=1}^{C} \\exp(x_{n, i})} \\right)\n",
        "$$\n",
        "\n",
        "#### Reduction\n",
        "\n",
        "-   **No Reduction** (`reduction='none'`):\n",
        "\n",
        "    When the reduction is set to 'none', the loss computation preserves the\n",
        "    original dimensionality of the input, effectively returning a tensor that\n",
        "    maps directly to each input element's loss. This tensor has the shape\n",
        "    $(\\mathcal{B}, d_1, \\ldots, d_K)$, where each element $l_{n}$ within this\n",
        "    tensor represents the computed loss for the corresponding input element\n",
        "    across all dimensions, including the batch and any $K$-dimensional space:\n",
        "\n",
        "    $$\n",
        "    \\mathcal{L} = \\{l_1, \\ldots, l_N\\}\n",
        "    $$\n",
        "\n",
        "    This preserves the granularity of loss across the dataset, allowing for\n",
        "    detailed analysis or custom reduction post hoc.\n",
        "\n",
        "-   **Mean Reduction** (`reduction='mean'`):\n",
        "\n",
        "    For the 'mean' reduction, the losses across all elements are averaged to\n",
        "    yield a single scalar value. This operation accounts for the total count of\n",
        "    elements ($N$), including those spanning batch and additional dimensions,\n",
        "    and is not merely an average over the batch size $\\mathcal{B}$, but over all\n",
        "    $N$ elements:\n",
        "\n",
        "    $$\n",
        "    \\mathcal{L}_{mean} = \\frac{1}{N} \\sum_{n=1}^{N} l_n\n",
        "    $$\n",
        "\n",
        "    Here, traditionally we think of $N$ as just the number of elements in the\n",
        "    batch, but in the implementation, it spans all elements across the batch and\n",
        "    $K$-dimensional spaces.\n",
        "\n",
        "-   **Sum Reduction** (`reduction='sum'`):\n",
        "\n",
        "    With 'sum' reduction, the losses for all elements are aggregated into a\n",
        "    single scalar through summation, without averaging. This sums the losses\n",
        "    across all elements, including those across the batch and $K$-dimensional\n",
        "    spaces:\n",
        "\n",
        "    $$\n",
        "    \\mathcal{L}_{sum} = \\sum_{n=1}^{N} l_n\n",
        "    $$\n",
        "\n",
        "    This scalar represents the total loss accumulated across the entire input\n",
        "    set, providing a measure of overall loss magnitude without\n",
        "    normalization by the number of elements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Simple Binary Classification Example\n",
        "\n",
        "-   `nn.CrossEntropyLoss` in PyTorch expects the input logits to be of shape\n",
        "    `[N, C, d1, d2, ..., dK]` (where `N` is the batch size, `C` is the number of\n",
        "    classes, and `d1` to `dK` are optional additional dimensions) and the target\n",
        "    to be of shape `[N, d1, d2, ..., dK]`.\n",
        "-   Let's look a simplified example in image classification. The target is a\n",
        "    single integer representing the class label, and the input logits are a\n",
        "    vector of length `C` (the number of classes).\n",
        "- More concretely, in the below example, we have $\\mathcal{B} = 4$ (batch size),\n",
        "  $C = 2$ (number of classes), $K = 0$ (no additional dimensions), and $N = 4$\n",
        "    (total number of elements across all dimensions).\n",
        "\n",
        "    - Our inputs (logits) are of shape $(\\mathcal{B}, C) = (4, 2)$.\n",
        "    - Our targets are of shape $(\\mathcal{B}) = (4)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rng = torch.Generator().manual_seed(composer.global_.seed)\n",
        "criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "B, C = 4, 2\n",
        "targets = torch.tensor([1, 0, 0, 0]) # indicating sample 1 is class 1 and sample 2 is class 0\n",
        "logits  = torch.tensor([[0.1, 0.9], [0.9, 0.1], [0.8, 0.2], [0.3, 0.7]])\n",
        "assert logits.size() == (B, C)\n",
        "loss   = criterion(logits, targets)\n",
        "pprint(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### GPT Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([256, 10, 18]), torch.Size([256, 10]))"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rng = torch.Generator().manual_seed(composer.global_.seed)\n",
        "input_tokens, targets, _, _ = next(iter(train_loader))\n",
        "model(input_tokens).shape, targets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss: torch.Tensor = self.criterion(logits.permute(0, 2, 1).contiguous(), targets.contiguous())\n",
        "loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpbLlCLRrdzc"
      },
      "outputs": [],
      "source": [
        "# criterion    = nn.CrossEntropyLoss(ignore_index=PAD, reduction=\"mean\")\n",
        "from omnivault.transformer.config.criterion import CRITERION_REGISTRY\n",
        "\n",
        "criterion_config_cls = CRITERION_REGISTRY[cfg.criterion.name]\n",
        "criterion_pydantic_config = criterion_config_cls(**cfg.criterion)\n",
        "\n",
        "criterion = criterion_pydantic_config.create_instance()\n",
        "assert criterion.ignore_index == vocabulary.token_to_index[vocabulary.PAD]\n",
        "\n",
        "pprint(criterion.ignore_index)\n",
        "pprint(criterion.reduction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from omnivault.transformer.core.state import State\n",
        "\n",
        "state = State(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    vocabulary=vocabulary,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "state.pretty_print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pprint(composer.trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "composer.generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    state=state,\n",
        "    composer=composer,\n",
        "    logger=LOGGER,\n",
        "    device=composer.trainer.device,  # type: ignore[arg-type]\n",
        ")\n",
        "trainer.remove_callback(event=TrainerEvent.ON_VALID_EPOCH_END.value, callback=save_state)\n",
        "\n",
        "# trainer.add_callback(\n",
        "#     TrainerEvent.ON_VALID_EPOCH_END.value,\n",
        "#     lambda trainer: evaluate_and_generate_on_valid_epoch_end(trainer, num_batches_to_eval=None),\n",
        "# )\n",
        "# _trained_state = trainer.fit(train_loader=train_loader, valid_loader=valid_loader, test_loader=test_loader)\n",
        "_trained_state.pretty_print()\n",
        "history = _trained_state.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch = next(iter(train_loader))\n",
        "pprint(batch)\n",
        "\n",
        "inputs, targets, target_padding_masks, future_masks = batch\n",
        "\n",
        "\n",
        "# Step 2: Pass the sample through the model\n",
        "trained_model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Assuming your model and sample require specific formatting, adjust as necessary\n",
        "    logits = model(inputs, target_padding_masks=target_padding_masks, future_masks=future_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "last_decoder_block = trained_model.decoder_blocks[-1] # take last decoder block? more feature?\n",
        "# pprint(last_decoder_block)\n",
        "\n",
        "masked_self_attention_mha = last_decoder_block.masked_self_attention_mha\n",
        "pprint(masked_self_attention_mha)\n",
        "\n",
        "context_vector, attention_weights = masked_self_attention_mha.context_vector, masked_self_attention_mha.attention_weights\n",
        "pprint(attention_weights.shape)\n",
        "# but has H=4 heads so do we take 1 head and check the heatmap?\n",
        "# torch.Size([208, 4, 10, 10])\n",
        "\n",
        "last_batch_last_sample_first_head_attention_weights = attention_weights[-1, 0:1, :, :].squeeze(0)\n",
        "pprint(last_batch_last_sample_first_head_attention_weights.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "the xy axis is keys and queries, which is correct `Q @ K.T`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Your existing setup\n",
        "last_decoder_block = trained_model.decoder_blocks[-1]\n",
        "masked_self_attention_mha = last_decoder_block.masked_self_attention_mha\n",
        "context_vector, attention_weights = masked_self_attention_mha.context_vector, masked_self_attention_mha.attention_weights\n",
        "\n",
        "# Number of heads\n",
        "num_heads = attention_weights.size(1)\n",
        "\n",
        "# Labels for each character in the sequence, including BOS\n",
        "labels = ['<BOS>'] + list('59+14=073')\n",
        "\n",
        "# Loop over each head and plot its heatmap\n",
        "for head in range(num_heads):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "\n",
        "    # Extract attention weights for the last sample in the last batch for this head\n",
        "    attention_matrix = attention_weights[-1, head, :, :].detach().numpy()\n",
        "\n",
        "    sns.heatmap(attention_matrix, annot=True, cmap='viridis', xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(f\"Attention Weights Heatmap for '<BOS>59+14=073' - Head {head+1}\")\n",
        "    plt.xlabel(\"Keys\")\n",
        "    plt.ylabel(\"Queries\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "x -> tensor([[15,  9,  8, 10,  3,  5, 13]])\n",
        "future_mask -> 7x7\n",
        "tensor([[ True, False, False, False, False, False, False],\n",
        "â”‚   â”‚   [ True,  True, False, False, False, False, False],\n",
        "â”‚   â”‚   [ True,  True,  True, False, False, False, False],\n",
        "â”‚   â”‚   [ True,  True,  True,  True, False, False, False],\n",
        "â”‚   â”‚   [ True,  True,  True,  True,  True, False, False],\n",
        "â”‚   â”‚   [ True,  True,  True,  True,  True,  True, False],\n",
        "â”‚   â”‚   [ True,  True,  True,  True,  True,  True,  True]])\n",
        "\n",
        "logits--> 1x7x18 because 1 sample\n",
        "tensor([[[  7.8,  -0.2,  -2.3,  -1.1,  -0.1,  -3.2,  -4.4,\n",
        "          -2.4,   3.7,  -0.9,  -5.1,  -4.5,  -5.6,  -2.2,\n",
        "          -0.5,  -4.2,  -2.9,  -4.9],\n",
        "        [  0.3,   3.7,   0.9,   1.7,   0.4,  -4.0,  -6.0,\n",
        "          -2.3,   8.5,   7.3,  -6.0,  -5.1,  -6.2,  -3.0,\n",
        "         -10.9,  -3.8,  -5.3,  -5.9],\n",
        "        [-10.5,  -0.4,   4.3,   2.4,  -6.3,  -8.9,  -0.1,\n",
        "           8.2,   8.6,   0.4,   1.2,   0.9,   0.7,   0.6,\n",
        "           6.9,   0.0,   0.4,   1.2],\n",
        "        [ -2.8,   9.6,   2.0,  -6.2,  -8.2,  -2.3,   5.7,\n",
        "           6.6,  -0.3,  -4.7,  -0.5,  -0.9,  -0.9,   1.2,\n",
        "           2.3,  -0.4,   0.1,  -1.5],\n",
        "        [ -2.9,   1.6,  -1.0,  -5.8,  -0.2,   6.2,  14.1,\n",
        "           8.0,  -4.0,  -9.7,  -2.1,  -3.4,  -3.2,  -1.4,\n",
        "           0.0,  -1.7,   0.0,  -3.0],\n",
        "        [ -9.4,   1.7,   5.4,  -1.3,  -6.6,  -4.7,   6.7,\n",
        "          10.2,   1.9,  -9.6,   0.8,   0.6,   0.7,   1.2,\n",
        "          10.2,   0.4,   1.3,   1.2],\n",
        "        [  0.3,  16.1,   3.2,  -4.4,  -5.7,  -2.9,  -3.7,\n",
        "          -6.1,  -2.1,   4.0,  -0.4,   0.1,  -0.4,   0.0,\n",
        "           0.6,  -0.6,  -1.2,  -0.7]]])\n",
        "\n",
        "logits.argmax(dim=-1) -> 1x7\n",
        "tensor([[0,  8,  8,  1,  6, 14,  1]])\n",
        "```\n",
        "\n",
        "`logits.argmax(dim=-1)` basically compress 1x7x18 to 1x7 where for each row of the\n",
        "7 rows, find the index that is maximum for example, first row 7.8 is max of all\n",
        "18 elements, so index 0 is returned. `tensor([[0,  8,  8,  1,  6, 14,  1]])`\n",
        "\n",
        "There is some meaning here too, remember our input `[15, 9, 8, 10, 3, 5, 13]`\n",
        "this is basically the BOS (15) up till the equal sign, then\n",
        "`[ 0, 8, 8, 1, 6, 14, 1]` is basically the prediction of each token what comes\n",
        "next.\n",
        "\n",
        "1. **Input Sequence**: Your input sequence is `[15, 9, 8, 10, 3, 5, 13]`. In\n",
        "   this context, `15` could be a special token like BOS (Beginning of Sentence)\n",
        "   or something else depending on your encoding scheme.\n",
        "\n",
        "2. **Output Tensor Interpretation**: The output tensor\n",
        "   `tensor([[ 0, 8, 8, 1, 6, 14, 1]])` represents the model's sequential\n",
        "   predictions for each step of the input:\n",
        "\n",
        "   - The first element `0` is the prediction following the first element `15` of\n",
        "     the input.\n",
        "   - The second element `8` is the prediction after seeing the first two\n",
        "     elements `15, 9` of the input.\n",
        "   - The third element `8` is predicted after seeing `15, 9, 8`.\n",
        "   - The fourth element `1` follows after `15, 9, 8, 10`.\n",
        "   - The sequence continues in this manner, with each new prediction based on an\n",
        "     increasingly longer prefix of the input sequence.\n",
        "\n",
        "3. **Sequential Predictions**: This output suggests that the model is working in\n",
        "   an autoregressive manner. It generates predictions one token at a time, and\n",
        "   each prediction is based on the sequence of tokens it has seen up to that\n",
        "   point.\n",
        "\n",
        "4. **Specific Meanings of Output Tokens**: The actual meaning of each token in\n",
        "   your output tensor (`0`, `8`, `1`, `6`, `14`, etc.) depends on your specific\n",
        "   encoding and task. In a language model, these would correspond to specific\n",
        "   words or characters. In a numerical context, they could represent numbers or\n",
        "   operations.\n",
        "\n",
        "In summary, the output tensor reflects the model's predictions for what comes\n",
        "next in the sequence, based on the current and all previous input tokens. Each\n",
        "element in the output is the model's guess for the next token, considering the\n",
        "sequence of tokens it has seen up to that point.\n",
        "\n",
        "> Then we move on to the concat operation:\n",
        "\n",
        "\n",
        "- In our model, after processing the input `[15, 9, 8, 10, 3, 5, 13]`, it\n",
        "  predicts the next token to be `1`. This prediction is based on the entire\n",
        "  sequence seen so far.\n",
        "\n",
        "- The process of extending the input sequence with this new token (`1`) and then\n",
        "  feeding this extended sequence back into the model for further predictions is\n",
        "  indeed an example of greedy decoding. The model is iteratively building a\n",
        "  longer sequence, one token at a time, always choosing the most likely next\n",
        "  token at each step.\n",
        "\n",
        "- This process would continue until a stopping condition is met, which might be\n",
        "  the prediction of an EOS (End of Sentence) token or reaching a maximum\n",
        "  sequence length.\n",
        "\n",
        "\n",
        "> for i in range(num_digits + 2):\n",
        "> now you know why loop over 4 times in total if num digits is 2.\n",
        "> This is because, after equal sign, we will have answer of 3 digits (xyz)\n",
        "> and an EOS token, our stop condition!\n",
        "\n",
        "Lastly: `tensor([[15,  9,  8, 10,  3,  5, 13,  1,  3,  3, 14]])` is the full predicted\n",
        "after EOS is met. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config.global_config.seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def construct_future_mask(seq_len: int) -> torch.BoolTensor:\n",
        "    future_mask = torch.triu(torch.ones((seq_len, seq_len), dtype=torch.bool), diagonal=1).to(torch.bool)\n",
        "    future_mask = future_mask.contiguous()\n",
        "    future_mask = future_mask == 0\n",
        "    return torch.BoolTensor(future_mask)\n",
        "\n",
        "def construct_padding_mask(input_sequence: torch.Tensor, pad_token_id: int) -> torch.BoolTensor:\n",
        "    padding_mask = input_sequence != pad_token_id\n",
        "    return torch.BoolTensor(padding_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def compute_sum(model, x) -> List[int]:\n",
        "    \"Function for computing the sum of two numbers.\"\n",
        "    # x=[[15,  9,  8, 10,  3,  5, 13]]\n",
        "    for _ in range(num_digits + 2):\n",
        "        # pprint(x)\n",
        "        pad_mask = (x != PAD).view(1, 1, 1, x.size(-1)).to(DEVICE)\n",
        "        future_mask = construct_future_mask(seq_len=x.size(1))\n",
        "        batch_size, seq_len = x.size()\n",
        "        future_mask = future_mask.view(1, seq_len, seq_len).expand(size=(batch_size, -1, -1)).unsqueeze(1)\n",
        "        #print(pad_mask.shape, future_mask.shape)\n",
        "        #inputs, targets, target_padding_masks, future_masks = construct_batches(x)\n",
        "        #print(target_padding_masks.shape, future_masks.shape)\n",
        "        logits = model(input_tokens=x, target_padding_masks=pad_mask, future_masks=future_mask)\n",
        "        pprint(logits.shape)\n",
        "        time.sleep(100)\n",
        "        #logits = model(inputs, target_padding_masks=target_padding_masks, future_masks=future_masks)\n",
        "\n",
        "        last_output = logits.argmax(-1)[:, -1].view(1, 1)\n",
        "        x = torch.cat((x, last_output), 1).to(DEVICE)\n",
        "        # STOPPING CONDITION!\n",
        "        if last_output.item() == EOS:\n",
        "            break\n",
        "        #return\n",
        "    return x[0]\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, num_batch=None):\n",
        "    \"\"\"\n",
        "    Function for evaluation the model.\n",
        "\n",
        "    This function take equations, and truncate them up to the equal-sign, and feed\n",
        "    them to the model to get the predictions, compare them with the correct answers,\n",
        "    and output the accuracy.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    acc, count = 0, 0\n",
        "    num_wrong_to_display = 5\n",
        "    for idx, batch in enumerate(dataloader):\n",
        "        (\n",
        "            inputs,\n",
        "            targets,\n",
        "            target_padding_masks,\n",
        "            future_masks,\n",
        "        ) = batch  # construct_batches(batch)\n",
        "        for equation in inputs:\n",
        "            # pprint(equation)\n",
        "            # add EOS behind equation\n",
        "            equation = torch.cat((equation, torch.tensor([EOS])), 0) # TODO: PLEASE DO NOT DO THIS - DO NOT MODIFY LIKE THIS.\n",
        "            # fmt: off\n",
        "            loc_equal_sign = equation.tolist().index(EQUAL)\n",
        "            loc_EOS        = equation.tolist().index(EOS)\n",
        "            input          = equation[0 : loc_equal_sign + 1].view(1, -1).to(DEVICE)\n",
        "            ans            = equation[: loc_EOS + 1].tolist()\n",
        "            ans_pred       = compute_sum(model, input)\n",
        "            count += 1\n",
        "            # fmt: on\n",
        "\n",
        "            if ans == ans_pred.tolist():\n",
        "                acc += 1\n",
        "            else:\n",
        "                if num_wrong_to_display > 0:\n",
        "                    print(\n",
        "                        f'correct equation: {decode_equation(vocab=vocab, equation=equation).replace(\"<PAD>\",\"\")}'\n",
        "                    )\n",
        "                    print(f\"wrongly predicted as:        {decode_equation(vocab=vocab, equation=ans_pred)}\")\n",
        "                    num_wrong_to_display -= 1\n",
        "        if num_batch and idx > num_batch:\n",
        "            break\n",
        "    return acc / count\n",
        "\n",
        "\n",
        "def what_is(question: str) -> str:\n",
        "    \"function for computing the sum of two numbers with input in literal string format\"\n",
        "    pred = compute_sum(model, encode_equation(question, num_digits).view(1, -1))\n",
        "    pred = decode_equation(pred)\n",
        "    pred = pred[pred.index(\"=\") + 1 :]\n",
        "    return question + pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The provided code implements a form of greedy decoding for sequence generation.\n",
        "Let's break down how it aligns with the principles of greedy decoding:\n",
        "\n",
        "1. **Greedy Decoding Principle**: Greedy decoding in sequence generation models\n",
        "   involves choosing the most probable next token at each step of the sequence\n",
        "   generation. This is done iteratively until a stopping condition is met (like\n",
        "   reaching an EOS token or a maximum length).\n",
        "\n",
        "2. **Implementation in Your Code**:\n",
        "\n",
        "   - The `compute_sum` function generates a sequence by repeatedly predicting\n",
        "     the next token and appending it to the input.\n",
        "   - For each iteration in `compute_sum`:\n",
        "     - The model (`model(x, pad_mask, future_mask)`) generates logits for the\n",
        "       next token based on the current sequence (`x`).\n",
        "     - `last_output = logits.argmax(-1)[:, -1].view(1, 1)` picks the most\n",
        "       probable next token (the token with the highest logit value) from the\n",
        "       logits. This is the essence of greedy decoding.\n",
        "     - This token is then appended to the sequence:\n",
        "       `x = torch.cat((x, last_output), 1)`.\n",
        "   - The process continues until the model generates an EOS token, as indicated\n",
        "     by `if last_output.item() == EOS: break`.\n",
        "\n",
        "3. **Evaluation Function**:\n",
        "\n",
        "   - The `evaluate` function further confirms this approach by feeding truncated\n",
        "     sequences (up to the equal sign) from the dataloader to the `compute_sum`\n",
        "     function and comparing the model's predictions to the correct answers.\n",
        "\n",
        "4. **Characteristics of Greedy Decoding**:\n",
        "   - Greedy decoding is computationally efficient and straightforward but may\n",
        "     not always produce the best possible sequence. It does not reconsider past\n",
        "     decisions; it always picks the most likely next token at each step without\n",
        "     considering the global context of the sequence.\n",
        "\n",
        "In summary, the provided code, especially the `compute_sum` function, implements\n",
        "a typical greedy decoding approach. It iteratively generates a sequence by\n",
        "choosing the most probable next token at each step, which is characteristic of\n",
        "greedy decoding in sequence generation tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ujaa9mSNrdze"
      },
      "outputs": [],
      "source": [
        "print('training set examples the model gives an incorrect result:')\n",
        "# rng = torch.Generator().manual_seed(config.global_config.seed)\n",
        "seed_all(1992, seed_torch=True)\n",
        "\n",
        "train_acc = evaluate(model, train_loader, 2)\n",
        "pprint(train_acc) #\n",
        "# print('validataion set examples the model gives an incorrect result:')\n",
        "val_acc = evaluate(model, valid_loader)\n",
        "pprint(val_acc)\n",
        "# print('test set examples the model gives an incorrect result:')\n",
        "test_acc = evaluate(model, test_loader)\n",
        "pprint(test_acc)\n",
        "# result = f'''train_size: {train_size}, test_acc: {test_acc}, val_acc: {val_acc}, train_acc: {train_acc}\n",
        "#                 '''\n",
        "# print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "QUESTION:\n",
        "\n",
        "another not so smart question of the day: For an input sequence x1,x2,...,x_L, when it forward pass all the way through the decoder model, up till before the pre-logits/head/linear layer, and assuming for simplicity that we squeeze out the first batch dimension (only 1 sample), the the shape of the pre-logits is [L, D] where L is seq len and D the hidden embedding dimension. Am I right to say that the last row of [L, D] being the last token's representation, holds info of the full context of all previous tokens.\n",
        "\n",
        "1. This means the last token in the input sequence (the last row in [L, D]) is a function of all previous tokens, so it is not surprising why the tutorial will just use the last row/token's corresponding prediction as the next predicted token/word, given all previous tokens.\n",
        "\n",
        "> Important to know the last token or last row of [L, D] is actually a function of all previous tokens, here it is unmasked already.\n",
        "> So if confused, just remember the pre logits last row, corresponding to the last token in the input sequence, is a function of all previous tokens.\n",
        "> It just means that row holds all information, context, of all previous tokens so we can say its conditioned on all previous tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFIW_XfMzKJe"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASUDASA_zKJe"
      },
      "outputs": [],
      "source": [
        "# fmt: off\n",
        "rng        = torch.Generator().manual_seed(config.global_config.seed)\n",
        "\n",
        "B, L, V    = 2, 3, 4                                                   # Assuming we have B = batch size, L = sequence length, V = vocab size\n",
        "\n",
        "logits     = torch.randn(B, L, V, generator=rng)                       # logits from the head\n",
        "targets    = torch.randint(low=0, high=V, size=(B, L), generator=rng)  # targets are the labels\n",
        "# fmt: on\n",
        "\n",
        "pprint(logits)\n",
        "pprint(targets)\n",
        "pprint(logits[0]) # logits for the first sequence [L=10, V=18]\n",
        "pprint(targets[0]) # target for the first sequence [L=10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTPmANxxzKJe"
      },
      "source": [
        "We establish some conceptual understanding first:\n",
        "\n",
        "- Each sample in the batch has the following characteristics:\n",
        "    - Denote `target` and `logit` as the target and logits for a particular sample in the batch.\n",
        "    - The `target` is of shape `[L] = [3]` and each element is the class/vocab label for each token in the sequence.\n",
        "    - The `logit` is of shape `[L, V] = [3, 4]` and each row is the logits for each token in the sequence.\n",
        "    - Therefore, we want to compare each row in `logit` with each element in `target` to compute the loss.\n",
        "    - We can think of each row in `logit` as the prediction for each token in the sequence, and each element in `target` as the ground truth for each token in the sequence.\n",
        "    - Intuitively this means that within each sample, there are many \"sub-samples\" where each sub-sample is a token in the sequence. If you can visualize this, then there should be no confusion.\n",
        "- In code, we can do so with the following manner:\n",
        "    - Calculate loss for each token in each sample individually and then sum them up.\n",
        "    - Reduction by mean will mean we need to divide our `total_loss` by the total number\n",
        "        of samples in the batch. But remember that even though technically we have\n",
        "        2 samples in the batch, we are actually treating each token in each sample\n",
        "        as a sub-sample, so the total samples is `B * L` where `B` is the batch size\n",
        "        and `L` is the sequence length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHWXzJ59zKJe"
      },
      "outputs": [],
      "source": [
        "criterion  = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "total_loss = 0\n",
        "for b in range(B):\n",
        "    for l in range(L):\n",
        "        logit      = logits[b, l].unsqueeze(0)\n",
        "        target     = targets[b, l].unsqueeze(0)\n",
        "        total_loss += criterion(logit, target)\n",
        "\n",
        "pprint(total_loss)\n",
        "total_loss  = total_loss / (B * L)\n",
        "pprint(total_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhQrjELfzKJf"
      },
      "source": [
        "In PyTorch however, if you have a logits tensor of shape `[B, S, V]`, you need to permute it to\n",
        "  `[B, V, S]` to align with the format that `CrossEntropyLoss` wants, so that `V` (vocab size) is\n",
        "  treated as `C` (number of classes), and `S` (sequence length) is treated as\n",
        "  one of the additional dimensions `d1, d2, ..., dK`.\n",
        "\n",
        "But all in all, if you understood the previous loop to calculate the loss for each token in each sample individually and then sum them up, then dividing to fulfill reduction of mean, then you should be fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lwkq0QDOzKJf"
      },
      "outputs": [],
      "source": [
        "# Permute logits to shape [B, V, S]\n",
        "logits_permuted = logits.permute(0, 2, 1)\n",
        "\n",
        "# Instantiate the CrossEntropyLoss\n",
        "# By default, it reduces by averaging the losses over each observation in the input\n",
        "criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "loss = criterion(logits_permuted, targets)\n",
        "pprint(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Masking and Ignore Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# fmt: off\n",
        "rng        = torch.Generator().manual_seed(config.global_config.seed)\n",
        "\n",
        "B, L, V    = 2, 3, 4                                                   # Assuming we have B = batch size, L = sequence length, V = vocab size\n",
        "\n",
        "logits     = torch.randn(B, L, V, generator=rng)                       # logits from the head\n",
        "targets    = torch.randint(low=0, high=V, size=(B, L), generator=rng)  # targets are the labels\n",
        "# fmt: on\n",
        "\n",
        "pprint(logits)\n",
        "pprint(targets)\n",
        "pprint(logits[0]) # logits for the first sequence [L=10, V=18]\n",
        "pprint(targets[0]) # target for the first sequence [L=10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "targets[:, 0] = -123"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PAD_ = -123"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion  = nn.CrossEntropyLoss(reduction=\"mean\", ignore_index=PAD_)\n",
        "\n",
        "NON_IGNORE_COUNT = 0\n",
        "\n",
        "total_loss = 0\n",
        "for b in range(B):\n",
        "    for l in range(L):\n",
        "        logit      = logits[b, l].unsqueeze(0)\n",
        "        target     = targets[b, l].unsqueeze(0)\n",
        "        if target == torch.tensor([PAD_]):\n",
        "            continue\n",
        "        total_loss += criterion(logit, target)\n",
        "        NON_IGNORE_COUNT += 1\n",
        "\n",
        "pprint(total_loss)\n",
        "total_loss  = total_loss / NON_IGNORE_COUNT\n",
        "pprint(total_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE: `NON_IGNORE_COUNT` is used instead of `BxL`, why? Cause we are averaging over\n",
        "all non-ignored guys!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Permute logits to shape [B, V, S]\n",
        "logits_permuted = logits.permute(0, 2, 1)\n",
        "\n",
        "# Instantiate the CrossEntropyLoss\n",
        "# By default, it reduces by averaging the losses over each observation in the input\n",
        "criterion  = nn.CrossEntropyLoss(reduction=\"mean\", ignore_index=PAD_)\n",
        "\n",
        "loss = criterion(logits_permuted, targets)\n",
        "pprint(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_12_'></a>[Training with GPT-like Model](#toc0_)\n",
        "\n",
        "If you're working with a GPT-like model, which is a decoder-only architecture, the training mechanics differ slightly compared to the encoder-decoder models like seq2seq. In a GPT-style model, the entire sequence (input and output) is provided to the model at once, and each token is predicted based on the tokens that came before it. The model is still autoregressive, but there's no separate encoder to produce an intermediate representation; the \"encoding\" is effectively built into the ongoing autoregressive decoding process.\n",
        "\n",
        "In your case, if the equations are like `90+38=128`, during training you'd provide `90+38=` as the input and then use the remaining part `128` as the expected output, potentially along with special tokens to demarcate sequence boundaries or to flag the equation/result parts. However, unlike an encoder-decoder model where the decoder gets to \"peek\" at the correct output during training (also known as \"teacher forcing\"), here every token in the output is predicted one by one, based solely on the preceding tokens.\n",
        "\n",
        "In such a setup, you can definitely feed the entire equation to the model and try to predict each subsequent token based on the preceding tokens. For example, given `90+38=`, the model should predict `1`, `2`, `8` in succession.\n",
        "\n",
        "### <a id='toc1_12_1_'></a>[Loss Computation](#toc0_)\n",
        "\n",
        "For training a GPT-like model, you'd usually use a standard loss function like cross-entropy loss for each token's prediction. You'd compare the token predicted by the model to the actual token in the target sequence to compute the loss. This is calculated for each token and then averaged over the sequence or batch, depending on your implementation.\n",
        "\n",
        "### <a id='toc1_12_2_'></a>[Example](#toc0_)\n",
        "\n",
        "In a GPT-like model, each token in the sequence is used to predict the next token. The model takes a sequence of tokens and produces a new sequence of the same length where each new token is predicted based on all the preceding tokens in the input sequence. The loss is then computed between the predicted sequence and the target sequence.\n",
        "\n",
        "Let's take a closer look at an example:\n",
        "\n",
        "- The original tensor: `[15, 9, 0, 10, 3, 8, 13, 1, 2, 8, 14]` which corresponds to `<SOS>90+38=128<EOS>`\n",
        "- Input tensor:  `[15, 9,  0,  10, 3,  8,  13, 1,  2, 8]`, which corresponds to `<SOS>90+38=128` without `EOS`\n",
        "- Target tensor:     `[9,  0,  10, 3,  8,  13, 1,  2,  8, 14]`\n",
        "                     `[16, 16, 16, 16, 16, 16, 1,  2,  8, 14]`\n",
        "\n",
        "During training:\n",
        "\n",
        "1. **First Timestep**: The model takes `[15]` (or `[<BOS>]` if 15 is your BOS token) and tries to predict the next token. Ideally, it should predict `9`. But here, your target sequence starts with masked tokens (`16`, if 16 is your masking token). So the loss is computed between the predicted token and the masked token `16`. But since `CrossEntropyLoss` has an `ignore_index` (now you know what they are right!), you can set it to say `16` or (default `-1` but you would need to change padding number) and tell the model that whenever the ground truth is `16`, the loss\n",
        "is zeroed out so it is not counted? This allows the model to focus on learning from the relevant parts of the sequence while ignoring the masked portions.\n",
        "\n",
        "2. **Second Timestep**: The model takes `[15, 9]` and predicts the next token, which should be `0`. Again, the target is a masked token `16`.\n",
        "\n",
        "3. **...**\n",
        "\n",
        "4. **Eighth Timestep**: The model takes `[15, 9,  0,  10, 3,  8,  13]` (which is `90+38=`) and predicts the next token. Now the target is `1`, so the loss is computed between the predicted token and `1`. There is no mask anymore here, so the loss will be computed.\n",
        "5. **Ninth Timestep**: The model takes `[15, 9,  0,  10, 3,  8,  13, 1]` (which is `90+38=1`) and predicts the next token. Now the target is `2`, so the loss is computed between the predicted token and `2`.\n",
        "   1. Here's an important thing for beginners (me), In a typical GPT-like architecture used for sequence-to-sequence tasks like this one, the model doesn't use its own predictions as input during training. Instead, it uses the original, ground-truth input sequence. This is known as \"teacher forcing.\" In teacher forcing, even if the model predicts a wrong token at some timestep, it doesn't affect the input sequence for subsequent timesteps. The model continues to get the original input sequence for the entire training epoch.\n",
        "   2. So if model predicts a `3` during the eighth timestep, where the ground trut is `1`, the model would simply incur a higher loss for that prediction. However, the input for the ninth timestep would still be the ground truth sequence up to that point, regardless of what the model predicted at the eighth timestep.\n",
        "   3. But it is noted that this behaviour is still autoregressive.\n",
        "6. **Tenth**: The model takes `[15, 9,  0,  10, 3,  8,  13, 1, 2]` and predicts the next token which is `8`.\n",
        "7. **Last**: The model takes `[15, 9,  0,  10, 3,  8,  13, 1, 2, 8]` and predicts the next token which is `14` the `EOS`.\n",
        "   1. The reason you need to predict `EOS` is simple intuitively, consider the case where there's no need for `EOS`, then the model will not know when to stop.\n",
        "\n",
        "This goes on until the entire sequence is processed. Note that the model never actually \"sees\" the target tokens during the prediction. It is solely relying on the tokens that came before the current token in the input sequence. After the model makes its prediction, then the predicted tokens are compared to the target tokens to compute the loss, which is then backpropagated to update the model weights.\n",
        "\n",
        "### <a id='toc1_12_3_'></a>[Confusion: Training versus Inference](#toc0_)\n",
        "\n",
        "The statement \"it generates one token at a time and uses its own previously generated tokens as context for generating subsequent tokens\" is generally true for GPT-like models during the inference stage, not during training. During inference (or generation), the model does indeed use its own previously generated tokens to produce the next token, since there is no ground truth sequence to rely on. In that case, if the model makes an incorrect prediction at a certain timestep, that incorrect token is used as part of the context for the following timestep.\n",
        "\n",
        "During training, however, the model typically uses the ground truth tokens for the preceding sequence as context for predicting each next token, as described in your example. This resembles teacher forcing, in that the ground truth, rather than the model's own predictions, is used to guide training.\n",
        "\n",
        "So there's no contradiction, but the behavior is context-dependent:\n",
        "\n",
        "- During training, the ground truth sequence is used for context.\n",
        "- During inference, the model's own previously generated tokens are used for context.\n",
        "\n",
        "Both approaches are consistent with the autoregressive nature of the model: in both cases, the token at each position is generated based on the tokens at all previous positions. The difference lies in whether those preceding tokens come from the ground truth (during training) or from the model's own previous outputs (during inference).\n",
        "\n",
        "### <a id='toc1_12_4_'></a>[Training vs Inference](#toc0_)\n",
        "\n",
        "In an autoregressive model like a Transformer decoder, the concept of \"learning\n",
        "the representation of the sequence as it goes\" does not refer to the model\n",
        "processing one token at a time during actual forward passes. Instead, it refers\n",
        "to the model's ability to generate or predict one token at a time during\n",
        "inference, while training on a full sequence in a batched manner.\n",
        "\n",
        "During training:\n",
        "\n",
        "- All tokens are processed in parallel for efficiency. This is possible because\n",
        "  the entire sequence is known beforehand (it's the training data).\n",
        "- The \"autoregressive\" property is enforced by using masks in the self-attention\n",
        "  mechanism. This masking ensures that the prediction for each token can only\n",
        "  depend on previously generated tokens, not on future tokens which the model\n",
        "  has no access to during inference. This is how the model learns the\n",
        "  conditional probability distribution of each token given the previous tokens,\n",
        "  despite the parallel processing of tokens.\n",
        "\n",
        "During inference:\n",
        "\n",
        "- The model starts with an initial token (such as a start-of-sequence token) and\n",
        "  generates the next token based on this single input.\n",
        "- Then, the model uses both the initial token and the newly generated token to\n",
        "  predict the third token, and so on.\n",
        "- This process is sequential and each new token is predicted based on the\n",
        "  previously generated tokens, creating a sequence one token at a time.\n",
        "\n",
        "So, when we say that the model learns the representation of the sequence as it\n",
        "goes, we mean that the model is trained to handle sequences in such a way that\n",
        "it can generate them one piece at a time, respecting the causal order inherent\n",
        "to the task (e.g., language modeling). The parallel processing during training\n",
        "does not contradict the autoregressive nature of the model; it is simply a\n",
        "computational efficiency that is enabled by knowing the full sequence in\n",
        "advance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Questions\n",
        "\n",
        "### Why Masked == 0 in some?\n",
        "\n",
        "The use of `mask == 0` in the `masked_fill` operation is a result of how the\n",
        "mask is constructed. Essentially, different implementations may represent masks\n",
        "differently:\n",
        "\n",
        "1. **Boolean Masking with True/False**: In some implementations, the mask might\n",
        "   be a Boolean tensor where `True` denotes the positions to mask (set to\n",
        "   negative infinity) and `False` for the positions to keep. In such cases, you\n",
        "   can directly use the mask in `masked_fill` as in your provided code:\n",
        "\n",
        "    ```python\n",
        "    attention_scores = attention_scores.masked_fill(mask, float(\"-inf\"))\n",
        "    ```\n",
        "\n",
        "    Here, if `mask[i][j]` is `True`, `attention_scores[i][j]` would be set to\n",
        "    `-inf`.\n",
        "\n",
        "2. **Integer Masking with 1/0**: In other implementations, the mask might be an\n",
        "   integer tensor where `1` denotes the positions to keep and `0` denotes the\n",
        "   positions to mask. In such cases, you'll often find the mask is inverted\n",
        "   (`mask == 0`) before using `masked_fill`:\n",
        "\n",
        "    ```python\n",
        "    attention_scores = attention_scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "    ```\n",
        "\n",
        "    Here, if `mask[i][j]` is `0`, `attention_scores[i][j]` would be set to\n",
        "    `-inf`.\n",
        "\n",
        "The core functionalityâ€”masking certain positions in the attention scoresâ€”is the\n",
        "same in both cases. The difference lies in how the mask tensor is constructed\n",
        "and interpreted. So, if you find an implementation using `mask == 0`, it's\n",
        "likely using an integer mask where `0` signifies positions to mask, whereas if\n",
        "it's directly using `mask`, it's probably a Boolean mask where `True` signifies\n",
        "positions to mask.\n",
        "\n",
        "### what is the reason of setting the attention scores's mask indexes to negative infinity\n",
        "\n",
        "In the attention mechanism, particularly in the Scaled Dot-Product Attention,\n",
        "attention scores are computed for each query-key pair and then passed through a\n",
        "softmax function to obtain attention weights. These weights are used to take a\n",
        "weighted sum of the value vectors, resulting in the final output or the context\n",
        "vectors. The purpose of the mask is to prevent certain tokens (like padding\n",
        "tokens) from being attended to.\n",
        "\n",
        "The reason for setting masked attention scores to negative infinity (`-inf`)\n",
        "lies in the properties of the softmax function:\n",
        "\n",
        "1. **Softmax Behavior**: The softmax function transforms its input (the\n",
        "   attention scores in this case) into a probability distribution.\n",
        "   Mathematically, the softmax function for a given vector $x$ is defined as:\n",
        "\n",
        "$$\n",
        "\\text{Softmax}(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^{N} e^{x_j}}\n",
        "$$\n",
        "\n",
        "2. **Impact of Negative Infinity**: When you pass negative infinity through the\n",
        "   softmax function, $e^{-\\infty}$ approaches zero. As a result, the masked\n",
        "   positions get a near-zero weight in the attention mechanism.\n",
        "\n",
        "$$\n",
        "\\text{Softmax}(-\\infty) = \\frac{e^{-\\infty}}{\\sum_{j=1}^{N} e^{x_j}} \\approx 0\n",
        "$$\n",
        "\n",
        "3. **Avoiding Unwanted Attention**: The point of setting these specific\n",
        "   positions to `-inf` is to ensure that when softmax is applied, these\n",
        "   positions get zero attention weights. This is a way of making sure that the\n",
        "   model does not attend to the positions we've masked (like padding tokens or\n",
        "   future tokens in the sequence, depending on the mask).\n",
        "\n",
        "In summary, setting the masked attention scores to `-inf` and then passing them\n",
        "through a softmax effectively nullifies the contribution of the masked positions\n",
        "in the resulting attention-weighted sum of the value vectors. This is a commonly\n",
        "used trick to impose a certain structure (like masking out future information in\n",
        "the decoder) or to handle variable-length sequences with padding.\n",
        "\n",
        "### <a id='toc1_13_3_'></a>[Why do we need both ignore index in Loss and also negative infinity mask](#toc0_)\n",
        "\n",
        "Using an \"ignore index\" in the `CrossEntropyLoss` function in PyTorch can ignore\n",
        "the effect of certain tokens (like padding tokens) during the loss computation.\n",
        "However, the purpose of the mask in the attention mechanism and the \"ignore\n",
        "index\" in the loss function serve different roles in the model, and they operate\n",
        "at different stages of the computational graph.\n",
        "\n",
        "1. **Ignore Index in Loss Function**: The \"ignore index\" in the loss function\n",
        "   ensures that the model's output at certain positions (typically corresponding\n",
        "   to padding tokens) does not contribute to the loss. This happens at the very\n",
        "   end of the forward pass, just before backpropagation begins.\n",
        "\n",
        "2. **Mask in Attention Mechanism**: The mask in the attention mechanism, on the\n",
        "   other hand, operates during the forward pass at the time when attention\n",
        "   scores are computed. This is a more \"internal\" operation and ensures that\n",
        "   certain positions do not contribute to the output at all, not just during the\n",
        "   loss computation but actually in the intermediate representations (i.e.,\n",
        "   context vectors) that the model computes.\n",
        "\n",
        "To put it another way, even if you're ignoring certain tokens in your loss\n",
        "calculation, those tokens can still influence the model's output unless they're\n",
        "masked out in the attention mechanism itself.\n",
        "\n",
        "For example, consider a decoder in a sequence-to-sequence model:\n",
        "\n",
        "-   If you don't use a mask in the attention mechanism, future tokens could\n",
        "    influence the output at the current timestep, which is not desirable.\n",
        "-   Even if you use an \"ignore index\" in your loss function, it doesn't prevent\n",
        "    the model from \"cheating\" by peeking at the future tokens if they are not\n",
        "    masked in the attention mechanism.\n",
        "\n",
        "So in summary, using an \"ignore index\" in `CrossEntropyLoss` is not a\n",
        "replacement for using attention masks. Both have specific roles in the model,\n",
        "and they are often used together to ensure both that the model attends to the\n",
        "right tokens and that it is trained properly.\n",
        "\n",
        "### <a id='toc1_13_4_'></a>[Target and Preds/Logits Shape](#toc0_)\n",
        "\n",
        "The target tensor for the cross-entropy loss function should typically have a\n",
        "shape of `[batch_size, sequence_length]` where each entry in the tensor is an\n",
        "integer representing the index of the true class (i.e., the actual word/token\n",
        "from the vocabulary) for that position in the sequence. Here `batch_size` refers\n",
        "to the number of sequences in each batch, and `sequence_length` is the length of\n",
        "each sequence.\n",
        "\n",
        "Let's break it down step-by-step:\n",
        "\n",
        "1. **Last Linear Layer of Decoder**: When you say that the last linear layer of\n",
        "   your decoder has shape `[bs, vocab_size]`, it means that for each example in\n",
        "   the batch, you're outputting a distribution over the vocabulary. The values\n",
        "   can be logit scores that represent the likelihood of each word in your\n",
        "   vocabulary being the next word in the sequence.\n",
        "\n",
        "2. **Target Shape**: In comparison, your target tensor should contain the actual\n",
        "   words (as integers) that appear at each position in your sequence for each\n",
        "   example in the batch. The target tensor does not need to have a `vocab_size`\n",
        "   dimension because it is not a distribution; it contains the indices of the\n",
        "   actual next words. Thus, it should have a shape `[bs, sequence_length]`.\n",
        "\n",
        "3. **Cross-Entropy Loss**: When using the cross-entropy loss, the logits (i.e.,\n",
        "   the output from your linear layer) should have a shape\n",
        "   `[bs, sequence_length, vocab_size]`, while the target should have a shape\n",
        "   `[bs, sequence_length]`. The cross-entropy loss function will internally\n",
        "   apply a softmax to the logits, and then compute the log-likelihood between\n",
        "   the predicted distribution and the target class.\n",
        "\n",
        "To sum up, if your decoder's last linear layer has shape `[bs, vocab_size]` for\n",
        "each time step, make sure that your target tensor has the shape\n",
        "`[bs, sequence_length]`, and your logits should be\n",
        "`[bs, sequence_length, vocab_size]` when you feed them into the cross-entropy\n",
        "loss function.\n",
        "\n",
        "### <a id='toc1_13_5_'></a>[Why do we flatten prediction and target (logits)?](#toc0_)\n",
        "\n",
        "Flattening both the predicted logits and the target labels serves a specific\n",
        "purpose when using the cross-entropy loss function for sequence data. Let's dig\n",
        "into each component to understand why this is done:\n",
        "\n",
        "#### <a id='toc1_13_5_1_'></a>[Background](#toc0_)\n",
        "\n",
        "1. **Logits Tensor**: In a sequence-to-sequence model, you usually generate a\n",
        "   sequence of logits for each item in your batch. The logits for each position\n",
        "   in the sequence form a vector of size `vocab_size`, which gives you a\n",
        "   probability distribution across all possible tokens.\n",
        "\n",
        "    Shape: `[batch_size, sequence_length, vocab_size]`\n",
        "\n",
        "2. **Targets Tensor**: Your ground truth data, the `targets`, are integers\n",
        "   representing the correct class labels (or tokens) at each sequence position.\n",
        "\n",
        "    Shape: `[batch_size, sequence_length]`\n",
        "\n",
        "#### <a id='toc1_13_5_2_'></a>[Traditional Loss Computation](#toc0_)\n",
        "\n",
        "Typically, the cross-entropy loss between predicted probabilities and target\n",
        "labels for one data point is computed, and then you average over all data\n",
        "points. In sequence-to-sequence models, you can think of each position in the\n",
        "sequence as a separate data point.\n",
        "\n",
        "#### <a id='toc1_13_5_3_'></a>[Why Flatten?](#toc0_)\n",
        "\n",
        "1. **Batch and Sequence Unification**: The idea of flattening both logits and\n",
        "   targets is to treat each `(batch, sequence_position)` pair as an independent\n",
        "   data point. Instead of having a batch of sequences, you have a \"flattened\"\n",
        "   batch of tokens. This simplifies the application of the loss function by\n",
        "   converting the 3D logits tensor and 2D targets tensor into 2D and 1D tensors,\n",
        "   respectively.\n",
        "\n",
        "2. **Efficiency**: Loss computations often benefit from vectorization for\n",
        "   computational efficiency. By flattening the tensors, you enable a more\n",
        "   efficient matrix operation, which is generally faster than using nested loops\n",
        "   over each sequence and batch.\n",
        "\n",
        "3. **Alignment**: The key is to ensure that each row in the flattened logits\n",
        "   corresponds to the same position in the flattened targets. This alignment is\n",
        "   crucial for the correct computation of the loss.\n",
        "\n",
        "#### <a id='toc1_13_5_4_'></a>[Step-by-step Flattening](#toc0_)\n",
        "\n",
        "1. **Logits Flattening**: `logits.view(-1, logits.size(-1))` will take the 3D\n",
        "   tensor `[batch_size, seq_length, vocab_size]` and reshape it into a 2D tensor\n",
        "   of shape `[batch_size * seq_length, vocab_size]`.\n",
        "\n",
        "2. **Targets Flattening**: `targets.view(-1)` will take the 2D tensor\n",
        "   `[batch_size, seq_length]` and convert it into a 1D tensor of shape\n",
        "   `[batch_size * seq_length]`.\n",
        "\n",
        "3. **Loss Calculation**: Both flattened tensors are then used in the\n",
        "   cross-entropy loss function. The loss between each row in the flattened\n",
        "   logits and the corresponding element in the flattened targets is computed.\n",
        "\n",
        "By flattening the tensors this way, you maintain the correspondence between each\n",
        "logit and its corresponding target, enabling you to correctly compute the loss\n",
        "for each token across all sequences and batches.\n",
        "\n",
        "### <a id='toc1_13_6_'></a>[Why sometimes unsqueeze masks?](#toc0_)\n",
        "\n",
        "The `unsqueeze` operation is used to add an additional dimension to the tensor.\n",
        "In attention mechanisms, particularly the scaled dot-product attention used in\n",
        "models like the Transformer, the masks usually need to have the same number of\n",
        "dimensions as the attention logits for proper broadcasting.\n",
        "\n",
        "For instance, let's say your source tensor (`src`) has a shape of $B \\times L$\n",
        "where $B$ is the batch size and $L$ is the sequence length. The attention logit\n",
        "tensor resulting from the query-key dot product would then have shape\n",
        "$B \\times N \\times L \\times L$, where $N$ is the number of attention heads.\n",
        "\n",
        "The mask needs to align with the $L \\times L$ dimensions of this 4D tensor. In\n",
        "order to accomplish that, you add singleton dimensions to make it compatible\n",
        "with the attention logit tensor. By unsqueezing the mask tensor from\n",
        "$B \\times L$ to $B \\times 1 \\times 1 \\times L$, you enable broadcasting such\n",
        "that the mask effectively gets expanded to $B \\times N \\times L \\times L$ during\n",
        "the attention calculation, perfectly aligning with the attention logits.\n",
        "\n",
        "That's why the line:\n",
        "\n",
        "```python\n",
        "self.src_mask = (src != pad).unsqueeze(-2)\n",
        "```\n",
        "\n",
        "adds a singleton dimension, converting the shape from $B \\times L$ to\n",
        "$B \\times 1 \\times 1 \\times L$ for proper broadcasting during the attention\n",
        "computations.\n",
        "\n",
        "### <a id='toc1_13_7_'></a>[Why does sequence length differ for source and target, usually I thought it is just all L, same.](#toc0_)\n",
        "\n",
        "...\n",
        "\n",
        "### <a id='toc1_13_8_'></a>[Am i right to assume that the core idea of autoregressive model like decoder only (GPT like) is that for a given sample, there will eventually be L rows where L is the seq length, and therefore I can intuitively view it as 1 sample having L samples, since for each row, we will compute the loss. Am I right in my understanding? Do not hesistate to correct me.](#toc0_)\n",
        "\n",
        "Your description captures an important aspect of autoregressive models like\n",
        "decoder-only Transformers (e.g., GPT). Specifically, you're talking about how\n",
        "the model treats a sequence during training. Let's break down your understanding\n",
        "step by step.\n",
        "\n",
        "1. **Sequence Length**: When you mention \"L rows,\" where \"L\" is the sequence\n",
        "   length, you're essentially pointing out that each sequence is divided into\n",
        "   \"L\" time steps (or tokens). Each time step becomes an input-output pair for\n",
        "   training the model.\n",
        "\n",
        "2. **One Sequence as Multiple Samples**: You're correct to intuit that a single\n",
        "   sequence of length \"L\" can be treated like \"L\" samples, at least in the\n",
        "   context of loss calculation. This is because, during training, the model\n",
        "   computes the loss at each time step by comparing the predicted token with the\n",
        "   actual next token in the sequence.\n",
        "\n",
        "3. **Loss Computation**: The loss is often computed at each position and then\n",
        "   averaged over the sequence length or summed up, depending on the specific\n",
        "   loss function or training regime.\n",
        "\n",
        "However, it's crucial to clarify that although a single sequence may contribute\n",
        "\"L\" terms to the loss function, this is not equivalent to having \"L\" independent\n",
        "samples. The key difference lies in the autoregressive property: the prediction\n",
        "at each time step is conditioned on the preceding tokens. This introduces a\n",
        "temporal dependency across the \"L\" positions, making them not entirely\n",
        "independent samples.\n",
        "\n",
        "In other words, while it's accurate to say that a single sequence contributes\n",
        "multiple terms to the loss function, these terms are correlated because they\n",
        "come from the same sequence and are generated in an autoregressive manner.\n",
        "\n",
        "To summarize, you're mostly correct in your understanding that a single sequence\n",
        "is broken down into multiple steps for the purpose of loss computation, but it's\n",
        "important to remember that these steps are not independent samples due to the\n",
        "autoregressive nature of the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Some Implementation Details\n",
        "\n",
        "```\n",
        "Performs one decoder forward pass given encoder hidden states, the decoder input tokens and attention masks.\n",
        "B = batch size\n",
        "S = source sequence length\n",
        "T = target sequence length\n",
        "E = embedding dimensionality\n",
        "V = vocabulary size\n",
        "```\n",
        "\n",
        "### Input\n",
        "\n",
        "Let's view input's first two samples:\n",
        "\n",
        "```\n",
        "tensor([[15,  4,  9, 10,  1,  3, 13,  0,  6,  2],\n",
        "â”‚   â”‚   [15,  3,  5, 10,  4,  6, 13,  0,  8,  1]])\n",
        "```\n",
        "\n",
        "which is\n",
        "\n",
        "-   shape is `[2, 10]` which is `BxL`.\n",
        "-   `49+13=62` but no `EOS` as we truncated last token.\n",
        "-   `35+46=81` but no `EOS` as we truncated last token.\n",
        "\n",
        "### Positional Encodings\n",
        "\n",
        "#### Why do we hardcode batch size of 1 when creating P?\n",
        "\n",
        "The tensor $P$ for positional encoding is initialized with a batch size of 1.\n",
        "This makes it easy to add to the actual input sequences later, during the\n",
        "forward pass. Positional encodings are not dependent on the specific input\n",
        "sequence but are a function of the position within the sequence. Therefore, they\n",
        "can be precomputed and stored. When you look at the forward pass:\n",
        "\n",
        "```python\n",
        "def forward(self, Z: torch.Tensor) -> torch.Tensor:\n",
        "    Z = self._add_positional_encoding(Z)\n",
        "    return self.dropout(Z)\n",
        "```\n",
        "\n",
        "and the `_add_positional_encoding` method:\n",
        "\n",
        "```python\n",
        "def _add_positional_encoding(self, Z: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Add the positional encoding tensor to the input tensor.\"\"\"\n",
        "    return Z + self.P[:, : Z.shape[1], :].to(Z.device)\n",
        "```\n",
        "\n",
        "You'll see that $P$ is sliced to match the sequence length of $Z$ and then added\n",
        "to $Z$. Because of broadcasting rules in PyTorch, $P$ will automatically be\n",
        "broadcasted to the batch size of $Z$ during this addition. This is why $P$ is\n",
        "initialized with a batch size of 1; it keeps the implementation flexible while\n",
        "making the broadcasting implicit.\n",
        "\n",
        "#### Why do we register P as a buffer in PyTorch?\n",
        "\n",
        "In your `PositionalEncoding` class, the tensor `self.P` holds the pre-computed\n",
        "positional encodings. If you intend for this tensor to be automatically moved to\n",
        "the correct device when the module is moved, and if it should not be a learnable\n",
        "parameter, then registering it as a buffer would be a good idea. This ensures\n",
        "that `self.P` is part of the module's state but is not updated during\n",
        "backpropagation.\n",
        "\n",
        "You could register `self.P` as a buffer right after you initialize it in the\n",
        "`_init_positional_encoding` method:\n",
        "\n",
        "```python\n",
        "def _init_positional_encoding(self) -> torch.Tensor:\n",
        "    \"\"\"Initialize the positional encoding tensor.\"\"\"\n",
        "    P = torch.zeros((1, self.max_seq_len, self.d_model))\n",
        "    position = self._get_position_vector()\n",
        "    div_term = self._get_div_term_vector()\n",
        "    P[:, :, 0::2] = torch.sin(position / div_term)\n",
        "    P[:, :, 1::2] = torch.cos(position / div_term)\n",
        "    self.register_buffer(\"P\", P, persistent=True)\n",
        "    return P\n",
        "```\n",
        "\n",
        "Using `register_buffer` ensures that:\n",
        "\n",
        "1. `self.P` is automatically moved to the device the model is moved to (e.g.,\n",
        "   from CPU to GPU).\n",
        "2. `self.P` is saved when you save the model using `torch.save` or `torch.load`.\n",
        "\n",
        "The `persistent=False` argument indicates that the buffer should not be part of\n",
        "the model's `state_dict`, meaning it won't be saved or loaded with the model. If\n",
        "you do want it to be part of the `state_dict`, you can simply omit this\n",
        "argument.\n",
        "\n",
        "### Attention\n",
        "\n",
        "#### Why do we call contiguous on Q, K and V?\n",
        "\n",
        "D2L's code uses `reshape` to reshape the `Q`, `K` and `V`, where other code such\n",
        "as from the Annotated Transformer uses `view`. When you use `view`, this assumes\n",
        "the tensor is `contiguous`, so it is better to call `contiguous` first.\n",
        "\n",
        "#### Why do we want to transpose Q, K, and V?\n",
        "\n",
        "The transposition of $Q$, $K$, and $V$ in multi-head attention serves a specific\n",
        "purpose: to allow for parallel computation across multiple attention heads. In\n",
        "the original shape, the \"heads\" dimension does not exist; the tensor is simply\n",
        "$B \\times L \\times D$, where $B$ is the batch size, $L$ is the sequence length,\n",
        "and $D$ is the model dimension. By transposing, we create a new shape\n",
        "$B \\times H \\times L \\times (D/H)$, where $H$ is the number of heads. This\n",
        "enables the following:\n",
        "\n",
        "1. **Parallelization**: Each head can now be computed in parallel since each\n",
        "   head operates independently of the others.\n",
        "2. **Optimization**: Modern hardware accelerators like GPUs are optimized for\n",
        "   certain tensor operations, and having a shape that aligns well with these\n",
        "   optimizations can result in faster computation.\n",
        "3. **Readability and Maintainability**: It's easier to understand and debug the\n",
        "   operations for each head when they're isolated like this.\n",
        "\n",
        "#### Why do we want to reverse transpose Q, K, and V?\n",
        "\n",
        "After the attention scores are computed and used to weight $V$, we get a new\n",
        "tensor for each head. However, these tensors are still in the transposed shape\n",
        "$B \\times H \\times L \\times (D/H)$, and they need to be concatenated and\n",
        "linearly transformed to continue through the network. The reverse transposition\n",
        "essentially does the following:\n",
        "\n",
        "1. **Concatenation**: Converts the multiple heads back into a single tensor.\n",
        "   This is required because subsequent layers (like feed-forward neural\n",
        "   networks) expect input in the original $D$-dimensional space.\n",
        "\n",
        "2. **Compatibility**: The rest of the neural network architecture often expects\n",
        "   input tensors to have a specific shape (usually $B \\times L \\times D$).\n",
        "   Reverse transposing ensures that the output of the multi-head attention block\n",
        "   can be fed into subsequent layers without issue.\n",
        "\n",
        "3. **Resource Efficiency**: By reducing the tensor back to its original\n",
        "   dimensions, we can save memory and computational resources, which is\n",
        "   beneficial when you're training large models or operating under hardware\n",
        "   constraints.\n",
        "\n",
        "In summary, the initial transposition is done to facilitate parallel computation\n",
        "across heads, and the reverse transposition is done to concatenate these heads\n",
        "and prepare the tensor for subsequent layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why we need Positional Vector\n",
        "\n",
        "Positional encoding is critical cause the cat ate the mouse is the same as the\n",
        "mouse ate the cat without it\n",
        "\n",
        "Without it the attention Q and K matmul would result in a permutation invariant\n",
        "matrix. So adding position info makes the last token in the attention matrix\n",
        "(say mouse from the cat ate the mouse) would allow the word mouse to hold info\n",
        "for every other word in the sentence as well as knowing every other token\n",
        "position (including knowing it's the last token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO\n",
        "\n",
        "1. Add Positional Encoding\n",
        "2. Add LR Scheduler\n",
        "3. Check why need to use `torch.nn.utils.clip_grad_norm_` to clip gradients\n",
        "4. Why unsqueeze mask?\n",
        "5. Can you init weights inside Encoder instead of outside?\n",
        "6. Add Epoch and Batch State see my old code.\n",
        "7. Important use `Vocab` class like in https://github.com/jsbaan/transformer-from-scratch/blob/main/vocabulary.py.\n",
        "\n",
        "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc1_15_'></a>[References and Further Readings](#toc0_)\n",
        "\n",
        "- https://slds-lmu.github.io/seminar_nlp_ss20/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
