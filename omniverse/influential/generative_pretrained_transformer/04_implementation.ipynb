{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Implementation of Generative Pre-trained Transformers (GPT)\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Organized_Chaos-orange)\n",
    "[![Code](https://img.shields.io/badge/View-Code-blue?style=flat-square&logo=github)](https://github.com/gao-hongnan/omniverse/tree/5221d5d8b9bd845568b2e323d908be282c6e8434/omnivault/transformer)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Literal, Optional, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.backends.cudnn\n",
    "import torch.nn.functional as F\n",
    "from numpy.typing import ArrayLike, NDArray\n",
    "from pydantic import BaseModel, Field, computed_field, model_validator\n",
    "from rich.pretty import pprint\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing the Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Composer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">seed</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">debug</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">dataset_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'tinyshakespeare'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">data_folder</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'./data/tinyshakespeare'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">train_path</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/tinyshakespeare/train.bin'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">valid_path</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/tinyshakespeare/valid.bin'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">encoding_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt2'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">batch_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">block_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">device_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cpu'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">device</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">device</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cpu'</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">d_model</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">d_ff</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">H</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">vocab_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50257</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">num_decoder_blocks</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mComposer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mseed\u001b[0m=\u001b[1;36m2024\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mdebug\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33murl\u001b[0m=\u001b[32m'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mdataset_name\u001b[0m=\u001b[32m'tinyshakespeare'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mdata_folder\u001b[0m=\u001b[32m'./data/tinyshakespeare'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mtrain_path\u001b[0m=\u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/tinyshakespeare/train.bin'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mvalid_path\u001b[0m=\u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/tinyshakespeare/valid.bin'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mencoding_name\u001b[0m=\u001b[32m'gpt2'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mbatch_size\u001b[0m=\u001b[1;36m2\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mblock_size\u001b[0m=\u001b[1;36m8\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mdevice_type\u001b[0m=\u001b[32m'cpu'\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mdevice\u001b[0m=\u001b[1;35mdevice\u001b[0m\u001b[1m(\u001b[0m\u001b[33mtype\u001b[0m=\u001b[32m'cpu'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33md_model\u001b[0m=\u001b[1;36m4\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33md_ff\u001b[0m=\u001b[1;36m4\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mH\u001b[0m=\u001b[1;36m2\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mvocab_size\u001b[0m=\u001b[1;36m50257\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mnum_decoder_blocks\u001b[0m=\u001b[1;36m6\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Composer(BaseModel):\n",
    "    seed: int = 2024\n",
    "    debug: bool = False\n",
    "\n",
    "    url: str = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    dataset_name: str = \"tinyshakespeare\"\n",
    "    data_folder: str = Field(default=\"./data/tinyshakespeare\", description=\"Path to the data folder\")\n",
    "\n",
    "    train_path: Path = Field(None, description=\"Path to the train file\")\n",
    "    valid_path: Path = Field(None, description=\"Path to the valid file\")\n",
    "\n",
    "    encoding_name: Literal[\"gpt2\", \"r50k_base\", \"p50k_base\", \"p50k_edit\", \"cl100k_base\"] = \"gpt2\"\n",
    "\n",
    "    batch_size: int = Field(default=64, description=\"Batch size\")\n",
    "    block_size: int = Field(\n",
    "        default=128, description=\"Block size, an alias for max length/context window size.\", alias=\"context_length\"\n",
    "    )\n",
    "    device_type: Literal[\"cpu\", \"cuda\"] = \"cpu\"\n",
    "    device: torch.device = Field(None, description=\"Device to use\")\n",
    "\n",
    "    # model parameters\n",
    "    d_model: int = Field(default=512, description=\"Dimension of the model\")\n",
    "    d_ff: int = Field(default=512, description=\"Dimension of the feed forward layer\")\n",
    "    H: int = Field(default=8, description=\"Number of heads\", alias=\"num_heads\")\n",
    "    vocab_size: int = Field(default=50257, description=\"Vocabulary size\")\n",
    "    num_decoder_blocks: int = Field(default=6, description=\"Number of decoder blocks\")\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def set_train_valid_paths(self) -> Composer:\n",
    "        self.train_path = Path(self.data_folder) / \"train.bin\"\n",
    "        self.valid_path = Path(self.data_folder) / \"valid.bin\"\n",
    "        return self\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def set_device(self) -> Composer:\n",
    "        self.device = torch.device(self.device_type)\n",
    "        return self\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def set_debug_fields(self) -> Composer:\n",
    "        if self.debug:\n",
    "            self.batch_size = 2\n",
    "            self.block_size = 8\n",
    "            self.d_model = 4\n",
    "            self.H = 2\n",
    "            self.d_ff = 4\n",
    "        return self\n",
    "\n",
    "    class Config:\n",
    "        extra = \"allow\"\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "\n",
    "composer = Composer(debug=True)\n",
    "pprint(composer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_deterministic_mode() -> None:\n",
    "    # fmt: off\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    torch.backends.cudnn.benchmark        = False\n",
    "    torch.backends.cudnn.deterministic    = True\n",
    "    torch.backends.cudnn.enabled          = False\n",
    "\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    # fmt: on\n",
    "    warnings.warn(\n",
    "        \"Deterministic mode is activated. This will negatively impact performance and may cause increase in CUDA memory footprint.\",\n",
    "        category=UserWarning,\n",
    "        stacklevel=2,\n",
    "    )\n",
    "\n",
    "\n",
    "def seed_all(\n",
    "    seed: int = 1992,\n",
    "    seed_torch: bool = True,\n",
    "    set_torch_deterministic: bool = True,\n",
    ") -> int:\n",
    "    # fmt: off\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)       # set PYTHONHASHSEED env var at fixed value\n",
    "    np.random.default_rng(seed)                    # numpy pseudo-random generator\n",
    "    random.seed(seed)                              # python's built-in pseudo-random generator\n",
    "\n",
    "    if seed_torch:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)           # pytorch (both CPU and CUDA)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.enabled = False\n",
    "\n",
    "        if set_torch_deterministic:\n",
    "            configure_deterministic_mode()\n",
    "    # fmt: on\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2024"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_all(composer.seed, seed_torch=True, set_torch_deterministic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def am_i_in_jupyter() -> bool:\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        if \"IPKernelApp\" not in get_ipython().config:\n",
    "            return False\n",
    "    except ImportError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "IN_JUPYTER = am_i_in_jupyter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Vocabulary\n",
    "\n",
    "-   Traditional tokenization methods often involve steps such as\n",
    "    **_lower-casing_**, **_punctuation stripping_**, and **_splitting on\n",
    "    whitespace_**. Additionally, these methods might encode out-of-vocabulary\n",
    "    words using a special token to enable the model to handle unseen words\n",
    "    during evaluation or testing phases. For instance, language models (LMs) may\n",
    "    struggle with interpreting emojis due to such constraints.\n",
    "-   These conventional approaches can inadvertently restrict the natural\n",
    "    language input space $\\mathcal{X}$, consequently limiting the model space\n",
    "    $\\mathcal{H}$. This limitation stems from the fact that the scope of\n",
    "    $\\mathcal{H}$ is inherently dependent on the comprehensiveness of\n",
    "    $\\mathcal{X}$ as we can see\n",
    "    $\\mathcal{H} = \\mathcal{H}(\\mathcal{X} ; \\boldsymbol{\\Theta})$, which means\n",
    "    that the model space $\\mathcal{H}$ is a function of the input space\n",
    "    $\\mathcal{X}$ and the parameter space $\\boldsymbol{\\Theta}$.\n",
    "-   To resolve this, the idea of **_byte-level encoding_** can be used - since\n",
    "    you theoretically can encode any character in the world in **_UTF-8\n",
    "    encoding_**.\n",
    "-   However, the limitation is current byte-level language models tend to\n",
    "    perform poorly on word level tasks.\n",
    "-   The authors then introduced the BPE algorithm (is \"byte-level\" because it\n",
    "    operates on UTF-8 encoded strings) where they striked a balance between\n",
    "    character-level and word-level tokenization.\n",
    "-   So in summary, BPE is the **tokenizer** used to encode the input text into a\n",
    "    sequence of tokens - which form the input representation to the model.\n",
    "\n",
    "Byte pair encoding (BPE) is a way of converting text into tokens and is used as\n",
    "the tokenizer in the training of GPT-2. It has a couple desirable\n",
    "properties[^1]:\n",
    "\n",
    "-   It's reversible and lossless, so you can convert tokens back into the\n",
    "    original text\n",
    "-   It works on arbitrary text, even text that is not in the tokeniser's\n",
    "    training data\n",
    "-   It compresses the text: the token sequence is shorter than the bytes\n",
    "    corresponding to the original text. On average, in practice, each token\n",
    "    corresponds to about 4 bytes.\n",
    "-   It attempts to let the model see common subwords. For instance, \"ing\" is a\n",
    "    common subword in English, so BPE encodings will often split \"encoding\" into\n",
    "    tokens like \"encod\" and \"ing\" (instead of e.g. \"enc\" and \"oding\"). Because\n",
    "    the model will then see the \"ing\" token again and again in different\n",
    "    contexts, it helps models generalise and better understand grammar.\n",
    "\n",
    "```{admonition} References\n",
    ":class: seealso\n",
    "\n",
    "-   [https://github.com/karpathy/minbpe](https://github.com/karpathy/minbpe)\n",
    "-   [https://github.com/openai/tiktoken](https://github.com/openai/tiktoken)\n",
    "-   [https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)\n",
    "-   [Tokenizers - HuggingFace](https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first download the dataset from the Karpathy's\n",
    "[repo](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url: str, dataset_name: str, dest_folder: Path | str) -> Tuple[Path, str]:\n",
    "    dest_folder_path = Path(dest_folder)\n",
    "\n",
    "    dest_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    filepath = dest_folder_path / f\"{dataset_name}.txt\"\n",
    "\n",
    "    response = requests.get(url, stream=True, timeout=30)\n",
    "    corpus = response.text\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    return filepath, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/tinyshakespeare/tinyshakespeare.txt'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/tinyshakespeare/tinyshakespeare.txt'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filepath, corpus = download(composer.url, composer.dataset_name, composer.data_folder)\n",
    "pprint(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the first $100$ characters from the corpus below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(corpus[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print out all Tiktoken encodings, and note to ourself that we will be using\n",
    "`gpt2` - in which the vocabulary size $\\lvert \\mathcal{V} \\rvert = 50257$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All TikToken encodings: ['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base']\n",
      "Vocabulary size: 50257\n"
     ]
    }
   ],
   "source": [
    "print(f\"All TikToken encodings: {tiktoken.list_encoding_names()}\")\n",
    "\n",
    "# encode with tiktoken gpt2 bpe\n",
    "tokenizer = tiktoken.get_encoding(composer.encoding_name)\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.n_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then slice the `corpus` into `train-valid` subsets with a ratio of \n",
    "$9:1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(corpus)\n",
    "train_data = corpus[: int(N * 0.9)]\n",
    "valid_data = corpus[int(N * 0.9) :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then encode `train_data` and `valid_data` using the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 301,966 tokens\n",
      "val has 36,059 tokens\n"
     ]
    }
   ],
   "source": [
    "train_ids = tokenizer.encode_ordinary(train_data)\n",
    "valid_ids = tokenizer.encode_ordinary(valid_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(valid_ids):,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n",
      "--------------------------------------------------------------------------------\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_ids[:2]))\n",
    "print(\"-\" * 80)\n",
    "print(tokenizer.decode(train_ids[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we save the tokenized corpus into `.bin` file for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "valid_ids = np.array(valid_ids, dtype=np.uint16)\n",
    "\n",
    "train_ids.tofile(composer.train_path)\n",
    "valid_ids.tofile(composer.valid_path)\n",
    "\n",
    "# train.bin has 301,966 tokens\n",
    "# val.bin has 36,059 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloading (Poor Man's Dataloader)\n",
    "\n",
    "To batch the corpus into mini-batch of $\\mathcal{B}$ for training using PyTorch\n",
    "framework, we would need to create an efficient way of loading. The easy way out\n",
    "is of course to use PyTorch's `Dataset` class and work from there, but to keep\n",
    "this post similar to what Karpathy used, we would try to understand how he \n",
    "approached it.\n",
    "\n",
    "As Karpathy puts it, he implemented a poor man's\n",
    "[dataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n",
    "We will start by dissecting the code and understanding how it works and finally,\n",
    "show that everything can be done with PyTorch's `Dataset` and `Dataloader`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Mapping\n",
    "\n",
    "Firstly, Karpathy uses `numpy`'s\n",
    "[memory mapping](https://numpy.org/doc/stable/reference/generated/numpy.memmap.html)\n",
    "(`numpy.memmap`) to load the data. Memory mapping is used to create a\n",
    "memory-mapped array from a binary file. This involves mapping the contents of a\n",
    "file directly into the virtual memory space of the calling process. This allows\n",
    "applications to access the file data as if it were loaded in memory, using\n",
    "pointer operations or array indexing, without the need for explicit read or\n",
    "write operations.\n",
    "\n",
    "This essentially means that you can access small segments of a large file\n",
    "without having to load the entire file into memory. The concept draws\n",
    "similarities to the use of [generators](https://wiki.python.org/moin/Generators)\n",
    "in Python, where you can iterate over a large dataset without having to load the\n",
    "entire dataset into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dtype: uint16, data_shape: (301966,)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.memmap(composer.train_path, dtype=np.uint16, mode=\"r\")\n",
    "train_data_dtype = train_data.dtype\n",
    "train_data_shape = train_data.shape\n",
    "\n",
    "print(f\"data_dtype: {train_data_dtype}, data_shape: {train_data_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the shape of train data is `(301966,)`, which means that it is a 1D (flattened) array \n",
    "with $301966$ elements - this is basically the length of the entire train corpus, in terms of\n",
    "tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Sequence\n",
    "\n",
    "However, we are not going to pass the entire training corpus as is to the model.\n",
    "Instead, we are going to pass a **batch** of sequences (each sequence of length\n",
    "`context_length`) to the model at a time.\n",
    "\n",
    "Let's consider a sequence\n",
    "$\\mathbf{x} = (x_1, x_2, \\ldots, x_T) \\in \\mathbb{Z}^{1 \\times T}$, where:\n",
    "\n",
    "-   $x_t \\in \\mathbf{x}$ represents the $t$-th token in the sequence,\n",
    "-   Each token $x_t$ is an element of a predefined vocabulary\n",
    "    $\\mathcal{V} := \\mathcal{X}$,\n",
    "-   $T$ denotes the total number of tokens in the sequence, i.e., the sequence\n",
    "    length.\n",
    "\n",
    "In practice, we handle multiple sequences at once by grouping them into a batch.\n",
    "The batch size, denoted as $\\mathcal{B}$, is then presented to the model for\n",
    "parallel processing.\n",
    "\n",
    "A batch of sequences is represented as a matrix $\\mathbf{x}^{\\mathcal{B}}$,\n",
    "where each row corresponds to a sequence in the batch. If the batch size is\n",
    "$\\mathcal{B}$ and each sequence within the batch has a fixed length $T$, then\n",
    "$\\mathbf{x}^{\\mathcal{B}}$ can be expressed as:\n",
    "\n",
    "$$\n",
    "    \\mathbf{x}^{\\mathcal{B}} = \\begin{bmatrix}\n",
    "    \\mathbf{x}_1 \\\\\n",
    "    \\mathbf{x}_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\mathbf{x}_\\mathcal{B}\n",
    "    \\end{bmatrix} =\n",
    "    \\begin{bmatrix}\n",
    "    x_{1,1} & x_{1,2} & \\ldots & x_{1,T} \\\\\n",
    "    x_{2,1} & x_{2,2} & \\ldots & x_{2,T} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{\\mathcal{B},1} & x_{\\mathcal{B},2} & \\ldots & x_{\\mathcal{B},T}\n",
    "    \\end{bmatrix} \\in \\mathbb{Z}^{\\mathcal{B} \\times T}\n",
    "$$\n",
    "\n",
    "Here, $\\mathbf{x}_i$ represents the $i$-th sequence in the batch, and $x_{i,j}$\n",
    "denotes the $j$-th token in the $i$-th sequence of the batch. It's important to\n",
    "note that while we represent the sequences in a real-valued space\n",
    "$\\mathbb{Z}^{\\mathcal{B} \\times T}$ for mathematical convenience, in practice,\n",
    "each $x_{i,j}$ corresponds to a discrete token from the vocabulary $\\mathcal{X}$\n",
    "so using $\\mathbb{Z}^{+}$ would be more appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Length / Context Window / Block Size\n",
    "\n",
    "$T$ is often referred to as the sequence length, or in the context of GPT, it is\n",
    "the `block_size` or `context_length` or `max_seq_len`.\n",
    "\n",
    "It is the length of the sequence that the model will be trained on and is also\n",
    "the context length/context window that we often hear about.\n",
    "\n",
    "For example,\n",
    "[Gemini 1.5](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024)\n",
    "was announced to have a standard $128,000$ token context window, up to a maximum\n",
    "of $1$ million max length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example, if we define our $T$ to be $32$, then we would expect\n",
    "each sequence to be of length $32$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">memmap</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5962</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22307</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8421</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">356</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5120</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">597</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2252</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │      </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3285</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">502</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2740</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3237</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5248</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">461</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2740</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5962</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22307</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1639</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">389</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">uint16</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mmemmap\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m5962\u001b[0m, \u001b[1;36m22307\u001b[0m,    \u001b[1;36m25\u001b[0m,   \u001b[1;36m198\u001b[0m,  \u001b[1;36m8421\u001b[0m,   \u001b[1;36m356\u001b[0m,  \u001b[1;36m5120\u001b[0m,   \u001b[1;36m597\u001b[0m,  \u001b[1;36m2252\u001b[0m,\n",
       "\u001b[2;32m│   │      \u001b[0m\u001b[1;36m11\u001b[0m,  \u001b[1;36m3285\u001b[0m,   \u001b[1;36m502\u001b[0m,  \u001b[1;36m2740\u001b[0m,    \u001b[1;36m13\u001b[0m,   \u001b[1;36m198\u001b[0m,   \u001b[1;36m198\u001b[0m,  \u001b[1;36m3237\u001b[0m,    \u001b[1;36m25\u001b[0m,\n",
       "\u001b[2;32m│   │     \u001b[0m\u001b[1;36m198\u001b[0m,  \u001b[1;36m5248\u001b[0m,   \u001b[1;36m461\u001b[0m,    \u001b[1;36m11\u001b[0m,  \u001b[1;36m2740\u001b[0m,    \u001b[1;36m13\u001b[0m,   \u001b[1;36m198\u001b[0m,   \u001b[1;36m198\u001b[0m,  \u001b[1;36m5962\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;36m22307\u001b[0m,    \u001b[1;36m25\u001b[0m,   \u001b[1;36m198\u001b[0m,  \u001b[1;36m1639\u001b[0m,   \u001b[1;36m389\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35muint16\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m32\u001b[0m,\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are\n"
     ]
    }
   ],
   "source": [
    "first_sequence = train_data[0:0+32]\n",
    "pprint(first_sequence)\n",
    "pprint(first_sequence.shape)\n",
    "\n",
    "first_sequence_decoded = tokenizer.decode(first_sequence)\n",
    "print(first_sequence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example is just extracting $1$ such sequence $\\mathbf{x}$ from the train\n",
    "corpus. To leverage the prowess of linear algebra operations in CUDA, we would\n",
    "typically pass a batch of sequences $\\mathcal{B}$ to the model at a time.\n",
    "\n",
    "Furthermore, we would require some level of randomness in the sequences that we\n",
    "pass to the model to enable generalisation. <strike>You really do not want the\n",
    "model to overfit to an ordered sequence of tokens in the training\n",
    "corpus.</strike>\n",
    "\n",
    "To this end, let's see how Karpathy implements batching and shuffling of the\n",
    "sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling and Discrete Uniform Sampling\n",
    "\n",
    "To enable shuffling, Karpathy generates a tensor of random integers (essentially a list of\n",
    "random integers), which serve as indices. These indices are used to select\n",
    "random sequences from the training (and validation) data.\n",
    "\n",
    "For simplicity, let's look at the case where batch size is reduced to $\\mathcal{B} = 1$.\n",
    "This means we only need to sample $1$ sequence from the training data - and consequently\n",
    "we only need $1$ random index.\n",
    "\n",
    "We can easily achieve this via `torch.randint` which generates random integers\n",
    "from a discrete uniform distribution over the half-open interval $[l, h)$,\n",
    "and since we only want to sample $1$ sequence, we set `size=(1,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">122484</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m122484\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = torch.Generator(device=composer.device)\n",
    "\n",
    "generator.manual_seed(25)\n",
    "\n",
    "low, high = 0, len(train_data) - composer.block_size\n",
    "size = (1,)\n",
    "indices: torch.Tensor = torch.randint(low=low, high=high, size=size, generator=generator)\n",
    "pprint(indices)\n",
    "pprint(indices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical operation performed by `torch.randint(low, high, size, generator)` can be described as drawing samples from a uniform discrete distribution. Each element of the resulting tensor is an independent and identically distributed {cite}`radford2019language` (i.i.d.) random variable $X_i$ with the following probability mass function (PMF):\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(X_i = k) = \\frac{1}{h - l} \\quad \\text{for} \\, k = l, \\ldots, h-1 \n",
    "$$\n",
    "\n",
    "This PMF implies that each integer in the range $[l, h-1]$ has an equal probability of being selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our demonstration, we selected a random index, specifically $136,016$, from\n",
    "our training dataset. This serves as a starting point for constructing a\n",
    "sequence, denoted as $\\mathbf{x}$. This sequence consists of the token found at\n",
    "the chosen index and extends to include the subsequent $T$ tokens, where $T$\n",
    "represents the block size. For the sake of simplicity, and to align with our\n",
    "predefined settings, we have chosen $T = 8$. This block size is predetermined in\n",
    "our `composer` configuration, activated specifically under a `debug` mode.\n",
    "\n",
    "In code, we can achieve this by slicing the training data from the random index\n",
    "to the random index plus the block size. This is done by `train_data[random_index:random_index+block_size]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">memmap</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11503</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21120</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">880</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">788</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29448</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│      </span><span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">uint16</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mmemmap\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m11503\u001b[0m,   \u001b[1;36m290\u001b[0m, \u001b[1;36m21120\u001b[0m,    \u001b[1;36m30\u001b[0m,   \u001b[1;36m880\u001b[0m,   \u001b[1;36m788\u001b[0m,    \u001b[1;36m11\u001b[0m, \u001b[1;36m29448\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│      \u001b[0m\u001b[33mdtype\u001b[0m=\u001b[35muint16\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m,\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " priest and clerk? well then, amen\n"
     ]
    }
   ],
   "source": [
    "random_sequence = train_data[indices : indices + composer.block_size]\n",
    "pprint(random_sequence)\n",
    "pprint(random_sequence.shape)\n",
    "\n",
    "random_sequence_decoded = tokenizer.decode(random_sequence)\n",
    "print(tokenizer.decode(random_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might wonder why the highest value of the random integers is\n",
    "`len(self.train_data) - self.block_size`. This is mostly to prevent index out of\n",
    "range errors. As we shall soon see, we are using these `indices` to slice a\n",
    "sequence of length `block_size` from the data where you start slicing from the\n",
    "index `index` and end at `index + block_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction Input Sequences\n",
    "\n",
    "Now that we understand how to sample a single sequence from the training data,\n",
    "let's look at how we can sample a batch of sequences.\n",
    "PyTorch made it easy for you, as we can just simply change the `size` parameter\n",
    "to `(batch_size,)` so we can sample $\\mathcal{B}$ number of indices - and\n",
    "consequently $\\mathcal{B}$ number of sequences.\n",
    "\n",
    "In our case, if we set $\\mathcal{B} = 2$, we would expect to get $2$ random\n",
    "indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">122484</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">196406</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m122484\u001b[0m, \u001b[1;36m196406\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = torch.Generator(device=composer.device)\n",
    "generator.manual_seed(25)\n",
    "low, high = 0, len(train_data) - composer.block_size\n",
    "size = (composer.batch_size,)\n",
    "indices: torch.Tensor = torch.randint(low=low, high=high, size=size, generator=generator)\n",
    "pprint(indices)\n",
    "pprint(indices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then construct a batch of input sequences $\\mathcal{B}$ by selecting the\n",
    "tokens at the indices $122,484$ and $196,406$ and the next $T$ tokens via a for\n",
    "loop - and using `torch.stack` to stack the sequences into a tensor of shape\n",
    "$\\mathbb{Z}^{\\mathcal{B} \\times T}$.\n",
    "\n",
    "So the first row of the batch would be the sequence starting at index $122,484$\n",
    "and the second row would be the sequence starting at index $196,406$, with each\n",
    "sequence having a length of $T=8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11503</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21120</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">880</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">788</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29448</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">326</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8616</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">373</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14855</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37286</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m11503\u001b[0m,   \u001b[1;36m290\u001b[0m, \u001b[1;36m21120\u001b[0m,    \u001b[1;36m30\u001b[0m,   \u001b[1;36m880\u001b[0m,   \u001b[1;36m788\u001b[0m,    \u001b[1;36m11\u001b[0m, \u001b[1;36m29448\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m  \u001b[1;36m326\u001b[0m,  \u001b[1;36m8616\u001b[0m,   \u001b[1;36m373\u001b[0m, \u001b[1;36m14855\u001b[0m,    \u001b[1;36m13\u001b[0m,   \u001b[1;36m198\u001b[0m,   \u001b[1;36m198\u001b[0m, \u001b[1;36m37286\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.stack([torch.from_numpy((train_data[index : index + composer.block_size]).astype(np.int64)) for index in indices])\n",
    "pprint(x)\n",
    "pprint(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth reconciling the fact that the slicing uses `[index:index + block_size]` and\n",
    "therefore completes the reasoning behind the `len(self.train_data) - self.block_size` in\n",
    "the `torch.randint` function call - to prevent index out of range errors. Consider\n",
    "that if we do not subtract `block_size` from the length of the training data, we might\n",
    "end up with an index that is the last index of the training data, and when we add\n",
    "`block_size` to it, we would end up with an index that is out of range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction Target Sequences\n",
    "\n",
    "As we will define more formally later, GPT model is an autoregressive\n",
    "self-supervised learning model {cite}`math11112451` that directly learns a\n",
    "conditional probability distribution $\\mathbb{P}(x_t | x_{<t} ; \\Theta)$ over\n",
    "the vocabulary $\\mathcal{V}$ of tokens, which is conditioned on the entire\n",
    "history of tokens $x_{<t} = (x_1, x_2, \\ldots, x_{t-1})$.\n",
    "\n",
    "We have seen earlier how to construct an input sequence $\\mathbf{x}$ from the\n",
    "training data. To put things into perspective, we consider again the first\n",
    "sequence that we constructed from the training data:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix} 11503 & 290 & 21120 & 30 & 880 & 788 & 11 & 29448 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "representing the sentence `'priest and clerk? well then, amen'`.\n",
    "\n",
    "Given the autoregressive and self-supervised nature, in order to construct the\n",
    "target sequence $\\mathbf{y}$, we simply shift the input sequence by one token to\n",
    "the left. This means that the target sequence $\\mathbf{y}$ is:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\begin{bmatrix} 290 & 21120 & 30 & 880 & 788 & 11 & 29448 & 13 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "representing the sentence `'and clerk? well then, amen.'`. Note here $13$ is the\n",
    "index of the next token after the last token in the input sequence.\n",
    "\n",
    "This behaviour is autoregressive because we are using the context tokens\n",
    "$x_{<t}$ to predict the next token $x_t$, and self-supervised because we are\n",
    "using the input sequence $\\mathbf{x}$ to construct the target sequence\n",
    "$\\mathbf{y}$ without any external labels.\n",
    "\n",
    "To illustrate further, the prediction process during training is cumulative:\n",
    "\n",
    "-   For predicting $x_2$, the model uses $x_1$ as context:\n",
    "    $\\mathbb{P}\\left(x_2 \\mid x_1\\right)$.\n",
    "-   For predicting $x_3$, the model uses both $x_1$ and $x_2$ as context:\n",
    "    $\\mathbb{P}\\left(x_3 \\mid x_1, x_2\\right)$.\n",
    "-   This pattern continues, such that for predicting $x_t$, the model uses\n",
    "    $x_1, x_2, \\ldots, x_{t-1}$ as context:\n",
    "    $\\mathbb{P}\\left(x_t \\mid x_1, x_2, \\ldots, x_{t-1}\\right)$\n",
    "\n",
    "In code, we can achieve this by simply slicing the adding a `1` to the `index`\n",
    "in the `train_data` slicing operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21120</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">880</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">788</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29448</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8616</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">373</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14855</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37286</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">406</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m  \u001b[1;36m290\u001b[0m, \u001b[1;36m21120\u001b[0m,    \u001b[1;36m30\u001b[0m,   \u001b[1;36m880\u001b[0m,   \u001b[1;36m788\u001b[0m,    \u001b[1;36m11\u001b[0m, \u001b[1;36m29448\u001b[0m,    \u001b[1;36m13\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m8616\u001b[0m,   \u001b[1;36m373\u001b[0m, \u001b[1;36m14855\u001b[0m,    \u001b[1;36m13\u001b[0m,   \u001b[1;36m198\u001b[0m,   \u001b[1;36m198\u001b[0m, \u001b[1;36m37286\u001b[0m,   \u001b[1;36m406\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "' and clerk? well then, amen.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.stack([torch.from_numpy((train_data[index + 1: index + 1 + composer.block_size]).astype(np.int64)) for index in indices])\n",
    "pprint(y)\n",
    "pprint(y.shape)\n",
    "\n",
    "tokenizer.decode(y[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous Data Loading and Prefetching\n",
    "\n",
    "As we approach the last part of the code, Karpathy moves `x` and `y` to the\n",
    "device and returns them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if composer.device_type == \"cuda\":\n",
    "    # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "    x, y = x.pin_memory().to(composer.device, non_blocking=True), y.pin_memory().to(composer.device, non_blocking=True)\n",
    "else:\n",
    "    x, y = x.to(composer.device), y.to(composer.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a common operation in PyTorch, where we move the data to the underlying\n",
    "device (CPU or GPU or MPS) to leverage the processing capabilities of the\n",
    "device. It goes without saying that modern deep learning models are trained on\n",
    "GPUs - and CUDA is the de facto standard for GPU-accelerated computing.\n",
    "\n",
    "CUDA allows a `pin_memory` and `non_blocking` parameter to be set when transferring\n",
    "tensor data from CPU to GPU. The `pin_memory` parameter is used to allow `.to(\"cuda\")`\n",
    "to be more [performant](https://devblogs.nvidia.com/how-optimize-data-transfers-cuda-cc/)\n",
    "as it avoids some implicit CPU-to-CPU copies. Tensors which are pinned in memory\n",
    "also allow the transfer from CPU to GPU to be done asynchronously via `non_blocking` with respect to\n",
    "the host[^2].\n",
    "\n",
    "It can be useful because we can do some other work in CPU while the data is being\n",
    "transferred to GPU. Consider the below scenario:\n",
    "\n",
    "- `tensor.pin_memory().to(\"cuda\", non_blocking=True)` will transfer the tensor\n",
    "  to the GPU asynchronously, and the CPU can continue doing some other work.\n",
    "- While waiting, CPU can do some other operations without waiting for the\n",
    "  transfer to complete,\n",
    "- Once `tensor` is transferred to the GPU, then we can do some other operations\n",
    "  on the GPU.\n",
    "\n",
    "What is worth noting is that CUDA manages the synchronization such that\n",
    "operations on the GPU will not start until the transfer is complete. However, CUDA\n",
    "programming is complex and is out of the scope of this post. Interested readers\n",
    "can see the reference section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collating Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Literal, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "__all__ = [\"get_batch\"]\n",
    "\n",
    "\n",
    "def get_batch(\n",
    "    composer: Composer,\n",
    "    *,\n",
    "    split: Literal[\"train\", \"valid\"],\n",
    "    batch_size: int,\n",
    "    block_size: int,\n",
    "    generator: torch.Generator,\n",
    "    device: torch.device,\n",
    "    device_type: Literal[\"cpu\", \"cuda\"] = \"cpu\",\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
    "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
    "    if split == \"train\":\n",
    "        data = np.memmap(composer.train_path, dtype=np.uint16, mode=\"r\")\n",
    "    else:\n",
    "        data = np.memmap(composer.valid_path, dtype=np.uint16, mode=\"r\")\n",
    "\n",
    "    low, high = 0, len(data) - block_size\n",
    "    size = (batch_size,)\n",
    "\n",
    "    indices = torch.randint(low=low, high=high, size=size, generator=generator)\n",
    "\n",
    "    x = torch.stack([torch.from_numpy((data[index : index + block_size]).astype(np.int64)) for index in indices])\n",
    "    y = torch.stack(\n",
    "        [torch.from_numpy((data[index + 1 : index + 1 + block_size]).astype(np.int64)) for index in indices]\n",
    "    )\n",
    "    if device_type == \"cuda\":\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11503</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21120</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">880</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">788</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29448</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">326</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8616</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">373</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14855</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37286</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m11503\u001b[0m,   \u001b[1;36m290\u001b[0m, \u001b[1;36m21120\u001b[0m,    \u001b[1;36m30\u001b[0m,   \u001b[1;36m880\u001b[0m,   \u001b[1;36m788\u001b[0m,    \u001b[1;36m11\u001b[0m, \u001b[1;36m29448\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m  \u001b[1;36m326\u001b[0m,  \u001b[1;36m8616\u001b[0m,   \u001b[1;36m373\u001b[0m, \u001b[1;36m14855\u001b[0m,    \u001b[1;36m13\u001b[0m,   \u001b[1;36m198\u001b[0m,   \u001b[1;36m198\u001b[0m, \u001b[1;36m37286\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21120</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">880</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">788</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29448</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8616</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">373</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14855</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37286</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">406</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m  \u001b[1;36m290\u001b[0m, \u001b[1;36m21120\u001b[0m,    \u001b[1;36m30\u001b[0m,   \u001b[1;36m880\u001b[0m,   \u001b[1;36m788\u001b[0m,    \u001b[1;36m11\u001b[0m, \u001b[1;36m29448\u001b[0m,    \u001b[1;36m13\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m8616\u001b[0m,   \u001b[1;36m373\u001b[0m, \u001b[1;36m14855\u001b[0m,    \u001b[1;36m13\u001b[0m,   \u001b[1;36m198\u001b[0m,   \u001b[1;36m198\u001b[0m, \u001b[1;36m37286\u001b[0m,   \u001b[1;36m406\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = torch.Generator(device=composer.device)\n",
    "generator.manual_seed(25)\n",
    "train_batch = get_batch(\n",
    "    composer,\n",
    "    split=\"train\",\n",
    "    batch_size=composer.batch_size,\n",
    "    block_size=composer.block_size,\n",
    "    device=composer.device,\n",
    "    generator=generator,\n",
    ")\n",
    "x, y = train_batch\n",
    "pprint(x)\n",
    "pprint(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyTorch's Dataset and Dataloader\n",
    "\n",
    "It is relatively simple to understand - and since there is not a need to\n",
    "[collate](https://pytorch.org/docs/stable/data.html#dataloader-collate-fn) the\n",
    "data, which makes things a bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2061</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8169</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2471</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20388</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3792</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42602</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2636</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">262</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1502</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">373</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17687</span><span style=\"font-weight: bold\">]])</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span>  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">198</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2061</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8169</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2471</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20388</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">66</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42602</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2636</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">262</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1502</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">373</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17687</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span><span style=\"font-weight: bold\">]])</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m   \u001b[1;36m13\u001b[0m,   \u001b[1;36m198\u001b[0m,  \u001b[1;36m2061\u001b[0m,    \u001b[1;36m11\u001b[0m,  \u001b[1;36m8169\u001b[0m,     \u001b[1;36m0\u001b[0m,  \u001b[1;36m2471\u001b[0m, \u001b[1;36m20388\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m3792\u001b[0m, \u001b[1;36m42602\u001b[0m,  \u001b[1;36m2636\u001b[0m,    \u001b[1;36m30\u001b[0m,   \u001b[1;36m262\u001b[0m,  \u001b[1;36m1502\u001b[0m,   \u001b[1;36m373\u001b[0m, \u001b[1;36m17687\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m  \u001b[1;36m198\u001b[0m,  \u001b[1;36m2061\u001b[0m,    \u001b[1;36m11\u001b[0m,  \u001b[1;36m8169\u001b[0m,     \u001b[1;36m0\u001b[0m,  \u001b[1;36m2471\u001b[0m, \u001b[1;36m20388\u001b[0m,    \u001b[1;36m66\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m42602\u001b[0m,  \u001b[1;36m2636\u001b[0m,    \u001b[1;36m30\u001b[0m,   \u001b[1;36m262\u001b[0m,  \u001b[1;36m1502\u001b[0m,   \u001b[1;36m373\u001b[0m, \u001b[1;36m17687\u001b[0m,    \u001b[1;36m13\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class ShakespeareDataset(Dataset[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "    def __init__(self, data_path: str, block_size: int) -> None:\n",
    "        self.data = np.memmap(data_path, dtype=np.uint16, mode=\"r\")\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = torch.from_numpy(self.data[idx : idx + self.block_size].astype(np.int64))\n",
    "        y = torch.from_numpy(self.data[idx + 1 : idx + 1 + self.block_size].astype(np.int64))\n",
    "        return x, y\n",
    "\n",
    "\n",
    "train_dataset = ShakespeareDataset(composer.train_path, composer.block_size)\n",
    "valid_dataset = ShakespeareDataset(composer.valid_path, composer.block_size)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=composer.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if composer.device_type == \"cuda\" else False,\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=composer.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Adjust based on your system\n",
    "    pin_memory=True if composer.device_type == \"cuda\" else False,\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    pprint(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the `__len__` method, we need to return the length of the dataset. Let's\n",
    "say there are 100 tokens in the dataset with a context window of 10. Then, we\n",
    "need to return 90 (100 - 10) for the `__len__` method. This means we can have a\n",
    "possible 90 sequences of 10 tokens each. Again, this is because if any token\n",
    "index after 90 would result in a sequence that cannot be formed (out of bounds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(301958, 36051)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__(), valid_dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Pre-trained Transformer (GPT)\n",
    "\n",
    "The GPT-2 architecture is a **_transformer_**-based model, and as the name\n",
    "suggests, it is a continuation of the GPT-1 model with some minor modifications.\n",
    "\n",
    "GPT-2 utilizes a **Transformer** architecture {cite}`vaswani2017attention` as\n",
    "its backbone, which is distinguished by **_self-attention mechanisms_**. In\n",
    "short, we switched from bi-directional cross attention to uni-directional\n",
    "self-attention.\n",
    "\n",
    "```{figure} ./assets/sebastian-decoder.jpeg\n",
    "---\n",
    "name: sebastian-decoder\n",
    "---\n",
    "GPT Architecture. Image Credit: Build a Large Language Model (From Scratch) by Sebastian Raschka\n",
    "```\n",
    "\n",
    "### Modifications from GPT-1 and Model Stability\n",
    "\n",
    "Modifications from GPT-1 include:\n",
    "\n",
    "-   **Layer normalization** is repositioned to the **_input_** of each\n",
    "    sub-block, mirroring a **_pre-activation residual network_**. This\n",
    "    modification is believed to offer training stability and model performance.\n",
    "    By normalizing the inputs to each sub-block, it is conjectured to alleviate\n",
    "    issues tied to **_internal covariate shift_**, thus aiding in smoother and\n",
    "    potentially faster training.\n",
    "-   GPT-2 introduces an **_additional layer normalization step_** that is\n",
    "    executed **_after the final self-attention block_** within the model. This\n",
    "    additional normalization step can help ensure that the outputs of the\n",
    "    transformer layers are normalized before being passed to subsequent layers\n",
    "    or used in further processing, further contributing to model stability.\n",
    "-   The GPT-2 paper introduces a modification to the standard weight\n",
    "    initialization for the model's residual layers. Specifically, the weights\n",
    "    are scaled by a factor of $\\frac{1}{\\sqrt{N_{\\text{decoder_blocks}}}}$,\n",
    "    where $N_{\\text{decoder_blocks}}$ represents the number of blocks (or\n",
    "    layers) in the Transformer's decoder.\n",
    "\n",
    "    The rationale, as quoted from the paper: _\"A modified initialization which\n",
    "    accounts for the accumulation on the residual path with model depth is\n",
    "    used\"_ {cite}`radford2019language`, is to ensure that the variance of the\n",
    "    input to the block is the same as the variance of the block's output. This\n",
    "    is to ensure that the signal is neither amplified nor diminished as it\n",
    "    passes through the block. As the model depth increases, the activations get\n",
    "    added/acculumated, and hence the scaling factor is\n",
    "    $\\frac{1}{\\sqrt{N_{\\text{decoder_blocks}}}}$, to scale it down.\n",
    "\n",
    "-   Clearly, we can see the empahsis on model stability. In training large\n",
    "    language models, **numerical stability** is paramount; the cost of training\n",
    "    is significantly high, with every loss and gradient spike that fails to\n",
    "    recover necessitating a return to a previous checkpoint, resulting in\n",
    "    substantial GPU hours and potentially tens of thousands of dollars wasted.\n",
    "-   The model's **vocabulary** is expanded to 50,257 tokens.\n",
    "-   The context window size is increased from 512 to 1024 tokens, enhancing the\n",
    "    model's ability to maintain coherence over longer text spans.\n",
    "-   A larger batch size of 512, GPT-2 benefits from more stable and effective\n",
    "    gradient estimates during training, contributing to improved learning\n",
    "    outcomes.\n",
    "\n",
    "-   The GPT-2 paper introduces a modification to the standard weight\n",
    "    initialization for the model's residual layers. Specifically, the weights\n",
    "    are scaled by a factor of $\\frac{1}{\\sqrt{N_{\\text{decoder_blocks}}}}$,\n",
    "    where $N_{\\text{decoder_blocks}}$ represents the number of blocks (or\n",
    "    layers) in the Transformer's decoder.\n",
    "\n",
    "    The rationale, as quoted from the paper: _\"A modified initialization which\n",
    "    accounts for the accumulation on the residual path with model depth is\n",
    "    used\"_ {cite}`radford2019language`, is to ensure that the variance of the\n",
    "    input to the block is the same as the variance of the block's output. This\n",
    "    is to ensure that the signal is neither amplified nor diminished as it\n",
    "    passes through the block. As the model depth increases, the activations get\n",
    "    added/acculumated, and hence the scaling factor is\n",
    "    $\\frac{1}{\\sqrt{N_{\\text{decoder_blocks}}}}$, to scale it down.\n",
    "\n",
    "    -   In practice, seeing how Karpathy implemented it, it seems that the\n",
    "        scalings are implemented on the _projection_ layers of the\n",
    "        MultiHeadAttention and PositionwiseFFN layers, as seen below:\n",
    "\n",
    "        ```python\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "        ```\n",
    "\n",
    "    -   My guess is that the projection layers in both MultiHeadAttention and\n",
    "        PositionwiseFFN are critical junctures where the model's representations\n",
    "        are linearly transformed. These layers significantly influence the\n",
    "        model's ability to learn and propagate signals effectively through its\n",
    "        depth. Scaling the weights of these projection layers helps to control\n",
    "        the rate at which information (and error gradients) is dispersed\n",
    "        throughout the network, directly affecting learning stability and\n",
    "        efficiency.\n",
    "\n",
    "-   I did not implement the custom scaling and just went ahead with default\n",
    "    weight scaling:\n",
    "\n",
    "    -   Weights initialization for the decoder:\n",
    "\n",
    "        ```python\n",
    "        def _init_weights(self, module: nn.Module) -> None:\n",
    "            normal_init_modules = (nn.Linear, nn.Embedding)\n",
    "            if isinstance(module, normal_init_modules):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if hasattr(module, \"bias\") and module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "        ```\n",
    "\n",
    "    -   Weights initialization for the context projection and the context fully\n",
    "        connected layers are done using Xavier Uniform initialization.\n",
    "\n",
    "        ```python\n",
    "        def _init_weights(self) -> None:\n",
    "            \"\"\"Initialize parameters of the linear layers.\"\"\"\n",
    "            nn.init.xavier_uniform_(self.ffn[\"context_fc\"].weight)\n",
    "            if self.ffn[\"context_fc\"].bias is not None:\n",
    "                nn.init.constant_(self.ffn[\"context_fc\"].bias, 0)\n",
    "\n",
    "            nn.init.xavier_uniform_(self.ffn[\"context_projection\"].weight)\n",
    "            if self.ffn[\"context_projection\"].bias is not None:\n",
    "                nn.init.constant_(self.ffn[\"context_projection\"].bias, 0)\n",
    "        ```\n",
    "\n",
    "### GPT-2 Variants\n",
    "\n",
    "To this end, we encapsulate some key parameters in\n",
    "{numref}`decoder-concept-gpt-2-family-duplicate` below, which provides\n",
    "specifications for several GPT-2 variants, distinguished by their scale.\n",
    "\n",
    "```{list-table} GPT-2 Family\n",
    ":header-rows: 1\n",
    ":name: decoder-concept-gpt-2-family-duplicate\n",
    "\n",
    "* - Parameters\n",
    "  - Layers\n",
    "  - d_model\n",
    "  - H\n",
    "  - d_ff\n",
    "  - Activation\n",
    "  - Vocabulary Size\n",
    "  - Context Window\n",
    "* - 117M\n",
    "  - 12\n",
    "  - 768\n",
    "  - 12\n",
    "  - 3072\n",
    "  - GELU\n",
    "  - 50,257\n",
    "  - 1024\n",
    "* - 345M\n",
    "  - 24\n",
    "  - 1024\n",
    "  - 16\n",
    "  - 4096\n",
    "  - GELU\n",
    "  - 50,257\n",
    "  - 1024\n",
    "* - 762M\n",
    "  - 36\n",
    "  - 1280\n",
    "  - 20\n",
    "  - 5120\n",
    "  - GELU\n",
    "  - 50,257\n",
    "  - 1024\n",
    "* - 1542M\n",
    "  - 48\n",
    "  - 1600\n",
    "  - 25\n",
    "  - 6400\n",
    "  - GELU\n",
    "  - 50,257\n",
    "  - 1024\n",
    "```\n",
    "\n",
    "### GPT-2 Model Architecture (HuggingFace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is without the head/softmax layer, from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Layer (type:depth-idx)                        Param #\n",
      "======================================================================\n",
      "GPT2Model                                     --\n",
      "├─Embedding: 1-1                              38,597,376\n",
      "├─Embedding: 1-2                              786,432\n",
      "├─Dropout: 1-3                                --\n",
      "├─ModuleList: 1-4                             --\n",
      "│    └─GPT2Block: 2-1                         --\n",
      "│    │    └─LayerNorm: 3-1                    1,536\n",
      "│    │    └─GPT2Attention: 3-2                2,362,368\n",
      "│    │    └─LayerNorm: 3-3                    1,536\n",
      "│    │    └─GPT2MLP: 3-4                      4,722,432\n",
      "│    └─GPT2Block: 2-2                         --\n",
      "│    │    └─LayerNorm: 3-5                    1,536\n",
      "│    │    └─GPT2Attention: 3-6                2,362,368\n",
      "│    │    └─LayerNorm: 3-7                    1,536\n",
      "│    │    └─GPT2MLP: 3-8                      4,722,432\n",
      "│    └─GPT2Block: 2-3                         --\n",
      "│    │    └─LayerNorm: 3-9                    1,536\n",
      "│    │    └─GPT2Attention: 3-10               2,362,368\n",
      "│    │    └─LayerNorm: 3-11                   1,536\n",
      "│    │    └─GPT2MLP: 3-12                     4,722,432\n",
      "│    └─GPT2Block: 2-4                         --\n",
      "│    │    └─LayerNorm: 3-13                   1,536\n",
      "│    │    └─GPT2Attention: 3-14               2,362,368\n",
      "│    │    └─LayerNorm: 3-15                   1,536\n",
      "│    │    └─GPT2MLP: 3-16                     4,722,432\n",
      "│    └─GPT2Block: 2-5                         --\n",
      "│    │    └─LayerNorm: 3-17                   1,536\n",
      "│    │    └─GPT2Attention: 3-18               2,362,368\n",
      "│    │    └─LayerNorm: 3-19                   1,536\n",
      "│    │    └─GPT2MLP: 3-20                     4,722,432\n",
      "│    └─GPT2Block: 2-6                         --\n",
      "│    │    └─LayerNorm: 3-21                   1,536\n",
      "│    │    └─GPT2Attention: 3-22               2,362,368\n",
      "│    │    └─LayerNorm: 3-23                   1,536\n",
      "│    │    └─GPT2MLP: 3-24                     4,722,432\n",
      "│    └─GPT2Block: 2-7                         --\n",
      "│    │    └─LayerNorm: 3-25                   1,536\n",
      "│    │    └─GPT2Attention: 3-26               2,362,368\n",
      "│    │    └─LayerNorm: 3-27                   1,536\n",
      "│    │    └─GPT2MLP: 3-28                     4,722,432\n",
      "│    └─GPT2Block: 2-8                         --\n",
      "│    │    └─LayerNorm: 3-29                   1,536\n",
      "│    │    └─GPT2Attention: 3-30               2,362,368\n",
      "│    │    └─LayerNorm: 3-31                   1,536\n",
      "│    │    └─GPT2MLP: 3-32                     4,722,432\n",
      "│    └─GPT2Block: 2-9                         --\n",
      "│    │    └─LayerNorm: 3-33                   1,536\n",
      "│    │    └─GPT2Attention: 3-34               2,362,368\n",
      "│    │    └─LayerNorm: 3-35                   1,536\n",
      "│    │    └─GPT2MLP: 3-36                     4,722,432\n",
      "│    └─GPT2Block: 2-10                        --\n",
      "│    │    └─LayerNorm: 3-37                   1,536\n",
      "│    │    └─GPT2Attention: 3-38               2,362,368\n",
      "│    │    └─LayerNorm: 3-39                   1,536\n",
      "│    │    └─GPT2MLP: 3-40                     4,722,432\n",
      "│    └─GPT2Block: 2-11                        --\n",
      "│    │    └─LayerNorm: 3-41                   1,536\n",
      "│    │    └─GPT2Attention: 3-42               2,362,368\n",
      "│    │    └─LayerNorm: 3-43                   1,536\n",
      "│    │    └─GPT2MLP: 3-44                     4,722,432\n",
      "│    └─GPT2Block: 2-12                        --\n",
      "│    │    └─LayerNorm: 3-45                   1,536\n",
      "│    │    └─GPT2Attention: 3-46               2,362,368\n",
      "│    │    └─LayerNorm: 3-47                   1,536\n",
      "│    │    └─GPT2MLP: 3-48                     4,722,432\n",
      "├─LayerNorm: 1-5                              1,536\n",
      "======================================================================\n",
      "Total params: 124,439,808\n",
      "Trainable params: 124,439,808\n",
      "Non-trainable params: 0\n",
      "======================================================================\n",
      "GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2LMHeadModel # type: ignore[import-untyped]\n",
    "from torchinfo import summary\n",
    "\n",
    "gpt = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "print(summary(gpt))\n",
    "print(gpt.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt_with_head = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "# print(summary(gpt_with_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt_medium = GPT2Model.from_pretrained('gpt2-medium')\n",
    "# print(gpt_medium.config)\n",
    "\n",
    "# gpt_large = GPT2Model.from_pretrained('gpt2-large')\n",
    "# print(gpt_large.config)\n",
    "\n",
    "# gpt_xl = GPT2Model.from_pretrained('gpt2-xl')\n",
    "# print(gpt_xl.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the config does not show the dimension of the feedforward network.\n",
    "In GPT-2 source code, we can see what the dimension of the feedforward network\n",
    "is. It is defined as:\n",
    "\n",
    "```python\n",
    "inner_dim = config.n_inner if config.n_inner is not None else 4 * hidden_size\n",
    "```\n",
    "\n",
    "This is why you do not see it in the `config` object as if not set, then it is\n",
    "simply set to `4 * hidden_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Embeddings\n",
    "\n",
    "First, we will look at the first sequence, given by\n",
    "`' priest and clerk? well then, amen'`, which we have already mapped to its\n",
    "corresponding token IDs.\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix} 11503 & 290 & 21120 & 30 & 880 & 788 & 11 & 29448 \\end{bmatrix} \\in \\mathbb{Z}^{1 \\times 8}\n",
    "$$\n",
    "\n",
    "The shape is $1 \\times 8$, which is a single sequence of $8$ tokens. And in this\n",
    "case, we have each word/punctuation mapped to a unique token ID, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: 11503, Token:  priest\n",
      "Token ID: 290, Token:  and\n",
      "Token ID: 21120, Token:  clerk\n",
      "Token ID: 30, Token: ?\n",
      "Token ID: 880, Token:  well\n",
      "Token ID: 788, Token:  then\n",
      "Token ID: 11, Token: ,\n",
      "Token ID: 29448, Token:  amen\n"
     ]
    }
   ],
   "source": [
    "for token in x[0]:\n",
    "    print(f\"Token ID: {token.item()}, Token: {tokenizer.decode([token.item()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to map each token to a vector (embeddings) in a high-dimensional\n",
    "space.\n",
    "\n",
    "The integer tokens, by themselves, do not carry much information. For example,\n",
    "the word `priest` is tokenized to be `11503`, which is an arbitrary integer. In\n",
    "a one-dimensional Euclidean space, the word `priest` and the next word `and`,\n",
    "indexed by `290`, **_would appear to be very far apart from each other_**.\n",
    "However, if we were to change a tokenizer, and somehow the word `priest` is now\n",
    "tokenized to be `291`, then the words `priest` and `and` **_would appear to be\n",
    "very near to each other_**.\n",
    "\n",
    "This means that the model could potentially learn the relationship of two tokens\n",
    "based solely on their tokenized integers. To address this, we use **embedding\n",
    "vectors**. While the initial mapping from words to vectors is dependent on the\n",
    "tokenizer and may be _arbitrary_, during training, the model adjusts these\n",
    "vectors so that words used in similar contexts come to have similar vectors.\n",
    "This allows the model to capture **semantic** relationships between words - and\n",
    "by extension, allows the model to capture relationships between tokens better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">' priest and clerk? well then, amen'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m' priest and clerk? well then, amen'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11503</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">290</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21120</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">880</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">788</span>,    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29448</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m11503\u001b[0m,   \u001b[1;36m290\u001b[0m, \u001b[1;36m21120\u001b[0m,    \u001b[1;36m30\u001b[0m,   \u001b[1;36m880\u001b[0m,   \u001b[1;36m788\u001b[0m,    \u001b[1;36m11\u001b[0m, \u001b[1;36m29448\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x0 = x[0]\n",
    "pprint(tokenizer.decode(x0.cpu().numpy()))\n",
    "pprint(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0213</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3146</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2616</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3730</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5715</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1229</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.8145</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4164</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4973</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1740</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6713</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1102</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.3167</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2943</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9573</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2935</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0623</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1054</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8182</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.4184</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4016</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3422</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.9704</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2435</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0576</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0596</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2764</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2403</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2707</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.5865</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4099</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.3797</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">EmbeddingBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-1.0213\u001b[0m,  \u001b[1;36m0.3146\u001b[0m, \u001b[1;36m-0.2616\u001b[0m,  \u001b[1;36m0.3730\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.5715\u001b[0m,  \u001b[1;36m0.1229\u001b[0m, \u001b[1;36m-0.8145\u001b[0m, \u001b[1;36m-1.4164\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.4973\u001b[0m, \u001b[1;36m-1.1740\u001b[0m, \u001b[1;36m-0.6713\u001b[0m, \u001b[1;36m-0.1102\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-2.3167\u001b[0m,  \u001b[1;36m0.2943\u001b[0m,  \u001b[1;36m0.9573\u001b[0m,  \u001b[1;36m0.2935\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0623\u001b[0m, \u001b[1;36m-0.1054\u001b[0m,  \u001b[1;36m0.8182\u001b[0m, \u001b[1;36m-2.4184\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-1.4016\u001b[0m,  \u001b[1;36m0.3422\u001b[0m, \u001b[1;36m-0.9704\u001b[0m, \u001b[1;36m-0.2435\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.0576\u001b[0m, \u001b[1;36m-0.0596\u001b[0m,  \u001b[1;36m0.2764\u001b[0m, \u001b[1;36m-0.2403\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.2707\u001b[0m, \u001b[1;36m-0.5865\u001b[0m, \u001b[1;36m-1.4099\u001b[0m, \u001b[1;36m-1.3797\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mEmbeddingBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(composer.seed)\n",
    "\n",
    "tok_embed = nn.Embedding(num_embeddings=composer.vocab_size, embedding_dim=composer.d_model)\n",
    "x0_tok_embed = tok_embed(x0)\n",
    "pprint(x0_tok_embed)\n",
    "pprint(x0_tok_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that for the sequence\n",
    "`x0 = ' priest and clerk? well then, amen' = [ 11503, 290, 21120, 30, 880, 788, 11, 29448]`,\n",
    "we have the following token embeddings:\n",
    "\n",
    "```python\n",
    "[[-1.0213,  0.3146, -0.2616,  0.3730],\n",
    " [ 0.5715,  0.1229, -0.8145, -1.4164],\n",
    " [ 0.4973, -1.1740, -0.6713, -0.1102],\n",
    " [-2.3167,  0.2943,  0.9573,  0.2935],\n",
    " [ 0.0623, -0.1054,  0.8182, -2.4184],\n",
    " [-1.4016,  0.3422, -0.9704, -0.2435],\n",
    " [-0.0576, -0.0596,  0.2764, -0.2403],\n",
    " [ 1.2707, -0.5865, -1.4099, -1.3797]]\n",
    "```\n",
    "\n",
    "Notice that for each token, the embedding vector is a $4$-dimensional vector.\n",
    "This is because we have set the embedding dimension to be $4$, which is a\n",
    "hyperparameter that we can set. In the case of GPT-2, the embedding dimension is\n",
    "$768$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Before we explain further, we will first implement the token embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Of The Embedding Layer\n",
    "\n",
    "Typically, the token embeddings are learned during training, and the learned\n",
    "embeddings are used to represent the tokens in the input sequence. \n",
    "\n",
    "1. We first unsqueeze the input tensor `x0` to add a batch dimension, resulting\n",
    "   in a shape of `[1, 8]` where `1` is the batch size $\\mathcal{B}$ and `8` is\n",
    "   the sequence length $T$. For simplicity, we do not consider batch size in\n",
    "   this example first.\n",
    "2. The `tok_embed` layer is then applied to the input tensor, resulting in a\n",
    "   tensor of shape `[1, 8, 4]`.\n",
    "    1. `z0_tok_embed` is the token embedding tensor, which is the transformed\n",
    "       input tensor `x0`. Here our `x0` was transformed from a sequence of\n",
    "       tokens to a sequence of token embeddings.\n",
    "    2. There is a weight matrix `W_e` that is `[V, D]` that transforms the input\n",
    "       tensor into the token embedding tensor via a matrix multiplication\n",
    "       `z0_tok_embed = x0_ohe @ W_e` (which we will see shortly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0213</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3146</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2616</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3730</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5715</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1229</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.8145</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4164</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4973</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1740</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6713</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1102</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.3167</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2943</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9573</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2935</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0623</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1054</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8182</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.4184</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4016</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3422</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.9704</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2435</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0576</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0596</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2764</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2403</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2707</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.5865</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4099</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.3797</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">EmbeddingBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-1.0213\u001b[0m,  \u001b[1;36m0.3146\u001b[0m, \u001b[1;36m-0.2616\u001b[0m,  \u001b[1;36m0.3730\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.5715\u001b[0m,  \u001b[1;36m0.1229\u001b[0m, \u001b[1;36m-0.8145\u001b[0m, \u001b[1;36m-1.4164\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.4973\u001b[0m, \u001b[1;36m-1.1740\u001b[0m, \u001b[1;36m-0.6713\u001b[0m, \u001b[1;36m-0.1102\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-2.3167\u001b[0m,  \u001b[1;36m0.2943\u001b[0m,  \u001b[1;36m0.9573\u001b[0m,  \u001b[1;36m0.2935\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0623\u001b[0m, \u001b[1;36m-0.1054\u001b[0m,  \u001b[1;36m0.8182\u001b[0m, \u001b[1;36m-2.4184\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-1.4016\u001b[0m,  \u001b[1;36m0.3422\u001b[0m, \u001b[1;36m-0.9704\u001b[0m, \u001b[1;36m-0.2435\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.0576\u001b[0m, \u001b[1;36m-0.0596\u001b[0m,  \u001b[1;36m0.2764\u001b[0m, \u001b[1;36m-0.2403\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.2707\u001b[0m, \u001b[1;36m-0.5865\u001b[0m, \u001b[1;36m-1.4099\u001b[0m, \u001b[1;36m-1.3797\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mEmbeddingBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = torch.Generator(device=composer.device)\n",
    "generator.manual_seed(25)\n",
    "torch.manual_seed(composer.seed)\n",
    "\n",
    "tok_embed = TokenEmbedding(vocab_size=composer.vocab_size, d_model=composer.d_model)\n",
    "\n",
    "# x0 = x0.unsqueeze(dim=0) if x0.ndim == 1 else x0 # [T] -> [B, T]\n",
    "\n",
    "z0_tok_embed: torch.Tensor = tok_embed(x0)\n",
    "assert z0_tok_embed.shape == (composer.block_size, composer.d_model) # [B, T, D] = [1, 8, 4]\n",
    "pprint(z0_tok_embed)\n",
    "pprint(z0_tok_embed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this operation above is essentially a lookup operation, where we look up the\n",
    "embedding vector for each token in the sequence. This is done by `tok_embed(x)`.\n",
    "We run it against the first sequence for simplicity, and `z0_tok_embed` is the\n",
    "resulting tensor, with a shape of $T \\times D$. In our case, the sequence length\n",
    "(block size) is $T = 8$, and the embedding dimension is $D = 4$. This means that\n",
    "we have essentially mapped each of the $8$ tokens representing\n",
    "`priest and clerk? well then, amen` to a $4$-dimensional vector.\n",
    "\n",
    "-   `priest` is mapped to `[-1.0213, 0.3146, -0.2616, 0.3730]`\n",
    "-   `and` is mapped to `[ 0.5715, 0.1229, -0.8145, -1.4164]`\n",
    "-   ...\n",
    "-   `amen` is mapped to `[ 1.2707, -0.5865, -1.4099, -1.3797]`\n",
    "\n",
    "With each token being a vector, not only does the token carry more information,\n",
    "it is also much easier to do linear algebra operations on the tokens. For\n",
    "example, we can easily calculate the mean/sum of the embeddings for pooling, or\n",
    "we can easily calculate the dot product between two tokens to measure their\n",
    "similarity in a high-dimensional space (as compared to it being an integer with\n",
    "only 1 dimension).\n",
    "\n",
    "Furthermore, the embeddings are learned during training, and the model would try\n",
    "to capture semantic relationships between tokens. For example, the model would\n",
    "try to learn that `priest` and `clerk` are related in some way because they\n",
    "refer to people, and `amen` is related to `priest` because it is often used in\n",
    "religious contexts.\n",
    "\n",
    "To this end, we denote the output of the token embedding layer as $\\mathbf{X}$.\n",
    "In what follows, we will see beneath how the token embedding layer is computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Representation of Input Sequence $\\mathbf{x}$\n",
    "\n",
    "First, we need to understand how the input sequence $\\mathbf{x}$ can be\n",
    "represented as a one-hot encoded matrix.\n",
    "\n",
    "The one-hot representation of the input sequence $\\mathbf{x}$ is denoted as\n",
    "$\\mathbf{X}^{\\text{ohe}}$. This representation converts each token in the\n",
    "sequence to a one-hot encoded vector, where each vector has a length equal to\n",
    "the size of the vocabulary $V$.\n",
    "\n",
    "#### Definition\n",
    "\n",
    "The one-hot encoded matrix $\\mathbf{X}^{\\text{ohe}}$ is defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^{\\text{ohe}} = \\begin{bmatrix}\n",
    "o_{1,1} & o_{1,2} & \\cdots & o_{1,V} \\\\\n",
    "o_{2,1} & o_{2,2} & \\cdots & o_{2,V} \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "o_{T,1} & o_{T,2} & \\cdots & o_{T,V}\n",
    "\\end{bmatrix} \\in \\{0, 1\\}^{T \\times V}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "-   $T$: Total length of the sequence $\\mathbf{x}$.\n",
    "-   $V$: Size of the vocabulary $\\mathcal{V}$.\n",
    "-   $o_{t,j}$: Element of the one-hot encoded matrix $\\mathbf{X}^{\\text{ohe}}$\n",
    "    at row $t$ and column $j$.\n",
    "\n",
    "In addition, we have:\n",
    "\n",
    "-   $\\mathbf{X}^{\\text{ohe}}$ is a $T \\times V$ matrix.\n",
    "-   Elements of $\\mathbf{X}^{\\text{ohe}}$ are binary, i.e., they belong to\n",
    "    $\\{0, 1\\}$.\n",
    "-   The row vector $\\mathbf{o}_{t, :}$ represents the one-hot encoded vector for\n",
    "    the token at position $t$ in the sequence $\\mathbf{x}$.\n",
    "\n",
    "#### One-Hot Encoding Process\n",
    "\n",
    "For each token $x_t$ at position $t$ in the sequence $\\mathbf{x}$\n",
    "($1 \\leq t \\leq T$), the corresponding row vector $\\mathbf{o}_{t, :}$ in\n",
    "$\\mathbf{X}^{\\text{ohe}}$ is defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{o}_{t, j} = \\begin{cases}\n",
    "1 & \\text{if } f_{\\text{stoi}}(x_t) = j-1\\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "for $j = 1, 2, \\ldots, V$.\n",
    "\n",
    "Here, $f_{\\text{stoi}}(x_t)$ maps the token $x_t$ to its index $j-1$ in the\n",
    "vocabulary $\\mathcal{V}$, the $j-1$ is because zero-based indexing used in\n",
    "python (where $0 \\leq j-1 < V$). Each row $\\mathbf{o}_{t, :}$ in\n",
    "$\\mathbf{X}^{\\text{ohe}}$ contains a single '1' at the column $j$ corresponding\n",
    "to the vocabulary index of $x_t$, and '0's elsewhere.\n",
    "\n",
    "```{prf:example} Example\n",
    ":label: gpt-notations-one-hot-example\n",
    "\n",
    "For example, if the vocabulary\n",
    "$\\mathcal{V} = \\{\\text{cat}, \\text{dog}, \\text{mouse}\\}$ and the sequence\n",
    "$\\mathbf{x} = (\\text{mouse}, \\text{dog})$, then the one-hot encoded matrix\n",
    "$\\mathbf{X}^{\\text{ohe}}$ will be:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^{\\text{ohe}} = \\begin{bmatrix}\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 1 & 0\n",
    "\\end{bmatrix} \\in \\{0, 1\\}^{2 \\times 3}\n",
    "$$\n",
    "\n",
    "In this example:\n",
    "\n",
    "-   The sequence length $T = 2$.\n",
    "-   The vocabulary size $V = 3$.\n",
    "-   \"mouse\" corresponds to the third position in the vocabulary, and \"dog\" to\n",
    "    the second, which is seen in their respective one-hot vectors.\n",
    "```\n",
    "\n",
    "We write the one hot encoding proces for the input sequence `x0` as follows\n",
    "in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_ohe: torch.Tensor = F.one_hot(x0, num_classes=composer.vocab_size).float()\n",
    "assert x0_ohe.shape == (composer.block_size, composer.vocab_size)  # [B, T, V] = [1, 8, 50257]\n",
    "for index, token_id in enumerate(x0):\n",
    "    assert x0_ohe[index, token_id].item() == 1.0  # check if the one-hot encoding is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer Is Matmul Of One-Hot Encoded Input Matrix And Embedding Matrix Weights\n",
    "\n",
    "Once the one hot encoding representation $\\mathbf{X}^{\\text{ohe}}$ is well\n",
    "defined, we can then pass it as input through our GPT model, in which the first\n",
    "layer is a embedding lookup table. In the GPT model architecture, the first\n",
    "layer typically involves mapping the one-hot encoded input vectors into a\n",
    "lower-dimensional, dense embedding space using the embedding matrix\n",
    "$\\mathbf{W}_e$.\n",
    "\n",
    "| Matrix Description             | Symbol                     | Dimensions            | Description                                                                              |\n",
    "| ------------------------------ | -------------------------- | --------------------- | ---------------------------------------------------------------------------------------- |\n",
    "| One-Hot Encoded Input Matrix   | $\\mathbf{X}^{\\text{ohe}}$  | $T \\times V$          | Each row corresponds to a one-hot encoded vector representing a token in the sequence.   |\n",
    "| Embedding Matrix               | $\\mathbf{W}_e$             | $V \\times D$          | Each row is the embedding vector of the corresponding token in the vocabulary.           |\n",
    "| Embedded Input Matrix          | $\\mathbf{X}$               | $T \\times D$          | Each row is the embedding vector of the corresponding token in the input sequence.       |\n",
    "| Embedding Vector for Token $t$ | $\\mathbf{X}_t$             | $1 \\times D$          | The embedding vector for the token at position $t$ in the input sequence.                |\n",
    "| Batched Input Tensor           | $\\mathbf{X}^{\\mathcal{B}}$ | $B \\times T \\times D$ | A batched tensor containing $B$ input sequences, each sequence is of shape $T \\times D$. |\n",
    "\n",
    "More concretely, we create an embedding matrix $\\mathbf{W}_{e}$ of size\n",
    "$V \\times D$, where $V$ is the vocabulary size, $D$ is the dimensions of the\n",
    "embeddings, we would then matrix multiply $\\mathbf{X}^{\\text{ohe}}$ with\n",
    "$\\mathbf{W}_{e}$ to get the output tensor $\\mathbf{X}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\mathbf{X}^{\\text{ohe}} \\cdot \\mathbf{W}_{e}\n",
    "$$\n",
    "\n",
    "Indeed, we see that the result of `tok_embed(x)` is the same as the result of\n",
    "`x0_ohe @ W_e`. In other words, you can one hot encoded the input\n",
    "sequence $\\mathbf{x} = (x_1, x_2, \\ldots, x_T)$ and then matrix multiply it with\n",
    "the embedding matrix $\\mathbf{W}^{e}$ (via a linear layer) to get the same\n",
    "result as `tok_embed(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50257</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m50257\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W_e = tok_embed.embedding.weight.data # [V, D]\n",
    "x0_ohe: torch.Tensor = F.one_hot(x0, num_classes=composer.vocab_size).float() # [T, V]\n",
    "z0_tok_embed_matmul: torch.Tensor = x0_ohe @ W_e # [T, D]\n",
    "torch.testing.assert_close(z0_tok_embed, z0_tok_embed_matmul, rtol=0.0, atol=0.0, msg=\"The matrix multiplication is not correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Recall our tokenized sequence is\n",
    "    `[11503, 290, 21120, 30, 880, 788, 11, 29448]`.\n",
    "-   Converting it to one-hot encoding, we would have a matrix of size\n",
    "    `[8, 50257]` (or more generally `[B, T, V]` in the presence of batch size\n",
    "    `B`).\n",
    "-   Each row is a one-hot vector of the token $x_{t} \\in \\mathbb{R}^{V}$ at\n",
    "    position $t$. For example, the first row would be a one-hot vector of the\n",
    "    token `11503`, so every element in the first row is $0$ except for the\n",
    "    $11503$-th element, which is $1$.\n",
    "-   A minute quirk here is that the token $x_{t}$ exists in the **_continuous\n",
    "    space_** instead of the **_discrete space_**. This is because we have to\n",
    "    perform the dot product between the one-hot vector and the embedding vector,\n",
    "    which is a continuous vector. This is more of a data type coercion.\n",
    "    Therefore, in our code, we also converted the one-hot vector to `.float()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition\n",
    "\n",
    "The embedding matrix $\\mathbf{W}_{e}$ is structured as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{W}_e &= \\left[\\begin{array}{cccc}\n",
    "w_{1,1} & w_{1,2} & \\cdots & w_{1, D} \\\\\n",
    "w_{2,1} & w_{2,2} & \\cdots & w_{2, D} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{V, 1} & w_{V, 2} & \\cdots & w_{V, D}\n",
    "\\end{array}\\right] \\in \\mathbb{R}^{V \\times D}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "-   $\\mathbf{w}_j = (w_{j,1}, w_{j,2}, \\ldots, w_{j,D}) \\in \\mathbb{R}^{1 \\times D}$:\n",
    "    -   Each row vector $\\mathbf{w}_j$ of the matrix $\\mathbf{W}_e$ represents\n",
    "        the $D$-dimensional embedding vector for the $j$-th token in the\n",
    "        vocabulary $\\mathcal{V}$.\n",
    "    -   The subscript $j$ ranges from 1 to $V$, indexing the tokens.\n",
    "-   $V$ is the vocabulary size.\n",
    "-   $D$ is the hidden embedding dimension.\n",
    "\n",
    "Here is a visual representation of how each embedding vector is selected through\n",
    "matrix multiplication:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{X}^{\\text{ohe}} \\cdot \\mathbf{W}_{e} &=\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & \\cdots & 0 \\\\\n",
    "1 & 0 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & 1\n",
    "\\end{bmatrix}_{T \\times V}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & \\cdots & w_{1,D} \\\\\n",
    "w_{2,1} & w_{2,2} & \\cdots & w_{2,D} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{V,1} & w_{V,2} & \\cdots & w_{V,D}\n",
    "\\end{bmatrix}_{V \\times D} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "w_{2,1} & w_{2,2} & \\cdots & w_{2,D} \\\\\n",
    "w_{1,1} & w_{1,2} & \\cdots & w_{1,D} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{T,1} & w_{T,2} & \\cdots & w_{T,D}\n",
    "\\end{bmatrix}_{T \\times D}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Each row in the resulting matrix $\\mathbf{X}$ is the embedding of the\n",
    "corresponding token in the input sequence, picked directly from $\\mathbf{W}_e$\n",
    "by the one-hot vectors. In other words, the matrix $\\mathbf{W}_e$ can be\n",
    "visualized as a table where each row corresponds to a token's embedding vector:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cccc}\n",
    "\\text{Token Index} & \\text{Dimension 1} & \\text{Dimension 2} & \\cdots & \\text{Dimension } D \\\\\n",
    "\\hline\n",
    "1 & w_{1,1} & w_{1,2} & \\cdots & w_{1,D} \\\\\n",
    "2 & w_{2,1} & w_{2,2} & \\cdots & w_{2,D} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "V & w_{V,1} & w_{V,2} & \\cdots & w_{V,D} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "#### Lookup\n",
    "\n",
    "When the one-hot encoded input matrix $\\mathbf{X}^{\\text{ohe}}$ multiplies with\n",
    "the embedding matrix $\\mathbf{W}_e$, each row of $\\mathbf{X}^{\\text{ohe}}$\n",
    "effectively selects a corresponding row from $\\mathbf{W}_e$. This operation\n",
    "simplifies to row selection because each row of $\\mathbf{X}^{\\text{ohe}}$\n",
    "contains exactly one '1' and the rest are '0's.\n",
    "\n",
    "#### Semantic Representation\n",
    "\n",
    "Now each row of the output tensor, indexed by $t$, $\\mathbf{X}_{t, :}$: is the\n",
    "$D$ dimensional embedding vector for the token $x_t$ at the $t$-th position in\n",
    "the sequence. In this context, each token in the sequence is represented by a\n",
    "$D$ dimensional vector. So, the output tensor $\\mathbf{X}$ captures the dense\n",
    "representation of the sequence. Each token in the sequence is replaced by its\n",
    "corresponding embedding vector from the embedding matrix $\\mathbf{W}_{e}$. As\n",
    "before, the output tensor $\\mathbf{X}$ carries semantic information about the\n",
    "tokens in the sequence. The closer two vectors are in this embedding space, the\n",
    "more semantically similar they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embeddings\n",
    "\n",
    "For the lack of a better phrase, we say that self-attention, the core function\n",
    "of GPTs, is permutation invariant. While it is obvious that the input sequence\n",
    "$\\mathbf{x}$ is ordered in the sense that $x_1$ comes before $x_2$, and $x_2$\n",
    "comes before $x_3$, and so on, this information gets lost in the self-attention\n",
    "mechanism. This means that the model does not differentiate \"the cat ate the\n",
    "mouse\" from \"the mouse ate the cat\" as long as the tokens are the same - and\n",
    "this is not desirable.\n",
    "\n",
    "The dominant approach for preserving information about the order of tokens is to\n",
    "represent this to the model as an additional input associated with each token.\n",
    "These inputs are called positional encodings, and they can either be learned or\n",
    "fixed _a priori_ {cite}`zhang2023dive`. What this means is that we can either\n",
    "construct a learnable parameter that is updated during training, or we can\n",
    "construct a fixed parameter that is not updated during training. For the sake of\n",
    "completeness, we will discuss briefly the scheme where the positional encodings\n",
    "are fixed _a priori_ based on sinusoidal functions - which is also the scheme\n",
    "described in the paper \"Attention is All You Need\" {cite}`vaswani2017attention`.\n",
    "\n",
    "### Definition\n",
    "\n",
    "```{prf:definition} Positional Encoding\n",
    ":label: decoder-positional-encoding\n",
    "\n",
    "The positional encoding function\n",
    "$\\mathrm{PE}: \\mathbb{N} \\times \\mathbb{N} \\rightarrow \\mathbb{R}$ computes the\n",
    "position encoding for each position $p := t \\in \\mathbb{N}$ and each dimension\n",
    "$d = 1, 2, \\ldots, D \\in \\mathbb{N}$ in the input embedding space as follows {cite}`math11112451`:\n",
    "\n",
    "$$\n",
    "\\operatorname{PE}(p, d)= \\begin{cases}\\sin \\left(\\frac{p}{10000^{\\frac{2 d}{D}}}\\right) & \\text { if } d \\text { is even, } \\\\ \\cos \\left(\\frac{p}{10000^{\\frac{2 d-1}{D}}}\\right) & \\text { if } d \\text { is odd. }\\end{cases}\n",
    "$$\n",
    "```\n",
    "\n",
    "Note that $10,000$ is an parameter that can be tuned.\n",
    "\n",
    "Thus, the entire tensor $\\mathbf{P}$ is defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{P} = \\begin{bmatrix} p_{1,1} & p_{1,2} & \\cdots & p_{1,D} \\\\ p_{2,1} & p_{2,2} & \\cdots & p_{2,D} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ p_{L,1} & p_{L,2} & \\cdots & p_{L,D} \\end{bmatrix} \\in \\mathbb{R}^{L \\times D}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "-   $L$: is the sequence length.\n",
    "-   $D$: is the embedding dimension.\n",
    "-   $p_{i, d}$: is the element at position $i, d$ in the tensor\n",
    "    $\\mathbf{P}$.\n",
    "\n",
    "Note that $\\mathbf{P}$ is independent of $\\mathbf{Z}$, and it's computed\n",
    "based on the positional encoding formula used in transformers, which uses\n",
    "sinusoidal functions of different frequencies.\n",
    "\n",
    "### More Intuition on Positional Encodings (Pretty bad analogy, to revise)\n",
    "\n",
    "Consider a sentence \"Mary visited the museum in Paris.\"\n",
    "\n",
    "The original word embeddings ($\\mathbf{Z}$) can be thought of as guests at a\n",
    "party, where each guest represents a word: 'Mary,' 'visited,' 'the,' 'museum,'\n",
    "'in,' and 'Paris.' The personality of each guest represents the semantic meaning\n",
    "of the word they represent.\n",
    "\n",
    "Now, let's think of the positional encoding ($\\mathbf{P}$) as the order in which\n",
    "these guests arrived at the party. This captures the positional information in\n",
    "the sentence. 'Mary' was the first to arrive, 'visited' the second, and so on.\n",
    "\n",
    "When we add the positional encoding to the original embeddings\n",
    "($\\mathbf{Z} = \\mathbf{Z} + \\mathbf{P}$), we're basically combining the\n",
    "personality of each guest (the meaning of the word) with their arrival order at\n",
    "the party (their position in the sentence).\n",
    "\n",
    "So, for example, 'Mary' isn't just represented as a proper noun referring to a\n",
    "person, but she's also identified as the first person who arrived at the party.\n",
    "Similarly, 'Paris' isn't just a proper noun referring to a city, but it's the\n",
    "last entity that arrived.\n",
    "\n",
    "This way, the combined tensor ($\\mathbf{Z} = \\mathbf{Z} + \\mathbf{P}$) captures\n",
    "both the semantic meaning and the positional information for each word, allowing\n",
    "the Transformer to understand both the meaning of the words and their order in\n",
    "the sentence. This is crucial because, in many languages, changing the order of\n",
    "words can drastically alter the meaning of a sentence.\n",
    "\n",
    "### An Example of Positional Encoding\n",
    "\n",
    "To demonstrate how positional encodings are calculated for an input sequence\n",
    "using the given formula, let's take the first three tokens from the example\n",
    "sequence `' priest and clerk? well then, amen'`. We'll assume these tokens are\n",
    "'priest', 'and', 'clerk' and that we're dealing with an embedding dimension\n",
    "$D = 4$ for simplicity. The positions $p$ of these tokens are 1, 2, and 3,\n",
    "respectively.\n",
    "\n",
    "For $D = 4$, each token's positional encoding will be a vector of 4 elements.\n",
    "Let's calculate the positional encodings for $p = 1, 2, 3$ (corresponding to\n",
    "'priest', 'and', 'clerk?') and for each dimension $d = 0, 1, 2, 3$:\n",
    "\n",
    "#### Positional Encoding for $p = 1$ ('priest')\n",
    "\n",
    "-   $d = 0$ (even): $\\sin\\left(\\frac{1}{10000^{0/4}}\\right) = \\sin(1)$\n",
    "-   $d = 1$ (odd):\n",
    "    $\\cos\\left(\\frac{1}{10000^{1/4}}\\right) =\n",
    "    \\cos\\left(\\frac{1}{\\sqrt[4]{10000}}\\right)$\n",
    "-   $d = 2$ (even):\n",
    "    $\\sin\\left(\\frac{1}{10000^{2/4}}\\right) =\n",
    "    \\sin\\left(\\frac{1}{\\sqrt{10000}}\\right)$\n",
    "-   $d = 3$ (odd):\n",
    "    $\\cos\\left(\\frac{1}{10000^{3/4}}\\right) =\n",
    "    \\cos\\left(\\frac{1}{\\sqrt[4]{10000^3}}\\right)$\n",
    "\n",
    "#### Positional Encoding for $p = 2$ ('and')\n",
    "\n",
    "-   $d = 0$ (even): $\\sin\\left(\\frac{2}{10000^{0/4}}\\right) = \\sin(2)$\n",
    "-   $d = 1$ (odd):\n",
    "    $\\cos\\left(\\frac{2}{10000^{1/4}}\\right) =\n",
    "    \\cos\\left(\\frac{2}{\\sqrt[4]{10000}}\\right)$\n",
    "-   $d = 2$ (even):\n",
    "    $\\sin\\left(\\frac{2}{10000^{2/4}}\\right) =\n",
    "    \\sin\\left(\\frac{2}{\\sqrt{10000}}\\right)$\n",
    "-   $d = 3$ (odd):\n",
    "    $\\cos\\left(\\frac{2}{10000^{3/4}}\\right) =\n",
    "    \\cos\\left(\\frac{2}{\\sqrt[4]{10000^3}}\\right)$\n",
    "\n",
    "#### Positional Encoding for $p = 3$ ('clerk?')\n",
    "\n",
    "-   $d = 0$ (even): $\\sin\\left(\\frac{3}{10000^{0/4}}\\right) = \\sin(3)$\n",
    "-   $d = 1$ (odd):\n",
    "    $\\cos\\left(\\frac{3}{10000^{1/4}}\\right) =\n",
    "    \\cos\\left(\\frac{3}{\\sqrt[4]{10000}}\\right)$\n",
    "-   $d = 2$ (even):\n",
    "    $\\sin\\left(\\frac{3}{10000^{2/4}}\\right) =\n",
    "    \\sin\\left(\\frac{3}{\\sqrt{10000}}\\right)$\n",
    "-   $d = 3$ (odd):\n",
    "    $\\cos\\left(\\frac{3}{10000^{3/4}}\\right) =\n",
    "    \\cos\\left(\\frac{3}{\\sqrt[4]{10000^3}}\\right)$\n",
    "\n",
    "The use of $10000^{(\\cdot/D)}$ with large exponents is designed to provide a\n",
    "wide range of frequencies, enabling the model to distinguish between positions\n",
    "at different scales. In practice, the positional encodings for high-dimensional\n",
    "embeddings (e.g., $D=512$ in real models) involve very small and precise values,\n",
    "facilitating the model's learning of complex positional relationships.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class PositionalEncoding(ABC, nn.Module):\n",
    "    def __init__(self, d_model: int, context_length: int, dropout: float = 0.0) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.context_length = context_length\n",
    "        self.dropout = nn.Dropout(p=dropout, inplace=False)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        ...\n",
    "\n",
    "\n",
    "class Sinusoid(PositionalEncoding):\n",
    "    P: torch.Tensor\n",
    "\n",
    "    def __init__(self, d_model: int, context_length: int, dropout: float = 0.0) -> None:\n",
    "        super().__init__(d_model, context_length, dropout)\n",
    "\n",
    "        P = self._init_positional_encoding()\n",
    "        self.register_buffer(\"P\", P, persistent=True)  # with this no need requires_grad=False\n",
    "\n",
    "    def _init_positional_encoding(self) -> torch.Tensor:\n",
    "        \"\"\"Initialize the positional encoding tensor.\"\"\"\n",
    "        P = torch.zeros((1, self.context_length, self.d_model))\n",
    "        position = self._get_position_vector()\n",
    "        div_term = self._get_div_term_vector()\n",
    "        P[:, :, 0::2] = torch.sin(position / div_term)\n",
    "        P[:, :, 1::2] = torch.cos(position / div_term)\n",
    "        return P\n",
    "\n",
    "    def _get_position_vector(self) -> torch.Tensor:\n",
    "        \"\"\"Return a vector representing the position of each token in a sequence.\"\"\"\n",
    "        return torch.arange(self.context_length, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    def _get_div_term_vector(self) -> torch.Tensor:\n",
    "        \"\"\"Return a vector representing the divisor term for positional encoding.\"\"\"\n",
    "        return torch.pow(\n",
    "            10000,\n",
    "            torch.arange(0, self.d_model, 2, dtype=torch.float32) / self.d_model,\n",
    "        )\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        z = self._add_positional_encoding(z)\n",
    "        z = self.dropout(z)\n",
    "        return z\n",
    "\n",
    "    def _add_positional_encoding(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Add the positional encoding tensor to the input tensor.\"\"\"\n",
    "        return z + self.P[:, : z.shape[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now do a sum operation between the output of the token embeddings $\\mathbf{Z}$\n",
    "and the positional encodings $\\mathbf{P}$ to get the final input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=composer.device)\n",
    "generator.manual_seed(25)\n",
    "\n",
    "pos_embed = Sinusoid(d_model=composer.d_model, context_length=composer.block_size, dropout=0.0)\n",
    "P = pos_embed.P\n",
    "\n",
    "z0_tok_embed_with_pos_embed = pos_embed(z0_tok_embed)\n",
    "z0_tok_embed_add_pos_embed = z0_tok_embed + P\n",
    "\n",
    "torch.testing.assert_close(z0_tok_embed_with_pos_embed, z0_tok_embed_add_pos_embed, rtol=0.0, atol=0.0) # just to show that adding P to the z0 is the same as pos_embed(z0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen earlier using manual calculations, the input sequence's first token/position at $t=1$\n",
    "has values of $[0, 1, 0, 1]$ for the positional encoding with $D=4$. We simply add this positional\n",
    "encoding to the token embeddings to get the final input embeddings. We can verify it visually below (or can add programmatically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(P)\n",
    "pprint(z0_tok_embed)\n",
    "pprint(z0_tok_embed_with_pos_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `forward` method of the `PositionalEncoding` class, the positional\n",
    "encoding is added to the input `X`:\n",
    "\n",
    "```python\n",
    "def forward(self, X):\n",
    "    X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "    return self.dropout(X)\n",
    "```\n",
    "\n",
    "This method slices the precalculated positional encodings tensor `self.P` to\n",
    "match the sequence length of `X`, adds it to `X`, and then applies dropout. The\n",
    "result, which is the sum of the original embeddings and the positional\n",
    "encodings, is returned. So there's no need to add the positional encodings to\n",
    "`X` outside of this class.\n",
    "\n",
    "So when you call `positional_encoding(Z)`, it adds the positional encodings to\n",
    "`Z` and applies dropout, then returns the result. You could store this result in\n",
    "`Z_with_pos_encodings` or just overwrite it as `Z`:\n",
    "\n",
    "```python\n",
    "Z = positional_encoding(Z)\n",
    "```\n",
    "\n",
    "Now, `Z` contains the original embeddings with the positional encodings added\n",
    "and dropout applied.\n",
    "\n",
    "In the context of our \"hello bot\" example, the original tensor `Z` represented\n",
    "the word embeddings, where each token in the sequence (i.e., `SOS`, `hello`,\n",
    "`bot`, `EOS`) was converted into a 2-dimensional vector capturing the semantic\n",
    "meaning of each token. After adding positional encoding, the new tensor\n",
    "represents both the semantic and positional information of each token in the\n",
    "sequence.\n",
    "\n",
    "-   The first row (`[1.1103, -0.6898]`) now encapsulates both the meaning of the\n",
    "    `SOS` token and the information that it's the first token in the sequence.\n",
    "\n",
    "-   The second row (`[0.0756, -0.2103]`) is now a representation of the word\n",
    "    `hello` that carries not just its semantics (e.g., being a greeting), but\n",
    "    also the information that it's the second word in the sentence.\n",
    "\n",
    "-   The third row (`[2.2618, 0.2702]`) likewise carries both the semantics of\n",
    "    `bot` (likely related to AI or technology), and its position as the third\n",
    "    word in the sentence.\n",
    "\n",
    "-   The last row (`[-0.8478, -0.0320]`) encapsulates the semantics of `EOS`\n",
    "    token, signifying end of a sentence, and the fact that it's the last token\n",
    "    in the sentence.\n",
    "\n",
    "The idea here is that in natural language, word order matters. The sentence\n",
    "\"hello bot\" is not the same as \"bot hello\" (okay maybe it is the same in this\n",
    "example, a better one is cat eat mouse isn't the same as mouse eat cat).\n",
    "\n",
    "So, in a language model, we want our representations to capture not just what\n",
    "words mean, but also where they are in a sentence. Positional encoding is a\n",
    "technique to achieve this goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Positional Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embed_visual = Sinusoid(d_model=48, context_length=96)\n",
    "\n",
    "P_visual = pos_embed_visual.P.squeeze().T.cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 3))\n",
    "pos = ax.imshow(P_visual, cmap=\"RdGy\", extent=(1, P_visual.shape[1] + 1, P_visual.shape[0] + 1, 1))\n",
    "fig.colorbar(pos, ax=ax)\n",
    "ax.set_xlabel(\"Position in sequence\")\n",
    "ax.set_ylabel(\"Hidden dimension\")\n",
    "ax.set_title(\"Positional encoding over hidden dimensions\")\n",
    "ax.set_xticks([1] + [i * 10 for i in range(1, 1 + P_visual.shape[1] // 10)])\n",
    "ax.set_yticks([1] + [i * 10 for i in range(1, 1 + P_visual.shape[0] // 10)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positional encodings are depicted through sine and cosine functions, each\n",
    "varying in wavelength across the hidden dimensions, to uniquely represent each\n",
    "position. By examining these functions within individual hidden dimensions, we\n",
    "gain deeper insights into the encoding patterns. Here, we present a\n",
    "visualization of the positional encodings across hidden dimensions $d = 0, 1,\n",
    "2, 3$ for the initial $16$ sequence positions {cite}`lippe2023uvadlc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_positional_encoding(pe: np.ndarray, block_size: int, figsize: Tuple[int, int] = (12, 4)) -> None:\n",
    "    \"\"\"Plot positional encoding for each hidden dimension.\n",
    "\n",
    "    Args:\n",
    "        pe: Positional encoding array.\n",
    "        composer_block_size: Block size of the composer.\n",
    "        figsize: Figure size for the plot.\n",
    "    \"\"\"\n",
    "    sns.set_theme()\n",
    "    fig, ax = plt.subplots(2, 2, figsize=figsize)\n",
    "    ax = [a for a_list in ax for a in a_list]\n",
    "\n",
    "    for i, a in enumerate(ax):\n",
    "        a.plot(np.arange(1, block_size + 1), pe[i, :block_size], color=f'C{i}', marker=\"o\", markersize=6, markeredgecolor=\"black\")\n",
    "        a.set_title(f\"Encoding in hidden dimension d={i+1}\")\n",
    "        a.set_xlabel(\"Position in sequence\", fontsize=10)\n",
    "        a.set_ylabel(\"Positional encoding\", fontsize=10)\n",
    "        a.set_xticks(np.arange(1, 17))\n",
    "        a.tick_params(axis='both', which='major', labelsize=10)\n",
    "        a.tick_params(axis='both', which='minor', labelsize=8)\n",
    "        a.set_ylim(-1.2, 1.2)\n",
    "\n",
    "    fig.subplots_adjust(hspace=0.8)\n",
    "    sns.reset_orig()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_positional_encoding(P_visual, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the patterns between the hidden dimension 1 and 2 only differ in\n",
    "the starting angle. The wavelength is $2\\pi$ , hence the repetition after\n",
    "position 6 . The hidden dimensions 2 and 3 have about twice the wavelength {cite}`lippe2023uvadlc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encodings via Embeddings\n",
    "\n",
    "In practice, the positional encodings are learned as part of the GPT-2 {cite}`radford2019language`.\n",
    "So we can replicate the same by using a `nn.Embedding` layer in PyTorch as in the token embeddings.\n",
    "\n",
    "$\\mathbf{W}_{p}$ is the positional embedding matrix. Each row of this matrix\n",
    "corresponds to the embedding of a position in a sequence. This matrix is usually\n",
    "of size $T \\times D$, where $T$ is the maximum length of a sequence we allow in\n",
    "the model, and $D$ is the dimension of the embedding space.\n",
    "\n",
    "In other words, the $\\mathbf{P}$ matrix introduced earlier has the same shape as\n",
    "$\\mathbf{W}_{p}$, and while the former is fixed, the latter is learned during\n",
    "the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model: int, context_length: int) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=context_length, embedding_dim=d_model)\n",
    "\n",
    "    def forward(self, positions: torch.Tensor) -> torch.Tensor:\n",
    "        return self.embedding(positions.to(device=self.embedding.weight.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=composer.device)\n",
    "generator.manual_seed(25)\n",
    "seed_all(composer.seed, seed_torch=True, set_torch_deterministic=False)\n",
    "\n",
    "pos_embed = PositionalEmbedding(d_model=composer.d_model, context_length=composer.block_size)\n",
    "positions = torch.arange(start=0, end=composer.block_size, dtype=torch.long) # shape (t)\n",
    "z0_pos_embed = pos_embed(positions)\n",
    "\n",
    "z0_tok_embed_with_pos_embed = z0_tok_embed + z0_pos_embed\n",
    "pprint(z0_tok_embed_with_pos_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we would have wrapped up the first two layers, where we first pass\n",
    "an input sequence $\\mathbf{x}$ through the token embedding layer to obtain the\n",
    "token embeddings $\\mathbf{Z} = \\mathbf{W}_{e} \\mathbf{x}$, and then add the\n",
    "positional embeddings to the token embeddings to obtain the final embeddings.\n",
    "\n",
    "The process to encode position into the embeddings is:\n",
    "\n",
    "Given an input sequence $\\mathbf{x} = \\left(x_1, x_2, ..., x_{T}\\right)$, where\n",
    "$x_t$ is the token at position $t$ in the sequence, we have transformed the\n",
    "input sequence into a sequence of token embeddings $\\mathbf{Z}$, holding both\n",
    "the static semantics and the positional information of the input sequence.\n",
    "\n",
    "$$\n",
    "\\mathbf{Z} = \\mathbf{W}_{e} \\mathbf{x} + \\mathbf{W}_{p} \\in \\mathbb{R}^{T \\times D}\n",
    "$$\n",
    "\n",
    "And note this is only for $1$ sequence, and we can extend this to $\\mathcal{B}$\n",
    "sequences in a batch.\n",
    "\n",
    "```{admonition} References\n",
    ":class: seealso\n",
    "\n",
    "-   [Positional Encodings - Aman Chadha](https://aman.ai/primers/ai/transformers/#positional-encoding)\n",
    "-   [Self-Attention and Positional Encoding - Dive into Deep Learning](https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html)\n",
    "-   [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\n",
    "-   [Positional Encoding - Brandon Rohrer](https://e2eml.school/transformers.html#positional_encoding)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "### Definition\n",
    "\n",
    "Layer normalization is a technique applied in the context of neural networks to\n",
    "stabilize the learning process by normalizing the inputs across the features for\n",
    "each token in a sequence. Given a data representation\n",
    "$\\mathbf{Z} \\in \\mathbb{R}^{T \\times D}$, where $T$ is the sequence length\n",
    "(number of tokens) and $D$ is the hidden dimension (feature space), layer\n",
    "normalization is applied **independently** to each vector (or token) across the\n",
    "**feature** dimension $D$. You can think of each token $t=1, \\ldots, T$ as a\n",
    "separate example, and $\\mathbf{Z}_{t}$ represents each row/token. We then\n",
    "compute the mean and variance for each row/token and then apply the\n",
    "normalization to each row/token. This process is repeated for each row/token in\n",
    "the input matrix $\\mathbf{Z}$.\n",
    "\n",
    "When considering a batch of such sequences, represented as\n",
    "$\\mathbf{Z}^{\\mathcal{B}} \\in \\mathbb{R}^{\\mathcal{B} \\times T \\times D}$, where\n",
    "$\\mathcal{B}$ is the batch size, layer normalization still focuses on\n",
    "normalizing each token vector within each sequence in the batch. The operation\n",
    "does not aggregate or normalize across different tokens ($T$ dimension) or\n",
    "different sequences in the batch ($\\mathcal{B}$ dimension); instead, it\n",
    "normalizes the values across the features ($D$ dimension) for each token.\n",
    "\n",
    "For a single token $\\mathbf{Z}_t \\in \\mathbb{R}^{1 \\times D}$ in a sequence\n",
    "$\\mathbf{Z} \\in \\mathbb{R}^{T \\times D}$, the the normalization process involves\n",
    "subtracting the mean $\\mu_t$ and dividing by the standard deviation $\\sigma_t$\n",
    "(adjusted with a small constant $\\epsilon$ for numerical stability) of its\n",
    "features. This process ensures that, for each token, the features are centered\n",
    "around zero with a unit variance.\n",
    "\n",
    "```{prf:definition} Layer Normalization\n",
    ":label: decoder-layer-normalization\n",
    "\n",
    "Given a token $\\mathbf{Z}_t \\in \\mathbb{R}^{1 \\times D}$ from the sequence\n",
    "$\\mathbf{Z} \\in \\mathbb{R}^{T \\times D}$, the normalized output\n",
    "$\\overline{\\mathbf{Z}}_t \\in \\mathbb{R}^{1 \\times D}$ for this token can be\n",
    "expressed as follows:\n",
    "\n",
    "$$\n",
    "\\overline{\\mathbf{Z}}_{t} = \\frac{\\mathbf{Z}_{t} - \\mu_{t}}{\\sqrt{\\sigma_{t}^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "-   $\\mathbf{Z}_t$ is the vector of features for the token at position $t$,\n",
    "-   $\\mu_t \\in \\mathbb{R}$ is the mean of the features for this token,\n",
    "-   $\\sigma_t^2 \\in \\mathbb{R}$ is the variance of the features for this token,\n",
    "-   $\\epsilon \\in \\mathbb{R}$ is a small constant added for numerical stability,\n",
    "    ensuring that we never divide by zero or approach zero in the denominator.\n",
    "\n",
    "The mean $\\mu_t$ and variance $\\sigma_t^2$ are computed as follows:\n",
    "\n",
    "$$\n",
    "\\mu_t \\stackrel{\\text { def }}{=} \\frac{1}{D} \\sum_{d=1}^D Z_{t d}, \\quad \\sigma_t^2 \\stackrel{\\text { def }}{=} \\frac{1}{D} \\sum_{d=1}^D\\left(Z_{t d}-\\mu_t\\right)^2\n",
    "$$\n",
    "\n",
    "Here, $\\mathbf{Z}_{td}$ represents the $d$-th feature of the token at position\n",
    "$t$. The division and subtraction are applied element-wise across the feature\n",
    "dimension $D$ for the token, normalizing each feature based on the statistics of\n",
    "the features within the same token.\n",
    "```\n",
    "\n",
    "Since layer normalization is performed for each token vector across the feature\n",
    "dimension $D$, the process can be vectorized and applied simultaneously to all\n",
    "$T$ token vectors in the sequence.\n",
    "\n",
    "-   For each token $t = 1, \\ldots, T$, the mean $\\mu_t$ and variance\n",
    "    $\\sigma_t^2$ are computed.\n",
    "-   Each token vector $\\mathbf{Z}_t$ is then normalized using its respective\n",
    "    $\\mu_t$ and $\\sigma_t^2$.\n",
    "\n",
    "This results in a normalized sequence\n",
    "$\\overline{\\mathbf{Z}} \\in \\mathbb{R}^{T \\times D}$, where each token vector\n",
    "$\\overline{\\mathbf{Z}}_t$ has been normalized independently. The normalized\n",
    "sequence retains its original shape $(T \\times D)$.\n",
    "\n",
    "When considering a batch of sequences, represented as\n",
    "$\\mathbf{Z}^{\\mathcal{B}} \\in \\mathbb{R}^{\\mathcal{B} \\times T \\times D}$, the\n",
    "layer normalization process extends naturally:\n",
    "\n",
    "-   The normalization process is applied independently to each token vector in\n",
    "    each sequence within the batch. This means for each sequence $b$ in the\n",
    "    batch $\\mathcal{B}$, and for each token $t$ in each sequence, the process\n",
    "    computes $\\mu_{bt}$ and $\\sigma_{bt}^2$, and normalizes each\n",
    "    $\\mathbf{Z}_{bt}$ accordingly.\n",
    "-   Since the operation is independent across tokens and sequences, it can be\n",
    "    parallelized, allowing for efficient computation over the entire batch.\n",
    "\n",
    "The result is a batch of normalized sequences,\n",
    "$\\overline{\\mathbf{Z}}^{\\mathcal{B}} \\in \\mathbb{R}^{\\mathcal{B} \\times T \\times D}$,\n",
    "where each token vector $\\overline{\\mathbf{Z}}_{bt}$ in each sequence of the\n",
    "batch has been normalized based on its own mean and variance.\n",
    "\n",
    "```{admonition} Broadcasting\n",
    ":class: tip\n",
    "\n",
    "It is worth noting that the notation above involves broadcasting, we are\n",
    "essentially subtracting a scalar value ($\\mu_t$) from a vector ($\\mathbf{Z}_t$)\n",
    "and dividing by another scalar value ($\\sigma_t$). This is fine in practice, as\n",
    "the scalar values are broadcasted to match the shape of the vector during the\n",
    "element-wise operations.\n",
    "\n",
    "We can however make the definition clearer by removing the implicit\n",
    "broadcasting, and say that for each activation $Z_{td}$ (feature $d$ of a token\n",
    "at position $t$), we compute the normalized activation $\\overline{Z}_{td}$\n",
    "\n",
    "$$\n",
    "\\overline{\\mathbf{Z}}_{td} = \\frac{\\mathbf{Z}_{td} - \\mu_{t}}{\\sqrt{\\sigma_{t}^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "where $\\mu_t$ and $\\sigma_t^2$ are computed as before.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=composer.device)\n",
    "generator.manual_seed(25)\n",
    "seed_all(composer.seed, seed_torch=True, set_torch_deterministic=False)\n",
    "\n",
    "B, T, D = 2, 3, 4\n",
    "embedding = torch.randn(B, T, D)\n",
    "\n",
    "first_sequence_ = embedding[0]\n",
    "\n",
    "first_sequence_mean = torch.empty_like(first_sequence_)\n",
    "first_sequence_var = torch.empty_like(first_sequence_)\n",
    "\n",
    "for index, token in enumerate(first_sequence_):\n",
    "\n",
    "    first_sequence_mean[index, :] = torch.mean(token, dim=-1)\n",
    "    first_sequence_var[index, :] = torch.var(token, dim=-1, unbiased=False)\n",
    "\n",
    "pprint(first_sequence_mean)\n",
    "pprint(first_sequence_var)\n",
    "\n",
    "first_sentence_norm = (first_sequence_ - first_sequence_mean) / torch.sqrt(first_sequence_var)\n",
    "pprint(first_sentence_norm)\n",
    "\n",
    "layer_norm = nn.LayerNorm(normalized_shape=D, eps=0)\n",
    "\n",
    "normalized_embedding = layer_norm(embedding)\n",
    "pprint(normalized_embedding.shape)\n",
    "pprint(normalized_embedding)\n",
    "\n",
    "torch.testing.assert_close(first_sentence_norm, normalized_embedding[0], rtol=1e-5, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that indeed the assertion passed, and our calculations are correct. Note we must\n",
    "set `unbiased=False` in the `torch.var` function to get the same result as the\n",
    "`LayerNorm` function because we are using population variance formula.\n",
    "\n",
    "We can further confirm below now the mean and variance close to 0 and 1 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.mean(normalized_embedding, dim=-1)\n",
    "std = torch.std(normalized_embedding, dim=-1)\n",
    "\n",
    "\n",
    "print(\"\\nExample of mean and std for a single sentence across embedding dimensions:\")\n",
    "print(\"Mean:\", mean[0])\n",
    "print(\"Standard deviation:\", std[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learnable Affine Transformation\n",
    "\n",
    "Normalizing the activations to have zero mean and unit variance can limit the\n",
    "representational power of the network, and thus after computing the normalized\n",
    "features $\\hat{\\mathbf{Z}}_t$ for each token, we introduce a **_learnable_**\n",
    "affine transformation (scaling and shifting), in terms of parameters $\\gamma$\n",
    "and $\\beta$, which are of the same dimensionality as the feature space $D$, to\n",
    "scale and shift the normalized features, allowing the model to \"undo\" the\n",
    "normalization if it is beneficial for the learning process.\n",
    "\n",
    "$$\n",
    "\\overline{\\mathbf{Z}}_t = \\dfrac{\\mathbf{Z}_t - \\mu_t}{\\sqrt{\\sigma_t^2 + \\epsilon}} \\odot \\gamma + \\beta\n",
    "$$\n",
    "\n",
    "where $\\overline{\\mathbf{Z}}_t$ represents the output of the layer normalization\n",
    "for the token at position $t$, and $\\odot$ denotes element-wise multiplication.\n",
    "And for each activation $\\mathbf{Z}_{td}$, we have:\n",
    "\n",
    "$$\n",
    "\\overline{\\mathbf{Z}}_{t d}=\\frac{\\mathbf{Z}_{t d}-\\mu_t}{\\sqrt{\\sigma_t^2+\\epsilon}} \\cdot \\gamma_d + \\beta_d\n",
    "$$\n",
    "\n",
    "where $\\gamma_d$ and $\\beta_d$ are the scaling and shifting parameters for the\n",
    "$d$-th feature of the token at position $t$. However notice that I did not\n",
    "index $\\gamma$ and $\\beta$ by $t$ because they are shared across all tokens in\n",
    "the sequence. \n",
    "\n",
    "The notation $\\gamma_{d}$ without indexing $t$ implies that the scaling\n",
    "parameter $\\gamma$ is feature-specific but shared across all tokens in the\n",
    "sequence. It means that each feature dimension $d$ across all tokens $t$ in the\n",
    "sequence has its own unique scaling parameter, but this parameter does not\n",
    "change with different tokens. This is the common setup in layer normalization,\n",
    "where $\\gamma$ and $\\beta$ parameters are learned for each feature dimension $D$\n",
    "and are applied identically across all tokens $T$.\n",
    "\n",
    "Overall, layer norm is just taking each row in $\\mathbf{Z}$, sum all $D$\n",
    "elements in the row, and then calculate the mean and variance. Then, we subtract\n",
    "the mean from each element in the row, divide by the standard deviation, and\n",
    "then scale and shift the result using $\\gamma$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.types import _device, _dtype\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    __constants__ = [\"normalized_shape\", \"eps\", \"elementwise_affine\"]\n",
    "\n",
    "    normalized_shape: Union[int, Tuple[int, ...]]\n",
    "    eps: float\n",
    "    elementwise_affine: bool\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        normalized_shape: Union[int, Tuple[int, ...]],\n",
    "        eps: float = 1e-5,\n",
    "        elementwise_affine: bool = True,\n",
    "        device: Optional[Union[_device, str, None]] = None,\n",
    "        dtype: Optional[_dtype] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "\n",
    "        if self.elementwise_affine:\n",
    "            self.gamma = nn.Parameter(torch.empty(self.normalized_shape, **factory_kwargs))  # type: ignore[arg-type]\n",
    "            self.beta = nn.Parameter(torch.empty(self.normalized_shape, **factory_kwargs))  # type: ignore[arg-type]\n",
    "        else:\n",
    "            self.register_parameter(\"gamma\", None)\n",
    "            self.register_parameter(\"beta\", None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        if self.elementwise_affine:\n",
    "            nn.init.ones_(self.gamma)\n",
    "            nn.init.zeros_(self.beta)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True, unbiased=False)\n",
    "        if self.elementwise_affine:\n",
    "            return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "        return (x - mean) / (std + self.eps)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return \"{normalized_shape}, eps={eps}, elementwise_affine={elementwise_affine}\".format(**self.__dict__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=composer.device)\n",
    "generator.manual_seed(25)\n",
    "seed_all(composer.seed, seed_torch=True, set_torch_deterministic=False)\n",
    "\n",
    "ln1 = LayerNorm(normalized_shape=composer.d_model, eps=1e-5, elementwise_affine=True)\n",
    "z0_tok_embed_with_pos_embed_ln1 = ln1(z0_tok_embed_with_pos_embed)\n",
    "pprint(z0_tok_embed_with_pos_embed_ln1)\n",
    "\n",
    "ln1_pytorch = nn.LayerNorm(normalized_shape=composer.d_model, eps=1e-5, elementwise_affine=True)\n",
    "z0_tok_embed_with_pos_embed_ln1_pytorch = ln1_pytorch(z0_tok_embed_with_pos_embed)\n",
    "pprint(z0_tok_embed_with_pos_embed_ln1_pytorch)\n",
    "\n",
    "torch.testing.assert_close(z0_tok_embed_with_pos_embed_ln1, z0_tok_embed_with_pos_embed_ln1_pytorch, rtol=1e-5, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Norm Stabilises Activation Distributions\n",
    "\n",
    "Besides the known fact that layer normalization enables convergence and provdies\n",
    "regularization {cite}`lippe2023uvadlc`, it also stabilizes the distributions of\n",
    "activations {cite}`zhang2023dive`. Training deep neural networks are\n",
    "challenging, loss can easily be exploded or vanished, and the gradients can be\n",
    "unstable. One simple way is to ensure each layer's activation has a similar\n",
    "distribution - the intuition is that if each layer's activation has a similar\n",
    "distribution, then the gradients will also have a similar distribution, and this\n",
    "will stabilize the training process. Layer normalization is one of the\n",
    "techniques that can help to achieve this.\n",
    "\n",
    "```{admonition} References\n",
    ":class: seealso\n",
    "\n",
    "-   [Layer Normalization - Dive into Deep Learning](https://d2l.ai/chapter_convolutional-modern/batch-norm.html#layer-normalization)\n",
    "-   [Residual Connection and Layer Normalization - Dive into Deep Learning](https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#residual-connection-and-layer-normalization)\n",
    "-   [Layer Normalization - arXiv](https://arxiv.org/abs/1607.06450)\n",
    "-   [PyTorch Documentation: torch.nn.LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Connection\n",
    "\n",
    "I have written a more detailed post on the intuition of\n",
    "[ResNet](https://gao-hongnan.github.io/gaohn-galaxy/deep_learning/computer_vision/modern_convolutional_neural_networks/resnets/concept.html)\n",
    "which is heavily adapted from the chapter\n",
    "[Residual Networks (ResNet) and ResNeXt](https://d2l.ai/chapter_convolutional-modern/resnet.html)\n",
    "from the Dive into Deep Learning book.\n",
    "\n",
    "For the sake of intuition, we can think of the residual connection as a way to\n",
    "ensure that the original input to a layer is not lost as it passes through the\n",
    "model layers.\n",
    "\n",
    "-   Deep neural networks are known to suffer from the vanishing gradient\n",
    "    problem, where gradients become increasingly small as they are\n",
    "    backpropagated through the layers during training. Since we are\n",
    "    backpropagating backwards, the earlier layers are therefore more susceptible\n",
    "    to this problem. The weak gradient signal could often be close to $0$, and\n",
    "    this could lead to the model not learning well. Consequently, we mitigate\n",
    "    this problem by adding the original input to the output of the layer, so\n",
    "    that the gradient signal has a direct path to flow through the network.\n",
    "\n",
    "    -   Furthermore, Eugene Yan's blog post\n",
    "        [Some Intuition on Attention and the Transformer](https://eugeneyan.com/writing/attention/)\n",
    "        also highlighted that attention acting as a filtering mechanism may\n",
    "        block information from passing through, directly resulting flat\n",
    "        gradients as a small change to the inputs of the attention layer may not\n",
    "        change the outputs that much. Skip (residual) connections help resolve\n",
    "        this.\n",
    "\n",
    "-   We will see later that Multi-Head Attention mechanism operates on a set of\n",
    "    tokens, instead of over a sequence. We encode positional information into\n",
    "    the tokens, but there is a risk that the positional information is lost in\n",
    "    the multi-head attention layers. The residual connection helps to ensure\n",
    "    that the positional information is not lost {cite}`lippe2023uvadlc`.\n",
    "-   The other well known property of the residual connection is that it helps to\n",
    "    learn the identity function. Perhaps the scenario is that the best thing a\n",
    "    layer or a series of layer can learn is itself - and we don't actually want\n",
    "    an update.\n",
    "\n",
    "We quote the following from the Dive into Deep Learning book:\n",
    "\n",
    "Consider $\\mathcal{F}$, the class of functions that a specific network\n",
    "architecture (together with learning rates and other hyperparameter settings)\n",
    "can reach. That is, for all $f \\in \\mathcal{F}$ there exists some set of\n",
    "parameters (e.g., weights and biases) that can be obtained through training on a\n",
    "suitable dataset. Let us assume that $f^*$ is th \"truth\" function that we really\n",
    "would like to find. If it is in $\\mathcal{F}$, we are in good shape but\n",
    "typically we will not b quite so lucky. Instead, we will try to find some\n",
    "$f_{\\mathcal{F}}^*$ which is our best bet within $\\mathcal{F}$. For instance,\n",
    "given a dataset with features $\\mathbf{X}$ and labels $\\mathbf{y}$, we might try\n",
    "finding it by solving the following optimization problem:\n",
    "\n",
    "$$\n",
    "f_{\\mathcal{F}}^* \\stackrel{\\text { def }}{=} \\underset{f}{\\operatorname{argmin}} L(\\mathbf{X}, \\mathbf{y}, f) \\text { subject to } f \\in \\mathcal{F} .\n",
    "$$\n",
    "\n",
    "It is only reasonable to assume that if we design a different and more powerful\n",
    "architecture $\\mathcal{F}^{\\prime}$ we should arrive at a better outcome. In\n",
    "other words, we would expect that $f_{\\mathcal{F}}^*$ is \"better\" than\n",
    "$f_{\\mathcal{F}}^*$. However, if $\\mathcal{F} \\nsubseteq \\mathcal{F}^{\\prime}$\n",
    "there is no guarantee that this should even happen. In fact,\n",
    "$f_{\\mathcal{F}^{\\prime}}^*$ might well be worse.\n",
    "\n",
    "As illustrated by :numref: fig_functionclasses, for non-nested function classes,\n",
    "a larger function class does not always move closer to the \"truth\" function\n",
    "$f^*$. For instance, on the left of :numref: fig_functionclasses, though\n",
    "$\\mathcal{F}_3$ is closer to $f^*$ than $\\mathcal{F}_1, \\mathcal{F}_6$ moves\n",
    "away and there is no guarantee that further increasing the complexity can reduce\n",
    "the distance from $f^*$. With nested function classes where\n",
    "$\\mathcal{F}_1 \\subseteq \\ldots \\subseteq \\mathcal{F}_6$ on the right of\n",
    ":numref: fig_functionclasses, we can avoid the aforementioned issue from the\n",
    "non-nested function classes.\n",
    "\n",
    "```{figure} ./assets/d2l-resnet-functionclasses.svg\n",
    "---\n",
    "name: d2l-resnet-functionclasses\n",
    "---\n",
    "\n",
    "For non-nested function classes, a larger (indicated by area) function class\n",
    "does not guarantee we will get closer to the \"truth\" function $f^*$. This\n",
    "does not happen for nested function classes.\n",
    "\n",
    "**Image Credit:**\n",
    "[8.6. Residual Networks (ResNet) and ResNeXt - Dive Into Deep Learing](https://d2l.ai/chapter_convolutional-modern/resnet.html)\n",
    "```\n",
    "\n",
    "### LayerNorm and Residual Connection\n",
    "\n",
    "In our context, we would have the following for GPT-1 and older Transformers:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{AddNorm}(\\mathbf{x}) &= \\operatorname{LayerNorm}(\\mathbf{x} + \\operatorname{Sublayer}(\\mathbf{x})) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where the layernorm is applied after the residual connection. In other words,\n",
    "if `Sublayer` is a function that represents a sublayer (e.g., `MultiHeadAttention`),\n",
    "then in each sub-block of the decoder, we would compute the output and then add\n",
    "the original input to the output, and then apply layer normalization to the sum.\n",
    "\n",
    "However, in GPT-2, there is a modification, more concretely, to shift the layer normalization\n",
    "to the input of the sub-block. This means now instead of applying layer normalization\n",
    "after the residual connection, we apply it before the residual connection. For example,\n",
    "if `Sublayer` is a function that represents a sublayer (e.g., `MultiHeadAttention`),\n",
    "then in each sub-block of the decoder, we would first apply layer normalization to the\n",
    "input, then pass the normalized input to the sublayer, and then add the original input\n",
    "to the output of the sublayer.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{AddNorm}(\\mathbf{x}) &= \\mathbf{x} + \\operatorname{Sublayer}(\\operatorname{LayerNorm}(\\mathbf{x}))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Implementation of Residual Block and AddNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        sublayer: Callable[[torch.Tensor], torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        return x + sublayer(x)\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, feature_dim: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # fmt: off\n",
    "        self.dropout    = nn.Dropout(p=dropout, inplace=False)\n",
    "        self.layer_norm = LayerNorm(normalized_shape=feature_dim, eps=1e-5, elementwise_affine=True)\n",
    "        # fmt: on\n",
    "\n",
    "    def forward(self, x: torch.Tensor, sublayer: Callable[[torch.Tensor], torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"G(F(x) + x) where G = layer norm and F = sublayer\"\"\"\n",
    "        # FIXME: GPT-2 should be x + self.dropout(sublayer(self.layer_norm(x)))\n",
    "        output: torch.Tensor = self.layer_norm(x + sublayer(self.dropout(x)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "### Intuition of Attention Mechanism\n",
    "\n",
    "Attention is not a new concept, and one of the most influencial papers came from\n",
    "_Neural Machine Translation by Jointly Learning to Align and Translate_\n",
    "{cite}`bahdanau2014neural`, a paper published during 2014. In the context of our\n",
    "post, we would stick to one intuitive interpretation, that _the attention\n",
    "mechanism describes a **weighted average** of (sequence) elements with the\n",
    "weights **dynamically** computed based on an input query and elements’ keys_\n",
    "{cite}`lippe2023uvadlc`. In other words, we want contextually relevant\n",
    "information to be weighted more heavily than less relevant information. For\n",
    "example, the sentence _the cat walks by the river bank_ would require the word\n",
    "_bank_ to be weighted more heavily than the word _the_ when the word _cat_ is\n",
    "being processed. The dynamic portion is also important because this allows the\n",
    "model to adjust the weights based on an input sequence (note that the learned\n",
    "weights are static but the interaction with the input sequence is dynamic). When\n",
    "attending to the first token _cat_ in the sequence, we would want the token\n",
    "_cat_ to be a **weighted average** of all the tokens in the sequence, including\n",
    "itself. This is the essence of the self-attention mechanism.\n",
    "\n",
    "### Token Embedding and Vector Representation Process\n",
    "\n",
    "Given an input sequence $\\mathbf{x} = \\left(x_1, x_2, \\ldots, x_T\\right)$, where\n",
    "$T$ is the length of the sequence, and each $x_t \\in \\mathcal{V}$ is a token in\n",
    "the sequence, we use a generic embedding function $h_{\\text{emb}}$ to map each\n",
    "token to a vector representation in a continuous vector space:\n",
    "\n",
    "$$\n",
    "\\begin{aligned} h_{\\text{emb}} : \\mathcal{V} &\\rightarrow \\mathbb{R}^{D} \\\\ x_t\n",
    "&\\mapsto \\mathbf{z}_t \\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{V}$ is the vocabulary of tokens (discrete space $\\mathbb{Z}$),\n",
    "and $D$ is the dimension of the embedding space (continuous space). The output\n",
    "of the embedding function $h_{\\text{emb}}$ is a sequence of vectors\n",
    "$\\mathbf{Z} = \\left(\\mathbf{z}_1, \\mathbf{z}_2, \\ldots, \\mathbf{z}_T\\right)$,\n",
    "where each $\\mathbf{z}_t \\in \\mathbb{R}^{D}$ is the vector representation of the\n",
    "token $x_t$ in the sequence. As seen earlier, we represent the sequence of\n",
    "vectors $\\mathbf{Z}$ as a matrix $\\mathbf{Z} \\in \\mathbb{R}^{T \\times D}$, where\n",
    "each row of the matrix represents the vector representation of each token in the\n",
    "sequence.\n",
    "\n",
    "### Queries, Keys, and Values\n",
    "\n",
    "#### Database Analogy\n",
    "\n",
    "Let's draw an analogy to understand the concept of queries, keys, and values in\n",
    "the context of the attention mechanism. Consider a database $\\mathcal{D}$\n",
    "consisting of tuples of keys and values. For instance, the database\n",
    "$\\mathcal{D}$ might consist of tuples\n",
    "`{(\"Zhang\", \"Aston\"), (\"Lipton\", \"Zachary\"), (\"Li\", \"Mu\"), (\"Smola\", \"Alex\"), (\"Hu\", \"Rachel\"), (\"Werness\", \"Brent\")}`\n",
    "with the last name being the key and the first name being the value\n",
    "{cite}`zhang2023dive`. Operations on the database $\\mathcal{D}$ can be performed\n",
    "using queries $q$ that operate on the keys and values in the database. More\n",
    "concretely, if our query is \"Li\", or more verbosely, \"What is the first name\n",
    "associated with the last name Li?\", the answer would be \"Mu\" - the **key**\n",
    "associated with the **query** \"What is the first name associated with the last\n",
    "name Li?\" is \"Li\", and the **value** associated with the key \"Li\" is \"Mu\".\n",
    "Furthermore, if we also allowed for approximate matches, we would retrieve\n",
    "(\"Lipton\", \"Zachary\") instead.\n",
    "\n",
    "More rigorously, we denote\n",
    "$\\mathcal{D} \\stackrel{\\text { def }}{=}\\left\\{\\left(\\mathbf{k}_1, \\mathbf{v}_1\\right), \\ldots\\left(\\mathbf{k}_m, \\mathbf{v}_m\\right)\\right\\}$\n",
    "a database of $m$ tuples of _keys_ and _values_, as well as a query\n",
    "$\\mathbf{q}$. Then we can define the attention over $\\mathcal{D}$ as\n",
    "\n",
    "$$\n",
    "\\operatorname{Attention}(\\mathbf{q}, \\mathcal{D})\n",
    "\\stackrel{\\operatorname{def}}{=} \\sum_{t=1}^T \\alpha\\left(\\mathbf{q},\n",
    "\\mathbf{k}_t\\right) \\mathbf{v}_t\n",
    "$$\n",
    "\n",
    "where\n",
    "$\\alpha\\left(\\mathbf{q}, \\mathbf{k}_t\\right) \\in \\mathbb{R}(t=1, \\ldots, T)$ are\n",
    "scalar attention weights {cite}`zhang2023dive`. The operation itself is\n",
    "typically referred to as\n",
    "[_attention pooling_](https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-pooling.html).\n",
    "The term \"attention\" is used because this operation focuses specifically on\n",
    "those terms that have a substantial weight, denoted as $\\alpha$, meaning it\n",
    "gives more importance to these terms. Consequently, the attention over\n",
    "$\\mathcal{D}$ generates a linear combination of values contained in the\n",
    "database. In fact, this contains the above example as a special case where all\n",
    "but one weight is zero. Why so? Because the query is an exact match for one of\n",
    "the keys.\n",
    "\n",
    "To illustrate why in the case of an exact match within a database the attention\n",
    "weights ($\\alpha$) are all zero except for one, let's use the attention formula\n",
    "provided and consider a simplified example with vectors.\n",
    "\n",
    "```{prf:example} Exact Match Scenario\n",
    ":label: decoder-concept-attention-exact-match-scenario\n",
    "\n",
    "Imagine a simplified database $\\mathcal{D}$ consisting of 3 key-value pairs,\n",
    "where each key $\\mathbf{k}_t$ and the query $\\mathbf{q}$ are represented as\n",
    "vectors in some high-dimensional space, and the values $\\mathbf{v}_t$ are also\n",
    "vectors (or can be scalar for simplicity in this example). For simplicity, let's\n",
    "assume our vectors are in a 2-dimensional space and represent them as follows:\n",
    "\n",
    "-   Keys (representing $3$ keys in the database):\n",
    "    -   $\\mathbf{k}_1 = [1, 0]$,\n",
    "    -   $\\mathbf{k}_2 = [0, 1]$,\n",
    "    -   $\\mathbf{k}_3 = [1, 1]$\n",
    "-   Values (corresponding to the keys):\n",
    "    -   $\\mathbf{v}_1 = [0.1, 0.9]$,\n",
    "    -   $\\mathbf{v}_2 = [0.2, 0.8]$,\n",
    "    -   $\\mathbf{v}_3 = [0.3, 0.7]$\n",
    "-   Query (looking for an item/concept similar to $\\mathbf{k}_1$):\n",
    "    -   $\\mathbf{q} = [1, 0]$\n",
    "\n",
    "The attention weights $\\alpha(\\mathbf{q}, \\mathbf{k}_t)$ indicate how similar or\n",
    "relevant each key is to the query. In an exact match scenario, the similarity\n",
    "calculation will result in a high value (e.g., $1$) when the query matches a key\n",
    "exactly, and low values (e.g., $0$) otherwise. For simplicity, let's use a\n",
    "simple matching criterion where the weight is $1$ for an exact match and $0$\n",
    "otherwise:\n",
    "\n",
    "-   $\\alpha(\\mathbf{q}, \\mathbf{k}_1) = 1$ (since\n",
    "    $\\mathbf{q} =\n",
    "    \\mathbf{k}_1$, exact match)\n",
    "-   $\\alpha(\\mathbf{q}, \\mathbf{k}_2) = 0$ (since\n",
    "    $\\mathbf{q} \\neq\n",
    "    \\mathbf{k}_2$, no match)\n",
    "-   $\\alpha(\\mathbf{q}, \\mathbf{k}_3) = 0$ (since\n",
    "    $\\mathbf{q} \\neq\n",
    "    \\mathbf{k}_3$, no match)\n",
    "\n",
    "Using the attention formula:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\operatorname{Attention}(\\mathbf{q}, \\mathcal{D}) &=\n",
    "\\sum_{t=1}^3 \\alpha(\\mathbf{q}, \\mathbf{k}_t) \\mathbf{v}_t \\\\ &= (1 \\cdot\n",
    "[0.1, 0.9]) + (0 \\cdot [0.4, 0.6]) + (0 \\cdot [0.7, 0.3]) \\\\ &= [0.1, 0.9]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This calculation shows that because the attention weights for $\\mathbf{k}_2$ and\n",
    "$\\mathbf{k}_3$ are zero (due to no exact match), they don't contribute to the\n",
    "final attention output. Only $\\mathbf{k}_1$, which exactly matches the query,\n",
    "has a non-zero weight (1), making it the sole contributor to the attention\n",
    "result. This is a direct consequence of the query being an exact match for one\n",
    "of the keys, leading to a scenario where \"all but one weight is zero.\"\n",
    "```\n",
    "\n",
    "#### Queries, Keys, and Values in Attention Mechanism\n",
    "\n",
    "The database example is a neat analogy to understand the concept of queries,\n",
    "keys, and values in the context of the attention mechanism. To put things into\n",
    "perspective, each token $x_t$ in the input sequence $\\mathbf{x}$ emits three\n",
    "vectors through projecting its corresponding token and positional embedding\n",
    "output $\\mathbf{z}_t$, a query vector $\\mathbf{q}_t$, a key vector\n",
    "$\\mathbf{k}_t$, and a value vector $\\mathbf{v}_t$. Consider the earlier example\n",
    "_cat walks by the river bank_, where each word is a token in the sequence. When\n",
    "we start to process the first token $\\mathbf{z}_1$, _cat_, we would consider a\n",
    "query vector $\\mathbf{q}_1$, projected from $\\mathbf{z}_1$, to be used to\n",
    "interact with the key vectors $\\mathbf{k}_t$ for $t \\in \\{1, 2, \\ldots, T\\}$, in\n",
    "the sequence - determining how much _attention_ \"cat\" should pay to every other\n",
    "token in the sequence (including itself). Consequently, it will also emit a key\n",
    "vector $\\mathbf{k}_1$ so that other tokens can interact with it. Subsequently,\n",
    "the attention pooling will form a linear combination of the query vector\n",
    "$\\mathbf{q}_1$ with every other key vector $\\mathbf{k}_t$ in the sequence,\n",
    "\n",
    "$$\n",
    "\\alpha(\\mathbf{q}_1, \\mathbf{k}_t) \\in \\mathbb{R} = \\mathbf{q}_1 \\cdot\n",
    "\\mathbf{k}_t \\quad \\text{for } t \\in \\{1, 2, \\ldots, T\\}\n",
    "$$\n",
    "\n",
    "and each $\\alpha(\\mathbf{q}_1, \\mathbf{k}_t)$ will indicate how much attention\n",
    "the token \"cat\" should pay to the token at position $t$ in the sequence. We\n",
    "would later see that we would add a softmax normalization to the attention\n",
    "scores to obtain the final attention weights.\n",
    "\n",
    "We would then use the attention scores $\\alpha(\\mathbf{q}_1, \\mathbf{k}_t)$ to\n",
    "create a weighted sum of the value vectors $\\mathbf{v}_t$ to form the new\n",
    "representation of the token \"cat\".\n",
    "\n",
    "$$\n",
    "\\operatorname{Attention}(\\mathbf{q}_1, \\mathbf{k}_t, \\mathbf{v}_t) =\n",
    "\\sum_{t=1}^T \\alpha(\\mathbf{q}_1, \\mathbf{k}_t) \\mathbf{v}_t\n",
    "$$\n",
    "\n",
    "Consequently, the first token must also emit a value vector $\\mathbf{v}_1$. You\n",
    "can think of the value vector as carrying the actual information or content that\n",
    "will be aggregated based on the attention scores.\n",
    "\n",
    "To reiterate, the output\n",
    "$\\operatorname{Attention}(\\mathbf{q}_1, \\mathbf{k}_t, \\mathbf{v}_t)$ will be the\n",
    "new representation of the token \"cat\" in the sequence, which is a weighted sum\n",
    "of the value vectors $\\mathbf{v}_t$ based on the attention scores\n",
    "$\\alpha(\\mathbf{q}_1, \\mathbf{k}_t)$ and now not only holds semantic and\n",
    "positional information about the token \"cat\" itself but also contextual\n",
    "information about the other tokens in the sequence. This allows the token \"cat\"\n",
    "to have a better understanding of itself in the context of the whole sentence.\n",
    "In this whole input sequence, the most ambiguous token is the token \"bank\" as it\n",
    "can refer to a financial institution or a river bank. The attention mechanism\n",
    "will help the token \"bank\" to understand its context in the sentence - likely\n",
    "focusing more on the token \"river\" than the token \"cat\" or \"walks\" to understand\n",
    "its context.\n",
    "\n",
    "The same process will be repeated for each token in the sequence, where each\n",
    "token will emit a query vector, a key vector, and a value vector. The attention\n",
    "scores will be calculated for each token in the sequence, and the weighted sum\n",
    "of the value vectors will be used to form the new representation of each token\n",
    "in the sequence.\n",
    "\n",
    "To end this off, we can intuitively think of the query, key and value as\n",
    "follows:\n",
    "\n",
    "-   **Query**: What does the token want to know? Maybe to the token _bank_, it\n",
    "    is trying to figure out if it is a financial institution or a river bank.\n",
    "    But obviously, when considering the token \"bank\" within such an input\n",
    "    sequence, the query vector generated for \"bank\" would not actually ask \"Am I\n",
    "    a financial institution or a river bank?\" but rather would be an abstract\n",
    "    feature vector in a $D$ dimensional subspace that somehow captures the\n",
    "    potential and context meanings of the token \"bank\" and once it is used to\n",
    "    interact with the key vectors, it will help to determine later on how much\n",
    "    attention the token \"bank\" should pay to the other tokens in the sequence.\n",
    "-   **Key**: Carrying on from the previous point, if the query vector for the\n",
    "    token \"bank\" is being matched with the key vectors of the other tokens in\n",
    "    the sequence, the key \"river\" will be a good match for the query \"bank\" as\n",
    "    it will help the token \"bank\" to understand its context in the sentence. In\n",
    "    this subspace, the key vector for \"river\" will be a good match for the query\n",
    "    because it is more of an \"offering\" service to the query vector, and it will\n",
    "    know when it is deemed to be important to the query vector. As such, the\n",
    "    vectors in this subspace are able to identify itself as important or not\n",
    "    based on the query vector.\n",
    "\n",
    "-   **Value**: The value vector is the actual information or content that will\n",
    "    be aggregated based on the attention scores. If the attention mechanism\n",
    "    determines that \"river\" is highly relevant to understanding the context of\n",
    "    \"bank\" within the sentence, the value vector associated with \"river\" will be\n",
    "    given more weight in the aggregation process. This means that the\n",
    "    characteristics or features encoded in the \"river\" value vector\n",
    "    significantly influence the representation of the sentence or the specific\n",
    "    context being analyzed.\n",
    "\n",
    "### Linear Projections\n",
    "\n",
    "We have discussed the concept of queries, keys, and values but have not yet\n",
    "discussed how these vectors are obtained. As we have continuously emphasized,\n",
    "the query, key, and value vectors lie in a $D$-dimensional subspace, and they\n",
    "encode various abstract information about the tokens in the sequence.\n",
    "Consequently, it is no surprise that these vectors are obtained through linear\n",
    "transformations/projections of the token embeddings $\\mathbf{Z}$ using learned\n",
    "weight matrices $\\mathbf{W}^{\\mathbf{Q}}$, $\\mathbf{W}^{\\mathbf{K}}$ and\n",
    "$\\mathbf{W}^{\\mathbf{V}}$.\n",
    "\n",
    "````{prf:definition} Linear Projections for Queries, Keys, and Values\n",
    ":label: decoder-concept-linear-projections-queries-keys-values\n",
    "\n",
    "In the self-attention mechanism, each token embedding\n",
    "$\\mathbf{z}_t \\in \\mathbb{R}^{D}$ is projected into a new context vector across\n",
    "different **subspaces**. This projection is accomplished through three distinct\n",
    "**linear transformations**, each defined by a unique weight matrix:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{W}^{\\mathbf{Q}} \\in \\mathbb{R}^{D \\times d_q}, \\quad\n",
    "\\mathbf{W}^{\\mathbf{K}} \\in \\mathbb{R}^{D \\times d_k}, \\quad\n",
    "\\mathbf{W}^{\\mathbf{V}} \\in \\mathbb{R}^{D \\times d_v}\n",
    "$$\n",
    "\n",
    "where $d_q, d_k, d_v \\in \\mathbb{Z}^+$ are the hidden dimensions of the\n",
    "subspaces for the query, key, and value vectors, respectively.\n",
    "\n",
    "\n",
    "```{prf:remark} Dimensionality of the Subspaces\n",
    ":label: decoder-concept-linear-projections-queries-keys-values-remark\n",
    "\n",
    "It is worth noting that this post is written in the context of understand\n",
    "GPT models, and the dimensionality of the query, key, and value vectors are\n",
    "the same and usually equal to the dimensionality of the token embeddings.\n",
    "Thus, we may use $D$ interchangeably to indicate $d_k, d_v$ and $d_q$. This\n",
    "is not always the case, as encoder-decoder models might have different\n",
    "dimensionalities for the query, key, and value vectors. However, query and key\n",
    "must have the same dimensionality for the dot product to work.\n",
    "```\n",
    "\n",
    "Each token embedding $\\mathbf{z}_t$ is transformed into three vectors:\n",
    "\n",
    "-   The **query vector** $\\mathbf{q}_t$, representing what the token is looking\n",
    "    for in other parts of the input,\n",
    "-   The **key vector** $\\mathbf{k}_t$, representing how other tokens can be\n",
    "    found or matched,\n",
    "-   The **value vector** $\\mathbf{v}_t$, containing the actual information to be\n",
    "    used in the output.\n",
    "\n",
    "These transformations are formally defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{q}_t = \\mathbf{z}_t \\mathbf{W}^{Q}, \\quad \\mathbf{k}_t =\n",
    "\\mathbf{z}_t \\mathbf{W}^{K}, \\quad \\mathbf{v}_t = \\mathbf{z}_t \\mathbf{W}^{V}\n",
    "$$\n",
    "\n",
    "with each residing in $d_q, d_k, d_v$-dimensional subspaces, respectively.\n",
    "\n",
    "Given an input sequence of $T$ tokens, the individual vectors for each token can\n",
    "be stacked into matrices:\n",
    "\n",
    "$$\n",
    "\\mathbf{Q} = \\begin{bmatrix} \\mathbf{q}_1 \\\\ \\mathbf{q}_2 \\\\ \\vdots \\\\\n",
    "\\mathbf{q}_T \\end{bmatrix} \\in \\mathbb{R}^{T \\times d_q}, \\quad \\mathbf{K} =\n",
    "\\begin{bmatrix} \\mathbf{k}_1 \\\\ \\mathbf{k}_2 \\\\ \\vdots \\\\ \\mathbf{k}_T\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{T \\times d_k}, \\quad \\mathbf{V} = \\begin{bmatrix}\n",
    "\\mathbf{v}_1 \\\\ \\mathbf{v}_2 \\\\ \\vdots \\\\ \\mathbf{v}_T \\end{bmatrix} \\in\n",
    "\\mathbb{R}^{T \\times d_v}\n",
    "$$\n",
    "\n",
    "where each row of these matrices corresponds to the query, key, and value\n",
    "vectors for each token, respectively.\n",
    "\n",
    "These matrices are generated through simple matrix multiplication of the token\n",
    "embedding matrix $\\mathbf{Z} \\in \\mathbb{R}^{T \\times D}$ with the weight\n",
    "matrices\n",
    "$\\mathbf{W}^{\\mathbf{Q}}, \\mathbf{W}^{\\mathbf{K}}$ and $\\mathbf{W}^{\\mathbf{V}}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{Q} = \\mathbf{Z} \\mathbf{W}^{\\mathbf{Q}}, \\quad \\mathbf{K} = \\mathbf{Z}\n",
    "\\mathbf{W}^{\\mathbf{K}}, \\quad \\mathbf{V} = \\mathbf{Z} \\mathbf{W}^{\\mathbf{V}}\n",
    "$$\n",
    "````\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "\n",
    "#### Definition\n",
    "\n",
    "```{prf:definition} Scaled Dot-Product Attention\n",
    ":label: decoder-concept-scaled-dot-product-attention\n",
    "\n",
    "The attention mechanism is a function that maps a set of queries, keys, and\n",
    "values to an output, all of which are represented as matrices in a\n",
    "$D$-dimensional space. Specifically, the function is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\text{Attention}: \\mathbb{R}^{T \\times d_q} \\times \\mathbb{R}^{T\n",
    "\\times d_k} \\times \\mathbb{R}^{T \\times d_v} & \\rightarrow \\mathbb{R}^{T \\times\n",
    "d_v} \\\\ (\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) & \\mapsto\n",
    "\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) \\end{aligned}\n",
    "$$\n",
    "\n",
    "where given a query matrix $\\mathbf{Q} \\in \\mathbb{R}^{T \\times d_q}$, a key\n",
    "matrix $\\mathbf{K} \\in \\mathbb{R}^{T \\times d_k}$, and a value matrix\n",
    "$\\mathbf{V} \\in \\mathbb{R}^{T \\times d_v}$, the attention mechanism computes the\n",
    "the output matrix as follows:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) =\n",
    "\\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}}{\\sqrt{d_k}}\\right)\\mathbf{V}\n",
    "\\in \\mathbb{R}^{T \\times d_v}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "-   $\\mathbf{Q}\\mathbf{K}^{\\top}$ represents the dot product between the query\n",
    "    and key matrices, resulting in a matrix of scores that indicate the degree\n",
    "    of alignment or relevance between each query and all keys.\n",
    "-   $\\sqrt{d_k}$ is a scaling factor used to normalize the scores, preventing them\n",
    "    from becoming too large and ensuring a stable gradient during training. This\n",
    "    scaling factor is particularly important as it helps maintain the softmax\n",
    "    output in a numerically stable range {cite}`vaswani2017attention`.\n",
    "-   $\\text{softmax}(\\cdot)$ is applied row-wise to convert scores into attention\n",
    "    weights, ensuring that for each query, the weights across all keys sum up\n",
    "    to 1. This normalization step allows the mechanism to effectively distribute\n",
    "    focus across the keys according to their relevance to each query.\n",
    "-   The resulting matrix of attention weights is then used to compute a weighted\n",
    "    sum of the values in $\\mathbf{V}$, producing the output matrix. This output\n",
    "    represents a series of context vectors, each corresponding to a query and\n",
    "    containing aggregated information from the most relevant parts of the input\n",
    "    sequence as determined by the attention weights.\n",
    "```\n",
    "\n",
    "In what follows, we will break down the components of the attention mechanism\n",
    "and explain how it works in detail:\n",
    "\n",
    "-   What is Attention Scoring Function?\n",
    "-   Why Softmax?\n",
    "-   Why Scale by $\\sqrt{d_k}$?\n",
    "-   What is Context Vector?\n",
    "\n",
    "#### Attention Scoring Function\n",
    "\n",
    "In order to know which tokens in the sequence are most relevant to the current\n",
    "token, we need to calculate the attention scores between the query and key\n",
    "vectors. Consequently, we would need a scoring function that measures the\n",
    "influence or contribution of the $j$-th position on the $i$-th position in the\n",
    "sequence. This is achieved through the dot product between the query and key\n",
    "vectors, the reasoning through a\n",
    "[Gaussian kernel](https://en.wikipedia.org/wiki/Gaussian_filter) is rigorous and\n",
    "provides a good\n",
    "[intuition](https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html)\n",
    "why we chose the dot product as the scoring function (other than the fact that\n",
    "it is a measure of similarity).\n",
    "\n",
    "```{prf:definition} Attention Scoring Function\n",
    ":label: decoder-concept-attention-scoring-function\n",
    "\n",
    "Define the attention scoring function $\\alpha(\\cdot)$ as a function\n",
    "that calculates the relevance or influence of each position $t$ in the sequence\n",
    "on position $i$, known as the attention scores. The attention scoring function\n",
    "$\\alpha(\\cdot)$ is defined using the dot product between query and key\n",
    "vectors, leveraging its property as a similarity measure.\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\alpha: \\mathbb{R}^{d_q} \\times \\mathbb{R}^{d_k} & \\rightarrow\n",
    "\\mathbb{R} \\\\ (\\mathbf{q}, \\mathbf{k}_t) & \\mapsto \\alpha(\\mathbf{q},\n",
    "\\mathbf{k}_t) \\end{aligned}\n",
    "$$\n",
    "\n",
    "Specifically, the function is expressed as:\n",
    "\n",
    "$$\n",
    "\\alpha(\\mathbf{q}, \\mathbf{k}_t) = \\langle \\mathbf{q}, \\mathbf{k}_t \\rangle =\n",
    "\\mathbf{q} \\cdot \\mathbf{k}_t \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "-   $\\mathbf{q}$ is a query vector representing in the sequence, seeking\n",
    "    information or context.\n",
    "-   $\\mathbf{k}_t$ is the key vector representing the $t$-th position in the\n",
    "    sequence, offering context or information.\n",
    "-   $\\langle \\mathbf{q}, \\mathbf{k}_t \\rangle$ denotes the dot product between\n",
    "    the query vector $\\mathbf{q}$ and the key vector $\\mathbf{k}_t$, which\n",
    "    quantifies the level of similarity or alignment between the current position\n",
    "    that $\\mathbf{q}$ is at (say $i$-th) and $t$-th positions in the sequence.\n",
    "\n",
    "The expression $\\mathbf{q} \\cdot \\mathbf{k}_t$ is a scalar value that indicates\n",
    "the degree of alignment or relevance between the query at $i$-th position and\n",
    "the key at $t$-th position in the sequence. We would need to calculate the\n",
    "attention scores for each token in the sequence with respect to the query vector\n",
    "$\\mathbf{q}$, and the key vectors $\\mathbf{k}_t$ for\n",
    "$t \\in \\{1, 2, \\ldots, T\\}$.\n",
    "\n",
    "So this leads us to:\n",
    "\n",
    "$$\n",
    "\\alpha(\\mathbf{q}, \\mathbf{K}) = \\mathbf{q}\\mathbf{K}^{\\top} \\in \\mathbb{R}^{1\n",
    "\\times T}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{K} = \\begin{bmatrix} \\mathbf{k}_1 \\\\ \\mathbf{k}_2 \\\\ \\vdots \\\\\n",
    "\\mathbf{k}_T \\end{bmatrix} \\in \\mathbb{R}^{T \\times d_k}\n",
    "$$\n",
    "\n",
    "is the matrix of key vectors for each token in the sequence, and the output\n",
    "$\\alpha(\\mathbf{q}, \\mathbf{K}) \\in \\mathbb{R}^{1 \\times T}$ is a row\n",
    "vector of attention scores for the query vector $\\mathbf{q}$ with respect to\n",
    "each key vector $\\mathbf{k}_t$ for $t \\in \\{1, 2, \\ldots, T\\}$.\n",
    "\n",
    "Lastly, there are $T$ such queries in the input sequence $\\mathbf{Q}$, and we\n",
    "can stack all the query vectors $\\mathbf{q}_t$ into a matrix\n",
    "$\\mathbf{Q} \\in \\mathbb{R}^{T \\times d_q}$ to calculate the attention scores for\n",
    "all the queries in the sequence with respect to all the key vectors in the\n",
    "sequence.\n",
    "\n",
    "$$\n",
    "\\alpha(\\mathbf{Q}, \\mathbf{K}) = \\mathbf{Q}\\mathbf{K}^{\\top} \\in \\mathbb{R}^{T\n",
    "\\times T}\n",
    "$$\n",
    "\n",
    "To this end, each row of the matrix $\\mathbf{Q}\\mathbf{K}^{\\top}$ represents the\n",
    "attention scores for each query vector at position $i$ in the sequence with\n",
    "respect to all the key vectors in the sequence.\n",
    "```\n",
    "\n",
    "#### Scaling Down the Dot Product of Query and Key Vectors\n",
    "\n",
    "```{prf:definition} Query and Key are Independent and Identically Distributed (i.i.d.)\n",
    ":label: decoder-concept-query-key-iid\n",
    "\n",
    "Under the assumption of the query $\\mathbf{q}$ and key $\\mathbf{k}_t$ are\n",
    "_**independent and identically distributed**_ (i.i.d.) random variables with a\n",
    "gaussian distribution of mean $0$ and variance $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "\\mathbf{q} \\overset{\\mathrm{iid}}{\\sim} \\mathcal{N}(0, \\sigma^2), \\quad\n",
    "\\mathbf{k}_t \\overset{\\mathrm{iid}}{\\sim} \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "```\n",
    "\n",
    "```{prf:definition} Variance of Dot Product\n",
    ":label: decoder-concept-variance-dot-product\n",
    "\n",
    "Given that $\\mathbf{q} \\overset{\\mathrm{iid}}{\\sim} \\mathcal{N}(0, \\sigma^2), \\quad \\mathbf{k}_t \\overset{\\mathrm{iid}}{\\sim} \\mathcal{N}(0, \\sigma^2)$,\n",
    "the variance of the dot product between the query vector $\\mathbf{q}$ and the key\n",
    "vector $\\mathbf{k}_t$ is:\n",
    "\n",
    "$$\n",
    "\\mathbb{V}[\\mathbf{q} \\cdot \\mathbf{k}_t] = \\sum_{i=1}^{d_k} \\mathbb{V}[q_i\n",
    "k_{ti}] = d_k \\cdot \\sigma^4.\n",
    "$$\n",
    "```\n",
    "\n",
    "```{prf:proof}\n",
    "The dot product between $\\mathbf{q}$ and $\\mathbf{k}_t$ can be expressed as the\n",
    "sum of the products of their components:\n",
    "\n",
    "$$\\mathbf{q} \\cdot \\mathbf{k}_t = \\sum_{i=1}^{d_k} q_i k_{ti},$$\n",
    "\n",
    "where $q_i$ and $k_{ti}$ are the $i$-th components of $\\mathbf{q}$ and\n",
    "$\\mathbf{k}_t$, respectively.\n",
    "\n",
    "The variance of the sum of random variables (when these variables are\n",
    "independent, which is our case since components are iid) is the sum of their\n",
    "variances. The product $q_i k_{ti}$ is a new random variable, and its variance\n",
    "can be calculated as follows for a single pair of components:\n",
    "\n",
    "$$\n",
    "\\mathbb{V}[q_i k_{ti}] = \\mathbb{E}[(q_i k_{ti})^2] - (\\mathbb{E}[q_i\n",
    "k_{ti}])^2.\n",
    "$$\n",
    "\n",
    "Given that $q_i$ and $k_{ti}$ are independent and both have mean 0:\n",
    "\n",
    "$$\\mathbb{E}[q_i k_{ti}] = \\mathbb{E}[q_i] \\cdot \\mathbb{E}[k_{ti}] = 0.$$\n",
    "\n",
    "The expectation of the square of the product is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[(q_i k_{ti})^2] = \\mathbb{E}[q_i^2] \\cdot \\mathbb{E}[k_{ti}^2] =\n",
    "\\sigma^2 \\cdot \\sigma^2 = \\sigma^4.\n",
    "$$\n",
    "\n",
    "Since $\\mathbb{E}[q_i k_{ti}] = 0$, the variance of the product $q_i k_{ti}$ is\n",
    "simply $\\sigma^4$.\n",
    "\n",
    "For the dot product, we sum across all $d_k$ components, and since the variance\n",
    "of the sum of independent random variables is the sum of their variances:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{V}[\\mathbf{q} \\cdot \\mathbf{k}_t] = \\sum_{i=1}^{d_k} \\mathbb{V}[q_i\n",
    "k_{ti}] = d_k \\cdot \\sigma^4.\n",
    "$$\n",
    "```\n",
    "\n",
    "We want to ensure that the variance of the dot product still remains the same as\n",
    "the variance of the query and key vectors at $\\sigma^2$ regardless of the vector\n",
    "dimensions. To do so, we scale down the dot product by $\\sqrt{d_k}$, which is\n",
    "the square root of the dimensionality of the key vectors, this operation would\n",
    "scale the variance of the dot product down by $\\sqrt{d_k}^2 = d_k$ (since\n",
    "variance of a scaled random variable is the square of the scale factor times the\n",
    "original variance).\n",
    "\n",
    "Now our variance would be $\\sigma^4$ - but it is still not the same as the\n",
    "variance of the query and key vectors. This is okay because the original paper\n",
    "assume the variance $\\sigma^2 = 1$ {cite}`vaswani2017attention`, and therefore\n",
    "it does not matter since $\\sigma^2 = \\sigma^4$ when $\\sigma^2 = 1$.\n",
    "\n",
    "```{prf:definition} Attention Scoring Function with Scaling\n",
    ":label: decoder-concept-attention-scoring-function-with-scaling\n",
    "\n",
    "To this end, the updated scoring function is:\n",
    "\n",
    "$$\n",
    "\\alpha(\\mathbf{Q}, \\mathbf{K}) = \\frac{\\mathbf{Q}\\mathbf{K}^{\\top}}{\\sqrt{d_k}}\n",
    "\\in \\mathbb{R}^{T \\times T}\n",
    "$$\n",
    "```\n",
    "\n",
    "Before we look into the reason why we scale down the dot product, let's first\n",
    "complete the final block of the attention mechanism, which is the softmax\n",
    "normalization.\n",
    "\n",
    "#### Softmax\n",
    "\n",
    "```{prf:definition} Attention Scores\n",
    ":label: decoder-concept-attention-scores\n",
    "\n",
    "Currently the attention scores $\\alpha(\\mathbf{Q}, \\mathbf{K})$ are raw scores\n",
    "that indicate the degree of alignment or relevance between each query and all\n",
    "keys. They can be negative or positive, and they can be large or small. We\n",
    "denote them as the raw **attention scores** $\\alpha(\\mathbf{Q}, \\mathbf{K}) \\in\n",
    "\\mathbb{R}^{T \\times T}$.\n",
    "```\n",
    "\n",
    "```{prf:definition} Softmax Normalization and Attention Weights\n",
    ":label: decoder-concept-softmax-normalization-attention-weights\n",
    "\n",
    "It is common in deep learning to form a convex combination {cite}`zhang2023dive`\n",
    "of the attention scores $\\alpha(\\mathbf{Q}, \\mathbf{K})$ to obtain the\n",
    "**attention weights**, denoted as $\\text{softmax}(\\alpha(\\mathbf{Q}, \\mathbf{K}))$, which\n",
    "are non-negative and sum to $1$. This is achieved through the softmax\n",
    "normalization function, which is defined as:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(\\alpha(\\mathbf{Q}, \\mathbf{K})) = \\frac{\\exp(\\alpha(\\mathbf{Q},\n",
    "\\mathbf{K}))}{\\sum_{t=1}^T \\exp(\\alpha(\\mathbf{Q}, \\mathbf{k}_t))} \\in\n",
    "\\mathbb{R}^{T \\times T}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "-   $\\exp(\\cdot)$ is the exponential function, which is applied element-wise to\n",
    "    the raw attention scores $\\alpha(\\mathbf{Q}, \\mathbf{K})$.\n",
    "-   The denominator is the sum of the exponentials of the raw attention scores\n",
    "      across the $T$ keys, ensuring that the attention weights sum to $1$ for\n",
    "      each query, allowing the mechanism to effectively distribute focus across\n",
    "      the keys according to their relevance to each query.\n",
    "```\n",
    "\n",
    "The choice of softmax is a convenient choice, but not the only choice. However,\n",
    "it is convenient because it is both _differentiable_, which is often a desirable\n",
    "property for training deep learning models that are optimized using\n",
    "gradient-based methods, and it is also _monotonic_, which means that the\n",
    "**attention weights** are preserved exactly in the order as the raw **attention\n",
    "scores**.\n",
    "\n",
    "```{prf:definition} Attention Scoring Function with Scaling and Softmax\n",
    ":label: decoder-concept-attention-scoring-function-with-scaling-softmax\n",
    "\n",
    "To this end, our final attention scoring function is:\n",
    "\n",
    "$$\n",
    "\\alpha(\\mathbf{Q}, \\mathbf{K}) =\n",
    "\\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}}{\\sqrt{d_k}}\\right) \\in\n",
    "\\mathbb{R}^{T \\times T}\n",
    "$$\n",
    "```\n",
    "\n",
    "#### Context Vector/Matrix\n",
    "\n",
    "Consequently, we complete the walkthrough of the scaled dot-product attention\n",
    "mechanism by calculating the context vector, which is the weighted sum of the\n",
    "value vectors based on the attention weights obtained from the softmax\n",
    "normalization.\n",
    "\n",
    "```{prf:definition} Context Vector/Matrix\n",
    ":label: decoder-concept-context-vector-matrix\n",
    "\n",
    "Given the attention weights $\\alpha(\\mathbf{Q}, \\mathbf{K})$ and the value\n",
    "matrix $\\mathbf{V}$, the context vector $\\mathbf{C}$ is defined as the output\n",
    "of the scaled dot-product attention mechanism:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{C} :=\n",
    "\\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}}{\\sqrt{d_k}}\\right)\\mathbf{V}\n",
    "\\in \\mathbb{R}^{T \\times d_v}\n",
    "$$\n",
    "\n",
    "where each row $\\mathbf{c}_t$ of the context matrix $\\mathbf{C}$ is the\n",
    "new embedding of the token at position $t$ in the sequence, containing\n",
    "not only the semantic and positional information of the token itself, but also\n",
    "contextual information from the other tokens in the sequence.\n",
    "```\n",
    "\n",
    "#### Numerical Stability and Gradient Saturation\n",
    "\n",
    "We can now revisit on the underlying reason why we scale down the dot product\n",
    "$\\mathbf{Q}\\mathbf{K}^{\\top}$ by $\\sqrt{d_k}$.\n",
    "\n",
    "First, the softmax function has all the desirable properties we want,\n",
    "_smoothness_, _monotonicity_, and _differentiability_, but it is _sensitive_ to\n",
    "large input values.\n",
    "\n",
    "The softmax function is defined as follows for a given logit $z_i$ among a set\n",
    "of logits $Z$:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "$$\n",
    "\n",
    "If the variance of the logits before applying softmax is too large (not scaled\n",
    "down to a more manageable range), the exponential function $e^{z_i}$ can lead to\n",
    "extremely large output values for any $z_i$ that is even slightly larger than\n",
    "others in the set. This is due to the exponential function's rapid growth with\n",
    "respect to its input value.\n",
    "\n",
    "```{prf:remark} Gradient Saturation\n",
    ":label: decoder-concept-gradient-saturation\n",
    "\n",
    "-   **For one random element:** If one of the logits $z_i$ is significantly\n",
    "    larger than the others (which is more likely when the variance of the logits\n",
    "    is high), $e^{z_i}$ will dominate the numerator and denominator of the\n",
    "    softmax function for this logit. This will cause the softmax output for this\n",
    "    logit to approach 1, as it essentially overshadows all other $e^{z_j}$ terms\n",
    "    in the denominator.\n",
    "\n",
    "-   **For all others:** Simultaneously, the softmax outputs for all other logits\n",
    "    $z_j$ (where $j \\neq i$) will approach 0, because their $e^{z_j}$\n",
    "    contributions to the numerator will be negligible compared to $e^{z_i}$ in\n",
    "    the denominator. Thus, the attention mechanism would almost exclusively\n",
    "    focus on the token corresponding to the dominant logit, ignoring valuable\n",
    "    information from other parts of the input sequence.\n",
    "-   Furthermore, the gradients through the softmax function will be very small\n",
    "    (close to zero) for all logits except the dominant one, which can lead to\n",
    "    _gradient saturation_ and even _vanishing gradients_ during training.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.exp(z) / torch.sum(torch.exp(z), axis=0)\n",
    "\n",
    "# Without scaling: large inputs\n",
    "logits_large = torch.tensor([10, 20, 30], dtype=torch.float32)\n",
    "softmax_large = softmax(logits_large)\n",
    "\n",
    "d_k = 512\n",
    "scaling_factor = torch.sqrt(torch.tensor(d_k))\n",
    "scaled_logits = logits_large / scaling_factor\n",
    "softmax_scaled = softmax(scaled_logits)\n",
    "\n",
    "print(\"Softmax without scaling:\", softmax_large)\n",
    "print(\"Softmax with scaling:\", softmax_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, a vector with large inputs can lead to a _sharpening_ effect on\n",
    "the output of the softmax function, essentially causing the output to be too\n",
    "peaky, converging to 1 for the largest input and 0 for the rest (one-hot).\n",
    "\n",
    "```{prf:remark} Numerical Stability\n",
    ":label: decoder-concept-numerical-stability\n",
    "\n",
    "We know the importance of weight initialization in deep learning models,\n",
    "this is because it dictates the variance of the activations and gradients\n",
    "throughout the network. Without going into the theory, it is intuitive\n",
    "to think that having similar variance across all layer activations is\n",
    "a desirable property for numerical stability.\n",
    "By doing so, the model helps to ensure that the gradients are stable\n",
    "during backpropagation, avoiding the vanishing or exploding gradients problem\n",
    "and enabling effective learning.\n",
    "\n",
    "In the specific context of the attention mechanism, the variance of the dot\n",
    "products used to calculate attention scores is scaled down by the factor\n",
    "$\\frac{1}{\\sqrt{d_k}}$ to prevent softmax saturation. This allows each element\n",
    "to have a chance to influence the model's learning, rather than having a single\n",
    "element dominate because of the variance scaling with $d_k$.\n",
    "```\n",
    "\n",
    "#### Visualizing Variance of Dot Product\n",
    "\n",
    "If we set $d_k = 512$, and mean $0$ with unit variance, we will see in action\n",
    "that indeed the scaled dot product has a variance of $1$ while the unscaled dot\n",
    "product has a variance of $512$, which coincides with our theoretical analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(92, True, False)\n",
    "\n",
    "# Set the dimensionality of the keys and queries\n",
    "d_k = 512\n",
    "# Set the batch size, number of heads, and sequence length\n",
    "B, H, L = 4, 8, 32\n",
    "# Standard deviation for initialization\n",
    "sigma = 1.0\n",
    "\n",
    "# Initialize Q and K with variance sigma^2\n",
    "Q = torch.randn(B, H, L, d_k) * sigma\n",
    "K = torch.randn(B, H, L, d_k) * sigma\n",
    "\n",
    "# Calculate dot products without scaling\n",
    "unscaled_dot_products = torch.matmul(Q, K.transpose(-2, -1))\n",
    "\n",
    "# Calculate the variance of the unscaled dot products\n",
    "unscaled_variance = unscaled_dot_products.var(unbiased=False)\n",
    "\n",
    "# Apply the scaling factor 1 / sqrt(d_k)\n",
    "scaled_dot_products = unscaled_dot_products / torch.sqrt(torch.tensor(d_k).float())\n",
    "\n",
    "# Calculate the variance of the scaled dot products\n",
    "scaled_variance = scaled_dot_products.var(unbiased=False)\n",
    "\n",
    "print(f\"Unscaled Variance: {unscaled_variance}\")\n",
    "print(f\"Scaled Variance: {scaled_variance}\")\n",
    "\n",
    "# Apply softmax to the scaled and unscaled dot products\n",
    "softmax_unscaled = torch.nn.functional.softmax(unscaled_dot_products, dim=-1)\n",
    "softmax_scaled = torch.nn.functional.softmax(scaled_dot_products, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projections Lead to Dynamic Context Vectors\n",
    "\n",
    "From the start, we mentioned _the attention mechanism describes a **weighted\n",
    "average** of (sequence) elements with the weights **dynamically** computed based\n",
    "on an input query and elements’ keys_. We can easily see the **weighted\n",
    "average** part through self-attention. The **dynamic** part comes from the fact\n",
    "that the context vectors are computed based on the input query and its\n",
    "corresponding keys. There should be no confusion that all the learnable weights\n",
    "in this self-attention mechanism are the weight matrices\n",
    "$\\mathbf{W}^{\\mathbf{Q}}$, $\\mathbf{W}^{\\mathbf{K}}$ and\n",
    "$\\mathbf{W}^{\\mathbf{V}}$, but the dynamic is really because the scoring\n",
    "function uses a dot product $\\mathbf{Q}\\mathbf{K}^{\\top}$, which is **dynamic**\n",
    "because it is solely decided by the full input sequence $\\mathbf{x}$. Unlike\n",
    "static embeddings, where the word \"cat\" will always have the same embedding\n",
    "vector, the context vector for the word \"cat\" will be different in different\n",
    "sentences because it now depends on the full input sequence $\\mathbf{x}$.\n",
    "\n",
    "Consequently, the projection of the token embeddings into the query and key\n",
    "space is needed.\n",
    "\n",
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(ABC, nn.Module):\n",
    "    def __init__(self, dropout: float = 0.0) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout, inplace=False)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.BoolTensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        raise NotImplementedError(\"The `forward` method must be implemented by the subclass.\")\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(Attention):\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: torch.BoolTensor | None = None,\n",
    "        debug: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # fmt: off\n",
    "        d_q               = query.size(dim=-1)\n",
    "\n",
    "        attention_scores  = torch.matmul(query, key.transpose(dim0=-2, dim1=-1)) / torch.sqrt(torch.tensor(d_q).float())\n",
    "        attention_scores  = attention_scores.masked_fill(mask == 0, float(\"-inf\")) if mask is not None else attention_scores\n",
    "\n",
    "        attention_weights = attention_scores.softmax(dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        context_vector    = torch.matmul(attention_weights, value)\n",
    "        # fmt: on\n",
    "        if debug:\n",
    "            return context_vector, attention_weights, attention_scores\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "B, H, L, D = 4, 8, 32, 512  # batch size, head, context length, embedding dimension\n",
    "Q = torch.rand(B, H, L, D)  # query\n",
    "K = torch.rand(B, H, L, D)  # key\n",
    "V = torch.rand(B, H, L, D)  # value\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "attention = ScaledDotProductAttention(dropout=0.0)\n",
    "context_vector, attention_weights = attention(Q, K, V)\n",
    "\n",
    "assert context_vector.shape == (B, H, L, D)\n",
    "assert attention_weights.shape == (B, H, L, L)\n",
    "pprint(context_vector.shape)\n",
    "pprint(attention_weights.shape)\n",
    "\n",
    "# assert each row of attention_weights sums to 1\n",
    "# assert each element of attention_weights is between 0 and 1\n",
    "attention_weights_summed_over_sequences = attention_weights.sum(dim=-1)\n",
    "assert torch.allclose(\n",
    "    attention_weights_summed_over_sequences, torch.ones(B, H, L)\n",
    "), \"The attention weights distribution induced by softmax should sum to 1.\"\n",
    "assert torch.all(\n",
    "    (0 <= attention_weights) & (attention_weights <= 1)\n",
    "), \"All attention weights should be between 0 and 1.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_attention_heatmaps(\n",
    "    attention_weights: torch.Tensor,\n",
    "    xlabel: str = \"Keys\",\n",
    "    ylabel: str = \"Queries\",\n",
    "    show_title: bool = False,\n",
    "    figure_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    plot_kwargs: Optional[Dict[str, Any]] = None,\n",
    ") -> None:\n",
    "    B, H, _, _ = attention_weights.shape\n",
    "\n",
    "    if isinstance(attention_weights, torch.Tensor):\n",
    "        attention_weights = attention_weights.detach().cpu().numpy()\n",
    "\n",
    "    figure_kwargs = figure_kwargs or {\"figsize\": (15, 15), \"sharex\": True, \"sharey\": True, \"squeeze\": False}\n",
    "    fig, axes = plt.subplots(B, H, **figure_kwargs)\n",
    "\n",
    "    plot_kwargs = plot_kwargs or {\"cmap\": \"viridis\"}\n",
    "\n",
    "    for b, (row_axes, attention_weight) in enumerate(zip(axes, attention_weights)):\n",
    "        for h, (ax, head_attention) in enumerate(zip(row_axes, attention_weight)):\n",
    "            pcm = ax.imshow(head_attention, **plot_kwargs)\n",
    "            if b == B - 1:\n",
    "                ax.set_xlabel(xlabel)  # Only the last batch will have the xlabel\n",
    "            if h == 0:\n",
    "                ax.set_ylabel(ylabel)  # Only the first head will have the ylabel\n",
    "\n",
    "            if show_title:\n",
    "                ax.set_title(f\"Batch {b + 1}, Head {h + 1}\")\n",
    "    fig.colorbar(pcm, ax=axes, shrink=0.6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting back to our earlier database query, the following identity matrix\n",
    "would represent when the _query_ and the _key_ to be an exact match, indicated here\n",
    "naively as if query is 0, then key is also 0, and so on. And thus\n",
    "the weight matrix would have a diagonal of 1s and 0s elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = torch.eye(10).reshape((1, 1, 10, 10))\n",
    "show_attention_heatmaps(attention_weights, xlabel='Keys', ylabel='Queries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masked/Causal Self-Attention\n",
    "\n",
    "In the context of GPT models, which is a decoder-only architecture, the\n",
    "self-attention mechanism is often referred to as **masked self-attention** or\n",
    "**causal attention**. The reason is that the attention mechanism is masked to\n",
    "prevent information flow from future tokens to the current token. Given the\n",
    "autoregressive and self-supervised nature of the GPT models, the prediction for\n",
    "the current token should not be influenced by future tokens, as they are not\n",
    "known during inference.\n",
    "\n",
    "First, let's connect to our earlier example of `z_0_tok_embed_with_pos_embed` to\n",
    "see the non-masked self-attention mechanism in action. \n",
    "We would create weights $\\mathbf{W}^{\\mathbf{Q}}$, $\\mathbf{W}^{\\mathbf{K}}$ and\n",
    "$\\mathbf{W}^{\\mathbf{V}}$ and project the token embeddings into the query, key\n",
    "and value space. We then pass the query, key and value matrices into the scaled\n",
    "dot-product attention mechanism to obtain the context matrix $\\mathbf{C}$\n",
    "as well as the attention weights $\\alpha(\\mathbf{Q}, \\mathbf{K})$.\n",
    "Note since it is for only one sample, we would use lower case\n",
    "letters instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "seed_all(composer.seed, seed_torch=True, set_torch_deterministic=False)\n",
    "\n",
    "B, H, T, D = 1, 1, composer.block_size, composer.d_model  # batch size, head, context length, embedding dimension\n",
    "W_q = nn.Linear(in_features=D, out_features=D, bias=False)\n",
    "W_k = nn.Linear(in_features=D, out_features=D, bias=False)\n",
    "W_v = nn.Linear(in_features=D, out_features=D, bias=False)\n",
    "\n",
    "q0 = W_q(z0_tok_embed_with_pos_embed)\n",
    "k0 = W_k(z0_tok_embed_with_pos_embed)\n",
    "v0 = W_v(z0_tok_embed_with_pos_embed)\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "attention = ScaledDotProductAttention(dropout=0.0)\n",
    "context_vector, attention_weights = attention(q0, k0, v0)\n",
    "attention_weights = attention_weights.reshape(B, H, T, T)\n",
    "\n",
    "show_attention_heatmaps(attention_weights, xlabel='Keys', ylabel='Queries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is just one problem. We are allowing the self-attention to attend\n",
    "each token to all other tokens in the sequence, including itself. Since we have\n",
    "$T$ tokens in the sequence, each sub-sequence $\\mathbf{x}_{1:t}$ would look into\n",
    "the future sub-sequence $\\mathbf{x}_{t+1:T}$, which is trivial now for the model\n",
    "to learn.\n",
    "\n",
    "```{admonition} Naive Way To Handle Future Mask\n",
    ":class: note\n",
    "\n",
    "Naively, we can dissect the input sequence $\\mathbf{x}$ into $T$ sub-sequences,\n",
    "and run the self-attention $T$ times independently. This means we are really\n",
    "treating one sequence as $T$ sub-samples. But this is inefficient.\n",
    "```\n",
    "\n",
    "To fix this, we introduce a _mask_, where the mask effectively zeros out the\n",
    "attention scores for the future tokens in the sequence. This would result in the\n",
    "usage of an upper-triangular matrix for the attention scores, and the softmax\n",
    "would then zero out the future tokens in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "seed_all(composer.seed, seed_torch=True, set_torch_deterministic=False)\n",
    "\n",
    "B, H, T, D = 1, 1, composer.block_size, composer.d_model  # batch size, head, context length, embedding dimension\n",
    "W_q = nn.Linear(in_features=D, out_features=D, bias=False)\n",
    "W_k = nn.Linear(in_features=D, out_features=D, bias=False)\n",
    "W_v = nn.Linear(in_features=D, out_features=D, bias=False)\n",
    "\n",
    "q0 = W_q(z0_tok_embed_with_pos_embed)\n",
    "k0 = W_k(z0_tok_embed_with_pos_embed)\n",
    "v0 = W_v(z0_tok_embed_with_pos_embed)\n",
    "\n",
    "tril_mask = torch.tril(torch.ones((T, T), dtype=torch.bool))\n",
    "pprint(tril_mask)\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "attention = ScaledDotProductAttention(dropout=0.0)\n",
    "context_vector, attention_weights, attention_scores = attention(q0, k0, v0, mask=tril_mask, debug=True)\n",
    "pprint(attention_scores.squeeze(0).squeeze(0))\n",
    "pprint(attention_weights)\n",
    "\n",
    "attention_weights = attention_weights.reshape(B, H, T, T)\n",
    "show_attention_heatmaps(attention_weights, xlabel='Keys', ylabel='Queries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_decoded = tokenizer.decode(x0.squeeze().detach().cpu().numpy())\n",
    "pprint(x0_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the print logs and plots above, it is pretty clear how for one sequence,\n",
    "the future mask is working. Consider our decoded first sample\n",
    "`priest and clerk? well then, amen`. The result of $\\mathbf{Q}\\mathbf{K}^{\\top}$\n",
    "would be a matrix of attention weights of size $T \\times T$. As we mentioned\n",
    "earlier, if we do not mask, then for instance, the second row being the token\n",
    "`and` would have information on every token, including `clerk`, `amen` etc. This\n",
    "is considered _leakage_ of information.\n",
    "\n",
    "So when we provide such a triangular mask:\n",
    "\n",
    "$$\n",
    "\\text{mask} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & \\dots & 0 \\\\\n",
    "1 & 1 & 0 & \\dots & 0 \\\\\n",
    "1 & 1 & 1 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & 1 & 1 & \\dots & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and apply it to the attention scores, we would see that the attention scores\n",
    "would have $-\\infty$ for the future tokens,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{masked_attention_scores} &= \\text{attention_scores} \\odot \\text{mask} + (1 - \\text{mask}) \\cdot (-\\infty) \\\\\n",
    "&= \\begin{bmatrix}\n",
    "q_1 \\cdot k_1 & -\\infty & -\\infty & \\dots & -\\infty \\\\\n",
    "q_2 \\cdot k_1  & q_2 \\cdot k_2 & -\\infty & \\dots & -\\infty \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "q_T \\cdot k_1 & q_T \\cdot k_2 & q_T \\cdot k_3 & \\dots & q_T \\cdot k_T \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and lastly applying the softmax function, we would see that the future tokens\n",
    "would have a softmax output of $0$, a neat trick to ask the loss function to not\n",
    "consider the future tokens in the sequence. We can even use `ignore_index` in\n",
    "PyTorch's loss function like `nn.CrossEntropyLoss` to ignore the future tokens\n",
    "via a given mask index.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{masked_attention_weights} &= \\text{softmax}(\\text{masked_attention_scores}) \\\\\n",
    "&= \\begin{bmatrix}\n",
    "\\text{softmax}(q_1 \\cdot k_1) & 0 & 0 & \\dots & 0 \\\\\n",
    "\\text{softmax}(q_2 \\cdot k_1) & \\text{softmax}(q_2 \\cdot k_2) & 0 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{softmax}(q_T \\cdot k_1) & \\text{softmax}(q_T \\cdot k_2) & \\text{softmax}(q_T \\cdot k_3) & \\dots & \\text{softmax}(q_T \\cdot k_T) \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Last Token has Full Context \n",
    "\n",
    "And finally, the `context_vector` is a matrix of size $T \\times d_v$ where each\n",
    "row is the context vector for each token in the sequence, containing not only\n",
    "the semantic and positional information of the token itself, but also contextual\n",
    "information from the preceding tokens in the sequence.\n",
    "\n",
    "For example:\n",
    "\n",
    "- The first token `priest` would have a context vector that contains information\n",
    "from the token `priest` itself.\n",
    "- The second token `and` would have a context vector that contains information\n",
    "from the token `priest` and `and`.\n",
    "- The third token `clerk` would have a context vector that contains information\n",
    "from the token `priest`, `and` and `clerk`.\n",
    "- ...\n",
    "- The last token `amen` would have a context vector that contains information\n",
    "from the token `priest`, `and`, `clerk`, ..., `amen`.\n",
    "\n",
    "As a consequence, the last token would be the only token that has the full\n",
    "context of the sequence, and this is why while generating text, we would only\n",
    "need to use the context vector of the last token to generate the next token.\n",
    "\n",
    "To this end, each sequence $\\mathbf{x}$ is transformed to a $T \\times d_v$\n",
    "\n",
    "It is worth noting that one can connect back to the theory earlier on how one sequence\n",
    "$\\mathbf{x}$ is decomposed to a $T \\times d_v$ matrix where $d_v = D$ when only one head is involved.\n",
    "The difference is huge because each row of the embedding $\\mathbf{z}$ before passing through attention\n",
    "would only hold that single token $x_t$'s semantic and positional info without notion\n",
    "of any other tokens in the sequence. Now, each row of the context vector $\\mathbf{c}_t$\n",
    "would hold the semantic and positional info of the token $x_t$ as well as the context\n",
    "information from the preceding tokens in the sequence. \n",
    "\n",
    "Lastly, each sequence can be thought of having $T$ sub-samples, this is in line with our objective\n",
    "to model the joint distribution of one sequence $\\mathbf{x}$ - which is decomposed into $T$ conditional\n",
    "probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Complexity of Self-Attention\n",
    "\n",
    "It is easy to see that computing the dot product in self-attention quadratic\n",
    "over its sequence length $T$. And naively, we would require\n",
    "$\\mathcal{O}(T^2 \\cdot D)$ time to compute the self-attention on a sequence of\n",
    "length $T$ and $D$-dimensional representations. We see a table listed below,\n",
    "referenced from the paper _Attention is All You Need_\n",
    "{cite}`vaswani2017attention`, with complexity per layer, the number of\n",
    "sequential operations, and maximum path length. The complexity is measured by\n",
    "the upper bound of the number of operations to perform, while the maximum path\n",
    "length represents the maximum number of steps a forward or backward signal has\n",
    "to traverse to reach any other position. The lower this length, the better\n",
    "gradient signals can backpropagate for long-range dependencies\n",
    "{cite}`lippe2023uvadlc`.\n",
    "\n",
    "```{list-table} Master Theorem Cases\n",
    ":header-rows: 1\n",
    ":name: self-attention-complexity\n",
    "\n",
    "*   - Layer Type\n",
    "    - Complexity per Layer\n",
    "    - Sequential Operations\n",
    "    - Maximum Path Length\n",
    "*   - Self-Attention\n",
    "    - $\\mathcal{O}(T^2 \\cdot D)$\n",
    "    - $\\mathcal{O}(1)$\n",
    "    - $\\mathcal{O}(1)$\n",
    "*   - Recurrent\n",
    "    - $\\mathcal{O}(T \\cdot D^2)$\n",
    "    - $\\mathcal{O}(T)$\n",
    "    - $\\mathcal{O}(T)$\n",
    "*   - Convolutional\n",
    "    - $\\mathcal{O}(K \\cdot T \\cdot D^2)$\n",
    "    - $\\mathcal{O}(1)$\n",
    "    - $\\mathcal{O}(\\log_K(T))$\n",
    "*   - Self-Attention (restricted)\n",
    "    - $\\mathcal{O}(R \\cdot T \\cdot D)$\n",
    "    - $\\mathcal{O}(1)$\n",
    "    - $\\mathcal{O}\\left(\\frac{T}{R}\\right)$\n",
    "```\n",
    "\n",
    "In the table, we have:\n",
    "\n",
    "-   $T$ is the sequence length,\n",
    "-   $D$ is the representation dimension,\n",
    "-   $K$ is the kernel size of the convolution,\n",
    "-   $R$ is the size of the neighborhood in restricted self-attention.\n",
    "\n",
    "It is not easy to see that the parallel computation of self-attention is\n",
    "favourable, why so? Because unlike recurrent, where we need to strictly follow\n",
    "the sequence, in self-attention, we _simultaneously_ compute the attention\n",
    "scores for all pairs of positions. This is why the maximum path length is\n",
    "$\\mathcal{O}(1)$ for self-attention, since the minimum number of processing\n",
    "steps required to propagate information from any input position to any other\n",
    "position is a constant, regardless of the sequence length, within the same\n",
    "layer, whereas for recurrent, it is $\\mathcal{O}(T)$ because information from\n",
    "the first token in the sequence needs to pass through all the tokens to reach\n",
    "the last token. It is worth noting that when $T >> D$, self-attention is more\n",
    "computationally expensive than RNNs.\n",
    "\n",
    "### Self-Attention Enables Parallelism\n",
    "\n",
    "RNNs process data sequences in a serial manner, meaning they handle one element\n",
    "of the sequence at a time in a step-wise fashion. This sequential processing is\n",
    "due to the RNN's architecture, which relies on the previous step's hidden state\n",
    "to compute the current step's output. Because each step depends on the outcome\n",
    "of the previous step, RNNs inherently have a difficult time leveraging parallel\n",
    "computation within a sequence. The computational complexity of processing a\n",
    "sequence is therefore $\\mathcal{O}(T)$, as the model must iterate through each\n",
    "element of the sequence one after the other.\n",
    "\n",
    "GPT and other transformer-based models, on the other hand, use a different\n",
    "approach that allows for much more parallelization. Transformers process entire\n",
    "sequences of data at once, rather than step-by-step. This is possible due to the\n",
    "attention mechanism, which computes relationships between all elements in a\n",
    "sequence in parallel and hence the computational complexity of processing a\n",
    "sequence is $\\mathcal{O}(1)$ - irrespective of the sequence length $T$.\n",
    "\n",
    "#### Complexity per Layer\n",
    "\n",
    "Essentialy, this refers to the total computational complexity for processing a\n",
    "layer given an input sequence of length $T$ and model dimension $D$. For\n",
    "self-attention, this is $\\mathcal{O}(T^2 \\cdot D)$, indicating the complexity\n",
    "grows quadratically with the length of the input sequence and linearly with the\n",
    "dimension of the model. This is due to the pairwise interactions between\n",
    "elements in the sequence.\n",
    "\n",
    "#### Sequential Operations\n",
    "\n",
    "This column refers to the depth of sequential operations that cannot be\n",
    "parallelized. For self-attention, it's marked as $\\mathcal{O}(1)$, meaning that,\n",
    "in terms of depth of operations that must be executed sequentially (one after\n",
    "the other), self-attention does not increase with the sequence length. This is\n",
    "because, despite the quadratic computational complexity in terms of total\n",
    "operations ($\\mathcal{O}(T^2 \\cdot D)$), the attention computation is essentially\n",
    "computed via matrix multiplication in one go.\n",
    "\n",
    "#### Maximum Path Length\n",
    "\n",
    "This refers to the longest distance information must travel in the network from\n",
    "input to output. For self-attention, this is also $\\mathcal{O}(1)$, indicating\n",
    "that any output unit can directly attend to any input unit without intermediate\n",
    "steps. This contrasts with RNNs, where the path length can grow linearly with\n",
    "the sequence length ($\\mathcal{O}(T)$).\n",
    "\n",
    "The $\\mathcal{O}(1)$ in the \"Sequential Operations\" and \"Maximum Path Length\"\n",
    "columns for self-attention highlights its parallelization advantage and direct\n",
    "information flow capability. It does not, however, denote the overall\n",
    "computational complexity of processing a layer, which remains\n",
    "$\\mathcal{O}(T^2 \\cdot D)$ due to the pairwise attention computations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} More Intuition in Andrej Karpathy's Video\n",
    ":class: seealso\n",
    "\n",
    "There's much more intuition in Andrej Karpathy's video on the\n",
    "[Let's Build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY).\n",
    "He mentions things like:\n",
    "\n",
    "-   Averaging past context with for loops, the weakest for of aggregation.\n",
    "-   Matrix multiply as weighted aggregation.\n",
    "-   Adding softmax to make it a weighted average.\n",
    "-   Attention as communication.\n",
    "-   Attention has no notion of space and operates over sets.\n",
    "-   There is no communication across batch dimension.\n",
    "\n",
    "This\n",
    "[blog post](https://blog.matdmiller.com/posts/2023-06-10_transformers/notebook.html#previous-token-averages---building-intuition-for-self-attention)\n",
    "implements what Andrej mentioned in the video.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "In practice, given the same set of queries, keys, and values we may want our\n",
    "model to combine knowledge from different behaviors of the same attention\n",
    "mechanism, such as capturing dependencies of various ranges (e.g., shorter-range\n",
    "vs. longer-range) within a sequence. Thus, it may be beneficial to allow our\n",
    "attention mechanism to jointly use different representation subspaces of\n",
    "queries, keys, and values {cite}`zhang2023dive`.\n",
    "\n",
    "What this implies is that the natural language is a complex space, and as\n",
    "highlighted in the book _Speech and Language Processing_ by Jurafsky and Martin,\n",
    "there are distinct semantic, syntactic and discourse relationships that can hold\n",
    "between words in a sentence. For example, the verb \"ate\" in the sentence \"The\n",
    "cat ate the mouse\" has a semantic relationship with the noun \"cat\" and \"mouse\",\n",
    "and a syntactic relationship with the noun \"the\". It would be difficult for a\n",
    "single head to hold all such representations over just a singled weighted\n",
    "average, and this is where multi-head attention comes in.\n",
    "\n",
    "Multi-head attention is a mechanism that allows the model to jointly attend to\n",
    "information from different representation subspaces at different positions.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Eugene Yan's article\n",
    "[Some Intuition on Attention and the Transformer](https://eugeneyan.com/writing/attention/)\n",
    "provides good intuition on multi-head attention. We use his sample sequence\n",
    "_\"The chicken crossed the road carelessly\"_ to illustrate the intuition.\n",
    "\n",
    "-   One head might specifically capture the action-subject relationship, linking\n",
    "    \"crossed\" with \"chicken.\"\n",
    "-   Another head could focus on the action-object relationship, associating\n",
    "    \"crossed\" with \"road.\"\n",
    "-   Yet another head might explore the manner in which the action is performed,\n",
    "    connecting \"crossed\" with \"carelessly.\"\n",
    "\n",
    "### Definition\n",
    "\n",
    "The multi-head attention is a function that maps a query matrix\n",
    "$\\mathbf{Q} \\in \\mathbb{R}^{T \\times d_q}$, a key matrix\n",
    "$\\mathbf{K} \\in \\mathbb{R}^{T \\times d_k}$, and a value matrix\n",
    "$\\mathbf{V} \\in \\mathbb{R}^{T \\times d_v}$ to an output matrix defined as\n",
    "$\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) \\in \\mathbb{R}^{T \\times d_v}$.\n",
    "The function is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{for } h = 1, 2, \\ldots, H: \\\\\n",
    "\\mathbf{Q}_h = \\mathbf{Z} \\mathbf{W}_{h}^{\\mathbf{Q}} \\quad \\mathbf{K}_h = \\mathbf{Z} \\mathbf{W}_{h}^{\\mathbf{K}} \\quad \\mathbf{V}_h = \\mathbf{Z} \\mathbf{W}_{h}^{\\mathbf{V}} \\\\\n",
    "\\text{head}_h = \\text{Attention}(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h) \\\\\n",
    "\\mathbf{A} := \\text{MultiHead}(\\mathbf{Z}) = \\left(\\text{head}_1 \\oplus \\text{head}_2 \\oplus \\ldots \\oplus \\text{head}_H \\right) \\mathbf{W}^O\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where each $\\text{head}_h$ is the context vector $\\mathbf{C}_h$ obtained from\n",
    "the $h$-th head of the self-attention mechanism. The $\\oplus$ operator denotes\n",
    "the concatenation operation that concatenates the context vectors\n",
    "$\\mathbf{C}_1,\n",
    "\\mathbf{C}_2, \\ldots, \\mathbf{C}_H$ along the feature dimension,\n",
    "which essentially still result in the dimension $T \\times D$. The $\\mathbf{W}^O$\n",
    "is a learnable weight matrix that projects the concatenated context vectors back\n",
    "to the original dimensionality $D$.\n",
    "\n",
    "Some other notations:\n",
    "\n",
    "-   $H$ is the number of attention heads, which is a hyperparameter of the\n",
    "    multi-head attention mechanism.\n",
    "-   $\\mathbf{W}_{h}^{\\mathbf{Q}} \\in \\mathbb{R}^{D \\times d_q}$: The learnable\n",
    "    query weight matrix for the $h$-th head.\n",
    "    -   Note that $d_q = \\frac{D}{H}$, where $D$ is the hidden dimension of the\n",
    "        token embeddings.\n",
    "-   $\\mathbf{W}_{h}^{\\mathbf{K}} \\in \\mathbb{R}^{D \\times d_k}$: The key weight\n",
    "    matrix for the $h$-th head.\n",
    "    -   Note that $d_k = \\frac{D}{H}$, where $D$ is the hidden dimension of the\n",
    "        token embeddings.\n",
    "-   $\\mathbf{W}_{h}^{\\mathbf{V}} \\in \\mathbb{R}^{D \\times d_v}$: The value\n",
    "    weight matrix for the $h$-th head.\n",
    "    -   Note that $d_v = \\frac{D}{H}$, where $D$ is the hidden dimension of the\n",
    "        token embeddings.\n",
    "-   $\\mathbf{Q}_h$, $\\mathbf{K}_h$, and $\\mathbf{V}_h$ are the query, key, and\n",
    "    value matrices for the $h$-th head, respectively.\n",
    "-   $\\text{head}_h = \\text{Attention}(\\mathbf{Q}_h, \\mathbf{K}_h, \\mathbf{V}_h)$\n",
    "    is the context vector $\\mathbf{C}_h$ obtained from the $h$-th head of the\n",
    "    self-attention mechanism.\n",
    "-   $\\oplus$ is just\n",
    "    $\\text{Concat}(\\cdot)$, the concatenation operation that concatenates the head/context matrices $\\mathbf{C}_1, \\mathbf{C}_2, \\ldots, \\mathbf{C}_H$\n",
    "    along the feature dimension, resulting in a matrix of context vectors of\n",
    "    shape $\\mathbb{R}^{T \\times H \\cdot d_v} = \\mathbb{R}^{T \\times D}$.\n",
    "-   $\\mathbf{W}^O \\in \\mathbb{R}^{d_v \\times H \\cdot d_v}$ is a learnable weight\n",
    "    matrix that projects the concatenated context vectors back to the original\n",
    "    dimensionality $D$.\n",
    "\n",
    "\n",
    "Without the batch dimension $\\mathcal{B}$, the output matrix $\\mathbf{A}$ is\n",
    "of shape $\\mathbb{R}^{T \\times D}$, where each row $\\mathbf{a}_t$ of the output\n",
    "matrix $\\mathbf{A}$ is the new embedding of the token at position $t$ in the\n",
    "sequence, containing not only the semantic and positional information of the\n",
    "token itself, but also contextual information from the other tokens in the\n",
    "sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    __slots__ = [\n",
    "        \"d_model\",\n",
    "        \"d_k\",\n",
    "        \"d_q\",\n",
    "        \"d_v\",\n",
    "        \"H\",\n",
    "        \"W_Q\",\n",
    "        \"W_K\",\n",
    "        \"W_V\",\n",
    "        \"W_O\",\n",
    "        \"attention\",\n",
    "        \"dropout\",\n",
    "        \"context_vector\",\n",
    "        \"attention_weights\",\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        attention: Attention,\n",
    "        H: int,\n",
    "        d_model: int,\n",
    "        dropout: float = 0.1,\n",
    "        bias: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        assert d_model % H == 0, \"The number of heads must divide the embedding dimension.\"\n",
    "\n",
    "        # fmt: off\n",
    "        self.d_model   = d_model       # D\n",
    "        self.d_k       = d_model // H  # stay true to notations\n",
    "        self.d_q       = d_model // H\n",
    "        self.d_v       = d_model // H\n",
    "\n",
    "        self.H         = H             # number of heads\n",
    "\n",
    "        # shadow my notations, actually they are of shape D x D.\n",
    "        self.W_Q       = nn.Linear(self.d_model, self.d_q * self.H, bias=bias)  # D x D\n",
    "        self.W_K       = nn.Linear(self.d_model, self.d_k * self.H, bias=bias)\n",
    "        self.W_V       = nn.Linear(self.d_model, self.d_v * self.H, bias=bias)\n",
    "        self.W_O       = nn.Linear(self.d_model, self.d_model, bias=bias)\n",
    "\n",
    "        self.attention = attention\n",
    "        self.dropout   = nn.Dropout(p=dropout, inplace=False)\n",
    "\n",
    "        self.context_vector: torch.Tensor\n",
    "        self.attention_weights: torch.Tensor\n",
    "\n",
    "        # self._init_weights()\n",
    "        # fmt: on\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: torch.BoolTensor | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Notations\n",
    "        ---------\n",
    "        B:      Batch size\n",
    "        S or L: Source sequence length\n",
    "        T or L: Target sequence length\n",
    "        D:      Embedding dimension\n",
    "        H:      Number of heads\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query:  Although named as query, it is the embeddings `z` from the token_embedding + positional_embedding layer.\n",
    "                type:  torch.Tensor\n",
    "                shape: (B, S or T, D)\n",
    "        key:    Although named as key, it is the embeddings `z` from the token_embedding + positional_embedding layer.\n",
    "                type:  torch.Tensor\n",
    "                shape: (B, S or T, D)\n",
    "        value:  Although named as value, it is the embeddings `z` from the token_embedding + positional_embedding layer.\n",
    "                type:  torch.Tensor\n",
    "                shape: (B, S or T, D)\n",
    "        mask:   Mask to be applied to the attention scores.\n",
    "                type:  torch.BoolTensor\n",
    "                shape: (B, 1, S or T, S or T)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        O:  The output of the multi-headed attention mechanism.\n",
    "            type:  torch.Tensor\n",
    "            shape: (B, S or T, D)\n",
    "\n",
    "        Variables\n",
    "        ---------\n",
    "        W_Q.weight (D, D)\n",
    "        W_K.weight (D, D)\n",
    "        W_V.weight (D, D)\n",
    "        W_O.weight (D, D)\n",
    "        \"\"\"\n",
    "        # fmt: off\n",
    "        if mask is not None:\n",
    "            assert mask.ndim     == 4, f\"Mask should have 4 dimensions but got {mask.ndim}.\"\n",
    "            assert mask.shape[0] == query.shape[0], (\"Batch size of mask and query must match.\")\n",
    "            assert mask.shape[1] == 1, (\"Mask should have shape (batch_size, 1, seq_len, seq_len).\")\n",
    "            assert mask.shape[2] == mask.shape[3] == query.shape[1], (\"Mask should have shape (batch_size, 1, seq_len, seq_len).\")\n",
    "\n",
    "\n",
    "        Q = self.W_Q(query).contiguous() # Z @ W_Q -> LxD @ DxD = LxD -> [B, L, D]\n",
    "        K = self.W_K(key).contiguous()   # Z @ W_K\n",
    "        V = self.W_V(value).contiguous() # Z @ W_V\n",
    "\n",
    "        Q = self.transpose_qkv(Q)        # splitting happens -> [B, H, L, D]\n",
    "        K = self.transpose_qkv(K)\n",
    "        V = self.transpose_qkv(V)\n",
    "\n",
    "        # Attention\n",
    "        self.context_vector, self.attention_weights = self.attention(Q, K, V, mask)\n",
    "        context_vector_concat                       = self.reverse_transpose_qkv(self.context_vector)\n",
    "        # fmt: on\n",
    "\n",
    "        # mypy complains because it infers `O` as `Any` but it is actually a tensor.\n",
    "        # You can either cast it to tensor or use `self.W_O.forward(context_vector_concat)`.\n",
    "        O = self.W_O(context_vector_concat)  # context_vector_concat @ W_O -> LxD @ DxD = LxD\n",
    "        return O  # type: ignore[no-any-return]\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        \"\"\"See PyTorch's MultiHeadAttention code for reference.\"\"\"\n",
    "        # we assume _qkv_same_embed_dim is True\n",
    "        nn.init.xavier_uniform_(self.W_Q.weight)\n",
    "        nn.init.xavier_uniform_(self.W_K.weight)\n",
    "        nn.init.xavier_uniform_(self.W_V.weight)\n",
    "        nn.init.xavier_uniform_(self.W_O.weight)\n",
    "\n",
    "    def transpose_qkv(self, q_or_k_or_v: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Transposition for parallel computation of multiple attention heads.\n",
    "        Why does transpose allow parallel computation? So originally the shape of\n",
    "        the query, key, and value is (B, L, D), and we want to split the D into H\n",
    "        heads to become (B, L, H, D / H). But this is not the shape we want (could\n",
    "        be due to efficiency reasons), so we transpose the shape to (B, H, L, D / H)\n",
    "        so all heads can be computed in parallel (efficiently).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        q_or_k_or_v: The query, key, or value tensor.\n",
    "            type:  torch.Tensor\n",
    "            shape: (B, L, D)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        q_or_k_or_v: The transposed query, key, or value tensor.\n",
    "            type:  torch.Tensor\n",
    "            shape: (B, H, L, D / H)\n",
    "        \"\"\"\n",
    "        # fmt: off\n",
    "        # 1. q_or_k_or_v is shape (B, L, D)\n",
    "        # 2. aim to make it of shape (B, L, H, D / H = d_qkv)\n",
    "        batch_size, seq_len, _ = q_or_k_or_v.shape\n",
    "        q_or_k_or_v            = q_or_k_or_v.view(batch_size, seq_len, self.H, self.d_model // self.H)\n",
    "\n",
    "        # 3. switch H from 3rd to 2nd dimension, or in python swap 2nd to 1st dimension and 1st to 2nd dimension\n",
    "        #    shape (B, H, L, D / H = d_qkv)\n",
    "        q_or_k_or_v            = q_or_k_or_v.permute(0, 2, 1, 3)\n",
    "        # fmt: on\n",
    "        return q_or_k_or_v\n",
    "\n",
    "    def reverse_transpose_qkv(self, q_or_k_or_v: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Reverse the transposition operation for concatenating multiple attention heads.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        q_or_k_or_v: The query, key, or value tensor.\n",
    "            type:  torch.Tensor\n",
    "            shape: (B, H, L, D / H)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        q_or_k_or_v: The transposed query, key, or value tensor.\n",
    "            type:  torch.Tensor\n",
    "            shape: (B, L, D)\n",
    "        \"\"\"\n",
    "        # fmt: off\n",
    "        # 1. q_or_k_or_v is shape (B, H, L, D / H = d_qkv)\n",
    "        # 2. aim to make it of shape (B, L, H, D / H = d_qkv)\n",
    "        q_or_k_or_v = q_or_k_or_v.permute(0, 2, 1, 3)\n",
    "\n",
    "        # 3. Merge H and d_qkv into D\n",
    "        batch_size, seq_len, _, _ = q_or_k_or_v.shape\n",
    "        q_or_k_or_v = q_or_k_or_v.contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        # fmt: on\n",
    "        return q_or_k_or_v\n",
    "\n",
    "def construct_dummy_batch_future_masks(batch_size: int, seq_len: int) -> torch.BoolTensor:\n",
    "    \"\"\"Broadcast future mask from shape (L, L) to (B, L, L) then (B, 1, L, L).\"\"\"\n",
    "    # Create a lower triangular mask for a single sequence\n",
    "    future_mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool), diagonal=0).to(torch.bool)\n",
    "    future_mask = future_mask.contiguous()\n",
    "    # broadcast future mask from shape (L, L) to (B, L, L)\n",
    "    future_masks = future_mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "    # broadcast future mask from shape (B, L, L) to (B, 1, L, L)\n",
    "    future_masks = future_masks.unsqueeze(1)\n",
    "    return torch.BoolTensor(future_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=composer.device)\n",
    "generator.manual_seed(25)\n",
    "seed_all(composer.seed, seed_torch=True, set_torch_deterministic=False)\n",
    "\n",
    "B, H, T, D = 1, 1, composer.block_size, composer.d_model  # batch size, head, context length, embedding dimension\n",
    "W_q = nn.Linear(in_features=D, out_features=D, bias=False)\n",
    "W_k = nn.Linear(in_features=D, out_features=D, bias=False)\n",
    "W_v = nn.Linear(in_features=D, out_features=D, bias=False)\n",
    "\n",
    "q0 = W_q(z0_tok_embed_with_pos_embed)\n",
    "k0 = W_k(z0_tok_embed_with_pos_embed)\n",
    "v0 = W_v(z0_tok_embed_with_pos_embed)\n",
    "pprint(q0.shape)\n",
    "\n",
    "causal_attention = ScaledDotProductAttention()\n",
    "causal_mha = MultiHeadedAttention(attention=causal_attention, H=composer.H, d_model=composer.d_model, dropout=0.0)\n",
    "tril_mask = construct_dummy_batch_future_masks(B, T)\n",
    "\n",
    "A_BHL = causal_mha(q0, k0, v0, mask=tril_mask)\n",
    "\n",
    "attention_weights = causal_mha.attention_weights\n",
    "pprint(attention_weights.shape)\n",
    "show_attention_heatmaps(attention_weights, xlabel='Keys', ylabel='Queries', show_title=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Invariance\n",
    "\n",
    "Neural networks without the notion of sequence will be permutation invariant\n",
    "with respect to the inputs and the multi-head attention mechanism is no\n",
    "exception. If we ignore the the batch $\\mathcal{B}$ dimension for now, then if\n",
    "we switch two tokens in the sequence via $\\text{permute}(\\mathbf{Z})$, or more\n",
    "concretely, say we swap $x_1$ and $x_2$ in the sequence, then the output of the\n",
    "multi-head attention mechanism would be the same up to a permutation of the rows\n",
    "($1 <-> 2$). This is why we would need the positional encoding to break the\n",
    "permutation invariance.\n",
    "\n",
    "Some extended ideas can be found in the chapter on\n",
    "[Numerical Stability and Initialization](https://d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#breaking-the-symmetry),\n",
    "from the book Dive into Deep Learning {cite}`zhang2023dive`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying LayerNorm and Residual Connections to Multi-Head Attention Output\n",
    "\n",
    "Recall earlier on the discussion of using Layer Normalization (LayerNorm) on the\n",
    "output of the Multi-Head Attention mechanism with residual connections.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{AddNorm}(\\mathbf{x}) &= \\operatorname{LayerNorm}(\\mathbf{x} + \\operatorname{Sublayer}(\\mathbf{x})) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "-   The Multi-Head Attention layer is the $\\operatorname{Sublayer}(\\cdot)$\n",
    "    function. So $\\text{MultiHead}(\\cdot) := \\operatorname{Sublayer}(\\cdot)$.\n",
    "-   The output from the Multi-Head Attention to be\n",
    "    $\\mathbf{A} \\in \\mathbb{R}^{T \\times D}$, we would pass it to\n",
    "    $\\operatorname{LayerNorm}(\\mathbf{A} + \\text{MultiHead}(\\mathbf{A}))$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=composer.device)\n",
    "generator.manual_seed(25)\n",
    "seed_all(composer.seed, seed_torch=True, set_torch_deterministic=False)\n",
    "\n",
    "B, H, T, D = 1, 1, composer.block_size, composer.d_model  # batch size, head, context length, embedding dimension\n",
    "W_q = nn.Linear(in_features=D, out_features=D, bias=False)\n",
    "W_k = nn.Linear(in_features=D, out_features=D, bias=False)\n",
    "W_v = nn.Linear(in_features=D, out_features=D, bias=False)\n",
    "\n",
    "q0 = W_q(z0_tok_embed_with_pos_embed)\n",
    "k0 = W_k(z0_tok_embed_with_pos_embed)\n",
    "v0 = W_v(z0_tok_embed_with_pos_embed)\n",
    "\n",
    "causal_attention = ScaledDotProductAttention()\n",
    "causal_mha = MultiHeadedAttention(attention=causal_attention, H=composer.H, d_model=composer.d_model, dropout=0.0)\n",
    "tril_mask = construct_dummy_batch_future_masks(B, T)\n",
    "\n",
    "### AddNorm\n",
    "add_norm_1 = AddNorm(feature_dim=composer.d_model, dropout=0.0)\n",
    "z0_tok_embed_with_pos_embed_with_mha_and_addnorm1 = add_norm_1(z0_tok_embed_with_pos_embed, lambda z: causal_mha(z, z, z, mask=tril_mask))\n",
    "pprint(z0_tok_embed_with_pos_embed_with_mha_and_addnorm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positionwise Feed-Forward Networks\n",
    "\n",
    "The term \"positionwise feed-forward network\" (FFN) in the context of Transformer\n",
    "models refers to a dense neural network (otherwise known as multilayer\n",
    "perceptron) that operates on the output of the Multi-Head Attention mechanism.\n",
    "This component is called \"positionwise\" because it applies the **same**\n",
    "feed-forward neural network (FFN) **independently** and **identically** to each\n",
    "position $t$ in the sequence of length $T$.\n",
    "\n",
    "### Independent Processing\n",
    "\n",
    "In the Transformer architecture, after the Multi-Head Attention mechanism\n",
    "aggregates information from different positions in the sequence based on\n",
    "attention scores, each element (or position) $t$ in the sequence has an updated\n",
    "representation. The positionwise FFN then processes each of these updated\n",
    "representations. However, rather than considering the sequence as a whole or how\n",
    "elements relate to each other at this stage, the FFN operates on each position\n",
    "separately. This means that for a sequence of length $T$, the same FFN is\n",
    "applied $T$ times independently, and by extension, given a batch of sequences,\n",
    "the FFN is applied $T \\times \\mathcal{B}$ times, where $\\mathcal{B}$ is the\n",
    "batch size.\n",
    "\n",
    "### Identical Application\n",
    "\n",
    "The term \"using the same FFN\" signifies that the same set of parameters (weights\n",
    "and biases) of the feed-forward neural network is used for each position in the\n",
    "sequence. The rationale is that the transformation is consistent across all\n",
    "sequence positions, so each element is transformed by the same learned function.\n",
    "This means the weight matrices and bias vectors of the FFN are shared across all\n",
    "positions in the sequence. In other words, if a sequence has $T=3$\n",
    "positions/tokens, the weight matrices and bias vectors of the FFN are the same\n",
    "for all three positions.\n",
    "\n",
    "### Definition\n",
    "\n",
    "Typically, a positionwise FFN consists of two linear transformations with a\n",
    "non-linear activation function in between. The general form can be represented\n",
    "as follows.\n",
    "\n",
    "```{prf:definition} Position-wise Feedforward Networks\n",
    ":label: def-positionwise-ffn\n",
    "\n",
    "Given an input matrix $\\mathbf{Z} \\in \\mathbb{R}^{T \\times D}$, the\n",
    "position-wise feedforward network computes the output matrix\n",
    "$\\mathbf{Z}^{\\prime} \\in \\mathbb{R}^{T \\times D}$ via the following operations:\n",
    "\n",
    "$$\n",
    "\\mathbf{Z}^{\\prime}=\\sigma_Z\\left(\\mathbf{Z} \\mathbf{W}^{\\text{FF}}_1 + \\mathbf{b}^{\\text{FF}}_1\\right) \\mathbf{W}^{\\text{FF}}_2 + \\mathbf{b}^{\\text{FF}}_2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "-   $\\mathbf{W}^{\\text{FF}}_1 \\in \\mathbb{R}^{D \\times d_{\\text{ff}}}$ and\n",
    "    $\\mathbf{W}^{\\text{FF}}_2 \\in \\mathbb{R}^{d_{\\text{ff}} \\times D}$ are\n",
    "    learnable weight matrices.\n",
    "-   $\\mathbf{b}^{\\text{FF}}_1 \\in \\mathbb{R}^{d_{\\text{ff}}}$ and\n",
    "    $\\mathbf{b}^{\\text{FF}}_2 \\in \\mathbb{R}^{D}$ are learnable bias vectors.\n",
    "-   $\\sigma_Z$ is a non-linear activation function, such as the Gaussian Error\n",
    "    Linear Unit (GELU) or the Rectified Linear Unit (ReLU).\n",
    "```\n",
    "\n",
    "### Projection to a Higher Dimension Space\n",
    "\n",
    "In the Transformer architecture, the dimensionality of the hidden layer in the\n",
    "positionwise FFN, denoted as $d_{\\text{ff}}$, is often chosen to be larger than\n",
    "the dimensionality of the input and output embeddings, $D$. This means that the\n",
    "FFN projects the input embeddings into a higher-dimensional space before\n",
    "projecting them back to the original dimensionality.\n",
    "\n",
    "The motivation behind this design choice is to allow the model to learn more\n",
    "complex and expressive representations. By projecting the input embeddings into\n",
    "a higher-dimensional space, the model capacity is increased, and the FFN can\n",
    "capture more intricate patterns and relationships among the features. We then\n",
    "project back (\"unembedding\") the higher-dimensional representations to the\n",
    "original dimensionality to maintain the consistency of the model.\n",
    "\n",
    "In practice, a common choice for the dimensionality of the hidden layer is to\n",
    "set $d_{\\text{ff}}$ to be a multiple of the input and output dimensionality $D$.\n",
    "For example, in the original Transformer paper {cite}`vaswani2017attention`, the\n",
    "authors used $d_{\\text{ff}} = 4 \\times D$.\n",
    "\n",
    "### Gaussian Error Linear Unit (GELU)\n",
    "\n",
    "The Gaussian Error Linear Unit (GELU) is a non-linear activation function used\n",
    "in the context of neural networks, which allows the model to capture more\n",
    "complex patterns in the data compared to traditional activation functions like\n",
    "ReLU. The GELU activation function is defined as:\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) = x \\cdot \\Phi(x)\n",
    "$$\n",
    "\n",
    "where $x$ is the input to the activation function, and $\\Phi(x)$ represents the\n",
    "cumulative distribution function (CDF) of the standard Gaussian distribution.\n",
    "The GELU function, effectively, models inputs with a non-linear transformation\n",
    "that weights inputs by their value, with a probabilistic gating mechanism\n",
    "derived from the Gaussian distribution.\n",
    "\n",
    "The cumulative distribution function $\\Phi(x)$ for a standard Gaussian\n",
    "distribution is given by:\n",
    "\n",
    "$$\n",
    "\\Phi(x) = \\frac{1}{2} \\left[1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right]\n",
    "$$\n",
    "\n",
    "where $\\text{erf}$ denotes the error function, which is a special function\n",
    "integral of the Gaussian distribution. Combining these, the GELU function can be\n",
    "expressed as:\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) = x \\cdot \\frac{1}{2} \\left[1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right]\n",
    "$$\n",
    "\n",
    "I will not pretend I have went through the entire paper and motivation of GELU,\n",
    "but usually, when new and \"better\" activation functions are proposed, they\n",
    "usually serve as an alternative to the common activation functions such as ReLU\n",
    "etc, where they solve some of the problems that the common activation functions\n",
    "have. From the formulation, we can see that GELU obeys the following properties:\n",
    "\n",
    "-   **Non-linearity**: GELU introduces non-linearity to the model, a given\n",
    "    requirement.\n",
    "-   **Differentiability**: GELU is smooth and differentiable everywhere, which\n",
    "    is beneficial for gradient-based optimization methods.\n",
    "-   **Boundedness**: GELU seems to be bounded below by $-0.17$ and not upper\n",
    "    bounded, but practice we can show there is an upper bound if we normalize\n",
    "    the input.\n",
    "\n",
    "```{prf:remark} Approximation of GELU\n",
    ":label: remark-approx-gelu\n",
    "\n",
    "To further simplify the GELU function and enhance computational efficiency, an\n",
    "approximation of the Gaussian CDF is commonly used in practice (extracted from\n",
    "[Mathematical Analysis and Performance Evaluation of the GELU Activation Function in Deep Learning](https://www.hindawi.com/journals/jmath/2023/4229924/)):\n",
    "\n",
    "$$\n",
    "\\Phi(\\alpha x) \\approx \\frac{1}{2}\\left(1+\\tanh \\left(\\beta\\left(\\alpha x+\\gamma(\\alpha x)^3\\right)\\right)\\right),\n",
    "$$\n",
    "\n",
    "where $\\beta>0$ and $\\gamma \\in \\mathbb{R}$ are constants, selected to minimize\n",
    "approximation error. Substituting this approximation into the GELU function, we\n",
    "arrive at the final approximate form of the GELU activation function (Figure 1):\n",
    "\n",
    "$$\n",
    "\\operatorname{GELU}(x)=0.5 x\\left(1+\\tanh \\left(\\sqrt{\\frac{2}{\\pi}}\\left(x+0.044715 x^3\\right)\\right)\\right) .\n",
    "$$\n",
    "```\n",
    "\n",
    "```{prf:definition} GELU Activation Function\n",
    ":label: def-gelu\n",
    "\n",
    "For a matrix $\\mathbf{Z}$ with elements $\\mathbf{Z}_{t d}$ where $t$ indexes the\n",
    "sequence (from 1 to $T$ ) and $d$ indexes the feature dimension (from 1 to $D$\n",
    "), the GELU activation is applied **element-wise** to each element\n",
    "$\\mathbf{Z}_{t d}$ independently:\n",
    "\n",
    "$$\n",
    "\\operatorname{GELU}\\left(x_{t d}\\right)=x_{t d} \\cdot \\frac{1}{2}\\left[1+\\operatorname{erf}\\left(\\frac{x_{t d}}{\\sqrt{2}}\\right)\\right]\n",
    "$$\n",
    "```\n",
    "\n",
    "```{admonition} References\n",
    ":class: seealso\n",
    "\n",
    "-   [Mathematical Analysis and Performance Evaluation of the GELU Activation Function in Deep Learning](https://www.hindawi.com/journals/jmath/2023/4229924/)\n",
    "-   [Gaussian Error Linear Units (GELUs) ](https://arxiv.org/abs/1606.08415)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a Position-wise FeedForward Network (FFN) used in Transformer models.\n",
    "\n",
    "    This module applies two linear transformations with a non-linear activation\n",
    "    in between. It is often used after the multi-head self-attention layer\n",
    "    in Transformer models.\n",
    "\n",
    "    The naming convention for the linear layers ('context_fc' and 'context_projection') is inspired by\n",
    "    the functionality within the Transformer architecture:\n",
    "\n",
    "    - 'context_fc' (context fully connected): This layer expands the dimensionality\n",
    "    of the input features, creating a richer representation. The expansion factor\n",
    "    is often 4 in Transformer models, meaning the intermediate size is 4 times the\n",
    "    size of the input/output dimensions.\n",
    "\n",
    "    - 'context_projection' (context projection): This layer projects the expanded\n",
    "    features back down to the original dimension, synthesizing the information\n",
    "    processed by the 'context_fc' layer.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_ff: Optional[int] = None,\n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "        dropout: float = 0.1,\n",
    "        bias: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # fmt: off\n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model # typical value for d_ff in Transformer models\n",
    "\n",
    "        self.ffn = nn.ModuleDict({\n",
    "            'context_fc': nn.Linear(d_model, d_ff, bias=bias),\n",
    "            'activation': activation,\n",
    "            'context_projection': nn.Linear(d_ff, d_model, bias=bias),\n",
    "            'dropout': nn.Dropout(p=dropout, inplace=False),\n",
    "\n",
    "        })\n",
    "\n",
    "        # self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        \"\"\"Initialize parameters of the linear layers.\"\"\"\n",
    "        nn.init.xavier_uniform_(self.ffn[\"context_fc\"].weight)\n",
    "        if self.ffn[\"context_fc\"].bias is not None:\n",
    "            nn.init.constant_(self.ffn[\"context_fc\"].bias, 0)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.ffn[\"context_projection\"].weight)\n",
    "        if self.ffn[\"context_projection\"].bias is not None:\n",
    "            nn.init.constant_(self.ffn[\"context_projection\"].bias, 0)\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        z = self.ffn[\"context_fc\"](z)\n",
    "        z = self.ffn[\"activation\"](z)\n",
    "        z = self.ffn[\"dropout\"](z)\n",
    "        z = self.ffn[\"context_projection\"](z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying LayerNorm and Residual Connections to Positionwise FFN Output\n",
    "\n",
    "We also apply Layer Normalization (LayerNorm) and residual connections to the\n",
    "output of the positionwise FFN in a similar manner to the Multi-Head Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=composer.device)\n",
    "generator.manual_seed(25)\n",
    "seed_all(composer.seed, seed_torch=True, set_torch_deterministic=False)\n",
    "\n",
    "ffn = PositionwiseFeedForward(d_model=composer.d_model, d_ff=composer.d_ff, activation=nn.GELU(approximate=\"tanh\"), dropout=0.0)\n",
    "\n",
    "### AddNorm\n",
    "add_norm_2 = AddNorm(feature_dim=composer.d_model, dropout=0.0)\n",
    "z0_tok_embed_with_pos_embed_with_mha_and_addnorm1_and_ffn_addnorm2 = add_norm_2(z0_tok_embed_with_pos_embed_with_mha_and_addnorm1, ffn)\n",
    "pprint(z0_tok_embed_with_pos_embed_with_mha_and_addnorm1_and_ffn_addnorm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Head\n",
    "\n",
    "The softmax head is the final layer of the Transformer model that maps the\n",
    "output of the positionwise FFN to the output vocabulary. The logits (without batch dimension) will have a shape\n",
    "of $\\mathbb{R}^{T \\times V}$, where $V$ is the size of the output vocabulary.\n",
    "\n",
    "We also add another layer of Layer Normalization (LayerNorm) to the output of the\n",
    "positionwise FFN before the softmax head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_before_head = nn.LayerNorm(composer.d_model)\n",
    "\n",
    "head = Head(d_model=composer.d_model, vocab_size=composer.vocab_size)\n",
    "logits: torch.FloatTensor = head(ln_before_head(z0_tok_embed_with_pos_embed_with_mha_and_addnorm1_and_ffn_addnorm2))\n",
    "pprint(logits.shape) # [B, T, V]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sub-sequence will be able to predict the next token in the sequence with\n",
    "a probability distribution over the output vocabulary $\\mathcal{V}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all Together to form the GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Composer Configuration with Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We use dataclass here for easy instantiating with hydra\"\"\"\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from torch import nn\n",
    "\n",
    "class MultiHeadedAttentionConfig(BaseModel):\n",
    "    attention: Attention\n",
    "    d_model: int\n",
    "    H: int\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pydantic config.\"\"\"\n",
    "\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "\n",
    "# TODO: add `field_validator` such that if `d_ff` is `None`, then `d_ff` is set to `4 * d_model`.\n",
    "class PositionwiseFeedForwardConfig(BaseModel):\n",
    "    d_model: int\n",
    "    d_ff: int\n",
    "    activation: nn.Module = Field(default=nn.GELU(approximate=\"tanh\"))\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = True\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pydantic config.\"\"\"\n",
    "\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "\n",
    "class AddNormConfig(BaseModel):\n",
    "    feature_dim: int\n",
    "    dropout: float\n",
    "\n",
    "\n",
    "class DecoderBlockConfig(BaseModel):\n",
    "    masked_self_attention_mha: MultiHeadedAttentionConfig\n",
    "    feed_forward: PositionwiseFeedForwardConfig\n",
    "    add_norm_1: AddNormConfig\n",
    "    add_norm_2: AddNormConfig\n",
    "\n",
    "\n",
    "class DecoderConfig(BaseModel):\n",
    "    d_model: int\n",
    "    vocab_size: int\n",
    "    context_length: int  # NOTE: alias=max_seq_len,block_size\n",
    "    num_decoder_blocks: int\n",
    "    dropout: float\n",
    "    decoder_block: DecoderBlockConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_self_attention_mha_config = MultiHeadedAttentionConfig(attention=ScaledDotProductAttention(), d_model=composer.d_model, H=composer.H, dropout=0.1)\n",
    "\n",
    "feed_forward_config = PositionwiseFeedForwardConfig(d_model=composer.d_model, d_ff=composer.d_ff, activation=nn.GELU(approximate=\"tanh\"), dropout=0.1, bias=True)\n",
    "\n",
    "add_norm_config_1 = AddNormConfig(feature_dim=composer.d_model, dropout=0.1)\n",
    "add_norm_config_2 = AddNormConfig(feature_dim=composer.d_model, dropout=0.1)\n",
    "\n",
    "# Create DecoderBlockConfig\n",
    "decoder_block_config = DecoderBlockConfig(\n",
    "    masked_self_attention_mha=masked_self_attention_mha_config,\n",
    "    feed_forward=feed_forward_config,\n",
    "    add_norm_1=add_norm_config_1,\n",
    "    add_norm_2=add_norm_config_2,\n",
    ")\n",
    "\n",
    "# Create the overall DecoderConfig\n",
    "model_config = DecoderConfig(\n",
    "    d_model=composer.d_model,\n",
    "    vocab_size=composer.vocab_size,\n",
    "    context_length=composer.block_size,\n",
    "    num_decoder_blocks=1,\n",
    "    dropout=0.1,\n",
    "    decoder_block=decoder_block_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch Composer\n",
    "\n",
    "composer.model_config = model_config\n",
    "pprint(composer.model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Blocks\n",
    "\n",
    "The decoder block consists of the following transformations in pseudo-code:\n",
    "\n",
    "```python\n",
    "z = z + self.attn(self.ln_1(z))\n",
    "z = z + self.mlp(self.ln_2(z))\n",
    "```\n",
    "\n",
    "Essentially, we take the embeddings output from the token and positional\n",
    "layers, and go through the following:\n",
    "\n",
    "```python\n",
    "z -> LayerNorm(z) -> MultiHeadAttention(LayerNorm(z)) -> z + MultiHeadAttention(LayerNorm(z))\n",
    "```\n",
    "\n",
    "then we pass the output through the positionwise FFN, abbreivate `z + MultiHeadAttention(LayerNorm(z))` as `z_mha`:\n",
    "\n",
    "```python\n",
    "z_mha -> LayerNorm(z_mha) -> PositionwiseFFN(LayerNorm(z_mha)) -> z_mha + PositionwiseFFN(LayerNorm(z_mha))\n",
    "```\n",
    "\n",
    "It is worth noting that my implementation of `AddNorm` is different from the GPT-2 paper, with \n",
    "`self.layer_norm(x + sublayer(self.dropout(x)))` instead of `x + sublayer(self.layer_norm(x))`.\n",
    "Both works, but the former is the one mentioned in the GPT-2 paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDecoderBlock(nn.Module):\n",
    "    \"\"\"GPTDecoderBlock focuses on masked self-attention and feed-forward layers.\n",
    "\n",
    "    The architecture follows the GPT-style decoder, which only has masked\n",
    "    self-attention and position-wise feed-forward layers, omitting the\n",
    "    encoder-decoder cross-attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: DecoderConfig) -> None:\n",
    "        super().__init__()\n",
    "        # fmt: off\n",
    "        self.masked_self_attention_mha = MultiHeadedAttention(**config.decoder_block.masked_self_attention_mha.model_dump(mode=\"python\"))\n",
    "        self.feed_forward              = PositionwiseFeedForward(**config.decoder_block.feed_forward.model_dump(mode=\"python\"))\n",
    "        self.add_norm_1                = AddNorm(**config.decoder_block.add_norm_1.model_dump(mode=\"python\"))\n",
    "        self.add_norm_2                = AddNorm(**config.decoder_block.add_norm_2.model_dump(mode=\"python\"))\n",
    "        # fmt: on\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        z: torch.Tensor,  # that's tgt in torch code base\n",
    "        *,\n",
    "        target_masks: torch.BoolTensor,  # that's tgt_mask in torch code base\n",
    "    ) -> torch.Tensor:\n",
    "        z = self.add_norm_1(\n",
    "            z,\n",
    "            lambda z: self.masked_self_attention_mha(query=z, key=z, value=z, mask=target_masks),\n",
    "        )\n",
    "        z = self.add_norm_2(z, self.feed_forward)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast, overload, Type\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing_extensions import override, TypeAlias\n",
    "\n",
    "class _NotGiven:\n",
    "\n",
    "    _instance: _NotGiven | None = None\n",
    "\n",
    "    def __new__(cls: Type[_NotGiven]) -> _NotGiven:  # noqa: PYI034\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(_NotGiven, cls).__new__(cls)  # noqa: UP008\n",
    "        return cls._instance\n",
    "\n",
    "    def __bool__(self) -> Literal[False]:\n",
    "        \"\"\"\n",
    "        This method is used to define the boolean value of an instance of `_NotGiven`.\n",
    "        By returning `False`, it allows `_NotGiven` to be used in boolean contexts (like\n",
    "        `if` statements) to signify the absence of a value. This is especially useful\n",
    "        for checking if an argument was provided or not in a function.\n",
    "        \"\"\"\n",
    "        return False\n",
    "\n",
    "    @override\n",
    "    def __repr__(self) -> Literal[\"NOT_GIVEN\"]:\n",
    "        return \"NOT_GIVEN\"\n",
    "\n",
    "    def __setattr__(self, key: str, value: Any) -> None:\n",
    "        raise AttributeError(f\"{self.__class__.__name__} instances are immutable\")\n",
    "\n",
    "    def __delattr__(self, key: str) -> None:\n",
    "        raise AttributeError(f\"{self.__class__.__name__} instances are immutable\")\n",
    "\n",
    "\n",
    "NOT_GIVEN = _NotGiven()\n",
    "NotGiven: TypeAlias = _NotGiven\n",
    "\n",
    "\n",
    "def construct_dummy_batch_target_padding_masks(batch_size: int, seq_len: int) -> torch.BoolTensor:\n",
    "    \"\"\"Construct a dummy batch of target padding masks of shape (B, 1, L, L) which\n",
    "    assumes there is no padding token involved.\"\"\"\n",
    "\n",
    "    return torch.BoolTensor(torch.ones((batch_size, 1, seq_len, seq_len), dtype=torch.bool))\n",
    "\n",
    "\n",
    "class BaseDecoder(nn.Module, ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for a decoder in a transformer-like architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: DecoderConfig,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(\n",
    "        self,\n",
    "        input_tokens: torch.LongTensor,\n",
    "        *,  # force keyword only arguments to prevent errors\n",
    "        target_padding_masks: torch.BoolTensor | NotGiven = NOT_GIVEN,\n",
    "        future_masks: torch.BoolTensor | NotGiven = NOT_GIVEN,\n",
    "        encoder_hidden_states: torch.Tensor | NotGiven = NOT_GIVEN,  # that's memory in torch code base\n",
    "        encoder_hidden_states_masks: torch.BoolTensor | NotGiven = NOT_GIVEN,  # that's memory_mask in torch code base\n",
    "    ) -> torch.FloatTensor:\n",
    "        ...\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        \"\"\"Initializes weights of the given module using Xavier uniform initialization.\"\"\"\n",
    "        for p in module.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "# NOTE: seq_len <= context_length == max_seq_len\n",
    "class GPTDecoder(BaseDecoder):\n",
    "    def __init__(self, config: DecoderConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        # fmt: off\n",
    "        self.d_model       : int           = config.d_model\n",
    "        self.tok_embed     : nn.Embedding  = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_embed     : nn.Parameter  = nn.Parameter(torch.zeros(1, config.context_length, config.d_model))\n",
    "        self.decoder_blocks: nn.ModuleList = nn.ModuleList([GPTDecoderBlock(config) for _ in range(config.num_decoder_blocks)]) # PyTorch did not make ModuleList a proper container, maybe open a PR to make it inherit Generic[T]???\n",
    "\n",
    "        self.dropout       : nn.Dropout    = nn.Dropout(config.dropout)\n",
    "        self.layer_norm    : nn.LayerNorm  = nn.LayerNorm(config.d_model)\n",
    "        self.head          : nn.Linear     = nn.Linear(config.d_model, config.vocab_size)  # last layer\n",
    "        # fmt: on\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for parameter_name, parameter in self.named_parameters():\n",
    "            if parameter_name.endswith(\"context_projection.weight\"):\n",
    "                mean = 0.0\n",
    "                std_dev = 0.02 / torch.sqrt(torch.tensor(2 * config.num_decoder_blocks, dtype=torch.float))\n",
    "                torch.nn.init.normal_(parameter, mean=mean, std=std_dev)\n",
    "\n",
    "    @property\n",
    "    def total_trainable_parameters(self) -> int:\n",
    "        \"\"\"Returns the number of trainable parameters in the model.\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    @property\n",
    "    def total_parameters(self) -> int:\n",
    "        \"\"\"Returns the total number of parameters in the model, including non-trainable.\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    @override\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        normal_init_modules = (nn.Linear, nn.Embedding)\n",
    "        if isinstance(module, normal_init_modules):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if hasattr(module, \"bias\") and module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    @overload\n",
    "    def create_target_masks(\n",
    "        self, batch_size: int, seq_len: int, target_padding_masks: torch.BoolTensor, future_masks: torch.BoolTensor\n",
    "    ) -> torch.BoolTensor:\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def create_target_masks(\n",
    "        self, batch_size: int, seq_len: int, target_padding_masks: torch.BoolTensor, future_masks: NotGiven\n",
    "    ) -> torch.BoolTensor:\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def create_target_masks(\n",
    "        self, batch_size: int, seq_len: int, target_padding_masks: NotGiven, future_masks: torch.BoolTensor\n",
    "    ) -> torch.BoolTensor:\n",
    "        ...\n",
    "\n",
    "    @overload\n",
    "    def create_target_masks(\n",
    "        self, batch_size: int, seq_len: int, target_padding_masks: NotGiven, future_masks: NotGiven\n",
    "    ) -> torch.BoolTensor:\n",
    "        ...\n",
    "\n",
    "    def create_target_masks(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        seq_len: int,\n",
    "        target_padding_masks: torch.BoolTensor | NotGiven = NOT_GIVEN,\n",
    "        future_masks: torch.BoolTensor | NotGiven = NOT_GIVEN,\n",
    "    ) -> torch.BoolTensor:\n",
    "        target_masks_shape = (batch_size, 1, seq_len, seq_len)\n",
    "        if target_padding_masks is NOT_GIVEN and future_masks is NOT_GIVEN:\n",
    "            target_padding_masks = cast(\n",
    "                torch.BoolTensor, construct_dummy_batch_target_padding_masks(batch_size, seq_len)\n",
    "            )\n",
    "            future_masks = cast(torch.BoolTensor, construct_dummy_batch_future_masks(batch_size, seq_len))\n",
    "\n",
    "        # FIXME: CAN SOMEONE PLEASE HELP ME WITH TYPING HERE?? I AM SO STUCK IN CASTING HELL.\n",
    "        if target_padding_masks is NOT_GIVEN:\n",
    "            target_padding_masks = cast(\n",
    "                torch.BoolTensor, construct_dummy_batch_target_padding_masks(batch_size, seq_len)\n",
    "            )\n",
    "\n",
    "        if future_masks is NOT_GIVEN:\n",
    "            future_masks = cast(torch.BoolTensor, construct_dummy_batch_future_masks(batch_size, seq_len))\n",
    "\n",
    "        assert target_padding_masks.shape == future_masks.shape == target_masks_shape  # type: ignore[union-attr]\n",
    "\n",
    "        return cast(\n",
    "            torch.BoolTensor,\n",
    "            torch.logical_and(cast(torch.Tensor, target_padding_masks), cast(torch.Tensor, future_masks)).bool(),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_tokens: torch.LongTensor,\n",
    "        *,  # force keyword only arguments to prevent errors\n",
    "        target_padding_masks: torch.BoolTensor | NotGiven = NOT_GIVEN,\n",
    "        future_masks: torch.BoolTensor | NotGiven = NOT_GIVEN,\n",
    "        # fmt: off\n",
    "        encoder_hidden_states: torch.Tensor | NotGiven = NOT_GIVEN,  # that's memory in torch code base and is ensured not used here\n",
    "        encoder_hidden_states_masks: torch.BoolTensor | NotGiven = NOT_GIVEN,\n",
    "        # that's memory_mask in torch code base and is ensured not used here\n",
    "        # fmt: on\n",
    "    ) -> torch.FloatTensor:\n",
    "        assert encoder_hidden_states is NOT_GIVEN, \"GPTDecoderBlock does not have encoder-decoder cross-attention\"\n",
    "        assert encoder_hidden_states_masks is NOT_GIVEN, \"GPTDecoderBlock does not have encoder-decoder cross-attention\"\n",
    "\n",
    "        # fmt: off\n",
    "        batch_size  : int              = input_tokens.size(0)\n",
    "        seq_len     : int              = input_tokens.size(1) # note seq_len <= context_length in decoder\n",
    "        target_masks: torch.BoolTensor = self.create_target_masks(batch_size=batch_size, seq_len=seq_len, target_padding_masks=target_padding_masks, future_masks=future_masks)\n",
    "\n",
    "        target_masks = target_masks.to(input_tokens.device) # type: ignore[assignment]\n",
    "\n",
    "        z = self.tok_embed(input_tokens) # * math.sqrt(self.d_model) for better optimization landscape\n",
    "        z = z + self.pos_embed[:, :seq_len, :]\n",
    "        z = self.dropout(z)\n",
    "\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            z  = decoder_block(z, target_masks=target_masks)\n",
    "\n",
    "        z      = self.layer_norm(z)\n",
    "        logits: torch.FloatTensor = self.head(z)\n",
    "        # fmt: on\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        starting_tokens: torch.LongTensor | List[int],  # alias is starting_tokens\n",
    "        *,\n",
    "        max_tokens: int = 100,  # max tokens to generate\n",
    "        temperature: float = 1.0,  # temperature for sampling\n",
    "        greedy: bool = False,  # if True, sample greedily\n",
    "        top_k: int | None = None,  # if not None, sample from top k tokens\n",
    "        top_p: float | None = None,  # neclueus sampling\n",
    "    ) -> torch.LongTensor:\n",
    "        if self.training:\n",
    "            # a safety check to make sure we are not in training mode\n",
    "            # this generate could be called outside after training, or during\n",
    "            # training as a form of validation/evaluation.\n",
    "            self.eval()\n",
    "\n",
    "        # NOTE: `starting_tokens` is a list of integers, or a torch.LongTensor of shape (S or T).\n",
    "        # the distinction between this `starting_tokens` versus the one in `forward` is this is\n",
    "        # not batched! It is a single sequence of tokens so in order for it to be compatible\n",
    "        # with the model, we need to expand the first dimension to 1 - making it a batch.\n",
    "        if isinstance(starting_tokens, list):\n",
    "            starting_tokens = cast(torch.LongTensor, torch.as_tensor(starting_tokens, dtype=torch.long)[None, ...])\n",
    "\n",
    "        if starting_tokens.dim() == 1:\n",
    "            starting_tokens = cast(torch.LongTensor, torch.as_tensor(starting_tokens, dtype=torch.long)[None, ...])  # type: ignore[no-redef]\n",
    "        assert starting_tokens.dim() == 2, \"starting_tokens must be a 1D or 2D tensor\"\n",
    "\n",
    "        for _ in range(max_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at context_length\n",
    "            starting_tokens_cropped = (\n",
    "                starting_tokens[:, -self.config.context_length :]\n",
    "                if starting_tokens.size(1) > self.config.context_length\n",
    "                else starting_tokens\n",
    "            )\n",
    "\n",
    "            batch_size = starting_tokens_cropped.size(0)\n",
    "            seq_len = starting_tokens_cropped.size(1)  # this must be less than or equal to self.config.context_length\n",
    "\n",
    "            target_padding_masks = construct_dummy_batch_target_padding_masks(batch_size, seq_len)\n",
    "            future_masks = construct_dummy_batch_future_masks(batch_size, seq_len)\n",
    "\n",
    "            logits = self(\n",
    "                starting_tokens_cropped,\n",
    "                target_padding_masks=target_padding_masks,\n",
    "                future_masks=future_masks,\n",
    "            )\n",
    "            assert logits.shape == (batch_size, seq_len, self.config.vocab_size)\n",
    "\n",
    "            # NOTE: we are only interested in the last token's logits because in\n",
    "            # autoregressive models, the last token's logits holds the contextual\n",
    "            # information of all previous tokens (because it is the only token\n",
    "            # not masked). But in any case, we need this last token's logits to\n",
    "            # sample the next token.\n",
    "            logits = logits[:, -1, :]  # shape: (batch_size, vocab_size)\n",
    "            assert logits.shape == (batch_size, self.config.vocab_size)\n",
    "\n",
    "            # now scale by temperature\n",
    "            logits = logits / (temperature + 1e-8)  # add epsilon to prevent division by zero\n",
    "\n",
    "            # optional cropping of logits to top k\n",
    "            if top_k is not None:\n",
    "                top_k_values, _ = torch.topk(logits, k=top_k)\n",
    "                # The masking out to -inf is to prevent the sampling from\n",
    "                # non-top k values, effectively making the sampling pool\n",
    "                # to be only the top k values. We are zeroing out the\n",
    "                # probabilities of non-top k values.\n",
    "                logits[logits < top_k_values[:, [-1]]] = float(\"-inf\")\n",
    "\n",
    "            if top_p is not None:\n",
    "\n",
    "                def top_p_logits(logits: torch.Tensor, p: float) -> torch.Tensor:\n",
    "                    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                    # Remove tokens with cumulative probability above the threshold\n",
    "                    sorted_indices_to_remove = cumulative_probs > p\n",
    "                    # Shift the indices to the right to keep also the first token above the threshold\n",
    "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                    sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                    # Scatter sorted tensors to original indexing\n",
    "                    indices_to_remove = sorted_indices.scatter(\n",
    "                        dim=-1, index=sorted_indices, src=sorted_indices_to_remove\n",
    "                    )\n",
    "                    logits[indices_to_remove] = float(\"-inf\")\n",
    "                    return logits\n",
    "\n",
    "                logits = top_p_logits(logits, top_p)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            next_token = (\n",
    "                torch.multinomial(probs, num_samples=1) if not greedy else torch.topk(probs, k=1, dim=-1).indices\n",
    "            )\n",
    "\n",
    "            # append the next token to the input tokens, aka append sampled index\n",
    "            # to the running sequence context and continue the generation\n",
    "            starting_tokens = torch.cat([starting_tokens, next_token], dim=1)  # type: ignore[assignment]\n",
    "        return starting_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator(device=composer.device)\n",
    "generator.manual_seed(25)\n",
    "seed_all(composer.seed, seed_torch=True, set_torch_deterministic=False)\n",
    "\n",
    "train_batch = get_batch(composer, split=\"train\", batch_size=composer.batch_size, block_size=composer.block_size, device=composer.device, generator=generator)\n",
    "x, y = train_batch\n",
    "assert x.shape == (composer.batch_size, composer.block_size)\n",
    "\n",
    "model = GPTDecoder(model_config).to(composer.device)\n",
    "pprint(model.total_trainable_parameters)\n",
    "\n",
    "logits = model(x)\n",
    "assert logits.shape == (composer.batch_size, composer.block_size, composer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end, we have covered the implementations of the GPT-2 architecture, for\n",
    "the training and inference mechanism, please see the next\n",
    "[series](https://www.gaohongnan.com/transformer/decoder/adder.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Further Readings\n",
    "\n",
    "```{admonition} References\n",
    ":class: seealso\n",
    "\n",
    "-   [Layer Normalization - Dive into Deep Learning](https://d2l.ai/chapter_convolutional-modern/batch-norm.html#layer-normalization)\n",
    "-   [Residual Connection and Layer Normalization - Dive into Deep Learning](https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#residual-connection-and-layer-normalization)\n",
    "-   [Layer Normalization - arXiv](https://arxiv.org/abs/1607.06450)\n",
    "-   [PyTorch Documentation: torch.nn.LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)\n",
    "-   [Positional Encodings - Aman Chadha](https://aman.ai/primers/ai/transformers/#positional-encoding)\n",
    "-   [Self-Attention and Positional Encoding - Dive into Deep Learning](https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html)\n",
    "-   [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\n",
    "-   [Positional Encoding - Brandon Rohrer](https://e2eml.school/transformers.html#positional_encoding)\n",
    "-   [Transformers from Scratch - Brandon Rohrer](https://e2eml.school/transformers.html)\n",
    "-   [Primers • Transformers - Aman Chadha](https://aman.ai/primers/ai/transformers/#positional-encoding)\n",
    "-   [The Illustrated GPT-2 (Visualizing Transformer Language Models) - Jay Alammar](https://jalammar.github.io/illustrated-gpt2/)\n",
    "-   [The Transformer Family Version 2.0 - Lilian Weng](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2)\n",
    "-   [The Random Transformer - hackerllama](https://osanseviero.github.io/hackerllama/blog/posts/random_transformer)\n",
    "-   [Transformers From Scratch - Mat Miller](https://blog.matdmiller.com/posts/2023-06-10_transformers/notebook.html)\n",
    "-   [Andrej Karpathy GPT Implementation Google Colab](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=JB82yzt44REI)\n",
    "-   [Transformers: a Primer - Justin Seonyong Lee](http://www.columbia.edu/~jsl2239/transformers.html)\n",
    "-   [OpenAI GPT Models - Lei Mao](https://leimao.github.io/article/OpenAI-GPT-Models/)\n",
    "-   [Transformer Explained in One Single Page - Lei Mao](https://leimao.github.io/blog/Transformer-Explained/)\n",
    "-   [Autoregressive models](https://deepgenerativemodels.github.io/notes/autoregressive/)\n",
    "-   [The Annotated Transformer - Harvard NLP](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "-   [Tutorial 6: Transformers and Multi-Head Attention - UvA](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)\n",
    "-   [Some Intuition on Attention and the Transformer - Eugene Yan](https://eugeneyan.com/writing/attention/)\n",
    "-   [Let's build GPT: from scratch, in code, spelled out - Andrej Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
    "-   [Why does the transformer do better than RNN and LSTM in long-range context dependencies?](https://ai.stackexchange.com/questions/20075/why-does-the-transformer-do-better-than-rnn-and-lstm-in-long-range-context-depen)\n",
    "-   [How Transformer is Bidirectional - Machine Learning](https://stackoverflow.com/questions/55158554/how-transformer-is-bidirectional-machine-learning)\n",
    "-   [Neural Machine Translation by Jointly Learning to Align and Translate - arXiv](https://arxiv.org/abs/1409.0473)\n",
    "-   [Building Language Models from Scratch - Sebastian Raschka](https://github.com/rasbt/LLMs-from-scratch)\n",
    "-   [Numerical Stability and Initialization - Dive into Deep Learning](https://d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#breaking-the-symmetry)\n",
    "-   [Background: What is a Generative Model? - Google Developers](https://developers.google.com/machine-learning/gan/generative) \n",
    "-   [Why can we approximate the joint probability distribution using the output vector of the GPT model?](https://ai.stackexchange.com/questions/12579/why-can-we-approximate-the-joint-probability-distribution-using-the-output-vecto)\n",
    "-   [Why Joint Probability in Generative Models? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/65806/why-joint-probability-in-generative-models)\n",
    "-   [CSC412 Winter 2020: Probabilsitic Machine Learning - University of Toronto](https://probmlcourse.github.io/csc412/lectures/week_2/)\n",
    "-   [CS324 - Large Language Models - Stanford University](https://stanford-cs324.github.io/winter2022/lectures/introduction/)\n",
    "-   [Joint Probability Mass Function (PMF)](https://www.probabilitycourse.com/chapter5/5_1_1_joint_pmf.php)\n",
    "-   [Difference Between Joint Probability Distribution and Conditional Probability Distribution - Math Stack Exchange](https://math.stackexchange.com/questions/1566215/difference-between-joint-probability-distribution-and-conditional-probability-di)\n",
    "-   [Residual Networks (ResNets) - Dive into Deep Learning](https://d2l.ai/chapter_convolutional-modern/resnet.html)\n",
    "-   [GPT-1, GPT-2, GPT-3, InstructGPT, ChatGPT, and GPT-4 Summary](https://songhuiming.github.io/pages/2023/05/28/gpt-1-gpt-2-gpt-3-instructgpt-chatgpt-and-gpt-4-summary/)\n",
    "-   [How to Optimize Data Transfers in CUDA C/C++](https://devblogs.nvidia.com/how-optimize-data-transfers-cuda-cc/) \n",
    "-   [How to Overlap Data Transfers in CUDA C/C++](https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^1]: [OpenAI tiktoken](https://github.com/openai/tiktoken)\n",
    "[^2]: [How to Optimize Data Transfers in CUDA C/C++](https://devblogs.nvidia.com/how-optimize-data-transfers-cuda-cc/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omniverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
