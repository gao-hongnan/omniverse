{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Brain_Dump-red)\n",
    "![Tag](https://img.shields.io/badge/Level-Beginner-green)\n",
    "[![Code](https://img.shields.io/badge/View-Code-blue?style=flat-square&logo=github)](https://github.com/gao-hongnan/omniverse/tree/main/omnivault/modules/lora.py)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge And Quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %pip install -U omniverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:57:07.988523Z",
     "iopub.status.busy": "2024-07-21T13:57:07.988143Z",
     "iopub.status.idle": "2024-07-21T13:57:25.264786Z",
     "shell.execute_reply": "2024-07-21T13:57:25.263801Z",
     "shell.execute_reply.started": "2024-07-21T13:57:07.988491Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "import math\n",
    "from typing import Any, Dict, List, Optional, TypedDict, Union\n",
    "\n",
    "import numpy as np\n",
    "import psutil\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from pydantic import BaseModel, Field\n",
    "from rich.pretty import pprint\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    Qwen2ForSequenceClassification,\n",
    "    Qwen2Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "\n",
    "from omnivault.utils.reproducibility.seed import seed_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T14:17:17.237495Z",
     "iopub.status.busy": "2024-07-21T14:17:17.236450Z",
     "iopub.status.idle": "2024-07-21T14:17:17.244263Z",
     "shell.execute_reply": "2024-07-21T14:17:17.243219Z",
     "shell.execute_reply.started": "2024-07-21T14:17:17.237456Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seed_all(42, seed_torch=True, set_torch_deterministic=False)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "MAX_LENGTH = 32\n",
    "PADDING = \"longest\"\n",
    "BATCH_SIZE = 32\n",
    "TRUNCATION = True\n",
    "RETURN_TENSORS = \"pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c843d6f8361243c5ad049faae3842dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/2037 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f5d0c9b8ff49df95465f3cd2ff2c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/227 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Batch(TypedDict):\n",
    "    sentence: List[str]\n",
    "    labels: List[int]\n",
    "\n",
    "\n",
    "class TokenizedBatch(TypedDict):\n",
    "    input_ids: List[int]\n",
    "    attention_mask: List[int]\n",
    "    labels: List[int]\n",
    "\n",
    "tokenizer = Qwen2Tokenizer.from_pretrained(\"Qwen/Qwen1.5-0.5B\", padding_side=\"left\")\n",
    "\n",
    "def preprocess_function(batch: Batch, **kwargs: Any) -> TokenizedBatch:\n",
    "    return tokenizer(batch[\"sentence\"], **kwargs)\n",
    "\n",
    "dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\", trust_remote_code=True)[\"train\"]\n",
    "dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "train_valid_split = dataset.train_test_split(test_size=0.1, shuffle=True, stratify_by_column=\"labels\")\n",
    "\n",
    "train_dataset = train_valid_split[\"train\"]\n",
    "valid_dataset = train_valid_split[\"test\"]\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    fn_kwargs={\"truncation\": TRUNCATION, \"padding\": PADDING, \"max_length\": MAX_LENGTH},\n",
    "    batched=True,\n",
    "    num_proc=psutil.cpu_count(logical=True),\n",
    "    batch_size=1000,\n",
    ").remove_columns([\"sentence\"])\n",
    "\n",
    "tokenized_valid_dataset = valid_dataset.map(\n",
    "    preprocess_function,\n",
    "    fn_kwargs={\"truncation\": TRUNCATION, \"padding\": PADDING, \"max_length\": MAX_LENGTH},\n",
    "    batched=True,\n",
    "    num_proc=psutil.cpu_count(logical=True),\n",
    "    batch_size=1000,\n",
    ").remove_columns([\"sentence\"])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T14:17:17.249922Z",
     "iopub.status.busy": "2024-07-21T14:17:17.249525Z",
     "iopub.status.idle": "2024-07-21T14:17:35.013715Z",
     "shell.execute_reply": "2024-07-21T14:17:35.012667Z",
     "shell.execute_reply.started": "2024-07-21T14:17:17.249895Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen1.5-0.5B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Qwen2ForSequenceClassification</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>model<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Qwen2Model</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>embed_tokens<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Embedding</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">151936</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>layers<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ModuleList</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span> x <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Qwen2DecoderLayer</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">(</span>self_attn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Qwen2Attention</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>q_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>k_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>v_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>o_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>rotary_emb<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Qwen2RotaryEmbedding</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">(</span>mlp<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Qwen2MLP</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>gate_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2816</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>up_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2816</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>down_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2816</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>act_fn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SiLU</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">(</span>input_layernorm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Qwen2RMSNorm</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">(</span>post_attention_layernorm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Qwen2RMSNorm</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>norm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Qwen2RMSNorm</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>score<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQwen2ForSequenceClassification\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mmodel\u001b[1m)\u001b[0m: \u001b[1;35mQwen2Model\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0membed_tokens\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m151936\u001b[0m, \u001b[1;36m1024\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0mlayers\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m-\u001b[1;36m23\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;36m24\u001b[0m x \u001b[1;35mQwen2DecoderLayer\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m(\u001b[0mself_attn\u001b[1m)\u001b[0m: \u001b[1;35mQwen2Attention\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mq_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mk_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mv_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mo_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mrotary_emb\u001b[1m)\u001b[0m: \u001b[1;35mQwen2RotaryEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mQwen2MLP\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mgate_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m2816\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mup_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m2816\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mdown_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m2816\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mact_fn\u001b[1m)\u001b[0m: \u001b[1;35mSiLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m(\u001b[0minput_layernorm\u001b[1m)\u001b[0m: \u001b[1;35mQwen2RMSNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m(\u001b[0mpost_attention_layernorm\u001b[1m)\u001b[0m: \u001b[1;35mQwen2RMSNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0mnorm\u001b[1m)\u001b[0m: \u001b[1;35mQwen2RMSNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mscore\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m3\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = Qwen2ForSequenceClassification.from_pretrained(\n",
    "    \"Qwen/Qwen1.5-0.5B\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"single_label_classification\",\n",
    ")\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "base_model = base_model.to(DEVICE)\n",
    "pprint(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters before LoRA: 463,990,784\n"
     ]
    }
   ],
   "source": [
    "def total_trainable_parameters(module: nn.Module) -> int:\n",
    "    \"\"\"Returns the number of trainable parameters in the model.\"\"\"\n",
    "    return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def total_parameters(module: nn.Module) -> int:\n",
    "    \"\"\"Returns the total number of parameters in the model, including non-trainable.\"\"\"\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "base_model_total_trainable = total_trainable_parameters(base_model)\n",
    "print(f\"Total trainable parameters before LoRA: {base_model_total_trainable:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_for_single_label_classification(eval_prediction: EvalPrediction) -> Dict[str, float | List[float]]:\n",
    "    logits, labels = eval_prediction.predictions, eval_prediction.label_ids\n",
    "    probs = softmax(logits, axis=-1)\n",
    "\n",
    "    num_classes = logits.shape[1]\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "\n",
    "    metrics = {\n",
    "        \"eval_log_loss\": log_loss(labels, probs),\n",
    "        \"eval_accuracy\": accuracy_score(labels, preds),\n",
    "        \"eval_precision_macro\": precision_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"eval_recall_macro\": recall_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"eval_f1_score_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"eval_precision_micro\": precision_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"eval_recall_micro\": recall_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"eval_f1_score_micro\": f1_score(labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"eval_confusion_matrix\": confusion_matrix(labels, preds).tolist(),\n",
    "        \"eval_roc_auc\": roc_auc_score(labels, probs, multi_class=\"ovr\"),\n",
    "        \"eval_pr_auc\": average_precision_score(labels, probs, average=\"macro\")\n",
    "    }\n",
    "\n",
    "    if num_classes == 2:\n",
    "        metrics[\"eval_brier_score\"] = brier_score_loss(labels, probs[:, 1], pos_label=1)\n",
    "    else:\n",
    "        brier_scores = [brier_score_loss(labels == i, probs[:, i]) for i in range(num_classes)]\n",
    "        metrics[\"eval_brier_score\"] = np.mean(brier_scores)\n",
    "\n",
    "    if num_classes > 2:\n",
    "        for class_index in range(num_classes):\n",
    "            fpr, tpr, _ = roc_curve(labels == class_index, probs[:, class_index])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            precision, recall, _ = precision_recall_curve(labels == class_index, probs[:, class_index])\n",
    "            pr_auc = auc(recall, precision)\n",
    "            metrics[f\"eval_roc_auc_class_{class_index}\"] = roc_auc\n",
    "            metrics[f\"eval_pr_auc_class_{class_index}\"] = pr_auc\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate With Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T14:17:35.015699Z",
     "iopub.status.busy": "2024-07-21T14:17:35.015371Z",
     "iopub.status.idle": "2024-07-21T14:17:35.592396Z",
     "shell.execute_reply": "2024-07-21T14:17:35.591195Z",
     "shell.execute_reply.started": "2024-07-21T14:17:35.015673Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/omniverse/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54e397a08b4424c81321ea96fb2510d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_log_loss'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.401785679417559</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.14096916299559473</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_precision_macro'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3000285877644368</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_recall_macro'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3223057644110276</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_f1_score_macro'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.11305118925439782</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_precision_micro'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.14096916299559473</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_recall_micro'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.14096916299559473</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_f1_score_micro'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.14096916299559473</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_confusion_matrix'</span>: <span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">132</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">53</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">]]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_roc_auc'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5319817168701279</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_pr_auc'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3596197342614926</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_brier_score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5561022597522776</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_roc_auc_class_0'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4730964467005076</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_pr_auc_class_0'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.12580756501576662</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_roc_auc_class_1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5628899835796387</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_pr_auc_class_1'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6492525648321494</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_roc_auc_class_2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5599587203302373</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_pr_auc_class_2'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2816445108714582</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_loss'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.431046962738037</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_runtime'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.0382</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_samples_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32.252</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'eval_steps_per_second'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.12</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_log_loss'\u001b[0m: \u001b[1;36m7.401785679417559\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_accuracy'\u001b[0m: \u001b[1;36m0.14096916299559473\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_precision_macro'\u001b[0m: \u001b[1;36m0.3000285877644368\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_recall_macro'\u001b[0m: \u001b[1;36m0.3223057644110276\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_f1_score_macro'\u001b[0m: \u001b[1;36m0.11305118925439782\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_precision_micro'\u001b[0m: \u001b[1;36m0.14096916299559473\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_recall_micro'\u001b[0m: \u001b[1;36m0.14096916299559473\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_f1_score_micro'\u001b[0m: \u001b[1;36m0.14096916299559473\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_confusion_matrix'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m27\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m132\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m6\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[1;36m53\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_roc_auc'\u001b[0m: \u001b[1;36m0.5319817168701279\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_pr_auc'\u001b[0m: \u001b[1;36m0.3596197342614926\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_brier_score'\u001b[0m: \u001b[1;36m0.5561022597522776\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_roc_auc_class_0'\u001b[0m: \u001b[1;36m0.4730964467005076\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_pr_auc_class_0'\u001b[0m: \u001b[1;36m0.12580756501576662\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_roc_auc_class_1'\u001b[0m: \u001b[1;36m0.5628899835796387\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_pr_auc_class_1'\u001b[0m: \u001b[1;36m0.6492525648321494\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_roc_auc_class_2'\u001b[0m: \u001b[1;36m0.5599587203302373\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_pr_auc_class_2'\u001b[0m: \u001b[1;36m0.2816445108714582\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_loss'\u001b[0m: \u001b[1;36m7.431046962738037\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_runtime'\u001b[0m: \u001b[1;36m7.0382\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_samples_per_second'\u001b[0m: \u001b[1;36m32.252\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'eval_steps_per_second'\u001b[0m: \u001b[1;36m4.12\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_valid_dataset,\n",
    "    compute_metrics=compute_metrics_for_single_label_classification,\n",
    ")\n",
    "\n",
    "valid_metrics = trainer.predict(tokenized_valid_dataset, metric_key_prefix=\"eval\")\n",
    "pprint(valid_metrics.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T14:17:36.596407Z",
     "iopub.status.busy": "2024-07-21T14:17:36.596052Z",
     "iopub.status.idle": "2024-07-21T14:17:36.610294Z",
     "shell.execute_reply": "2024-07-21T14:17:36.609367Z",
     "shell.execute_reply.started": "2024-07-21T14:17:36.596376Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LoraConfig(BaseModel):\n",
    "    r: int = Field(..., description=\"Lora attention dimension (the 'rank').\")\n",
    "    lora_alpha: int = Field(..., description=\"The alpha parameter for Lora scaling.\")\n",
    "    lora_dropout: float = Field(..., description=\"The dropout probability for Lora layers.\")\n",
    "    target_modules: List[str] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"The names of the modules to apply the adapter to. If specified, only the modules with the specified \"\n",
    "            \"names will be replaced. When passing a string, a regex match will be performed. When passing a list of \"\n",
    "            \"strings, either an exact match will be performed or it is checked if the name of the module ends with any \"\n",
    "            \"of the passed strings. If specified as 'all-linear', all linear/Conv1D modules are chosen, excluding the \"\n",
    "            \"output layer. If not specified, modules are chosen according to the model architecture. If the architecture \"\n",
    "            \"is unknown, an error will be raised—manual specification of target modules is required in such cases.\"\n",
    "        ),\n",
    "    )\n",
    "    linear_bias: bool = Field(default=True, description=\"To include linear bias or not.\")\n",
    "    modules_to_save: List[str] = Field(\n",
    "        default=None,\n",
    "        description=(\n",
    "            \"\"\"List of modules apart from adapter layers to be set as\n",
    "               trainable and saved in the final checkpoint.\"\"\"\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LoraConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">r</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">lora_alpha</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">lora_dropout</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">target_modules</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'q_proj'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'k_proj'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'v_proj'</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">linear_bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">modules_to_save</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'score'</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mLoraConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mr\u001b[0m=\u001b[1;36m4\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mlora_alpha\u001b[0m=\u001b[1;36m8\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mlora_dropout\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.1\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mtarget_modules\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'q_proj'\u001b[0m, \u001b[32m'k_proj'\u001b[0m, \u001b[32m'v_proj'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mlinear_bias\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mmodules_to_save\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'score'\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4, lora_alpha=8, lora_dropout=0.1, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"], modules_to_save=[\"score\"]\n",
    ")\n",
    "pprint(lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print out the target modules below. For simplicity, we target only the `q`, `k` and `v` layers for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T14:17:36.630816Z",
     "iopub.status.busy": "2024-07-21T14:17:36.630235Z",
     "iopub.status.idle": "2024-07-21T14:17:36.639150Z",
     "shell.execute_reply": "2024-07-21T14:17:36.638279Z",
     "shell.execute_reply.started": "2024-07-21T14:17:36.630786Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj\n",
      "model.layers.0.self_attn.k_proj\n",
      "model.layers.0.self_attn.v_proj\n",
      "model.layers.1.self_attn.q_proj\n",
      "model.layers.1.self_attn.k_proj\n",
      "model.layers.1.self_attn.v_proj\n",
      "model.layers.2.self_attn.q_proj\n",
      "model.layers.2.self_attn.k_proj\n",
      "model.layers.2.self_attn.v_proj\n",
      "model.layers.3.self_attn.q_proj\n",
      "model.layers.3.self_attn.k_proj\n",
      "model.layers.3.self_attn.v_proj\n",
      "model.layers.4.self_attn.q_proj\n",
      "model.layers.4.self_attn.k_proj\n",
      "model.layers.4.self_attn.v_proj\n",
      "model.layers.5.self_attn.q_proj\n",
      "model.layers.5.self_attn.k_proj\n",
      "model.layers.5.self_attn.v_proj\n",
      "model.layers.6.self_attn.q_proj\n",
      "model.layers.6.self_attn.k_proj\n",
      "model.layers.6.self_attn.v_proj\n",
      "model.layers.7.self_attn.q_proj\n",
      "model.layers.7.self_attn.k_proj\n",
      "model.layers.7.self_attn.v_proj\n",
      "model.layers.8.self_attn.q_proj\n",
      "model.layers.8.self_attn.k_proj\n",
      "model.layers.8.self_attn.v_proj\n",
      "model.layers.9.self_attn.q_proj\n",
      "model.layers.9.self_attn.k_proj\n",
      "model.layers.9.self_attn.v_proj\n",
      "model.layers.10.self_attn.q_proj\n",
      "model.layers.10.self_attn.k_proj\n",
      "model.layers.10.self_attn.v_proj\n",
      "model.layers.11.self_attn.q_proj\n",
      "model.layers.11.self_attn.k_proj\n",
      "model.layers.11.self_attn.v_proj\n",
      "model.layers.12.self_attn.q_proj\n",
      "model.layers.12.self_attn.k_proj\n",
      "model.layers.12.self_attn.v_proj\n",
      "model.layers.13.self_attn.q_proj\n",
      "model.layers.13.self_attn.k_proj\n",
      "model.layers.13.self_attn.v_proj\n",
      "model.layers.14.self_attn.q_proj\n",
      "model.layers.14.self_attn.k_proj\n",
      "model.layers.14.self_attn.v_proj\n",
      "model.layers.15.self_attn.q_proj\n",
      "model.layers.15.self_attn.k_proj\n",
      "model.layers.15.self_attn.v_proj\n",
      "model.layers.16.self_attn.q_proj\n",
      "model.layers.16.self_attn.k_proj\n",
      "model.layers.16.self_attn.v_proj\n",
      "model.layers.17.self_attn.q_proj\n",
      "model.layers.17.self_attn.k_proj\n",
      "model.layers.17.self_attn.v_proj\n",
      "model.layers.18.self_attn.q_proj\n",
      "model.layers.18.self_attn.k_proj\n",
      "model.layers.18.self_attn.v_proj\n",
      "model.layers.19.self_attn.q_proj\n",
      "model.layers.19.self_attn.k_proj\n",
      "model.layers.19.self_attn.v_proj\n",
      "model.layers.20.self_attn.q_proj\n",
      "model.layers.20.self_attn.k_proj\n",
      "model.layers.20.self_attn.v_proj\n",
      "model.layers.21.self_attn.q_proj\n",
      "model.layers.21.self_attn.k_proj\n",
      "model.layers.21.self_attn.v_proj\n",
      "model.layers.22.self_attn.q_proj\n",
      "model.layers.22.self_attn.k_proj\n",
      "model.layers.22.self_attn.v_proj\n",
      "model.layers.23.self_attn.q_proj\n",
      "model.layers.23.self_attn.k_proj\n",
      "model.layers.23.self_attn.v_proj\n"
     ]
    }
   ],
   "source": [
    "for module_name, _module in base_model.named_modules():\n",
    "    if any(target_module in module_name for target_module in lora_config.target_modules):\n",
    "        print(module_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T14:17:36.642238Z",
     "iopub.status.busy": "2024-07-21T14:17:36.641629Z",
     "iopub.status.idle": "2024-07-21T14:17:36.657713Z",
     "shell.execute_reply": "2024-07-21T14:17:36.656809Z",
     "shell.execute_reply.started": "2024-07-21T14:17:36.642206Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"LoRA: Low-Rank Adaptation of Large Language Models.\n",
    "\n",
    "References\n",
    "----------\n",
    "[1] https://pytorch.org/torchtune/stable/tutorials/lora_finetune.html\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def _lora_a_init_params(x: nn.Linear) -> None:\n",
    "    \"\"\"\n",
    "    Initialize LoRA A weight to Kaiming uniform.\n",
    "    \"\"\"\n",
    "    nn.init.kaiming_uniform_(x.weight, a=math.sqrt(5))\n",
    "\n",
    "\n",
    "def _lora_b_init_params(x: nn.Linear) -> None:\n",
    "    \"\"\"\n",
    "    Initialize LoRA B weight to zeros.\n",
    "    \"\"\"\n",
    "    nn.init.zeros_(x.weight)\n",
    "\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, bias: bool, rank: int, alpha: float, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # These are the weights from the original pretrained model\n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=bias)  # weight shape=[out_dim, in_dim]\n",
    "\n",
    "        # These are the new LoRA params. In general rank << in_dim, out_dim - do not put bias here\n",
    "        self.lora_a = nn.Linear(in_features=in_dim, out_features=rank, bias=False)  # weight shape=[rank, in_dim]\n",
    "        self.lora_b = nn.Linear(in_features=rank, out_features=out_dim, bias=False)  # weight shape=[out_dim, rank]\n",
    "\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self) -> None:\n",
    "        \"\"\"See https://github.com/microsoft/LoRA/blob/4c0333854cb905966f8cc4e9a74068c1e507c7b7/loralib/layers.py#L119.\"\"\"\n",
    "\n",
    "        _lora_a_init_params(self.lora_a)\n",
    "        _lora_b_init_params(self.lora_b)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # This would be the output of the original model\n",
    "        frozen_out = x @ self.linear.weight.T\n",
    "        if self.linear.bias is not None:\n",
    "            frozen_out += self.linear.bias\n",
    "\n",
    "        # lora_a projects inputs down to the much smaller self.rank,\n",
    "        # then lora_b projects back up to the output dimension\n",
    "        x = self.dropout(x)\n",
    "        lora_out = (x @ self.lora_a.weight.T) @ self.lora_b.weight.T\n",
    "        # Finally, scale by the alpha parameter (normalized by rank)\n",
    "        # and add to the original model's outputs\n",
    "        return frozen_out + (self.alpha / self.rank) * lora_out\n",
    "\n",
    "\n",
    "def apply_lora_to_base_model(\n",
    "    model: nn.Module, rank: int, alpha: float, dropout: float, target_modules: List[str] | None = None\n",
    ") -> None:\n",
    "    \"\"\"Recursively apply LoRA to a model. Only supports applying on `nn.Linear` layers.\"\"\"\n",
    "\n",
    "    for module_name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if target_modules is None or any(target in module_name for target in target_modules):\n",
    "                setattr(\n",
    "                    model,\n",
    "                    module_name,\n",
    "                    LoRALinear(\n",
    "                        in_dim=module.in_features,\n",
    "                        out_dim=module.out_features,\n",
    "                        rank=rank,\n",
    "                        alpha=alpha,\n",
    "                        dropout=dropout,\n",
    "                        bias=module.bias is not None,\n",
    "                    ),\n",
    "                )\n",
    "        else:\n",
    "            # Recursively apply LoRA to children modules\n",
    "            apply_lora_to_base_model(model=module, rank=rank, alpha=alpha, dropout=dropout, target_modules=target_modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T14:17:36.674509Z",
     "iopub.status.busy": "2024-07-21T14:17:36.674242Z",
     "iopub.status.idle": "2024-07-21T14:17:36.745588Z",
     "shell.execute_reply": "2024-07-21T14:17:36.744846Z",
     "shell.execute_reply.started": "2024-07-21T14:17:36.674487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_model_with_adapter = copy.deepcopy(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply recursively the `LoRA` module to the `q`, `k` and `v` layers\n",
    "via `apply_lora_to_base_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T14:18:14.886061Z",
     "iopub.status.busy": "2024-07-21T14:18:14.884997Z",
     "iopub.status.idle": "2024-07-21T14:18:17.774911Z",
     "shell.execute_reply": "2024-07-21T14:18:17.774068Z",
     "shell.execute_reply.started": "2024-07-21T14:18:14.886015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "apply_lora_to_base_model(\n",
    "    model=base_model_with_adapter,\n",
    "    rank=lora_config.r,\n",
    "    alpha=lora_config.lora_alpha,\n",
    "    dropout=lora_config.lora_dropout,\n",
    "    target_modules=lora_config.target_modules,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T14:18:17.777260Z",
     "iopub.status.busy": "2024-07-21T14:18:17.776796Z",
     "iopub.status.idle": "2024-07-21T14:18:17.788139Z",
     "shell.execute_reply": "2024-07-21T14:18:17.787111Z",
     "shell.execute_reply.started": "2024-07-21T14:18:17.777226Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters after LoRA before freezing: 464,580,608\n"
     ]
    }
   ],
   "source": [
    "base_model_with_adapter_total_trainable = total_trainable_parameters(base_model_with_adapter)\n",
    "print(f\"Total trainable parameters after LoRA before freezing: {base_model_with_adapter_total_trainable:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First `bias` is default to `True` in original model, but in LoRA we need to have it as `False`. \n",
    "You also see that currently the total trainable parameters are more than base model. Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589824"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_with_adapter_total_trainable - base_model_total_trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T14:18:17.789667Z",
     "iopub.status.busy": "2024-07-21T14:18:17.789302Z",
     "iopub.status.idle": "2024-07-21T14:18:17.804199Z",
     "shell.execute_reply": "2024-07-21T14:18:17.803305Z",
     "shell.execute_reply.started": "2024-07-21T14:18:17.789635Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = base_model_with_adapter.model.layers[0].self_attn.q_proj.linear.weight.shape[0]\n",
    "layers = base_model_with_adapter.model.layers.__len__()\n",
    "rank = lora_config.r\n",
    "num_target_modules = len(lora_config.target_modules)\n",
    "\n",
    "qkv_lora_weight_params = (dim * rank * 2) * layers * num_target_modules # 2 is the AB 1 each\n",
    "\n",
    "base_model_with_adapter_total_trainable - base_model_total_trainable ==  qkv_lora_weight_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional parameters is basically because we apply to `qkv` where each `qkv` has 24 layers each, so for each layer, say `q_proj` we would have an additional\n",
    "of `1024 * 4 * 2` because matrix A and B are mirrored to version of `[dim, rank]`. \n",
    "\n",
    "Now of course the next step is to freeze the base pretrained weights.\n",
    "Note we DO NOT want to freeze the `score` module as that is our classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T14:18:17.806542Z",
     "iopub.status.busy": "2024-07-21T14:18:17.806225Z",
     "iopub.status.idle": "2024-07-21T14:18:17.822881Z",
     "shell.execute_reply": "2024-07-21T14:18:17.821968Z",
     "shell.execute_reply.started": "2024-07-21T14:18:17.806510Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for parameter_name, parameter in base_model_with_adapter.named_parameters():\n",
    "    # We will set requires_grad to False if 'lora_' is not in the parameter name AND the parameter name does not contain any of the module names specified in modules_to_save\n",
    "    if \"lora_\" not in parameter_name and not any(\n",
    "        module_name in parameter_name for module_name in lora_config.modules_to_save\n",
    "    ):\n",
    "        parameter.requires_grad = False\n",
    "    else:\n",
    "        # Safeguard here parameters that are part of LoRA or specified modules are trainable\n",
    "        parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T14:18:17.824230Z",
     "iopub.status.busy": "2024-07-21T14:18:17.823904Z",
     "iopub.status.idle": "2024-07-21T14:18:17.838416Z",
     "shell.execute_reply": "2024-07-21T14:18:17.837492Z",
     "shell.execute_reply.started": "2024-07-21T14:18:17.824205Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters after LoRA after freezing: 592,896\n"
     ]
    }
   ],
   "source": [
    "base_model_with_adapter_total_trainable = total_trainable_parameters(base_model_with_adapter)\n",
    "print(f\"Total trainable parameters after LoRA after freezing: {base_model_with_adapter_total_trainable:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T14:18:17.840090Z",
     "iopub.status.busy": "2024-07-21T14:18:17.839705Z",
     "iopub.status.idle": "2024-07-21T14:18:17.848973Z",
     "shell.execute_reply": "2024-07-21T14:18:17.848161Z",
     "shell.execute_reply.started": "2024-07-21T14:18:17.840060Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1277818483567122"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(base_model_with_adapter_total_trainable / base_model_total_trainable) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only training on `~0.1277%` of the total parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    modules_to_save=[\"score\"],\n",
    ")\n",
    "pprint(lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    do_eval=True,\n",
    "    do_predict=False,\n",
    "    do_train=True,\n",
    "    warmup_ratio=0.0,\n",
    "    learning_rate=6e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"./artifacts\",\n",
    "    overwrite_output_dir=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    logging_steps=25,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=32,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=128,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.0,\n",
    "    save_total_limit=2,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    half_precision_backend=\"auto\",\n",
    "    optim=\"adamw_torch\",\n",
    "    label_smoothing_factor=0.0,\n",
    "    max_grad_norm=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T14:18:17.850422Z",
     "iopub.status.busy": "2024-07-21T14:18:17.850080Z",
     "iopub.status.idle": "2024-07-21T14:18:18.183048Z",
     "shell.execute_reply": "2024-07-21T14:18:18.182178Z",
     "shell.execute_reply.started": "2024-07-21T14:18:17.850390Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model_with_adapter,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_valid_dataset,\n",
    "    compute_metrics=compute_metrics_for_single_label_classification,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T14:18:19.344677Z",
     "iopub.status.busy": "2024-07-21T14:18:19.344346Z",
     "iopub.status.idle": "2024-07-21T14:22:47.214358Z",
     "shell.execute_reply": "2024-07-21T14:22:47.213395Z",
     "shell.execute_reply.started": "2024-07-21T14:18:19.344646Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='640' max='640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [640/640 04:26, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Score Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Score Micro</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "      <th>Brier Score</th>\n",
       "      <th>Roc Auc Class 0</th>\n",
       "      <th>Pr Auc Class 0</th>\n",
       "      <th>Roc Auc Class 1</th>\n",
       "      <th>Pr Auc Class 1</th>\n",
       "      <th>Roc Auc Class 2</th>\n",
       "      <th>Pr Auc Class 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.492000</td>\n",
       "      <td>1.867992</td>\n",
       "      <td>1.867991</td>\n",
       "      <td>0.436123</td>\n",
       "      <td>0.429408</td>\n",
       "      <td>0.397243</td>\n",
       "      <td>0.374103</td>\n",
       "      <td>0.436123</td>\n",
       "      <td>0.436123</td>\n",
       "      <td>0.436123</td>\n",
       "      <td>[[5, 9, 16], [4, 60, 76], [7, 16, 34]]</td>\n",
       "      <td>0.597633</td>\n",
       "      <td>0.434514</td>\n",
       "      <td>0.284848</td>\n",
       "      <td>0.537902</td>\n",
       "      <td>0.181680</td>\n",
       "      <td>0.668309</td>\n",
       "      <td>0.717904</td>\n",
       "      <td>0.586687</td>\n",
       "      <td>0.380112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.396700</td>\n",
       "      <td>1.043167</td>\n",
       "      <td>1.043167</td>\n",
       "      <td>0.590308</td>\n",
       "      <td>0.419821</td>\n",
       "      <td>0.488931</td>\n",
       "      <td>0.413872</td>\n",
       "      <td>0.590308</td>\n",
       "      <td>0.590308</td>\n",
       "      <td>0.590308</td>\n",
       "      <td>[[0, 3, 27], [0, 85, 55], [0, 8, 49]]</td>\n",
       "      <td>0.780956</td>\n",
       "      <td>0.545472</td>\n",
       "      <td>0.195950</td>\n",
       "      <td>0.700677</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.865928</td>\n",
       "      <td>0.894582</td>\n",
       "      <td>0.776264</td>\n",
       "      <td>0.482198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.063500</td>\n",
       "      <td>0.784113</td>\n",
       "      <td>0.784113</td>\n",
       "      <td>0.757709</td>\n",
       "      <td>0.808035</td>\n",
       "      <td>0.553718</td>\n",
       "      <td>0.575001</td>\n",
       "      <td>0.757709</td>\n",
       "      <td>0.757709</td>\n",
       "      <td>0.757709</td>\n",
       "      <td>[[5, 11, 14], [0, 138, 2], [0, 28, 29]]</td>\n",
       "      <td>0.848063</td>\n",
       "      <td>0.663932</td>\n",
       "      <td>0.131309</td>\n",
       "      <td>0.814890</td>\n",
       "      <td>0.475911</td>\n",
       "      <td>0.905255</td>\n",
       "      <td>0.913337</td>\n",
       "      <td>0.824045</td>\n",
       "      <td>0.588326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.734300</td>\n",
       "      <td>1.009085</td>\n",
       "      <td>1.009085</td>\n",
       "      <td>0.731278</td>\n",
       "      <td>0.881481</td>\n",
       "      <td>0.532749</td>\n",
       "      <td>0.579467</td>\n",
       "      <td>0.731278</td>\n",
       "      <td>0.731278</td>\n",
       "      <td>0.731278</td>\n",
       "      <td>[[9, 20, 1], [0, 140, 0], [0, 40, 17]]</td>\n",
       "      <td>0.895576</td>\n",
       "      <td>0.791378</td>\n",
       "      <td>0.148736</td>\n",
       "      <td>0.867513</td>\n",
       "      <td>0.659774</td>\n",
       "      <td>0.940887</td>\n",
       "      <td>0.956902</td>\n",
       "      <td>0.878328</td>\n",
       "      <td>0.751395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.502600</td>\n",
       "      <td>1.525477</td>\n",
       "      <td>1.525477</td>\n",
       "      <td>0.585903</td>\n",
       "      <td>0.397773</td>\n",
       "      <td>0.569841</td>\n",
       "      <td>0.408853</td>\n",
       "      <td>0.585903</td>\n",
       "      <td>0.585903</td>\n",
       "      <td>0.585903</td>\n",
       "      <td>[[29, 1, 0], [36, 104, 0], [52, 5, 0]]</td>\n",
       "      <td>0.893914</td>\n",
       "      <td>0.804567</td>\n",
       "      <td>0.219748</td>\n",
       "      <td>0.906430</td>\n",
       "      <td>0.707196</td>\n",
       "      <td>0.933826</td>\n",
       "      <td>0.958734</td>\n",
       "      <td>0.841486</td>\n",
       "      <td>0.742477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.888100</td>\n",
       "      <td>0.520843</td>\n",
       "      <td>0.520843</td>\n",
       "      <td>0.828194</td>\n",
       "      <td>0.790419</td>\n",
       "      <td>0.766291</td>\n",
       "      <td>0.758882</td>\n",
       "      <td>0.828194</td>\n",
       "      <td>0.828194</td>\n",
       "      <td>0.828194</td>\n",
       "      <td>[[23, 5, 2], [9, 131, 0], [10, 13, 34]]</td>\n",
       "      <td>0.933432</td>\n",
       "      <td>0.861995</td>\n",
       "      <td>0.088546</td>\n",
       "      <td>0.917090</td>\n",
       "      <td>0.735546</td>\n",
       "      <td>0.943268</td>\n",
       "      <td>0.957575</td>\n",
       "      <td>0.939938</td>\n",
       "      <td>0.888554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.567000</td>\n",
       "      <td>0.563086</td>\n",
       "      <td>0.563086</td>\n",
       "      <td>0.784141</td>\n",
       "      <td>0.730080</td>\n",
       "      <td>0.794486</td>\n",
       "      <td>0.747735</td>\n",
       "      <td>0.784141</td>\n",
       "      <td>0.784141</td>\n",
       "      <td>0.784141</td>\n",
       "      <td>[[23, 0, 7], [10, 106, 24], [5, 3, 49]]</td>\n",
       "      <td>0.941179</td>\n",
       "      <td>0.882236</td>\n",
       "      <td>0.099551</td>\n",
       "      <td>0.948393</td>\n",
       "      <td>0.822162</td>\n",
       "      <td>0.967816</td>\n",
       "      <td>0.965795</td>\n",
       "      <td>0.907327</td>\n",
       "      <td>0.855146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.487300</td>\n",
       "      <td>0.777104</td>\n",
       "      <td>0.777104</td>\n",
       "      <td>0.779736</td>\n",
       "      <td>0.779006</td>\n",
       "      <td>0.691688</td>\n",
       "      <td>0.646585</td>\n",
       "      <td>0.779736</td>\n",
       "      <td>0.779736</td>\n",
       "      <td>0.779736</td>\n",
       "      <td>[[25, 5, 0], [3, 137, 0], [24, 18, 15]]</td>\n",
       "      <td>0.932729</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.110960</td>\n",
       "      <td>0.935195</td>\n",
       "      <td>0.808896</td>\n",
       "      <td>0.966913</td>\n",
       "      <td>0.973352</td>\n",
       "      <td>0.896078</td>\n",
       "      <td>0.814109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.301500</td>\n",
       "      <td>0.518010</td>\n",
       "      <td>0.518011</td>\n",
       "      <td>0.881057</td>\n",
       "      <td>0.832276</td>\n",
       "      <td>0.789348</td>\n",
       "      <td>0.806632</td>\n",
       "      <td>0.881057</td>\n",
       "      <td>0.881057</td>\n",
       "      <td>0.881057</td>\n",
       "      <td>[[18, 2, 10], [0, 137, 3], [5, 7, 45]]</td>\n",
       "      <td>0.960663</td>\n",
       "      <td>0.891463</td>\n",
       "      <td>0.066480</td>\n",
       "      <td>0.954653</td>\n",
       "      <td>0.788685</td>\n",
       "      <td>0.980172</td>\n",
       "      <td>0.983320</td>\n",
       "      <td>0.947162</td>\n",
       "      <td>0.898182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.276500</td>\n",
       "      <td>0.517759</td>\n",
       "      <td>0.517759</td>\n",
       "      <td>0.859031</td>\n",
       "      <td>0.815993</td>\n",
       "      <td>0.774102</td>\n",
       "      <td>0.792039</td>\n",
       "      <td>0.859031</td>\n",
       "      <td>0.859031</td>\n",
       "      <td>0.859031</td>\n",
       "      <td>[[20, 4, 6], [1, 136, 3], [6, 12, 39]]</td>\n",
       "      <td>0.950659</td>\n",
       "      <td>0.884996</td>\n",
       "      <td>0.072842</td>\n",
       "      <td>0.946701</td>\n",
       "      <td>0.812426</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.976206</td>\n",
       "      <td>0.933849</td>\n",
       "      <td>0.862842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.148300</td>\n",
       "      <td>0.556958</td>\n",
       "      <td>0.556958</td>\n",
       "      <td>0.867841</td>\n",
       "      <td>0.812477</td>\n",
       "      <td>0.827652</td>\n",
       "      <td>0.818891</td>\n",
       "      <td>0.867841</td>\n",
       "      <td>0.867841</td>\n",
       "      <td>0.867841</td>\n",
       "      <td>[[24, 1, 5], [4, 130, 6], [6, 8, 43]]</td>\n",
       "      <td>0.947634</td>\n",
       "      <td>0.881595</td>\n",
       "      <td>0.072743</td>\n",
       "      <td>0.959898</td>\n",
       "      <td>0.819215</td>\n",
       "      <td>0.964532</td>\n",
       "      <td>0.975784</td>\n",
       "      <td>0.918473</td>\n",
       "      <td>0.845316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.164200</td>\n",
       "      <td>1.004750</td>\n",
       "      <td>0.958535</td>\n",
       "      <td>0.845815</td>\n",
       "      <td>0.859036</td>\n",
       "      <td>0.733835</td>\n",
       "      <td>0.778149</td>\n",
       "      <td>0.845815</td>\n",
       "      <td>0.845815</td>\n",
       "      <td>0.845815</td>\n",
       "      <td>[[17, 8, 5], [0, 138, 2], [2, 18, 37]]</td>\n",
       "      <td>0.955712</td>\n",
       "      <td>0.899185</td>\n",
       "      <td>0.085987</td>\n",
       "      <td>0.958206</td>\n",
       "      <td>0.836613</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>0.979499</td>\n",
       "      <td>0.937049</td>\n",
       "      <td>0.882014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>0.103600</td>\n",
       "      <td>0.576214</td>\n",
       "      <td>0.555393</td>\n",
       "      <td>0.903084</td>\n",
       "      <td>0.864440</td>\n",
       "      <td>0.825647</td>\n",
       "      <td>0.842744</td>\n",
       "      <td>0.903084</td>\n",
       "      <td>0.903084</td>\n",
       "      <td>0.903084</td>\n",
       "      <td>[[20, 4, 6], [0, 138, 2], [5, 5, 47]]</td>\n",
       "      <td>0.969390</td>\n",
       "      <td>0.925581</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>0.968020</td>\n",
       "      <td>0.869500</td>\n",
       "      <td>0.980501</td>\n",
       "      <td>0.980907</td>\n",
       "      <td>0.959649</td>\n",
       "      <td>0.927190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>0.134100</td>\n",
       "      <td>0.630044</td>\n",
       "      <td>0.630045</td>\n",
       "      <td>0.863436</td>\n",
       "      <td>0.791531</td>\n",
       "      <td>0.800877</td>\n",
       "      <td>0.792307</td>\n",
       "      <td>0.863436</td>\n",
       "      <td>0.863436</td>\n",
       "      <td>0.863436</td>\n",
       "      <td>[[22, 2, 6], [5, 133, 2], [10, 6, 41]]</td>\n",
       "      <td>0.956208</td>\n",
       "      <td>0.893957</td>\n",
       "      <td>0.075615</td>\n",
       "      <td>0.956937</td>\n",
       "      <td>0.823854</td>\n",
       "      <td>0.977011</td>\n",
       "      <td>0.974442</td>\n",
       "      <td>0.934675</td>\n",
       "      <td>0.882540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.858376</td>\n",
       "      <td>0.832104</td>\n",
       "      <td>0.889868</td>\n",
       "      <td>0.854262</td>\n",
       "      <td>0.795781</td>\n",
       "      <td>0.817613</td>\n",
       "      <td>0.889868</td>\n",
       "      <td>0.889868</td>\n",
       "      <td>0.889868</td>\n",
       "      <td>[[17, 6, 7], [0, 137, 3], [4, 5, 48]]</td>\n",
       "      <td>0.964955</td>\n",
       "      <td>0.915977</td>\n",
       "      <td>0.066761</td>\n",
       "      <td>0.967343</td>\n",
       "      <td>0.853372</td>\n",
       "      <td>0.974425</td>\n",
       "      <td>0.976990</td>\n",
       "      <td>0.953096</td>\n",
       "      <td>0.918129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>0.694719</td>\n",
       "      <td>0.655884</td>\n",
       "      <td>0.898678</td>\n",
       "      <td>0.853911</td>\n",
       "      <td>0.823266</td>\n",
       "      <td>0.836484</td>\n",
       "      <td>0.898678</td>\n",
       "      <td>0.898678</td>\n",
       "      <td>0.898678</td>\n",
       "      <td>[[20, 2, 8], [0, 137, 3], [5, 5, 47]]</td>\n",
       "      <td>0.966060</td>\n",
       "      <td>0.919359</td>\n",
       "      <td>0.061878</td>\n",
       "      <td>0.965144</td>\n",
       "      <td>0.858776</td>\n",
       "      <td>0.979269</td>\n",
       "      <td>0.981300</td>\n",
       "      <td>0.953767</td>\n",
       "      <td>0.918822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>0.047100</td>\n",
       "      <td>0.742118</td>\n",
       "      <td>0.700290</td>\n",
       "      <td>0.903084</td>\n",
       "      <td>0.881214</td>\n",
       "      <td>0.829114</td>\n",
       "      <td>0.850509</td>\n",
       "      <td>0.903084</td>\n",
       "      <td>0.903084</td>\n",
       "      <td>0.903084</td>\n",
       "      <td>[[20, 4, 6], [0, 137, 3], [3, 6, 48]]</td>\n",
       "      <td>0.969409</td>\n",
       "      <td>0.927101</td>\n",
       "      <td>0.061708</td>\n",
       "      <td>0.969543</td>\n",
       "      <td>0.873595</td>\n",
       "      <td>0.980788</td>\n",
       "      <td>0.984931</td>\n",
       "      <td>0.957895</td>\n",
       "      <td>0.923976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.823952</td>\n",
       "      <td>0.765865</td>\n",
       "      <td>0.898678</td>\n",
       "      <td>0.877139</td>\n",
       "      <td>0.818003</td>\n",
       "      <td>0.841421</td>\n",
       "      <td>0.898678</td>\n",
       "      <td>0.898678</td>\n",
       "      <td>0.898678</td>\n",
       "      <td>[[19, 5, 6], [0, 137, 3], [3, 6, 48]]</td>\n",
       "      <td>0.968208</td>\n",
       "      <td>0.923295</td>\n",
       "      <td>0.062997</td>\n",
       "      <td>0.968190</td>\n",
       "      <td>0.862989</td>\n",
       "      <td>0.980296</td>\n",
       "      <td>0.985568</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.922566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.814503</td>\n",
       "      <td>0.754467</td>\n",
       "      <td>0.898678</td>\n",
       "      <td>0.868936</td>\n",
       "      <td>0.814536</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.898678</td>\n",
       "      <td>0.898678</td>\n",
       "      <td>0.898678</td>\n",
       "      <td>[[19, 5, 6], [0, 138, 2], [4, 6, 47]]</td>\n",
       "      <td>0.967228</td>\n",
       "      <td>0.920084</td>\n",
       "      <td>0.062920</td>\n",
       "      <td>0.968697</td>\n",
       "      <td>0.860490</td>\n",
       "      <td>0.976847</td>\n",
       "      <td>0.982438</td>\n",
       "      <td>0.956140</td>\n",
       "      <td>0.921672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.811079</td>\n",
       "      <td>0.751328</td>\n",
       "      <td>0.898678</td>\n",
       "      <td>0.868936</td>\n",
       "      <td>0.814536</td>\n",
       "      <td>0.837095</td>\n",
       "      <td>0.898678</td>\n",
       "      <td>0.898678</td>\n",
       "      <td>0.898678</td>\n",
       "      <td>[[19, 5, 6], [0, 138, 2], [4, 6, 47]]</td>\n",
       "      <td>0.967069</td>\n",
       "      <td>0.919679</td>\n",
       "      <td>0.062797</td>\n",
       "      <td>0.968528</td>\n",
       "      <td>0.860226</td>\n",
       "      <td>0.976847</td>\n",
       "      <td>0.982438</td>\n",
       "      <td>0.955831</td>\n",
       "      <td>0.920710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=640, training_loss=0.47741791264852507, metrics={'train_runtime': 267.3562, 'train_samples_per_second': 76.19, 'train_steps_per_second': 2.394, 'total_flos': 1235801533317120.0, 'train_loss': 0.47741791264852507, 'epoch': 10.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy hits around $90\\%$ after 10 epochs. This is a far cry from what\n",
    "an encoder like Deberta can achieve. But this is a good start and shows that\n",
    "the implementation is working."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
