{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">124439808</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m124439808\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from omnivault.utils.torch_utils.model_utils import total_trainable_parameters, total_parameters\n",
    "from rich.pretty import pprint\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "pprint(total_trainable_parameters(model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(418330.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.sqrt(torch.tensor(175_000_000_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52291.253316759496"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "175_000_000_000 / (418330 * 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83600"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "41800 * 1 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea 1. Low intrinsic\n",
    "\n",
    "-   Low intrinsic: We take inspiration from Li et al. (2018a); Aghajanyan et al.\n",
    "    (2020) which show that the learned over-parametrized models in fact reside\n",
    "    on a low intrinsic dimension. We hypothesize that the change in weights\n",
    "    during model adaptation also has a low “intrinsic rank”, leading to our\n",
    "    proposed Low-Rank Adaptation (LoRA) approach. LoRA allows us to train some\n",
    "    dense layers in a neural network indirectly by optimizing rank decomposition\n",
    "    matrices of the dense layers’ change during adaptation instead, while\n",
    "    keeping the pre-trained weights frozen, as shown in Figure 1. Using GPT-3\n",
    "    175B as an example, we show that a very low rank (i.e., r in Figure 1 can be\n",
    "    one or two) suffices even when the full rank (i.e., d) is as high as 12,288,\n",
    "    making LoRA both storage- and compute-efficient.\n",
    "\n",
    "-   Use image from lora paper, it has better latex.\n",
    "\n",
    "-   LoRA makes training more efficient and lowers the hardware barrier to entry\n",
    "    by up to 3 times when using adaptive optimizers since we do not need to\n",
    "    calculate the gradients or maintain the optimizer states for most\n",
    "    parameters. Instead, we only optimize the injected, much smaller low-rank\n",
    "    matrices.\n",
    "\n",
    "    -   This idea is good to know, adam for eg depends on trainable params, and\n",
    "        now stores less due to lora.\n",
    "\n",
    "-   Our simple linear design allows us to merge the trainable matrices with the\n",
    "    frozen weights when deployed, introducing no inference latency compared to a\n",
    "    fully fine-tuned model, by construction.\n",
    "\n",
    "    -   Not too sure how it works but I know you can add matrices.\n",
    "\n",
    "    -   After read below i know exactly how it works. Just distributive law\n",
    "        because W @ x then if have a gradient update of $\\nabla W$ you get\n",
    "        $(W + \\nabla W) @ x$ but note you can do $Wx + \\nabla W x$ so $\\nabla W$\n",
    "        is separate. so means you have original weights of pretrained model as\n",
    "        $W$ and final finetuned weight update to be $\\nabla W$ and during\n",
    "        inference u can add them (merge) but what is attractive is u can store\n",
    "        nabla W outside of pretrained model and if nabla W is v small then\n",
    "        hoseh! So nabla W = B @ A here.\n",
    "\n",
    "    If we paid close attention, the full finetuning and LoRA depictions in the\n",
    "    figure above look slightly different from the formulas I have shown earlier.\n",
    "    That's due to the distributive law of matrix multiplication: we don't have\n",
    "    to add the weights with the updated weights but can keep them separate. For\n",
    "    instance, if\n",
    "\n",
    "    x is the input data, then we can write the following for regular finetuning:\n",
    "    The fact that we can keep the LoRA weight matrices separate makes LoRA\n",
    "    especially attractive. In practice, this means that we don't have to modify\n",
    "    the weights of the pretrained model at all, as we can apply the LoRA\n",
    "    matrices on the fly. This is especially useful if you are considering\n",
    "    hosting a model for multiple customers. Instead of having to save the large\n",
    "    updated models for each customer, you only have to save the small set of\n",
    "    LoRA weights alongside the original pretrained model.\n",
    "\n",
    "-   Suppose we are given a pre-trained autoregressive language model PΦ(y|x)\n",
    "    parametrized by Φ. For instance, PΦ(y|x) can be a generic multi-task learner\n",
    "    such as GPT (Radford et al., b; Brown et al., 2020) based on the Transformer\n",
    "    architecture (Vaswani et al., 2017). Consider adapting this pre-trained\n",
    "    model to downstream conditional text generation tasks, such as\n",
    "    summarization, machine reading comprehension (MRC), and natural language to\n",
    "    SQL (NL2SQL). Each downstream task is represented by a training dataset of\n",
    "    context-target pairs: Z = {(xi , yi)}i=1,..,N , where both xi and yi are\n",
    "    sequences of tokens. For example, in NL2SQL, xi is a natural language query\n",
    "    and yi its corresponding SQL command; for summarization, xi is the content\n",
    "    of an article and yi its summary.\n",
    "\n",
    "    -   here what is the notation phi represent.\n",
    "    -   also check if classification means y is single target.\n",
    "\n",
    "-   During full fine-tuning, the model is initialized to pre-trained weights Φ0\n",
    "    and updated to Φ0 + ∆Φ by repeatedly following the gradient to maximize the\n",
    "    conditional language modeling objective: max Φ X (x,y)∈Z X |y| t=1 log\n",
    "    (PΦ(yt|x, y<t)) (1) One of the main drawbacks for full fine-tuning is that\n",
    "    for each downstream task, we learn a different set of parameters ∆Φ whose\n",
    "    dimension |∆Φ| equals |Φ0|. Thus, if the pre-trained model is large (such as\n",
    "    GPT-3 with |Φ0| ≈ 175 Billion), storing and deploying many independent\n",
    "    instances of fine-tuned models can be challenging, if at all feasible.\n",
    "    -   here full finetuning drawback is easy, because your weights size and\n",
    "        grads dont change.\n",
    "    -   note the dimension |\\phi| refers to the total params.\n",
    "-   Lit review on existing solns, can try skip.\n",
    "-   section 4.1 is hard.\n",
    "\n",
    "-   A Generalization of Full Fine-tuning. A more general form of fine-tuning\n",
    "    allows the training of a subset of the pre-trained parameters. LoRA takes a\n",
    "    step further and does not require the accumulated gradient update to weight\n",
    "    matrices to have full-rank during adaptation. This means that when applying\n",
    "    LoRA to all weight matrices and training all biases2 , we roughly recover\n",
    "    the expressiveness of full fine-tuning by setting the LoRA rank r to the\n",
    "    rank of the pre-trained weight matrices. In other words, as we increase the\n",
    "    number of trainable parameters 3 , training LoRA roughly converges to\n",
    "    training the original model, while adapter-based methods converges to an MLP\n",
    "    and prefix-based methods to a model that cannot take long input sequences.\n",
    "\n",
    "    -   Do not understand.\n",
    "\n",
    "-   No Additional Inference Latency\n",
    "-   even though the output dimension is usually sliced into attention heads. We\n",
    "    limit our study to only adapting the attention weights for downstream tasks\n",
    "    and freeze the MLP modules (so they are not trained in downstream tasks)\n",
    "    both for simplicity and parameter-efficiency.W\n",
    "    -   This just means they just take the concat W matrices and not care about\n",
    "        each sub-weight matrices.\n",
    "-   LoRA adds trainable pairs of rank decomposition matrices in parallel to\n",
    "    existing weight matrices. As mentioned in Section 4.2, we only apply LoRA to\n",
    "    Wq and Wv in most experiments for simplicity. The number of trainable\n",
    "    parameters is determined by the rank r and the shape of the original\n",
    "    weights: |Θ| = 2 × LˆLoRA × dmodel × r, where LˆLoRA is the number of weight\n",
    "    matrices we apply LoRA to.\n",
    "-   Low-Rank Structures in Deep Learning. L too deep!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omniverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
