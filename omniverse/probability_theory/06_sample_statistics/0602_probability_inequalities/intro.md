# Probability Inequalities

Probability bounds are important in machine learning because they provide rigorous guarantees on the performance of machine learning models, especially in cases where the amount of available data is limited. In practice, machine learning models are often trained on a limited sample of data, and it is important to understand the degree to which the model's performance on this sample can be extrapolated to new, unseen data.

The PAC framework is also based on probability bounds. In the PAC framework, we are interested in the performance of a machine learning model on a fixed, finite set of data points. The PAC framework provides a rigorous way to quantify the probability that the model's performance on this set of data points is within a certain tolerance of the true performance of the model on all possible data points.

## Further Readings

For a rigorous and concise treatment, see the following:

- Chan, Stanley H. "Chapter 6.2. Probability Inequalities." In Introduction to Probability for Data Science. Ann Arbor, Michigan: Michigan Publishing Services, 2021.
- Pishro-Nik, Hossein. "Chapter 6.2.0. Probability Bounds." In Introduction to Probability, Statistics, and Random Processes. Kappa Research, 2014.

For code walkthrough, see:

- [Nathaniel Dake: Probability Inequalities](https://www.nathanieldake.com/Mathematics/03-Probability-03-Inequalities.html)