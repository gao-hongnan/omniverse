# Conditional PMF and PDF

## Introduction

This chapter is very important, as it is tightly coupled with Bayes' rule. In machine learning,
there are many model structure that stems from conditional distributions. For example, the target
variable $Y$ is often conditioned on the input variable $\mathbf{X}$, and we are tasked to model
$\hat{Y}=\mathbb{P}(Y \mid \mathbf{X})$.

## Learning Objectives

- To learn the distinction between a joint probability distribution and a conditional probability distribution.
- To recognize that a conditional probability distribution is simply a probability distribution for a sub-population.
- To learn the formal definition of a conditional probability mass function of a discrete r.v.  given a discrete r.v. .
- To learn how to calculate the conditional mean and conditional variance of a discrete r.v.  given a discrete r.v. .
- To be able to apply the methods learned in the lesson to new problems.

## Further Readings

- Chan, Stanley H. "Chapter 5.3. Conditional PMF and PDF." In Introduction to Probability for Data Science, 266-275. Ann Arbor, Michigan: Michigan Publishing Services, 2021.
- [PSU Stat 414. Lesson 19: Conditional Distributions](https://online.stat.psu.edu/stat414/lesson/19)