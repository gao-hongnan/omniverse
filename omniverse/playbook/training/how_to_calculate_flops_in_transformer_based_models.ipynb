{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Calculate the Number of FLOPs in Transformer Based Models?\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Brain_Dump-red)\n",
    "[![Code](https://img.shields.io/badge/View-Code-blue?style=flat-square&logo=github)](https://github.com/gao-hongnan/omniverse/blob/main/omnivault/utils/torch_utils/speed_monitor.py)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "\n",
    "This notebook references from\n",
    "[Andrej Karpathy's NanoGPT](https://github.com/karpathy/nanoGPT/blob/master/transformer_sizing.ipynb),\n",
    "which originally stores a bunch of analysis about a Transformer, e.g. estimates\n",
    "the number of FLOPs, parameters, peak memory footprint, checkpoint size, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum, IntEnum\n",
    "from typing import Dict, Literal, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "from rich.pretty import pprint\n",
    "from tabulate import tabulate\n",
    "from torch import nn\n",
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations, Constants and Enums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    num_decoder_blocks: int = 12\n",
    "    context_length: int = 1024\n",
    "    n_embd: int = 768\n",
    "    ffw_size: int = 3072  # note, this is 4 * n_embd\n",
    "    n_head: int = 12\n",
    "    vocab_size: int = 50257\n",
    "    bias: Literal[False] = False\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        assert self.ffw_size == 4 * self.n_embd, \"ffw_size must be 4 * n_embd\"\n",
    "        assert self.bias is False, \"bias must be False in this experiment.\"\n",
    "\n",
    "\n",
    "class GPT2ModelType(Enum):\n",
    "    GPT2 = \"gpt2\"\n",
    "    GPT2_MEDIUM = \"gpt2-medium\"\n",
    "    GPT2_LARGE = \"gpt2-large\"\n",
    "    GPT2_XL = \"gpt2-xl\"\n",
    "\n",
    "\n",
    "class ByteUnits(IntEnum):\n",
    "    B = 1  # Byte = 1 byte\n",
    "    KB = 1000  # Kilobyte = 10^3 bytes\n",
    "    MB = 1000**2  # Megabyte = 10^6 bytes\n",
    "    GB = 1000**3  # Gigabyte = 10^9 bytes\n",
    "\n",
    "\n",
    "class FloatingPointPrecision(IntEnum):\n",
    "    FP32 = 4  # 32-bit floating-point, 4 bytes\n",
    "    FP16 = 2  # 16-bit floating-point, 2 bytes\n",
    "    BFLOAT16 = 2  # bfloat16, 16-bit, 2 bytes\n",
    "\n",
    "\n",
    "class GPUMemory(Enum):\n",
    "    A100_40GB = 40e9  # 40 GB for NVIDIA A100\n",
    "    V100_16GB = 16e9  # 16 GB for NVIDIA V100\n",
    "    V100_32GB = 32e9  # 32 GB for NVIDIA V100\n",
    "    T4_16GB = 16e9  # 16 GB for NVIDIA T4\n",
    "    P100_16GB = 16e9  # 16 GB for NVIDIA P100\n",
    "\n",
    "\n",
    "class GPU:\n",
    "    def __init__(self, name: str, flops: Dict[FloatingPointPrecision, float]) -> None:\n",
    "        self.name = name\n",
    "        self.flops = flops\n",
    "\n",
    "class A100(GPU):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\"A100\", {\n",
    "            FloatingPointPrecision.FP32: 19.5e12,\n",
    "            FloatingPointPrecision.FP16: 312e12,\n",
    "            FloatingPointPrecision.BFLOAT16: 312e12\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPTConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">num_decoder_blocks</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">context_length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">n_embd</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">ffw_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3072</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">n_head</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">vocab_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50257</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mGPTConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mnum_decoder_blocks\u001b[0m=\u001b[1;36m12\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mcontext_length\u001b[0m=\u001b[1;36m1024\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mn_embd\u001b[0m=\u001b[1;36m768\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mffw_size\u001b[0m=\u001b[1;36m3072\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mn_head\u001b[0m=\u001b[1;36m12\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mvocab_size\u001b[0m=\u001b[1;36m50257\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2_config = GPTConfig()\n",
    "pprint(gpt2_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Trainable Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_trainable_parameters(model: nn.Module, include_bias: bool = True) -> int:\n",
    "    \"\"\"Returns the number of trainable parameters in the model.\"\"\"\n",
    "    if not include_bias:\n",
    "        return sum(p.numel() for name, p in model.named_parameters() if p.requires_grad and \"bias\" not in name)\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2 = GPT2LMHeadModel.from_pretrained(GPT2ModelType.GPT2.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in GPT2 model: 124337664 (excluding bias) and 124439808 (including bias).\n"
     ]
    }
   ],
   "source": [
    "gpt2_params_no_bias = total_trainable_parameters(gpt2, include_bias=False)\n",
    "gpt2_params_with_bias = total_trainable_parameters(gpt2, include_bias=True)\n",
    "\n",
    "print(\n",
    "    f\"Number of trainable parameters in GPT2 model: {gpt2_params_no_bias} (excluding bias) and {gpt2_params_with_bias} (including bias).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Karpathy's blog post assumed that there is no bias for simplicity, we will\n",
    "also assume that there is no bias in the linear layers. We confirmed that the\n",
    "number of params (`124337664`) for the smallest GPT-2 model indeed matches the\n",
    "number of params given by Karpathy.\n",
    "\n",
    "In what follows, we would assume the smallest GPT-2 model and work out the\n",
    "theoretical model for the Transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_args = {\n",
    "#     'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "#     'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "#     'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "#     'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "# }[model_type]\n",
    "\n",
    "\n",
    "def params(\n",
    "    num_decoder_blocks: int = 12,\n",
    "    context_length: int = 1024,\n",
    "    n_embd: int = 768,\n",
    "    ffw_size: int = 3072,\n",
    "    vocab_size: int = 50257,\n",
    ") -> OrderedDict[str, int]:\n",
    "    \"\"\"estimates the number of parameters in the model\"\"\"\n",
    "    out = OrderedDict()\n",
    "\n",
    "    # token and position embeddings\n",
    "    out[\"embedding/position\"] = n_embd * context_length\n",
    "    out[\"embedding/token\"] = n_embd * vocab_size\n",
    "    out[\"embedding\"] = out[\"embedding/position\"] + out[\"embedding/token\"]\n",
    "\n",
    "    # attention blocks\n",
    "    out[\"attention/ln\"] = n_embd  # note, bias=False in our LN\n",
    "    out[\"attention/kqv\"] = n_embd * 3 * n_embd\n",
    "    out[\"attention/proj\"] = n_embd**2\n",
    "    out[\"attention\"] = out[\"attention/ln\"] + out[\"attention/kqv\"] + out[\"attention/proj\"]\n",
    "\n",
    "    # MLP blocks\n",
    "    assert ffw_size == 4 * n_embd, \"ffw_size must be 4 * n_embd\"\n",
    "    out[\"mlp/ln\"] = n_embd\n",
    "    out[\"mlp/ffw\"] = n_embd * ffw_size\n",
    "    out[\"mlp/proj\"] = ffw_size * n_embd\n",
    "    out[\"mlp\"] = out[\"mlp/ln\"] + out[\"mlp/ffw\"] + out[\"mlp/proj\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = num_decoder_blocks * out[\"block\"]\n",
    "    out[\"ln_f\"] = n_embd  # final layernorm\n",
    "    out[\"dense\"] = 0  # 0 because of parameter sharing. This layer uses the weights from the embedding layer\n",
    "\n",
    "    # total\n",
    "    out[\"total\"] = out[\"embedding\"] + out[\"transformer\"] + out[\"ln_f\"] + out[\"dense\"]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We see: 124337664, Expected: 124337664, Match: True\n",
      "\n",
      "+--------------------+------------+-----------------------+\n",
      "|        Name        | Parameters |       Ratio (%)       |\n",
      "+--------------------+------------+-----------------------+\n",
      "| embedding/position |   786432   |  0.6324970042866496   |\n",
      "|  embedding/token   |  38597376  |  31.042384711361475   |\n",
      "|     embedding      |  39383808  |  31.674881715648123   |\n",
      "|    attention/ln    |    768     | 0.0006176728557486812 |\n",
      "|   attention/kqv    |  1769472   |  1.4231182596449616   |\n",
      "|   attention/proj   |   589824   |  0.47437275321498723  |\n",
      "|     attention      |  2360064   |  1.8981086857156975   |\n",
      "|       mlp/ln       |    768     | 0.0006176728557486812 |\n",
      "|      mlp/ffw       |  2359296   |   1.897491012859949   |\n",
      "|      mlp/proj      |  2359296   |   1.897491012859949   |\n",
      "|        mlp         |  4719360   |   3.795599698575646   |\n",
      "|       block        |  7079424   |   5.693708384291344   |\n",
      "|    transformer     |  84953088  |   68.32450061149613   |\n",
      "|        ln_f        |    768     | 0.0006176728557486812 |\n",
      "|       dense        |     0      |          0.0          |\n",
      "|       total        | 124337664  |         100.0         |\n",
      "+--------------------+------------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "params_dict = params()\n",
    "gpt2_params_no_bias_manual = params_dict[\"total\"]\n",
    "\n",
    "# Compare to expected PyTorch model parameter count\n",
    "expected_params = gpt2_params_no_bias\n",
    "comparison_result = gpt2_params_no_bias_manual == expected_params\n",
    "comparison_msg = f\"We see: {gpt2_params_no_bias_manual}, Expected: {expected_params}, Match: {comparison_result}\"\n",
    "\n",
    "data = {\n",
    "    \"Name\": params_dict.keys(),\n",
    "    \"Parameters\": params_dict.values(),\n",
    "    \"Ratio (%)\": [value / gpt2_params_no_bias_manual * 100 for value in params_dict.values()],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Printing comparison result and parameter distribution table\n",
    "print(comparison_msg + \"\\n\")\n",
    "print(tabulate(df, headers=\"keys\", tablefmt=\"pretty\", showindex=False, numalign=\"right\", floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Checkpoint Size and Fluff Ratio\n",
    "\n",
    "The functions below perform a series of calculations related to the size\n",
    "of a GPT-2 model checkpoint, both measured and estimated, and computes the\n",
    "\"fluff ratio\" to compare these sizes. The purpose of these calculations is to\n",
    "evaluate how closely the estimated size of a GPT-2 model checkpoint matches the\n",
    "actual measured size, and to quantify any overhead or additional data in the\n",
    "checkpoint file as a percentage of the estimated size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_checkpoint_size(params_count: int, precision: FloatingPointPrecision, units: ByteUnits) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the estimated checkpoint size in specified units.\n",
    "\n",
    "    This function estimates the checkpoint size for a model given the number\n",
    "    of parameters, the precision of these parameters, and\n",
    "    the desired units for the result. It accounts for the AdamW optimizer's\n",
    "    storage requirements by adding two times the parameter bytes to account\n",
    "    for the optimizer's moment and velocity vectors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params_count : int\n",
    "        The number of parameters excluding biases.\n",
    "    precision : FloatingPointPrecision\n",
    "        The floating point precision of the parameters.\n",
    "    units : ByteUnits\n",
    "        The units for the resulting checkpoint size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The estimated checkpoint size in the specified units.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The AdamW optimizer requires additional storage for each parameter\n",
    "    for maintaining momentum and variance vectors, hence the calculation\n",
    "    includes 2 * params_bytes to accommodate these.\n",
    "    \"\"\"\n",
    "    params_bytes = params_count * precision.value\n",
    "    params_and_buffers_bytes = params_bytes + 2 * params_bytes  # AdamW optimizer buffers\n",
    "    return params_and_buffers_bytes / units.value\n",
    "\n",
    "\n",
    "def calculate_fluff_ratio(measured_bytes: int, estimated_bytes: float, units: ByteUnits) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the fluff ratio between measured and estimated checkpoint sizes.\n",
    "\n",
    "    The fluff ratio is a measure of the overhead or additional data in the\n",
    "    checkpoint file, expressed as a percentage of the estimated size. This\n",
    "    function converts the estimated size from gigabytes (or specified units)\n",
    "    to bytes before calculating the ratio to ensure consistency in units.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    measured_bytes : int\n",
    "        The actual size of the checkpoint file, in bytes.\n",
    "    estimated_bytes : float\n",
    "        The estimated size of the checkpoint file, in the specified units.\n",
    "    units : ByteUnits\n",
    "        The units in which the estimated bytes are provided.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The fluff ratio, expressed as a percentage.\n",
    "    \"\"\"\n",
    "    estimated_bytes_in_bytes = estimated_bytes * units.value\n",
    "    return (measured_bytes / estimated_bytes_in_bytes) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Measured Checkpoint Size in Bytes**:\n",
    "\n",
    "   - `gpt2_checkpoint_size_measured_in_bytes` is assigned a numerical value\n",
    "     that represents the actual size of a GPT-2 model checkpoint file in bytes.\n",
    "     This value is obtained from the output of the Unix command\n",
    "     `wc -c ckpt.pt`, which counts the number of bytes in the file `ckpt.pt`.\n",
    "\n",
    "2. **Estimated Checkpoint Size in Bytes**:\n",
    "\n",
    "   - The `calculate_checkpoint_size` function is called with the number of\n",
    "     parameters excluding biases (`gpt2_params_no_bias`), the precision of the\n",
    "     model's parameters (`FloatingPointPrecision.FP32`), and the unit of\n",
    "     measurement (`ByteUnits.B` for bytes). This function calculates the\n",
    "     estimated total size of the checkpoint in bytes, taking into account the\n",
    "     parameters and the additional storage required for the AdamW optimizer's\n",
    "     buffers.\n",
    "   - It is worth noting we are assuming floating-point precision of 32 bits (4\n",
    "     bytes) for the model's parameters, and hence we are multiplying the number\n",
    "     of parameters by 4 to obtain the size in bytes.\n",
    "\n",
    "   - The AdamW optimizer, which is commonly used in training deep learning\n",
    "     models for tasks like those involving GPT-2, maintains two additional\n",
    "     values (buffers) for each parameter: the first for the moment vector (`m`)\n",
    "     and the second for the squared moment vector (`v`). These buffers are used\n",
    "     to adapt the learning rates for each parameter during training. This is\n",
    "     why the storage requirement triples (`params_bytes + 2 * params_bytes`),\n",
    "     accounting for the original parameters plus the two buffers.\n",
    "\n",
    "3. **Fluff Ratio Calculation**:\n",
    "\n",
    "   - The `calculate_fluff_ratio` function is called with the measured size in\n",
    "     bytes, the estimated size in bytes, and the unit of measurement for the\n",
    "     estimated size (bytes). This function calculates the fluff ratio, which\n",
    "     indicates the percentage of overhead or additional data in the measured\n",
    "     checkpoint file compared to the estimated size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+-------------------+\n",
      "|              Metric               |       Value       |\n",
      "+-----------------------------------+-------------------+\n",
      "| Measured Checkpoint Size (bytes)  |    1542470366     |\n",
      "|   Measured Checkpoint Size (GB)   |    1.542470366    |\n",
      "| Estimated Checkpoint Size (bytes) |   1492051968.0    |\n",
      "|  Estimated Checkpoint Size (GB)   |    1.492051968    |\n",
      "|            Fluff Ratio            | 103.3791314968461 |\n",
      "+-----------------------------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "gpt2_checkpoint_size_measured_in_bytes = 1542470366  # from 'wc -c ckpt.pt'\n",
    "gpt2_checkpoint_size_measured_in_gb = gpt2_checkpoint_size_measured_in_bytes / ByteUnits.GB\n",
    "\n",
    "gpt2_checkpoint_size_estimated_in_bytes = calculate_checkpoint_size(\n",
    "    params_count=gpt2_params_no_bias,\n",
    "    precision=FloatingPointPrecision.FP32,\n",
    "    units=ByteUnits.B,\n",
    ")\n",
    "gpt2_checkpoint_size_estimated_in_gb = gpt2_checkpoint_size_estimated_in_bytes / ByteUnits.GB\n",
    "\n",
    "\n",
    "fluff_ratio = calculate_fluff_ratio(\n",
    "    measured_bytes=gpt2_checkpoint_size_measured_in_bytes,\n",
    "    estimated_bytes=gpt2_checkpoint_size_estimated_in_bytes,\n",
    "    units=ByteUnits.B,\n",
    ")\n",
    "\n",
    "data = [\n",
    "    [\"Measured Checkpoint Size (bytes)\", gpt2_checkpoint_size_measured_in_bytes],\n",
    "    [\"Measured Checkpoint Size (GB)\", gpt2_checkpoint_size_measured_in_gb],\n",
    "    [\"Estimated Checkpoint Size (bytes)\", gpt2_checkpoint_size_estimated_in_bytes],\n",
    "    [\"Estimated Checkpoint Size (GB)\", gpt2_checkpoint_size_estimated_in_gb],\n",
    "    [\"Fluff Ratio\", fluff_ratio],\n",
    "]\n",
    "\n",
    "print(tabulate(data, headers=[\"Metric\", \"Value\"], tablefmt=\"pretty\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Memory Footprint of Loading Model and Optimizer\n",
    "\n",
    "We can roughly understand that a checkpoint represents the amount of memory\n",
    "needed to store not just the model itself (its weights) but also additional\n",
    "information related to the optimizer state when you're using GPUs for deep\n",
    "learning tasks.\n",
    "\n",
    "When loading a model from a checkpoint for further training or inference, the\n",
    "GPU memory must accommodate the model weights and the optimizer state (if\n",
    "continuing training).\n",
    "\n",
    "Below, we estimate the ratio of our GPU memory that will be taken up by\n",
    "the model and optimizer state when loading a GPT-2 model from a checkpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory ratio taken up just for parameters: 3.73%\n"
     ]
    }
   ],
   "source": [
    "def calculate_memory_ratio(checkpoint_size: float, gpu_memory: GPUMemory) -> str:\n",
    "    memory_ratio = checkpoint_size / gpu_memory.value * 100\n",
    "    return f\"Memory ratio taken up just for parameters: {memory_ratio:.2f}%\"\n",
    "\n",
    "\n",
    "print(calculate_memory_ratio(checkpoint_size=gpt2_checkpoint_size_estimated_in_bytes, gpu_memory=GPUMemory.A100_40GB))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming an A100 GPU with roughly 40GB memory, then the code calculates the\n",
    "percentage of the GPU memory that the estimated checkpoint size (in bytes)\n",
    "occupies. This calculation gives an insight into how much of the GPU's memory is\n",
    "dedicated to storing the model's weights and the optimizer's buffers, without\n",
    "considering other memory usages such as activations during forward and backward\n",
    "passes.\n",
    "\n",
    "This percentage is relatively small, implying that most of the GPU memory is\n",
    "actually used for activations. Activations are the intermediate outputs of\n",
    "layers during the forward pass and their gradients during the backward pass,\n",
    "which can consume significant amounts of memory, especially in deep models and\n",
    "with large batch sizes.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating FLOPs for a Single Forward Pass\n",
    "\n",
    "In order to estimate FLOPs for a single forward pass, we would first need to\n",
    "define what is a FLOPS.\n",
    "\n",
    "### Basics of Floating Point Numbers\n",
    "\n",
    "- **Floating Point Representation**: In computers, numbers can be represented\n",
    "  in various formats, and one common format is floating point. This format is\n",
    "  used to represent real numbers (numbers with fractions) using a fixed amount\n",
    "  of memory, allowing for a wide range of values. A floating point number is\n",
    "  composed of a sign, an exponent, and a mantissa (or significand). This\n",
    "  representation can handle very large numbers, very small numbers, and\n",
    "  fractions.\n",
    "- **Operations on Floating Point Numbers**: Operations on floating point\n",
    "  numbers include addition, subtraction, multiplication, and division. Each of\n",
    "  these operations takes one or more floating point numbers as input and\n",
    "  produces a floating point number as output.\n",
    "\n",
    "### Floating Point Operations (FLOPs)\n",
    "\n",
    "Floating Point Operations, or FLOPs, refer to individual mathematical operations\n",
    "(additions, subtractions, multiplications, divisions) performed on\n",
    "[floating point numbers](https://en.wikipedia.org/wiki/Floating-point_arithmetic).\n",
    "Each operation counts as one FLOP.\n",
    "\n",
    "### Counting FLOPs of Matrix Multiplications\n",
    "\n",
    "In the context of deep learning, many operations are done via matrix\n",
    "multiplications, we will take a look at how to count FLOPs for matrix\n",
    "multiplications next.\n",
    "\n",
    "Deep learning, particularly in neural networks, relies heavily on matrix\n",
    "multiplications. A single matrix multiplication operation involves multiple\n",
    "floating point multiplications and additions.\n",
    "\n",
    "Consider two matrices $\\mathbf{A}$ and $\\mathbf{B}$ of size $m \\times n$ and\n",
    "$n \\times p$:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn}\n",
    "\\end{bmatrix}_{m \\times n} \\quad \\mathbf{B} = \\begin{bmatrix}\n",
    "b_{11} & b_{12} & \\cdots & b_{1p} \\\\\n",
    "b_{21} & b_{22} & \\cdots & b_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "b_{n1} & b_{n2} & \\cdots & b_{np}\n",
    "\\end{bmatrix}_{n \\times p}\n",
    "$$\n",
    "\n",
    "It is easy to see that if we want to compute the product\n",
    "$\\mathbf{C} = \\mathbf{A} \\mathbf{B}$, the element $c_{ij}$ of $\\mathbf{C}$ is\n",
    "given by:\n",
    "\n",
    "$$\n",
    "c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}\n",
    "$$\n",
    "\n",
    "and therefore there are a total of $m \\times n \\times p$ multiplications and\n",
    "$m \\times (n-1) \\times p$ additions. This amounts to roughly:\n",
    "\n",
    "$$\n",
    "m \\times n \\times p + m \\times (n-1) \\times p \\approx 2 \\times m \\times n \\times p\n",
    "$$ (playbook-transformer-flops-approximation)\n",
    "\n",
    "FLOPs. Note this is basically because matrix multiplication is a series of dot\n",
    "products, and each dot product involves $n$ multiplications and $n-1$ additions.\n",
    "\n",
    "### Estimating FLOPs for a Single Forward Pass of GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flops(\n",
    "    num_decoder_blocks: int = 12,\n",
    "    context_length: int = 1024,\n",
    "    n_embd: int = 768,\n",
    "    n_head: int = 12,\n",
    "    ffw_size: int = 3072,\n",
    "    vocab_size: int = 50257,\n",
    ") -> OrderedDict[str, int]:\n",
    "    # we only count Weight FLOPs, all other layers (LayerNorm, Softmax, etc) are effectively irrelevant\n",
    "    # we count actual FLOPs, not MACs. Hence 2* all over the place\n",
    "    # basically for any matrix multiply A (BxC) @ B (CxD) -> (BxD) flops are 2*B*C*D\n",
    "\n",
    "    out = OrderedDict()\n",
    "    head_size = n_embd // n_head\n",
    "\n",
    "    # attention blocks\n",
    "    # 1) the projection to key, query, values\n",
    "    out[\"attention/kqv\"] = 2 * context_length * (n_embd * 3 * n_embd)\n",
    "    # 2) calculating the attention scores\n",
    "    out[\"attention/scores\"] = 2 * context_length * context_length * n_embd\n",
    "    # 3) the reduction of the values (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "    out[\"attention/reduce\"] = 2 * n_head * (context_length * context_length * head_size)\n",
    "    # 4) the final linear projection\n",
    "    out[\"attention/proj\"] = 2 * context_length * (n_embd * n_embd)\n",
    "    out[\"attention\"] = sum(out[\"attention/\" + k] for k in [\"kqv\", \"scores\", \"reduce\", \"proj\"])\n",
    "\n",
    "    # MLP blocks\n",
    "    ffw_size = 4 * n_embd  # feed forward size\n",
    "    out[\"mlp/ffw1\"] = 2 * context_length * (n_embd * ffw_size)\n",
    "    out[\"mlp/ffw2\"] = 2 * context_length * (ffw_size * n_embd)\n",
    "    out[\"mlp\"] = out[\"mlp/ffw1\"] + out[\"mlp/ffw2\"]\n",
    "\n",
    "    # the transformer and the rest of it\n",
    "    out[\"block\"] = out[\"attention\"] + out[\"mlp\"]\n",
    "    out[\"transformer\"] = num_decoder_blocks * out[\"block\"]\n",
    "    out[\"dense\"] = 2 * context_length * (n_embd * vocab_size)\n",
    "\n",
    "    # forward,backward,total\n",
    "    out[\"forward_total\"] = out[\"transformer\"] + out[\"dense\"]\n",
    "    out[\"backward_total\"] = 2 * out[\"forward_total\"]  # use common estimate of bwd = 2*fwd\n",
    "    out[\"total\"] = out[\"forward_total\"] + out[\"backward_total\"]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flops function calculates the **total** number of **floating point\n",
    "operations** required to process a **single** sample\n",
    "$\\mathbf{x} = \\left(x_1, x_2, \\ldots, x_{T}\\right)$ of length $T$ through the\n",
    "entire model for a single **forward** pass.\n",
    "\n",
    "We take one sample snippet of code to explain how the flops are calculated:\n",
    "\n",
    "```python\n",
    "# 2) calculating the attention scores\n",
    "out[\"attention/scores\"] = 2 * context_length * context_length * n_embd\n",
    "```\n",
    "\n",
    "This is not difficult to see if one recalls the attention mechanism:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}}\\right) \\mathbf{V}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$ are the query, key, and value\n",
    "matrices of size $T \\times d_q$, $T \\times d_k$, and $T \\times d_v$,\n",
    "respectively. For simplicity, we assume that $d_q = d_k = d_v = D$ (which is\n",
    "`n_embd` in the code).\n",
    "\n",
    "In particular `attention_scores` is calculated as:\n",
    "\n",
    "```python\n",
    "attention_scores  = torch.matmul(query, key.transpose(dim0=-2, dim1=-1)) / torch.sqrt(torch.tensor(d_q).float())\n",
    "```\n",
    "\n",
    "which is the dot product of the query and key matrices, divided by the square\n",
    "root of the dimension of the query matrix and is of shape $T \\times T$. However,\n",
    "recall that ultimately the matrix multiplication of the two matrices\n",
    "$\\mathbf{Q}$ and $\\mathbf{K}^{\\top}$ is of shape $T \\times D$ and $D \\times T$,\n",
    "and by our earlier equation {eq}`playbook-transformer-flops-approximation`, this\n",
    "would be a total of $2 \\times T \\times T \\times D$ FLOPs, coinciding with the\n",
    "code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+---------------------+\n",
      "|       name       |    flops     |      ratio (%)      |\n",
      "+------------------+--------------+---------------------+\n",
      "|  attention/kqv   |  3623878656  | 1.2425508965889174  |\n",
      "| attention/scores |  1610612736  | 0.5522448429284077  |\n",
      "| attention/reduce |  1610612736  | 0.5522448429284077  |\n",
      "|  attention/proj  |  1207959552  | 0.41418363219630583 |\n",
      "|    attention     |  8053063680  | 2.7612242146420387  |\n",
      "|     mlp/ffw1     |  4831838208  | 1.6567345287852233  |\n",
      "|     mlp/ffw2     |  4831838208  | 1.6567345287852233  |\n",
      "|       mlp        |  9663676416  | 3.3134690575704466  |\n",
      "|      block       | 17716740096  |  6.074693272212485  |\n",
      "|   transformer    | 212600881152 |  72.89631926654981  |\n",
      "|      dense       | 79047426048  |  27.10368073345018  |\n",
      "|  forward_total   | 291648307200 |        100.0        |\n",
      "|  backward_total  | 583296614400 |        200.0        |\n",
      "|      total       | 874944921600 |        300.0        |\n",
      "+------------------+--------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "f = flops()\n",
    "flops_total = f[\"forward_total\"]\n",
    "\n",
    "table = [(\"name\", \"flops\", \"ratio (%)\")]\n",
    "for k, v in f.items():\n",
    "    table.append((k, v, v / flops_total * 100))\n",
    "\n",
    "print(tabulate(table, headers=\"firstrow\", tablefmt=\"pretty\", numalign=\"right\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check with Palm Paper's FLOPs Calculation\n",
    "\n",
    "NOTE: The notations below are purely based on the PaLM 2 paper, and is not\n",
    "what I usually use.\n",
    "\n",
    "The `palm_flops` function, as inspired by the\n",
    "[technical report of Google's PaLM 2](https://ai.google/static/documents/palm2techreport.pdf),\n",
    "calculates an estimate of the model's floating point operations (FLOPs) but\n",
    "introduces a specific formula for computing model FLOPs utilization (MFU) per\n",
    "token and for the entire model.\n",
    "\n",
    "1. **Non-Embedding Model Parameters (`N`)**: This calculation starts by\n",
    "   estimating the number of non-embedding model parameters. In transformer\n",
    "   models like PaLM, a significant portion of the parameters resides in the\n",
    "   embedding layers. This formula adjusts for that by subtracting the\n",
    "   embedding/position parameters from the total, focusing on the parameters\n",
    "   actively involved in computations outside of embeddings.\n",
    "\n",
    "2. **Model Dimensions (`L`, `H`, `Q`, `T`)**:\n",
    "\n",
    "   - `L` = Number of layers (`n_layer`)\n",
    "   - `H` = Number of attention heads (`n_head`)\n",
    "   - `Q` = Size of each attention head (`n_embd // n_head`)\n",
    "   - `T` = Sequence length (`block_size`), also referred to as context length\n",
    "     in other discussions.\n",
    "\n",
    "3. **MF Per Token (`mf_per_token`)**: This represents the estimated FLOPs for\n",
    "   processing a single token, calculated as `6*N + 12*L*H*Q*T`.\n",
    "\n",
    "4. **Total Model FLOPs for a sequence (`mf_per_sequence`)**: This is calculated\n",
    "   by multiplying the per-token FLOPs estimate (`mf_per_token`) by the sequence\n",
    "   length (`block_size` or `T`). This gives the total estimated FLOPs for\n",
    "   processing a sequence of length `T`.\n",
    "\n",
    "This is more of a sanity check for Karpathy, and he confirms if using PaLM's\n",
    "`palm_flops` function to calculate FLOPs for our GPT-2 model yields similar\n",
    "results to the ones he wrote himself (`flops` function).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now here is an estimate copy pasted from the PaLM paper\n",
    "# this formula is often used to calculate MFU (model flops utilization)\n",
    "def palm_flops(\n",
    "    params: OrderedDict[str, int], num_decoder_blocks: int, n_head: int, n_embd: int, context_length: int\n",
    ") -> int:\n",
    "    \"\"\"Estimate of the model flops following PaLM paper formula.\"\"\"\n",
    "    # non-embedding model parameters. note that we do not subtract the\n",
    "    # embedding/token params because those are tied and get used in the last layer.\n",
    "    N = params()[\"total\"] - params()[\"embedding/position\"]\n",
    "    L, H, Q, T = num_decoder_blocks, n_head, n_embd // n_head, context_length\n",
    "    mf_per_token = 6 * N + 12 * L * H * Q * T\n",
    "    mf_per_sequence = mf_per_token * context_length\n",
    "    return mf_per_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaLM paper estimate of GPT2 flops: 875062886400, Our estimate: 874944921600\n",
      "Ratio: 100.01348%\n"
     ]
    }
   ],
   "source": [
    "gpt2_flops_using_palm_flops_calculation = palm_flops(\n",
    "    params=params,\n",
    "    num_decoder_blocks=gpt2_config.num_decoder_blocks,\n",
    "    n_head=gpt2_config.n_head,\n",
    "    n_embd=gpt2_config.n_embd,\n",
    "    context_length=gpt2_config.context_length,\n",
    ")\n",
    "gpt2_flops_using_own_flops_calculation = f[\"total\"]\n",
    "print(\n",
    "    f\"PaLM paper estimate of GPT2 flops: {gpt2_flops_using_palm_flops_calculation}, Our estimate: {gpt2_flops_using_own_flops_calculation}\"\n",
    ")\n",
    "print(f\"Ratio: {gpt2_flops_using_palm_flops_calculation / gpt2_flops_using_own_flops_calculation * 100:.5f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Floating Point Operations Per Second (FLOPS)\n",
    "\n",
    "In computational tasks and processor performance assessment, we measure the\n",
    "capacity for floating-point computation in terms of\n",
    "[FLOPS](https://en.wikipedia.org/wiki/FLOPS), an acronym that stands for\n",
    "**_Floating Point Operations Per Second_**. This metric indicates the quantity\n",
    "of floating-point arithmetic operations—specifically, additions, subtractions,\n",
    "multiplications, and divisions—that a computing system is capable of performing\n",
    "_every second_. For example, a processor with the capability to execute one\n",
    "trillion such operations within a second is said to have a computational\n",
    "performance of 1 teraFLOP (TFLOP) or simply 1 TFLOPS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Practical Considerations for FLOPs in Deep Learning\n",
    "\n",
    "- In deep learning models, the number of FLOPs required for a forward pass\n",
    "  through the network gives an indication of the model's complexity and\n",
    "  efficiency. Models with higher FLOPs require more computational resources,\n",
    "  which can affect training and inference times, especially on large datasets.\n",
    "- Hardware wise we also can gauge what types of GPU is needed for the model,\n",
    "  especially when high FLOPS models are used, we would likely need a high-end\n",
    "  GPU that can operate at a higher FLOPS per second.\n",
    "\n",
    "### FLOPS Per Second in GPUs\n",
    "\n",
    "Given the **total number of FLOPs required for a forward pass through a deep\n",
    "learning model** and **the theoretical FLOPS per second of a GPU**, you can\n",
    "estimate the time it takes to perform the forward pass on the GPU by dividing\n",
    "the total FLOPs by the GPU's FLOPS capacity. This calculation assumes ideal\n",
    "conditions where the model fully utilizes the GPU's computational capabilities.\n",
    "\n",
    "$$\n",
    "\\text{Time for Forward Pass (seconds)} = \\frac{\\text{Total FLOPs for Forward Pass}}{\\text{Theoretical FLOPS of GPU}}\n",
    "$$\n",
    "\n",
    "This is assuming a single forward pass on a single sample.\n",
    "However, in practice, we use Model FLOPs Utilization (MFU) to measure the\n",
    "efficiency of the model, which is the ratio of the actual FLOPs to the\n",
    "theoretical FLOPS of the GPU.\n",
    "\n",
    "Note that different GPUs have different FLOPS, and the FLOPS of a GPU can be\n",
    "affected by what floating point precision is used (e.g. FP16, FP32, FP64)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model FLOPs Utilization (MFU)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given:\n",
    "\n",
    "-   Batch size with gradient accumulation factored in:\n",
    "    $\\text{batch_size} = 20 \\times 5$ **samples**.\n",
    "-   Measured time per iteration: $\\text{measured_time} = 0.755$ **seconds**\n",
    "    (note this is Karpathy's own measured time for the forward and backward\n",
    "    pass - 1 iteration)\n",
    "-   Total FLOPs required by the model for one forward and backward pass for a\n",
    "    single sample/sequence $\\mathbf{x} = \\left(x_1, x_2, \\ldots, x_{T}\\right)$:\n",
    "    $f[\\text{total}]$ having a unit of **FLOPs/sample**.\n",
    "-   The calculation of total FLOPs for a model's operations is independent of\n",
    "    the floating-point precision used (e.g., FP32, FP16, BF16). However when\n",
    "    training models, we may choose different floating-point precisions and by\n",
    "    extension, different hardware GPUs has different FLOPS for different\n",
    "    precisions.\n",
    "\n",
    "We will use FLOPS and TFLOPS (teraFLOPS) interchangeably where $1$ TFLOPS\n",
    "represents $10^{12}$ FLOPS.\n",
    "\n",
    "1. **Measured Throughput**:\n",
    "\n",
    "    The throughput, in terms of samples processed per second, can be calculated\n",
    "    as follows:\n",
    "\n",
    "    $$\n",
    "    \\text{measured_throughput} = \\frac{\\text{batch_size}}{\\text{measured_time}} \\text{ samples/second}\n",
    "    $$\n",
    "\n",
    "    Substituting the given values:\n",
    "\n",
    "    $$\n",
    "    \\text{measured_throughput} = \\frac{20 \\times 5}{0.755} = 132.4503311258278 \\text{ samples/second}\n",
    "    $$\n",
    "\n",
    "2. **FLOPs Achieved**:\n",
    "\n",
    "    The total floating point operations per second (FLOPs) achieved, based on\n",
    "    the measured throughput, is:\n",
    "\n",
    "    $$\n",
    "    \\text{flops_achieved_per_second} = f[\\text{total}] \\times \\text{measured_throughput}\n",
    "    $$\n",
    "\n",
    "    Here, $f[\\text{total}]$ represents the total FLOPs required by the model for\n",
    "    one complete pass (forward and backward) of a single sample, **with a unit\n",
    "    of FLOPs/sample**. And multiplying it by the measured throughput gives the\n",
    "    effective FLOPs achieved per second.\n",
    "\n",
    "    $$\n",
    "    \\text{flops_achieved_per_second} = \\left( \\frac{\\text{FLOPs}}{\\text{sample}} \\right)\n",
    "    \\times \\left( \\frac{\\text{sample}}{\\text{second}} \\right)\n",
    "    $$\n",
    "\n",
    "    On a side note, there should be no confusion in the cancellation of the\n",
    "    `sample` term in the numerator and denominator even though I used `sample`\n",
    "    and `samples` earlier. It is like saying if I can process $100$ samples per\n",
    "    second, and my model requires $1000$ FLOPs per sample, then I can process\n",
    "    $100 \\times 1000 = 100000$ FLOPs per second because I processed $100$\n",
    "    samples in $1$ second, and each sample required $1000$ FLOPs.\n",
    "\n",
    "3. **Fraction of A100 Utilization**:\n",
    "\n",
    "    Given the\n",
    "    [A100's promised performance for `bfloat16` operations](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf):\n",
    "\n",
    "    $$\n",
    "    \\text{a100_bfloat16_flops_promised} = 312 \\times 10^{12} \\text{ FLOPS}\n",
    "    $$\n",
    "\n",
    "    The fraction of the A100 GPU utilized can be expressed as a percentage of\n",
    "    the promised FLOPs:\n",
    "\n",
    "    $$\n",
    "    \\text{fraction of A100 used (\\%)} = \\left( \\frac{\\text{flops_achieved_per_second}}{\\text{a100_bfloat16_flops_promised}} \\right) \\times 100\n",
    "    $$\n",
    "\n",
    "    Substituting $\\text{flops_achieved_per_second}$ and\n",
    "    $\\text{a100_bfloat16_flops_promised}$ with their respective values provides\n",
    "    the percentage utilization of the A100 GPU's computational capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction of A100 used: 37.14%\n"
     ]
    }
   ],
   "source": [
    "# here is what we currently roughly measure\n",
    "batch_size = 20 * 5  # 5 is grad_accum, so total batch size is 100\n",
    "measured_time = 0.755  # in seconds per iteration\n",
    "measured_throughput = batch_size / measured_time # number of samples processed per second\n",
    "flops_achieved_per_second = f[\"total\"] * measured_throughput\n",
    "\n",
    "# A100 is cited to be 312 TFLOPS of bfloat16 running on tensor cores\n",
    "a100_bfloat16_promised_flops = 312e12\n",
    "\n",
    "# the fraction of the A100 that we are using:\n",
    "print(f\"fraction of A100 used: {flops_achieved_per_second / a100_bfloat16_promised_flops * 100:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Model FLOPs Utilization (MFU) Indicates a Rough Benchmark of Efficiency\n",
    "\n",
    "The Model FLOPs Utilization (MFU) of 37% in this context signifies that, for a\n",
    "specific model (like GPT-2), on a particular GPU (e.g., A100), and using a given\n",
    "floating point precision (e.g., bfloat16), the model's training or inference\n",
    "process is utilizing 37% of the GPU's theoretical maximum FLOPS capability. This\n",
    "percentage reflects the efficiency with which the model leverages the\n",
    "computational power of the GPU under those specific conditions.\n",
    "\n",
    "If we treat this metric of 37% as a benchmark reported by some paper, and if\n",
    "your actual MFU is significantly lower than this 37%, it suggests that there\n",
    "might be room for optimization in how the model is executed on the hardware. It\n",
    "could indicate inefficiencies in data loading, model architecture not fully\n",
    "leveraging the GPU's capabilities, or potential bottlenecks in the computation\n",
    "process that are preventing the GPU from being fully utilized. Consequently, MFU\n",
    "is a very common metric to monitor in the context of training large language\n",
    "models like GPT-2, as it can provide insights into the efficiency of the\n",
    "training process and help identify areas for potential optimization. After all,\n",
    "the GPUs are expensive to run, and we want to make sure we are getting the most\n",
    "out of them!\n",
    "\n",
    "### Relation of MFU and TFLOPS\n",
    "\n",
    "As we have seen earlier, we can denote the MFU formula as:\n",
    "\n",
    "$$\n",
    "\\operatorname{MFU} = \\frac{\\operatorname{MODEL_FLOPS}}{\\operatorname{GPU_FLOPS}},\n",
    "$$\n",
    "\n",
    "where $\\operatorname{MODEL_FLOPS}$ is the number of floating point operations per\n",
    "second and $\\operatorname{GPU_FLOPS}$ is the number of floating point operations\n",
    "per second that the GPU can perform.\n",
    "\n",
    "Then we can easily see that the $\\operatorname{MODEL_FLOPS}$ is given by a\n",
    "re-arrangement of the MFU formula:\n",
    "\n",
    "$$\n",
    "\\operatorname{MODEL_FLOPS} = \\operatorname{MFU} \\times \\operatorname{GPU_FLOPS}.\n",
    "$$\n",
    "\n",
    "This means if a report says they achieved $250$ TFLOPS, and they are using\n",
    "NVIDIA A100 GPU with `bfloat16` precision at a rate of $312$ TFLOPS, then we can\n",
    "calculate the MFU as:\n",
    "\n",
    "$$\n",
    "\\operatorname{MFU} = \\frac{250}{312} \\approx 0.8.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical FLOPs in Transformer Models\n",
    "\n",
    "The excerpt from Appendix B, page 66 of the PaLM paper _PaLM: Scaling Language\n",
    "Modeling with Pathways_ provides a detailed explanation of how Model FLOPs\n",
    "Utilization (MFU) is calculated for a dense Transformer language model, focusing\n",
    "on the relationship between observed throughput (tokens-per-second) and the\n",
    "theoretical maximum throughput based on the model's computational demands and\n",
    "the hardware's peak FLOPs.\n",
    "\n",
    "Given a corpus $\\mathcal{S}$ with $M$ sequences:\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\left\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_M\\right\\},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}_m$ is a sequence of length $T$ consisting of tokens:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_m = \\left\\{x_{m,1}, x_{m,2}, \\ldots, x_{m,T}\\right\\},\n",
    "$$\n",
    "\n",
    "Then we can easily see that there are a total of $D = M \\times T$ tokens. If we\n",
    "further denote the model (in our case GPT-2) as $\\mathcal{G}$, and the model's\n",
    "learnable parameters as $\\theta \\in \\Theta$, then the total number of learnable\n",
    "parameters in the model can be denoted as $M = |\\theta|$.\n",
    "\n",
    "1. **Basic Computation per Token for Non-Attention Components**: The paper notes\n",
    "   that, excluding self-attention, a decoder-only model with $N$ parameters\n",
    "   requires $6N$ matrix multiplication (matmul) FLOPs per token — $2N$ FLOPs for\n",
    "   the forward pass and $4N$ FLOPs for the backward pass. This doubling accounts\n",
    "   for the additional computations required during backpropagation (gradient\n",
    "   calculation and weight updates), where each matmul involves one\n",
    "   multiplication and one addition per pair of input values.\n",
    "\n",
    "2. **Self-Attention Computation**: For the self-attention mechanism, an\n",
    "   additional $6LHQ(2T)$ FLOPs per token are needed, with $L$, $H$, $Q$, and $T$\n",
    "   representing the number of layers/blocks, heads, head dimension (embedding\n",
    "   size of 1 head's query matrix) and sequence/context length, respectively.\n",
    "   This term accounts for the computations within the self-attention layers that\n",
    "   are more complex due to their dependency on the sequence length and the\n",
    "   architecture of the Transformer model.\n",
    "\n",
    "3. **Total Computational Requirement for 1 Token**: The total FLOPs per token,\n",
    "   considering both matmuls for the non-attention components and the\n",
    "   self-attention computations, are therefore summarized as $6N + 12LHQ(2T)$.\n",
    "   However, the self-attention part is noted to contribute a much smaller value\n",
    "   for large models, suggesting that the primary computational load comes from\n",
    "   the non-attention components.\n",
    "\n",
    "4. **Total Computational Requirement for the Entire Corpus**: The total\n",
    "   computational requirement for the entire corpus is then given by\n",
    "   $D(6N + 12LHQ(2T))$, where $D$ is the total number of tokens in the corpus.\n",
    "\n",
    "    Sometimes people just use $6ND$ as a rough estimate of the total FLOPs\n",
    "    required for the entire corpus, which is a reasonable approximation for\n",
    "    large models - in a sense $\\mathcal{O}(ND)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time needed to train the model: 3.46 days\n"
     ]
    }
   ],
   "source": [
    "# Finally let's check out the 6ND approximation as total cost of training in FLOPs\n",
    "N = params()[\"total\"]  # this is number of parameters, N\n",
    "D = 300e9  # 300B tokens, this is dataset size in tokens, D\n",
    "a100_bfloat16_promised_flops = 312e12  # 312 TFLOPS\n",
    "assumed_mfu = 0.3  # assume this model flops utilization (take the current 37% from above and add some DDP overhead)\n",
    "flops_throughput = a100_bfloat16_promised_flops * 8 * assumed_mfu  # assume an 8XA100 node at 30% utilization\n",
    "flops_needed = 6 * N * D  # 6ND\n",
    "time_needed_over_all_tokens_in_seconds = flops_needed / flops_throughput  # in seconds\n",
    "print(f\"time needed to train the model: {time_needed_over_all_tokens_in_seconds/3600/24:.2f} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karpathy reported that this number 3.46 days is close to his training time ~4 days.\n",
    "We see a modular function `estimate_mfu` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFUEstimationResult(BaseModel):\n",
    "    flops_per_token_per_fwdbwd: float\n",
    "    flops_per_sequence_per_fwdbwd: float\n",
    "    flops_per_iter_per_fwdbwd: float\n",
    "    flops_achieved_per_second: float\n",
    "    mfu: float\n",
    "\n",
    "\n",
    "def estimate_mfu(\n",
    "    num_decoder_blocks: int,\n",
    "    num_heads: int,\n",
    "    d_model: int,\n",
    "    context_length: int,\n",
    "    model_total_parameters: int,\n",
    "    effective_batch_size_per_iter: int,\n",
    "    time_taken_per_iter: float,\n",
    "    gpu_promised_flops: float = 312e12,  # A100 GPU bfloat16/float16 peak flops is 312 TFLOPS\n",
    ") -> MFUEstimationResult:\n",
    "    \"\"\"\n",
    "    Estimate Model FLOPs Utilization (MFU) as a ratio of achieved FLOPs to\n",
    "    the A100 GPU's peak FLOPs capability.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_decoder_blocks : int\n",
    "        Number of decoder blocks in the Transformer model.\n",
    "    num_heads : int\n",
    "        Number of attention heads in each Transformer block.\n",
    "    d_model : int\n",
    "        Dimension of the model's embeddings.\n",
    "    context_length : int\n",
    "        Number of tokens in each input sequence.\n",
    "    model_total_parameters : int\n",
    "        Total number of learnable parameters in the model.\n",
    "    effective_batch_size_per_iter : int\n",
    "        Effective batch size processed in one iteration, accounting for\n",
    "        gradient accumulation.\n",
    "    time_taken_per_iter : float\n",
    "        Time taken per training iteration in seconds.\n",
    "    gpu_promised_flops : float, optional\n",
    "        Theoretical peak performance of the GPU in FLOPs (default is\n",
    "        312e12 for A100 GPU bfloat16/float16 operations).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MFUEstimationResult:\n",
    "        Pydantic model with the following fields:\n",
    "        - flops_per_token_per_fwdbwd: FLOPs required for forward and backward pass of a single token\n",
    "        - flops_per_sequence_per_fwdbwd: FLOPs required for forward and backward pass of a single sequence\n",
    "        - flops_per_iter_per_fwdbwd: FLOPs required for forward and backward pass of the effective batch size\n",
    "        - flops_achieved_per_second: FLOPs achieved per second\n",
    "        - mfu: Model FLOPs Utilization (MFU) as a ratio of achieved FLOPs to the A100 GPU's peak FLOPs capability\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> estimate_mfu(\n",
    "    ...     num_decoder_blocks=6,\n",
    "    ...     num_heads=8,\n",
    "    ...     d_model=512,\n",
    "    ...     context_length=1024,\n",
    "    ...     model_total_parameters=1_000_000,\n",
    "    ...     effective_batch_size_per_iter=20 * 8,  # 20 sequences per GPU, 8 GPUs\n",
    "    ...     time_taken_per_iter=0.1, # 0.1 seconds per iteration\n",
    "    ...     gpu_promised_flops=312e12, # A100 GPU bfloat16/float16 peak flops\n",
    "    ... )\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function utilizes the formula from the PaLM paper Appendix B\n",
    "    (https://arxiv.org/abs/2204.02311) for estimating the FLOPs required\n",
    "    for one forward and backward pass of a single token and scales it up to\n",
    "    the effective batch size and the given model architecture to calculate MFU.\n",
    "    You can likely use it as a callback in your `Trainer` to log the MFU during training.\n",
    "    \"\"\"\n",
    "    # fmt: off\n",
    "    N, L, H, Q, T = model_total_parameters, num_decoder_blocks, num_heads, d_model // num_heads, context_length\n",
    "    flops_per_token_per_fwdbwd = 6 * N + 12 * L * H * Q * T # 1 token forward and backward flops\n",
    "    flops_per_sequence_per_fwdbwd = flops_per_token_per_fwdbwd * T # 1 sequence = T tokens\n",
    "    flops_per_iter_per_fwdbwd = flops_per_sequence_per_fwdbwd * effective_batch_size_per_iter # 1 iter means if batch size is 100, then 100 sequences are processed in 1 iter\n",
    "    # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "    flops_achieved_per_second = flops_per_iter_per_fwdbwd * (1.0 / time_taken_per_iter)  # per second\n",
    "    mfu = flops_achieved_per_second / gpu_promised_flops\n",
    "    # fmt: on\n",
    "    return MFUEstimationResult(\n",
    "        flops_per_token_per_fwdbwd=flops_per_token_per_fwdbwd,\n",
    "        flops_per_sequence_per_fwdbwd=flops_per_sequence_per_fwdbwd,\n",
    "        flops_per_iter_per_fwdbwd=flops_per_iter_per_fwdbwd,\n",
    "        flops_achieved_per_second=flops_achieved_per_second,\n",
    "        mfu=mfu,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MFUEstimationResult</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">flops_per_token_per_fwdbwd</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">854553600.0</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">flops_per_sequence_per_fwdbwd</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">875062886400.0</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">flops_per_iter_per_fwdbwd</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">87506288640000.0</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">flops_achieved_per_second</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">115902369059602.66</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">mfu</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.37148195211411106</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mMFUEstimationResult\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mflops_per_token_per_fwdbwd\u001b[0m=\u001b[1;36m854553600\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mflops_per_sequence_per_fwdbwd\u001b[0m=\u001b[1;36m875062886400\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mflops_per_iter_per_fwdbwd\u001b[0m=\u001b[1;36m87506288640000\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mflops_achieved_per_second\u001b[0m=\u001b[1;36m115902369059602\u001b[0m\u001b[1;36m.66\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mmfu\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.37148195211411106\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gradient_accumulation = 5\n",
    "batch_size = 20\n",
    "effective_batch_size = gradient_accumulation * batch_size\n",
    "measured_time = 0.755  # in seconds per iteration\n",
    "model_total_parameters = gpt2_params_no_bias - params()[\"embedding/position\"]\n",
    "\n",
    "mfu_estimates = estimate_mfu(\n",
    "    num_decoder_blocks=gpt2_config.num_decoder_blocks,\n",
    "    num_heads=gpt2_config.n_head,\n",
    "    d_model=gpt2_config.n_embd,\n",
    "    context_length=gpt2_config.context_length,\n",
    "    model_total_parameters=model_total_parameters,\n",
    "    effective_batch_size_per_iter=effective_batch_size,\n",
    "    time_taken_per_iter=measured_time,\n",
    "    gpu_promised_flops=312e12,  # A100 GPU bfloat16/float16 peak flops\n",
    ")\n",
    "\n",
    "pprint(mfu_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_training_days(\n",
    "    total_tokens_in_corpus: float,\n",
    "    mfu_result: MFUEstimationResult,\n",
    "    gpu_promised_flops: float = 312e12,  # Default A100 GPU peak FLOPs\n",
    "    num_gpus: int = 8,  # Default number of GPUs\n",
    "    assumed_mfu: Optional[float] = None,  # Optional manual MFU override\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Estimate the total training time in days based on the model FLOPs\n",
    "    utilization and other training parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    total_tokens_in_corpus : float\n",
    "        Total number of tokens to be processed during training.\n",
    "    mfu_result : MFUEstimationResult\n",
    "        The result from the estimate_mfu function, containing FLOPs\n",
    "        metrics and Model FLOPs Utilization.\n",
    "    gpu_promised_flops : float, optional\n",
    "        Theoretical peak FLOPs performance of a single GPU.\n",
    "    num_gpus : int, optional\n",
    "        Number of GPUs used in the training setup.\n",
    "    assumed_mfu : float, optional\n",
    "        If provided, overrides the MFU calculated in mfu_result to\n",
    "        manually adjust the utilization rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    training_days : float\n",
    "        Estimated total training time in days.\n",
    "    \"\"\"\n",
    "    mfu = assumed_mfu if assumed_mfu is not None else mfu_result.mfu\n",
    "\n",
    "    # Total FLOPs needed for the entire dataset\n",
    "    total_flops_needed = total_tokens_in_corpus * mfu_result.flops_per_token_per_fwdbwd\n",
    "\n",
    "    # Effective throughput considering MFU\n",
    "    effective_flops_per_second = gpu_promised_flops * num_gpus * mfu\n",
    "\n",
    "    # Total training time in seconds\n",
    "    total_training_time_seconds = total_flops_needed / effective_flops_per_second\n",
    "    total_training_time_days = total_training_time_seconds / (3600 * 24)\n",
    "\n",
    "    return total_training_time_days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9626068376068373"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens_in_corpus = 300e9  # 300B tokens\n",
    "assumed_mfu = 0.3  # assume this model flops utilization\n",
    "num_gpus = 8  # 8 GPUs\n",
    "training_days = estimate_training_days(\n",
    "    total_tokens_in_corpus=total_tokens_in_corpus,\n",
    "    mfu_result=mfu_estimates,\n",
    "    gpu_promised_flops=A100().flops[FloatingPointPrecision.BFLOAT16],\n",
    "    num_gpus=8,\n",
    "    assumed_mfu=assumed_mfu,\n",
    ")\n",
    "training_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For practical usage in training, see the following:\n",
    "\n",
    "-   [Karpathy's NanoGPT](https://github.com/karpathy/nanoGPT/blob/325be85d9be8c81b436728a420e85796c57dba7e/train.py#L326)\n",
    "    here.\n",
    "-   [https://github.com/mosaicml/composer/blob/dev/composer/callbacks/speed_monitor.py](https://github.com/mosaicml/composer/blob/dev/composer/callbacks/speed_monitor.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Further Readings\n",
    "\n",
    "-   B. Shazeer, N. Parmar, Z. Lan, Y. Zhu, P. J. Liu, J. Kudugunta, E. Michel,\n",
    "    and A. Roberts,\n",
    "    [\"PaLM: Scaling Language Modeling with Pathways\"](https://arxiv.org/pdf/2204.02311.pdf),\n",
    "    arXiv preprint arXiv:2204.02311, 2022.\n",
    "-   Google Research,\n",
    "    [\"PaLM-2: Scaling Language Model Capacity and Customization with Pathways\"](https://ai.google/static/documents/palm2techreport.pdf),\n",
    "    Google AI Blog, 2022.\n",
    "-   [NVIDIA A100 Datasheet](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf)\n",
    "-   [nanoGPT - Andrej Karpathy](https://github.com/karpathy/nanoGPT/blob/master/transformer_sizing.ipynb)\n",
    "-   [The FLOPs Calculus of Language Model Training - Dzmitry Bahdanau on Medium](https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4)\n",
    "-   [TF-Keras Issue #6 on GitHub](https://github.com/keras-team/tf-keras/issues/6)\n",
    "-   [Composer Callbacks - Speed Monitor on GitHub by MosaicML](https://github.com/mosaicml/composer/blob/dev/composer/callbacks/speed_monitor.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f5833218766b48e6e35e4452ee875aac0e2188d05bbe5298f2c62b79f08b222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
