{"cells":[{"cell_type":"markdown","metadata":{},"source":["# How To Do Teacher-Student Knowledge Distillation?\n","\n","[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n","[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n","[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n","![Tag](https://img.shields.io/badge/Tag-Brain_Dump-red)\n","![Tag](https://img.shields.io/badge/Level-Beginner-green)\n","\n","\n","```{contents}\n",":local:\n","```"]},{"cell_type":"markdown","metadata":{},"source":["## Dependencies\n","\n","```bash\n","pip install -U omniverse==0.0.63\n","P100 16GB\n","```"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T11:14:58.729305Z","iopub.status.busy":"2024-08-25T11:14:58.728920Z","iopub.status.idle":"2024-08-25T11:15:41.822301Z","shell.execute_reply":"2024-08-25T11:15:41.821196Z","shell.execute_reply.started":"2024-08-25T11:14:58.729268Z"},"trusted":true},"outputs":[],"source":["# %pip install omniverse==0.0.63"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T11:15:41.824701Z","iopub.status.busy":"2024-08-25T11:15:41.824371Z","iopub.status.idle":"2024-08-25T11:16:00.679410Z","shell.execute_reply":"2024-08-25T11:16:00.678480Z","shell.execute_reply.started":"2024-08-25T11:15:41.824666Z"},"trusted":true},"outputs":[],"source":["from __future__ import annotations\n","\n","import logging\n","from collections import Counter, OrderedDict\n","from typing import Any, Dict, List, Tuple, TypedDict, overload, cast\n","\n","import gc\n","import numpy as np\n","import pandas as pd\n","import psutil\n","import torch\n","from datasets import load_dataset\n","from rich.pretty import pprint\n","from scipy.special import softmax\n","from sklearn.metrics import (\n","    accuracy_score,\n","    auc,\n","    average_precision_score,\n","    brier_score_loss,\n","    confusion_matrix,\n","    f1_score,\n","    log_loss,\n","    precision_recall_curve,\n","    precision_score,\n","    recall_score,\n","    roc_auc_score,\n","    roc_curve,\n",")\n","from torch import nn\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm.notebook import tqdm  # Use notebook version for better UI in notebooks\n","from transformers import (\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    GPT2ForSequenceClassification,\n","    GPT2Tokenizer,\n","    PreTrainedModel,\n","    PreTrainedTokenizer,\n","    PreTrainedTokenizerBase,\n","    PreTrainedTokenizerFast,\n","    Trainer,\n","    TrainingArguments,\n",")\n","from transformers.trainer_utils import EvalPrediction\n","\n","from omnivault.transformer.config.decoder import (\n","    AddNormConfig,\n","    DecoderBlockConfig,\n","    DecoderConfig,\n","    MultiHeadedAttentionConfig,\n","    PositionwiseFeedForwardConfig,\n",")\n","from omnivault.transformer.modules.attention.core import MultiHeadedAttention, ScaledDotProductAttention\n","from omnivault.transformer.modules.layers.addnorm import AddNorm\n","from omnivault.transformer.modules.layers.mlp import PositionwiseFeedForward\n","from omnivault.utils.reproducibility.seed import seed_all\n","from omnivault.utils.torch_utils.model_utils import total_trainable_parameters\n"]},{"cell_type":"markdown","metadata":{},"source":["## Setting Up"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T11:16:00.681301Z","iopub.status.busy":"2024-08-25T11:16:00.680573Z","iopub.status.idle":"2024-08-25T11:16:00.693616Z","shell.execute_reply":"2024-08-25T11:16:00.692785Z","shell.execute_reply.started":"2024-08-25T11:16:00.681262Z"},"trusted":true},"outputs":[{"data":{"text/plain":["2024"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["seed_all(seed=2024, seed_torch=True, set_torch_deterministic=False)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T11:16:00.695846Z","iopub.status.busy":"2024-08-25T11:16:00.695551Z","iopub.status.idle":"2024-08-25T11:16:00.768480Z","shell.execute_reply":"2024-08-25T11:16:00.767706Z","shell.execute_reply.started":"2024-08-25T11:16:00.695814Z"},"trusted":true},"outputs":[],"source":["LOGGER = logging.getLogger(__name__)\n","LOGGER.setLevel(logging.INFO)\n","handler = logging.StreamHandler()\n","formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","handler.setFormatter(formatter)\n","LOGGER.addHandler(handler)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T11:16:00.769805Z","iopub.status.busy":"2024-08-25T11:16:00.769484Z","iopub.status.idle":"2024-08-25T11:16:00.818947Z","shell.execute_reply":"2024-08-25T11:16:00.818182Z","shell.execute_reply.started":"2024-08-25T11:16:00.769763Z"},"trusted":true},"outputs":[],"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","MAX_LENGTH = 64\n","PADDING = \"longest\"\n","BATCH_SIZE = 32\n","TRUNCATION = True\n","RETURN_TENSORS = \"pt\""]},{"cell_type":"markdown","metadata":{},"source":["## Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T11:16:00.820516Z","iopub.status.busy":"2024-08-25T11:16:00.820097Z","iopub.status.idle":"2024-08-25T11:16:05.140263Z","shell.execute_reply":"2024-08-25T11:16:05.139411Z","shell.execute_reply.started":"2024-08-25T11:16:00.820466Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71663a055def47d6bb6eb5c49494e5ab","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.04k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b5b112bbf8948e9a11e771be3c1963d","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/8.88k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d3bf5f713b4949768b73d1358b6b86fd","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/682k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a152e8232c4943ac9271785071b52c08","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/2264 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["class Batch(TypedDict):\n","    sentence: List[str]\n","    labels: List[int]\n","\n","\n","class TokenizedBatch(TypedDict):\n","    input_ids: List[int]\n","    attention_mask: List[int]\n","    labels: List[int]\n","\n","\n","def preprocess_function(batch: Batch, **kwargs: Any) -> TokenizedBatch:\n","    return cast(TokenizedBatch, tokenizer(batch[\"sentence\"], **kwargs))\n","\n","\n","dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\", trust_remote_code=True)[\"train\"]\n","dataset = dataset.rename_column(\"label\", \"labels\")\n","\n","train_valid_split = dataset.train_test_split(test_size=0.1, shuffle=True, stratify_by_column=\"labels\")\n","\n","train_dataset = train_valid_split[\"train\"]\n","valid_dataset = train_valid_split[\"test\"]"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T11:16:05.141845Z","iopub.status.busy":"2024-08-25T11:16:05.141489Z","iopub.status.idle":"2024-08-25T11:16:05.150220Z","shell.execute_reply":"2024-08-25T11:16:05.149246Z","shell.execute_reply.started":"2024-08-25T11:16:05.141800Z"},"trusted":true},"outputs":[],"source":["class FinancialPhraseDataset(torch.utils.data.Dataset):\n","    def __init__(\n","        self,\n","        sentences: List[str],\n","        labels: List[int],\n","        tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n","        **tokenizer_kwargs: Any,\n","    ) -> None:\n","        self.sentences = sentences\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.tokenizer_kwargs = tokenizer_kwargs or {\n","            \"max_length\": 64,\n","            \"padding\": \"longest\",\n","            \"truncation\": True,\n","            \"return_tensors\": \"pt\",\n","        }\n","\n","    def __len__(self) -> int:\n","        return len(self.sentences)\n","\n","    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n","        sentence = self.sentences[idx]\n","        label = self.labels[idx]\n","\n","        encoding = self.tokenizer(sentence, **self.tokenizer_kwargs)\n","\n","        return {\n","            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n","            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n","            \"labels\": torch.tensor(label, dtype=torch.long),\n","        }"]},{"cell_type":"markdown","metadata":{},"source":["Note again the causal masks is handled internally in huggingface models and we do not need to create it ourselves, hence here all masks are 1s."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T11:16:05.151862Z","iopub.status.busy":"2024-08-25T11:16:05.151514Z","iopub.status.idle":"2024-08-25T11:16:06.863945Z","shell.execute_reply":"2024-08-25T11:16:06.863013Z","shell.execute_reply.started":"2024-08-25T11:16:05.151818Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f24a69b9f1a24c39ab3cce673b354807","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a577403064dd42efb72d646553d24e84","version_major":2,"version_minor":0},"text/plain":["spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8143241d3fba479e98b4b2f77e46be32","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}],"source":["from transformers import DebertaV2Tokenizer\n","\n","tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n","\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","# Set padding side (usually 'right' for DeBERTa)\n","tokenizer.padding_side = \"right\""]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T11:16:06.865544Z","iopub.status.busy":"2024-08-25T11:16:06.865221Z","iopub.status.idle":"2024-08-25T11:16:06.907194Z","shell.execute_reply":"2024-08-25T11:16:06.906459Z","shell.execute_reply.started":"2024-08-25T11:16:06.865509Z"},"trusted":true},"outputs":[],"source":["train_dataset = FinancialPhraseDataset(\n","    sentences=train_dataset[\"sentence\"],\n","    labels=train_dataset[\"labels\"],\n","    tokenizer=tokenizer,\n",")\n","\n","valid_dataset = FinancialPhraseDataset(\n","    sentences=valid_dataset[\"sentence\"],\n","    labels=valid_dataset[\"labels\"],\n","    tokenizer=tokenizer,\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T11:16:06.911550Z","iopub.status.busy":"2024-08-25T11:16:06.911201Z","iopub.status.idle":"2024-08-25T11:16:06.923945Z","shell.execute_reply":"2024-08-25T11:16:06.923151Z","shell.execute_reply.started":"2024-08-25T11:16:06.911518Z"},"trusted":true},"outputs":[],"source":["class FinancialPhraseCollator:\n","    def __init__(\n","        self,\n","        tokenizer: PreTrainedTokenizer | PreTrainedTokenizerFast,\n","        max_length: int,\n","        padding_side: str = \"right\",\n","    ) -> None:\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.padding_side = padding_side\n","\n","    def pad_sequence(self, sequence: torch.Tensor, target_length: int) -> torch.Tensor:\n","        pad_length = target_length - sequence.size(0)\n","        if pad_length <= 0:\n","            return sequence[:target_length]\n","\n","        pad_tensor = torch.full((pad_length,), self.tokenizer.pad_token_id, dtype=sequence.dtype)\n","\n","        if self.padding_side == \"right\":\n","            return torch.cat([sequence, pad_tensor])\n","        else:  # padding_side == \"left\"\n","            return torch.cat([pad_tensor, sequence])\n","\n","    def create_attention_mask(self, sequence: torch.Tensor, target_length: int) -> torch.Tensor:\n","        attention_mask = torch.ones(target_length, dtype=torch.long)\n","        if self.padding_side == \"right\":\n","            attention_mask[sequence.size(0) :] = 0\n","        else:  # padding_side == \"left\"\n","            attention_mask[: target_length - sequence.size(0)] = 0\n","        return attention_mask\n","\n","    def __call__(self, batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n","        max_length = max(len(item[\"input_ids\"]) for item in batch)\n","        max_length = min(max_length, self.max_length)\n","\n","        input_ids = torch.stack([self.pad_sequence(item[\"input_ids\"], max_length) for item in batch])\n","        attention_mask = torch.stack([self.create_attention_mask(item[\"input_ids\"], max_length) for item in batch])\n","        labels = torch.stack([item[\"labels\"] for item in batch])\n","\n","        return {\n","            \"input_ids\": input_ids,\n","            \"attention_mask\": attention_mask,\n","            \"labels\": labels,\n","        }"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T11:16:06.925197Z","iopub.status.busy":"2024-08-25T11:16:06.924906Z","iopub.status.idle":"2024-08-25T11:16:06.939620Z","shell.execute_reply":"2024-08-25T11:16:06.938753Z","shell.execute_reply.started":"2024-08-25T11:16:06.925161Z"},"trusted":true},"outputs":[],"source":["collator = FinancialPhraseCollator(\n","    tokenizer=tokenizer,\n","    max_length=MAX_LENGTH,\n","    padding_side=tokenizer.padding_side\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T11:16:06.941229Z","iopub.status.busy":"2024-08-25T11:16:06.940830Z","iopub.status.idle":"2024-08-25T11:16:07.059850Z","shell.execute_reply":"2024-08-25T11:16:07.058954Z","shell.execute_reply.started":"2024-08-25T11:16:06.941180Z"},"trusted":true},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'input_ids'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10029</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">275</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16246</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8156</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">486</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">262</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5670</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5174</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11764</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">48850</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span>    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">585</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2784</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]])</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'attention_mask'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #808000; text-decoration-color: #808000\">...</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]])</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'labels'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>\n","<span style=\"font-weight: bold\">}</span>\n","</pre>\n"],"text/plain":["\u001b[1m{\u001b[0m\n","\u001b[2;32m│   \u001b[0m\u001b[32m'input_ids'\u001b[0m: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m    \u001b[1;36m1\u001b[0m, \u001b[1;36m10029\u001b[0m,   \u001b[1;36m275\u001b[0m,  \u001b[33m...\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m    \u001b[1;36m1\u001b[0m, \u001b[1;36m16246\u001b[0m,  \u001b[1;36m8156\u001b[0m,  \u001b[33m...\u001b[0m,     \u001b[1;36m2\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m    \u001b[1;36m1\u001b[0m,   \u001b[1;36m486\u001b[0m,   \u001b[1;36m262\u001b[0m,  \u001b[33m...\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\u001b[33m...\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m    \u001b[1;36m1\u001b[0m,  \u001b[1;36m5670\u001b[0m,  \u001b[1;36m5174\u001b[0m,  \u001b[33m...\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m    \u001b[1;36m1\u001b[0m, \u001b[1;36m11764\u001b[0m, \u001b[1;36m48850\u001b[0m,  \u001b[33m...\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m    \u001b[1;36m1\u001b[0m,   \u001b[1;36m585\u001b[0m,  \u001b[1;36m2784\u001b[0m,  \u001b[33m...\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m,     \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n","\u001b[2;32m│   \u001b[0m\u001b[32m'attention_mask'\u001b[0m: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\u001b[33m...\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m,  \u001b[33m...\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n","\u001b[2;32m│   \u001b[0m\u001b[32m'labels'\u001b[0m: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m0\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[1m}\u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["seed_all(seed=2024, seed_torch=True, set_torch_deterministic=False)\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    collate_fn=collator\n",")\n","\n","valid_dataloader = DataLoader(\n","    valid_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=False,\n","    collate_fn=collator\n",")\n","\n","for batch in train_dataloader:\n","    pprint(batch)\n","    break"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T11:16:07.062237Z","iopub.status.busy":"2024-08-25T11:16:07.061915Z","iopub.status.idle":"2024-08-25T11:16:07.113047Z","shell.execute_reply":"2024-08-25T11:16:07.112183Z","shell.execute_reply.started":"2024-08-25T11:16:07.062203Z"},"trusted":true},"outputs":[],"source":["from __future__ import annotations\n","\n","from typing import Tuple\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from transformers import DebertaV2ForSequenceClassification, get_linear_schedule_with_warmup\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","\n","\n","def train_one_epoch(\n","    model: nn.Module,\n","    dataloader: DataLoader,\n","    optimizer: torch.optim.Optimizer,\n","    scheduler: torch.optim.lr_scheduler.LambdaLR,\n","    device: torch.device,\n",") -> Tuple[float, float]:\n","    model.train()\n","    total_loss = 0.0\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","\n","        optimizer.zero_grad()\n","        outputs: SequenceClassifierOutput = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        logits = outputs.logits\n","\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        total_loss += loss.item()\n","        _, preds = torch.max(logits, dim=1)\n","        correct_predictions += torch.sum(preds == labels).item()\n","        total_predictions += labels.size(0)\n","\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy = correct_predictions / total_predictions\n","    return avg_loss, accuracy\n","\n","\n","def valid_one_epoch(model: nn.Module, dataloader: DataLoader, device: torch.device) -> Tuple[float, float]:\n","    model.eval()\n","    total_loss = 0.0\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc=\"Validation\", leave=False):\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","\n","            outputs: SequenceClassifierOutput = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            logits = outputs.logits\n","\n","            total_loss += loss.item()\n","            _, preds = torch.max(logits, dim=1)\n","            correct_predictions += torch.sum(preds == labels).item()\n","            total_predictions += labels.size(0)\n","\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy = correct_predictions / total_predictions\n","    return avg_loss, accuracy\n","\n","\n","def train_model(\n","    model: nn.Module,\n","    train_dataloader: DataLoader,\n","    valid_dataloader: DataLoader,\n","    optimizer: torch.optim.Optimizer,\n","    scheduler: torch.optim.lr_scheduler.LambdaLR,\n","    num_epochs: int,\n","    device: torch.device,\n",") -> nn.Module:\n","    for epoch in range(num_epochs):\n","        train_loss, train_accuracy = train_one_epoch(model, train_dataloader, optimizer, scheduler, device)\n","        val_loss, val_accuracy = valid_one_epoch(model, valid_dataloader, device)\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        print(f\"Training loss: {train_loss:.4f}, Training accuracy: {train_accuracy:.4f}\")\n","        print(f\"Validation loss: {val_loss:.4f}, Validation accuracy: {val_accuracy:.4f}\")\n","        print()\n","\n","    return model\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T11:16:07.114406Z","iopub.status.busy":"2024-08-25T11:16:07.114088Z","iopub.status.idle":"2024-08-25T11:16:27.614051Z","shell.execute_reply":"2024-08-25T11:16:27.613104Z","shell.execute_reply.started":"2024-08-25T11:16:07.114373Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa439f31c58447738dc9f27c72f79c21","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/241M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Training student model naively...\n"]},{"name":"stderr","output_type":"stream","text":["                                                         \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","Training loss: 0.8688, Training accuracy: 0.5886\n","Validation loss: 0.7205, Validation accuracy: 0.6167\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                         \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/2\n","Training loss: 0.6349, Training accuracy: 0.6937\n","Validation loss: 0.6428, Validation accuracy: 0.8018\n","\n"]}],"source":["seed_all(seed=42, seed_torch=True, set_torch_deterministic=False)\n","\n","NUM_LABELS = 3\n","NUM_EPOCHS = 2\n","\n","# Create student model (DeBERTa-v2-xlarge-mnli)\n","student_model = DebertaV2ForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-xsmall\", num_labels=NUM_LABELS)\n","student_model.to(DEVICE)\n","\n","# Cross Entropy Loss\n","ce_loss = nn.CrossEntropyLoss()\n","\n","# Train student model naively (without distillation)\n","print(\"Training student model naively...\")\n","student_optimizer = torch.optim.AdamW(student_model.parameters(), lr=2e-5)\n","student_scheduler = get_linear_schedule_with_warmup(\n","    student_optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * NUM_EPOCHS\n",")\n","_ = train_model(student_model, train_dataloader, valid_dataloader, student_optimizer, student_scheduler, NUM_EPOCHS, DEVICE)\n","\n","gc.collect()\n","\n","# Delete unnecessary objects\n","del student_model\n","del student_optimizer\n","del student_scheduler\n","del _\n","\n","# Empty CUDA cache\n","if torch.cuda.is_available():\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T11:16:27.615866Z","iopub.status.busy":"2024-08-25T11:16:27.615553Z","iopub.status.idle":"2024-08-25T11:18:22.453003Z","shell.execute_reply":"2024-08-25T11:18:22.452017Z","shell.execute_reply.started":"2024-08-25T11:16:27.615834Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c4df77ded4254fdfb378ed4376171431","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"17cea360f021431185444e103859cab3","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Training teacher model...\n"]},{"name":"stderr","output_type":"stream","text":["                                                         \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","Training loss: 0.5893, Training accuracy: 0.7320\n","Validation loss: 0.3581, Validation accuracy: 0.8238\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                         \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/2\n","Training loss: 0.2083, Training accuracy: 0.9293\n","Validation loss: 0.1159, Validation accuracy: 0.9692\n","\n"]}],"source":["seed_all(seed=42, seed_torch=True, set_torch_deterministic=False)\n","\n","# Create teacher model (DeBERTa-v2-xlarge)\n","teacher_model = DebertaV2ForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-large\", num_labels=NUM_LABELS)\n","teacher_model.to(DEVICE)\n","\n","# Train teacher model\n","print(\"Training teacher model...\")\n","teacher_optimizer = torch.optim.AdamW(teacher_model.parameters(), lr=2e-5)\n","teacher_scheduler = get_linear_schedule_with_warmup(\n","    teacher_optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * NUM_EPOCHS\n",")\n","teacher_model =train_model(teacher_model, train_dataloader, valid_dataloader, teacher_optimizer, teacher_scheduler, NUM_EPOCHS, DEVICE)\n","\n","gc.collect()\n","\n","# Delete unnecessary objects\n","del teacher_optimizer\n","del teacher_scheduler\n","\n","# Empty CUDA cache\n","if torch.cuda.is_available():\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T11:18:22.456284Z","iopub.status.busy":"2024-08-25T11:18:22.455872Z","iopub.status.idle":"2024-08-25T11:18:22.478271Z","shell.execute_reply":"2024-08-25T11:18:22.477290Z","shell.execute_reply.started":"2024-08-25T11:18:22.456239Z"},"trusted":true},"outputs":[],"source":["\n","def distillation_loss(\n","    student_logits: torch.Tensor,\n","    teacher_logits: torch.Tensor,\n","    labels: torch.Tensor,\n","    temperature: float = 2.0,\n","    alpha: float = 0.5,\n",") -> torch.Tensor:\n","    \"\"\"\n","    Compute the knowledge distillation loss.\n","\n","    This loss combines the soft targets from the teacher model with the\n","    hard targets from the true labels.\n","\n","    Parameters\n","    ----------\n","    student_logits : torch.Tensor\n","        Raw, unnormalized output scores from the student model.\n","    teacher_logits : torch.Tensor\n","        Raw, unnormalized output scores from the teacher model.\n","    labels : torch.Tensor\n","        True class labels for the input data.\n","    temperature : float, optional\n","        Controls the softness of probability distributions (default 2.0).\n","    alpha : float, optional\n","        Balances contribution of soft and hard losses (default 0.5).\n","\n","    Returns\n","    -------\n","    torch.Tensor\n","        The computed distillation loss.\n","\n","    Variables\n","    ---------\n","    soft_targets : torch.Tensor\n","        Teacher's predictions converted to a smoothed probability\n","        distribution.\n","    soft_prob : torch.Tensor\n","        Log of the student's smoothed probability predictions.\n","    soft_loss : torch.Tensor\n","        KL-Divergence between soft probabilities and soft targets,\n","        scaled by temperature^2.\n","    hard_loss : torch.Tensor\n","        Cross-entropy loss between student logits and true labels.\n","\n","    Notes\n","    -----\n","    The loss is computed as:\n","    L = α * L_soft + (1 - α) * L_hard\n","\n","    Where:\n","    - L_soft is the KL divergence between soft student and teacher probs\n","    - L_hard is the cross-entropy between student logits and true labels\n","    \"\"\"\n","    soft_targets = (teacher_logits / temperature).softmax(dim=-1)\n","    soft_prob = (student_logits / temperature).log_softmax(dim=-1)\n","    soft_loss = nn.KLDivLoss(reduction=\"batchmean\")(soft_prob, soft_targets) * (temperature**2)\n","    hard_loss = ce_loss(student_logits, labels)\n","    return alpha * soft_loss + (1 - alpha) * hard_loss\n","\n","\n","def train_student_one_epoch(\n","    student_model: nn.Module,\n","    teacher_model: nn.Module,\n","    dataloader: DataLoader,\n","    optimizer: torch.optim.Optimizer,\n","    scheduler: torch.optim.lr_scheduler.LambdaLR,\n","    device: torch.device,\n","    *,\n","    temperature: float = 2.0,\n","    alpha: float = 0.5,\n",") -> Tuple[float, float]:\n","    student_model.train()  # set student model to train mode\n","    teacher_model.eval()  # set teacher model to eval mode\n","    total_loss = 0.0\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","\n","        optimizer.zero_grad()\n","        with torch.no_grad():\n","            teacher_outputs: SequenceClassifierOutput = teacher_model(\n","                input_ids=input_ids, attention_mask=attention_mask\n","            )\n","        student_outputs: SequenceClassifierOutput = student_model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","        loss = distillation_loss(\n","            student_outputs.logits, teacher_outputs.logits, labels, temperature=temperature, alpha=alpha\n","        )\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        total_loss += loss.item()\n","        _, preds = torch.max(student_outputs.logits, dim=1)\n","        correct_predictions += torch.sum(preds == labels).item()\n","        total_predictions += labels.size(0)\n","\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy = correct_predictions / total_predictions\n","    return avg_loss, accuracy\n","\n","\n","def train_student_model(\n","    student_model: nn.Module,\n","    teacher_model: nn.Module,\n","    train_dataloader: DataLoader,\n","    valid_dataloader: DataLoader,\n","    optimizer: torch.optim.Optimizer,\n","    scheduler: torch.optim.lr_scheduler.LambdaLR,\n","    num_epochs: int,\n","    device: torch.device,\n","    *,\n","    temperature: float = 2.0,\n","    alpha: float = 0.5,\n",") -> None:\n","    for epoch in range(num_epochs):\n","        train_loss, train_accuracy = train_student_one_epoch(\n","            student_model,\n","            teacher_model,\n","            train_dataloader,\n","            optimizer,\n","            scheduler,\n","            device,\n","            temperature=temperature,\n","            alpha=alpha,\n","        )\n","        val_loss, val_accuracy = valid_one_epoch(student_model, valid_dataloader, device)\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        print(f\"Training loss: {train_loss:.4f}, Training accuracy: {train_accuracy:.4f}\")\n","        print(f\"Validation loss: {val_loss:.4f}, Validation accuracy: {val_accuracy:.4f}\")\n","        print()\n"]},{"cell_type":"markdown","metadata":{},"source":["| Term                                   | Definition                                                           | Explanation                                                                                                                                                                               |\n","| -------------------------------------- | -------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n","| Student Logits ($z_s$)                 | Raw unnormalized scores from the student model                       | $z_s = f_s(x)$ where $f_s$ is the student model and $x$ is the input                                                                                                                      |\n","| Teacher Logits ($z_t$)                 | Raw unnormalized scores from the teacher model                       | $z_t = f_t(x)$ where $f_t$ is the teacher model, and the parameter space $\\theta_t$ is larger than the parameter space $\\theta_s$ of the student model.                                   |\n","| Labels ($y$)                           | True class labels for the input data                                 | $y \\in \\{1, \\ldots, K\\}$ for $K$ classes                                                                                                                                                  |\n","| Temperature ($T$)                      | Hyperparameter controlling distribution softness                     | $T \\in \\mathbb{R}^+$, typically $T > 1$. Higher temperatures produce softer probability distributions, emphasizing the relative differences between class probabilities.                  |\n","| Alpha ($\\alpha$)                       | Balancing hyperparameter for soft and hard losses                    | $\\alpha \\in [0, 1] $ and $\\alpha$ is the weight given to the distillation (soft) loss and $1 - \\alpha$ gives the weight for the hard loss.                                                |\n","| Soft Targets ($p_t$)                   | $p_t = \\text{softmax}(z_t / T)$                                      | $p_t^{(i)} = \\frac{\\exp\\left(z_t^{(i)}/T\\right)}{\\sum_j \\exp\\left(z_t^{(j)}/T\\right)}$ The teacher's predictions converted to a probability distribution and smoothed by the temperature. |\n","| Soft Probabilities ($p_s$)             | $p_s = \\log\\text{softmax}(z_s / T)$                                  | $p_s^{(i)} = \\log\\frac{\\exp\\left(z_s^{(i)}/T\\right)}{\\sum_j \\exp\\left(z_s^{(j)}/T\\right)}$                                                                                                |\n","| Soft Loss ($L_\\text{soft}$)            | $L_\\text{soft} = T^2 \\cdot \\text{KL}(p_t \\Vert p_s)$                 | Measures divergence between soft probabilities and soft targets. Measures how well the student's predictions match the teacher's smoothed predictions.                                    |\n","| Hard Loss ($L_\\text{hard}$)            | $L_\\text{hard} = \\text{CE}(z_s, y)$                                  | The standard supervised learning loss, measuring how well the student's predictions match the true labels.                                                                                |\n","| Distillation Loss ($L_\\text{distill}$) | $L_\\text{distill} = \\alpha L_\\text{soft} + (1-\\alpha) L_\\text{hard}$ | The combined loss function that the student model optimizes, balancing between mimicking the teacher and predicting true labels.                                                          |\n","| KL-Divergence                          | $\\text{KL}(p \\Vert q) = \\sum_i p_i \\log\\frac{p_i}{q_i}$              | Measures difference between two probability distributions, and in this scenario, it quantifies how much the student's prediction distribution differs from the teacher's.                 |\n","| Cross-Entropy Loss                     | $\\text{CE}(z, y) = -\\sum_i y_i \\log(\\text{softmax}(z)_i)$            | Standard loss for multi-class classification                                                                                                                                              |\n","\n","Note: In the table, $i$ and $j$ are used as indices for classes, $\\exp$ denotes\n","the exponential function, and $\\log$ is the natural logarithm. The softmax\n","function is defined as\n","$\\text{softmax}(x)_i = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T11:18:22.480010Z","iopub.status.busy":"2024-08-25T11:18:22.479609Z","iopub.status.idle":"2024-08-25T11:19:09.647805Z","shell.execute_reply":"2024-08-25T11:19:09.646822Z","shell.execute_reply.started":"2024-08-25T11:18:22.479959Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","                                                         \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","Training loss: 0.3327, Training accuracy: 0.5925\n","Validation loss: 0.7193, Validation accuracy: 0.6167\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                         "]},{"name":"stdout","output_type":"stream","text":["Epoch 2/2\n","Training loss: 0.2407, Training accuracy: 0.7472\n","Validation loss: 0.6435, Validation accuracy: 0.8150\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["seed_all(seed=42, seed_torch=True, set_torch_deterministic=False)\n","\n","student_model = DebertaV2ForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-xsmall\", num_labels=NUM_LABELS)\n","student_model.to(DEVICE)\n","\n","# Train student model\n","student_optimizer = torch.optim.AdamW(student_model.parameters(), lr=2e-5)\n","student_scheduler = get_linear_schedule_with_warmup(\n","    student_optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * NUM_EPOCHS\n",")\n","train_student_model(\n","    student_model,\n","    teacher_model,\n","    train_dataloader,\n","    valid_dataloader,\n","    student_optimizer,\n","    student_scheduler,\n","    NUM_EPOCHS,\n","    DEVICE,\n","    temperature=0.5,\n","    alpha=0.8\n",")"]},{"cell_type":"markdown","metadata":{},"source":["We see an improvement over vanilla student model.\n","\n","```text\n","Epoch 1/2\n","Training loss: 0.8688, Training accuracy: 0.5886\n","Validation loss: 0.7205, Validation accuracy: 0.6167\n","\n","                                                         \n","Epoch 2/2\n","Training loss: 0.6349, Training accuracy: 0.6937\n","Validation loss: 0.6428, Validation accuracy: 0.8018\n","```"]},{"cell_type":"markdown","metadata":{},"source":["## References And Further Readings\n","\n","-   G. Hinton, O. Vinyals, and J. Dean, \"Distilling the Knowledge in a Neural\n","    Network,\" arXiv preprint arXiv:1503.02531, 2015. [Online]. Available:\n","    https://arxiv.org/abs/1503.02531\n","\n","-   https://douglasorr.github.io/2021-10-training-objectives/2-teacher/article.html\n","-   https://keras.io/examples/keras_recipes/better_knowledge_distillation/\n","-   https://keras.io/examples/vision/knowledge_distillation/\n","-   https://github.com/haitongli/knowledge-distillation-pytorch\n","-   https://pytorch.org/tutorials/beginner/knowledge_distillation_tutorial.html"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
