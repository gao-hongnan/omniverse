{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Do We Warmup Cosine Decay Schedulers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decays the learning rate according to the decreasing part of a cosine curve,\n",
    "with an initial warmup.\n",
    "\n",
    "The `CosineAnnealingWithWarmupScheduler` modulates the learning rate according\n",
    "to a two-phase process: a warmup phase followed by a cosine annealing phase. The\n",
    "learning rate _multiplier_ (this is a multiplier and not the real learning rate)\n",
    "$\\alpha_{t}$ at time (step) $t$ is given by:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\alpha_{t} = \\begin{cases}\n",
    "    \\frac{t}{t_{\\text{warmup}}}, & \\text{if } t < t_{\\text{warmup}} \\\\\n",
    "    \\alpha_f + (1 - \\alpha_f) \\times \\frac{1}{2} \\left[1 + \\cos(\\tau_w\\pi) \\right], & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where we denote:\n",
    "\n",
    "-   $\\tau_w = \\frac{t - t_{\\text{warmup}}}{t_{\\max}}$, the fraction of\n",
    "    post-warmup time elapsed,\n",
    "    -   $t$ as the **current** training step,\n",
    "    -   $t_{\\text{warmup}}$ as the **warmup** time (in steps),\n",
    "    -   $t_{\\max}$ as the **maximum** number of training steps, or maximum\n",
    "        number of iterations in an epoch (see\n",
    "        [here](https://github.com/skorch-dev/skorch/issues/610)). Note that if\n",
    "        you set it to the total number of training steps, the scheduler will\n",
    "        only decay the learning rate after the warmup phase and not oscillate\n",
    "        it.\n",
    "-   $\\alpha_f$ is a _scaling_ factor that determines the **final** learning rate\n",
    "    multiplier to decay to (a value between $0$ and $1$), and this is a _fixed_\n",
    "    value. For example, if $\\alpha_f = 0.1$ and the initial learning rate is\n",
    "    $\\eta_{\\max} = 3e-4$, then the final learning rate will be\n",
    "    $\\eta_{\\min} = 3e-4 \\times 0.1 = 3e-5$.\n",
    "\n",
    "-   $\\eta_{\\max}$ as the **initial** learning rate and will be the **maximum**\n",
    "    learning rate reached during training.\n",
    "\n",
    "The actual learning rate $\\eta_{t}$ at time (step) $t$ is then computed as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\eta_{t} = \\alpha_{t} \\times \\eta_{\\max}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where we emphasize again that $\\eta_{\\max}$ is the **initial** learning rate and\n",
    "will be the **maximum** learning rate reached during training.\n",
    "\n",
    "## Running Example\n",
    "\n",
    "For simplicity, we assume that there are a total of $10$ training steps (or\n",
    "epoches) depending on how you define it. Consequently, the $t_{\\text{max}}$ is\n",
    "$10$.\n",
    "\n",
    "-   $\\eta_{\\max} = 3e-4$\n",
    "-   $t_{\\text{warmup}} = 5$\n",
    "-   $t_{\\max} = 10$\n",
    "-   $\\alpha_f = 0.5$\n",
    "\n",
    "## 1. Warmup Phase\n",
    "\n",
    "During the warmup phase, when the **current** training step $t$ is less than the\n",
    "warmup time $t_{\\text{warmup}}$, the learning rate multiplier is **linearly**\n",
    "increased from $0$ to $1$.\n",
    "\n",
    "Mathematically, the learning rate multiplier $\\alpha_{t}$ at time (step) $t$ is\n",
    "\n",
    "$$\n",
    "\\alpha_{t} = \\frac{t}{t_{\\text{warmup}}}\n",
    "$$\n",
    "\n",
    "The learning rate at this phase is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\eta_{t} &=  \\alpha_{t} \\times \\eta_{\\max} \\\\\n",
    "&= \\frac{t}{t_{\\text{warmup}}} \\times \\eta_{\\max}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "During the warmup phase, the learning rate will linearly increase from $0$ to\n",
    "$\\eta_{\\max}$ in the first $t_{\\text{warmup}}$ steps. Since\n",
    "$\\eta_{\\max} = 3 \\times 10^{-4}$ and $t_{\\text{warmup}} = 5$, the learning rate\n",
    "will be increased as follows:\n",
    "\n",
    "-   $t = 1$:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\alpha_1 &= \\frac{t}{t_{\\text{warmup}}} = \\frac{1}{5} = 0.2 \\\\\n",
    "    \\eta_1 &= \\alpha_1 \\times \\eta_{\\max} = 0.2 \\times 3 \\times 10^{-4} = 6 \\times 10^{-5}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "-   $t = 2$:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\alpha_2 &= \\frac{t}{t_{\\text{warmup}}} = \\frac{2}{5} = 0.4 \\\\\n",
    "    \\eta_2 &= \\alpha_2 \\times \\eta_{\\max} = 0.4 \\times 3 \\times 10^{-4} = 1.2 \\times 10^{-4}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "-   $t = 3$:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\alpha_3 &= \\frac{t}{t_{\\text{warmup}}} = \\frac{3}{5} = 0.6 \\\\\n",
    "    \\eta_3 &= \\alpha_3 \\times \\eta_{\\max} = 0.6 \\times 3 \\times 10^{-4} = 1.8 \\times 10^{-4}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "-   $t = 4$:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\alpha_4 &= \\frac{t}{t_{\\text{warmup}}} = \\frac{4}{5} = 0.8 \\\\\n",
    "    \\eta_4 &= \\alpha_4 \\times \\eta_{\\max} = 0.8 \\times 3 \\times 10^{-4} = 2.4 \\times 10^{-4}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "-   $t = 5$:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\alpha_5 &= \\frac{t}{t_{\\text{warmup}}} = \\frac{5}{5} = 1 \\\\\n",
    "    \\eta_5 &= \\alpha_5 \\times \\eta_{\\max} = 3 \\times 10^{-4} \\times 1 = 3 \\times 10^{-4}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "The linear relationship for the warmup phase can be represented as a function of\n",
    "the current training step $x$:\n",
    "\n",
    "$$\n",
    "f(t) = \\frac{t}{t_{\\text{warmup}}}\n",
    "$$\n",
    "\n",
    "where $t_{\\text{warmup}}$is the total number of steps in the warmup phase. This\n",
    "function describes how the learning rate multiplier $\\alpha_t$ grows linearly\n",
    "from $0$ to $1$ as $t$ progresses from $0$ to $t_{\\text{warmup}}$.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The warmup phase in learning rate scheduling, where the learning rate linearly\n",
    "increases from a small value to a larger value, has its own unique intuition and\n",
    "benefits. Here's an exploration of the underlying reasoning:\n",
    "\n",
    "#### Practical Intuition\n",
    "\n",
    "1. **Gentle Start**: Starting the training with a small learning rate allows the\n",
    "   model to make small adjustments initially. This can be beneficial if the\n",
    "   initial parameters are far from optimal, as it reduces the risk of\n",
    "   overshooting or diverging.\n",
    "\n",
    "2. **Adaptation to Data**: The gradual increase in learning rate allows the\n",
    "   model to adapt to the data distribution and the structure of the loss\n",
    "   landscape. This can be particularly valuable when training deep models, where\n",
    "   the loss surface can be complex and non-convex.\n",
    "\n",
    "3. **Acceleration of Convergence**: By gradually increasing the learning rate,\n",
    "   the warmup phase can accelerate convergence by guiding the optimizer towards\n",
    "   a good region of the loss landscape. Once the learning rate reaches its\n",
    "   maximum value, the optimizer can take larger steps to explore this region.\n",
    "\n",
    "4. **Compatibility with Adaptive Methods**: When used with adaptive optimization\n",
    "   methods like Adam, the warmup phase can help stabilize the moving averages of\n",
    "   gradients and squared gradients, leading to more robust optimization.\n",
    "\n",
    "5. **Preventing Poor Local Minima**: By gradually increasing the learning rate,\n",
    "   the optimizer may escape shallow or poor local minima in the early stages of\n",
    "   training, enabling it to find a better solution.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "The warmup phase in learning rate scheduling serves as a controlled and gradual\n",
    "start to the training process. By linearly increasing the learning rate, it\n",
    "helps the model adapt to the data, avoids potential pitfalls in the loss\n",
    "landscape, and sets the stage for efficient optimization. The mathematical\n",
    "formulation is simple, but it embodies a thoughtful approach to training\n",
    "dynamics, often leading to improved convergence and generalization.\n",
    "\n",
    "## 2. Cosine Decay Phase\n",
    "\n",
    "After the warmup phase, the learning rate multiplier follows a cosine decay\n",
    "pattern. This phase commences once the current training step $t$ is greater than\n",
    "or equal to the warmup time $t_{\\text{warmup}}$, and it continues until the\n",
    "maximum training step $t_{\\text{max}}$.\n",
    "\n",
    "### 2.1. Tau Fraction\n",
    "\n",
    "We first define a variable $\\tau_w$ to represent the fraction of post-warmup\n",
    "time elapsed. Mathematically, it is defined as:\n",
    "\n",
    "$$\n",
    "\\tau_w = \\frac{t - t_{\\text{warmup}}}{t_{\\text{max}}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "-   $t$: Current training step.\n",
    "-   $t_{\\text{warmup}}$: Warmup time in training steps.\n",
    "-   $t_{\\text{max}}$: Total duration of the scheduler in training steps.\n",
    "\n",
    "### 2.2. Learning Rate Multiplier\n",
    "\n",
    "The learning rate multiplier $\\alpha_t$ during the cosine decay phase is given\n",
    "by:\n",
    "\n",
    "$$\n",
    "\\alpha_{t} = \\alpha_f + \\frac{1}{2}(1 - \\alpha_f)  \\left(1 + \\cos \\left(\\tau_w\\pi\\right)\\right)\n",
    "$$\n",
    "\n",
    "where $\\alpha_f$ is the scaling factor that determines the final learning rate\n",
    "multiplier to decay to.\n",
    "\n",
    "### 2.3. Learning Rate\n",
    "\n",
    "The actual learning rate $\\eta_t$ during this phase is then computed as:\n",
    "\n",
    "$$\n",
    "\\eta_{t} = \\alpha_{t} \\times \\eta_{\\text{max}}\n",
    "$$\n",
    "\n",
    "### Example\n",
    "\n",
    "Using the running example with:\n",
    "\n",
    "-   $\\eta_{\\text{max}} = 3 \\times 10^{-4}$\n",
    "-   $t_{\\text{warmup}} = 5$\n",
    "-   $t_{\\text{max}} = 10$\n",
    "-   $\\alpha_f = 0.5$\n",
    "\n",
    "The learning rate will be computed as follows:\n",
    "\n",
    "-   $t = 6$:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\tau_w &= \\frac{6 - 5}{10} = 0.1 \\\\\n",
    "    \\alpha_6 &= 0.5 + \\frac{1}{2}(1 - 0.5) \\left(1 + \\cos \\left(0.1\\pi\\right)\\right) = 0.975445 \\\\\n",
    "    \\eta_6 &= 3 \\times 10^{-4} \\times 0.975445 = 2.92634 \\times 10^{-4}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "-   $t = 7$:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\tau_w &= \\frac{7 - 5}{10} = 0.2 \\\\\n",
    "    \\alpha_7 &= 0.5 + \\frac{1}{2}(1 - 0.5) \\left(1 + \\cos \\left(0.2\\pi\\right)\\right) = 0.904508 \\\\\n",
    "    \\eta_7 &= 3 \\times 10^{-4} \\times 0.904508 = 2.71352 \\times 10^{-4}\n",
    "    \\end{align*}\n",
    "    $$\n",
    "-   ... (and so on for the remaining steps)\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The cosine decay phase in the learning rate scheduler is designed to gradually\n",
    "reduce the learning rate according to the cosine function. The intuition behind\n",
    "this decay can be understood both mathematically and from a practical\n",
    "standpoint.\n",
    "\n",
    "#### Mathematical Intuition\n",
    "\n",
    "1. **Cosine Function**: The cosine function oscillates between -1 and 1. By\n",
    "   taking a scaled and shifted version of the cosine function, we can create a\n",
    "   curve that starts at its highest point and gradually descends to its lowest\n",
    "   point over the interval $[0, t_{\\text{max}}]$.\n",
    "\n",
    "2. **Decay Formula**: The formula for the learning rate multiplier during the\n",
    "   decay phase is:\n",
    "\n",
    "    $$\n",
    "    \\alpha_{t} = \\alpha_f + \\frac{1}{2}(1 - \\alpha_f)\\left(1 + \\cos \\left(\\tau_w\\pi\\right)\\right)\n",
    "    $$\n",
    "\n",
    "    Here, $\\tau_w$ is the fraction of time elapsed since the warmup phase, and\n",
    "    it ranges from 0 to 1. The $\\cos(\\tau_w\\pi)$ term creates a curve that\n",
    "    starts at 1 (when $\\tau_w = 0$) and ends at -1 (when $\\tau_w = 1$). The\n",
    "    scaling and shifting ensure that $\\alpha_t$ starts at 1 and decays to\n",
    "    $\\alpha_f$.\n",
    "\n",
    "More concretely, the expression\n",
    "\n",
    "$$\n",
    "\\alpha_{t} = \\alpha_f + \\frac{1}{2}(1 - \\alpha_f)\\left(1 + \\cos \\left(\\tau_w\\pi\\right)\\right)\n",
    "$$\n",
    "\n",
    "describes the learning rate multiplier during the decay phase, where $\\tau_w$ is\n",
    "the fraction of time elapsed since the warmup phase where $\\tau_w$ is the\n",
    "fraction of time elapsed since the warmup phase, and it ranges from 0 to 1.\n",
    "\n",
    "Let's zoom into the cosine decay part in more details:\n",
    "\n",
    "-   The term $\\cos(\\tau_w\\pi)$ oscillates between 1 and -1 as $\\tau_w$ varies\n",
    "    from 0 to 1.\n",
    "-   When you add 1 to this term, the expression $1 + \\cos(\\tau_w\\pi)$ oscillates\n",
    "    between 0 and 2.\n",
    "-   Multiplying this by $\\frac{1}{2}$ scales it down, so the expression\n",
    "    $\\frac{1}{2}\\left(1 + \\cos(\\tau_w\\pi)\\right)$ oscillates between 0 and 1.\n",
    "-   The term $\\frac{1}{2}(1 - \\alpha_f)$ scales this oscillation so that the\n",
    "    amplitude is adjusted based on the desired final learning rate multiplier\n",
    "    $\\alpha_f$. This means if $\\alpha_f = 0.5$, the expression\n",
    "    $\\frac{1}{2}(1 - \\alpha_f)\\left(1 + \\cos(\\tau_w\\pi)\\right)$ oscillates\n",
    "    between 0 and 0.5.\n",
    "-   Adding $\\alpha_f$ shifts the entire expression so that it starts at 1 when\n",
    "    $\\tau_w = 0$ and decays to $\\alpha_f$ when $\\tau_w = 1$.\n",
    "\n",
    "The addition of $\\alpha_f$ in the decay formula serves the purpose of setting\n",
    "the final value of the learning rate multiplier $\\alpha_t$ to $\\alpha_f$ at the\n",
    "end of training. Let's break down the equation step by step to understand why\n",
    "$\\alpha_f$ is added back.\n",
    "\n",
    "Given the formula:\n",
    "\n",
    "$$\n",
    "\\alpha_{t} = \\alpha_f + \\frac{1}{2}(1 - \\alpha_f)\\left(1 + \\cos \\left(\\tau_w\\pi\\right)\\right)\n",
    "$$\n",
    "\n",
    "First, consider the case where $\\tau_w = 0$ (i.e., the beginning of the decay\n",
    "phase):\n",
    "\n",
    "-   The cosine term becomes $\\cos(0) = 1$.\n",
    "-   The entire expression inside the parentheses becomes $1 + 1 = 2$.\n",
    "-   The scaling factor $\\frac{1}{2}(1 - \\alpha_f)$ then multiplies this by\n",
    "    $\\frac{1 - \\alpha_f}{2}$.\n",
    "-   So the expression becomes\n",
    "    $\\alpha_f + \\frac{1 - \\alpha_f}{2} \\times 2 = \\alpha_f + (1 - \\alpha_f) = 1$.\n",
    "\n",
    "So at $\\tau_w = 0$, $\\alpha_t$ starts at 1.\n",
    "\n",
    "Now consider the case where $\\tau_w = 1$ (i.e., the end of training):\n",
    "\n",
    "-   The cosine term becomes $\\cos(\\pi) = -1$.\n",
    "-   The entire expression inside the parentheses becomes $1 - 1 = 0$.\n",
    "-   The scaling factor then multiplies this by\n",
    "    $\\frac{1 - \\alpha_f}{2}\\times 0 = 0$.\n",
    "-   So the expression becomes $\\alpha_f + 0 = \\alpha_f$.\n",
    "\n",
    "So at $\\tau_w = 1$, $\\alpha_t$ decays to $\\alpha_f$.\n",
    "\n",
    "By adding $\\alpha_f$, you ensure that the learning rate multiplier starts at 1\n",
    "and smoothly decays to the desired final value $\\alpha_f$. Without adding\n",
    "$\\alpha_f$, the expression would start at 1 but decay to 0, rather than the\n",
    "intended final value. The addition of $\\alpha_f$ shifts the entire decay curve\n",
    "so that it aligns with the desired starting and ending values.\n",
    "\n",
    "#### Practical Intuition\n",
    "\n",
    "1. **Gradual Reduction**: The cosine decay provides a smooth and gradual\n",
    "   reduction in the learning rate. Unlike a step decay, where the learning rate\n",
    "   suddenly drops at specific intervals, the cosine decay reduces the learning\n",
    "   rate in a more continuous manner.\n",
    "\n",
    "2. **Avoiding Local Minima**: The smooth decay helps the optimization process\n",
    "   explore the loss landscape more thoroughly. It can avoid getting stuck in\n",
    "   local minima in the early stages of training by keeping the learning rate\n",
    "   relatively high. Then, as training progresses, the learning rate decreases,\n",
    "   allowing the optimizer to converge to a local minimum.\n",
    "\n",
    "3. **Flexibility**: By controlling parameters like $\\alpha_f$ (final learning\n",
    "   rate multiplier) and $t_{\\text{max}}$ (total duration of the scheduler), you\n",
    "   can fine-tune the decay behavior to suit your specific training scenario.\n",
    "\n",
    "#### Summary\n",
    "\n",
    "The cosine decay in the learning rate scheduler is designed to balance\n",
    "exploration and exploitation in the optimization process. It starts with a\n",
    "relatively high learning rate to explore the loss landscape and then gradually\n",
    "reduces it to allow for fine-tuning and convergence. The mathematical\n",
    "formulation is inspired by the properties of the cosine function, providing a\n",
    "smooth and controlled decay that can be tailored to different training needs.\n",
    "\n",
    "## PyTorch's CosAnnealingLR\n",
    "\n",
    "In PyTorch's\n",
    "[CosAnnealingLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR),\n",
    "they implemented the cosine annealing scheduler without warmup, but the base\n",
    "formula should be similar. Let's take a look to see how they coincide.\n",
    "\n",
    "1. PyTorchâ€™s `CosineAnnealing` without warmup:\n",
    "   <https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR>\n",
    "2. Composers `CosineAnnealing` without warmup:\n",
    "   <https://docs.mosaicml.com/projects/composer/en/stable/api_reference/generated/composer.optim.CosineAnnealingScheduler.html#composer.optim.CosineAnnealingScheduler>\n",
    "\n",
    "The formula looks a bit different at first glance (without loss of generality,\n",
    "we can ignore warmup here), after digging a bit deeper, I tried to establish the\n",
    "equivalence by setting `eta_min` of pytorch to `alpha_f x eta_max`,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\eta_{t} = \\eta_{\\min} + \\frac{1}{2}(\\eta_{\\max} - \\eta_{\\min})\\left(1 + \\cos\\left(\\frac{t}{t_{\\max}}\\pi\\right)\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where there is a new notation $\\eta_{\\min}$, which is the minimum learning rate.\n",
    "Furthermore, they do not have the $\\alpha_f$ term, which is the scaling factor\n",
    "that determines the final learning rate multiplier to decay to.\n",
    "\n",
    "To prove their equivalence, set $\\eta_{\\min} = \\alpha_f \\times \\eta_{\\max}$, and\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\eta_{t} & = \\eta_{\\min} + \\frac{1}{2}(\\eta_{\\max} - \\eta_{\\min})\\left(1 + \\cos\\left(\\frac{t}{t_{\\max}}\\pi\\right)\\right) \\\\\n",
    "         & = \\underbrace{\\alpha_f \\times \\eta_{\\max}}_{\\eta_{\\min}} + \\frac{1}{2}\\left(\\eta_{\\max} - \\underbrace{\\alpha_f \\times \\eta_{\\max}}_{\\eta_{\\min}}\\right)\\left(1 + \\cos\\left(\\frac{t}{t_{\\max}}\\pi\\right)\\right) \\\\\n",
    "         & = \\alpha_f \\times \\eta_{\\max} + \\frac{1}{2}\\left(1 - \\alpha_f\\right) \\eta_{\\max}\\left(1 + \\cos\\left(\\frac{t}{t_{\\max}}\\pi\\right)\\right) \\\\\n",
    "         & = \\eta_{\\max} \\left(\\alpha_f + \\frac{1}{2}\\left(1 - \\alpha_f\\right) \\left(1 + \\cos\\left(\\frac{t}{t_{\\max}}\\pi\\right)\\right)\\right) \\\\\n",
    "         & = \\eta_{\\max} \\underbrace{\\left(\\alpha_f + \\frac{1}{2}(1 - \\alpha_f)  \\left(1 + \\cos \\left(\\tau_w\\pi\\right)\\right)\\right)}_{\\alpha_t} \\\\\n",
    "         & = \\alpha_t \\times \\eta_{\\max}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Setting $\\eta_{\\min} = \\alpha_f \\times \\eta_{\\max}$ is also not an arbitrary\n",
    "choice. If we interpret $\\eta_{\\min}$ as the minimum learning rate, then it\n",
    "makes sense to set it to $\\alpha_f \\times \\eta_{\\max}$, since $\\alpha_f$ is the\n",
    "scaling factor that determines the final learning rate multiplier to decay to\n",
    "from $\\eta_{\\max}$. More concretely, if the initial learning rate\n",
    "$\\eta_{\\max} = 3e-4$ and $\\alpha_f = 0.1$, then the final learning rate will be\n",
    "$\\eta_{\\min} = 3e-4 \\times 0.1 = 3e-5$.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Now, let's write code to plot the graph of this scheduler, showing how the\n",
    "learning rate multiplier changes over the training steps.\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "import math\n",
    "from typing import Union, List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function for the learning rate scheduler\n",
    "def cosine_annealing_with_warmup(\n",
    "    t: int, t_warmup: Union[int, float], t_max: Union[int, float], alpha_f: float\n",
    ") -> float:\n",
    "    \"\"\"Computes the learning rate multiplier using cosine annealing with warmup.\n",
    "\n",
    "    Args:\n",
    "        t (int): Current training step.\n",
    "        t_warmup (int or float): Warmup time in training steps.\n",
    "        t_max (int or float): Total duration of the scheduler in training steps.\n",
    "        alpha_f (float): Learning rate multiplier to decay to.\n",
    "\n",
    "    Returns:\n",
    "        alpha (float): The learning rate multiplier at the given training step.\n",
    "    \"\"\"\n",
    "    if t < t_warmup:\n",
    "        alpha = t / t_warmup\n",
    "    else:\n",
    "        tau_w = (t - t_warmup) / t_max\n",
    "        tau_w = min(1.0, tau_w)\n",
    "        alpha = alpha_f + (1 - alpha_f) * (1 + math.cos(math.pi * tau_w)) / 2\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def plot_scheduler(\n",
    "    steps: np.ndarray,\n",
    "    lr_values: List[float],\n",
    "    t: int,\n",
    "    lr_at_t: Optional[float] = None,\n",
    "    save: bool = False,\n",
    ") -> None:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(steps, lr_values, label=\"Cosine Annealing with Warmup\")\n",
    "    plt.xlabel(\"Training Steps\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.title(\"Cosine Annealing with Warmup Scheduler\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if t is not None:\n",
    "        # Draw the red vertical line at the specified step t, with annotation\n",
    "        plt.axvline(x=t, color=\"r\", linestyle=\"--\", label=f\"Step {t}\")\n",
    "    if lr_at_t is not None:\n",
    "        plt.annotate(\n",
    "            f\"Step {t}, LR: {lr_at_t}\",\n",
    "            (t, lr_at_t),\n",
    "            xytext=(5, 5),\n",
    "            textcoords=\"offset points\",\n",
    "            color=\"r\",\n",
    "        )\n",
    "    if save:\n",
    "        plt.savefig(\"cosine_annealing_with_warmup.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Cosine Annealing with Warmup Learning Rate Scheduler\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--t_warmup\", type=int, default=100, help=\"Warmup time in training steps\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--t_max\",\n",
    "        type=int,\n",
    "        default=399998,\n",
    "        help=\"Duration of the scheduler in training steps\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--alpha_f\",\n",
    "        type=float,\n",
    "        default=0.1,\n",
    "        help=\"Learning rate multiplier to decay to\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eta_max\", type=float, default=1.6e-4, help=\"Initial learning rate\"\n",
    "    )\n",
    "    # optional argument\n",
    "    parser.add_argument(\"--t\", type=int, help=\"Training step to plot\")\n",
    "    parser.add_argument(\n",
    "        \"--save\", action=\"store_true\", help=\"Save the plot as a PNG file\"\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Steps\n",
    "    steps = np.arange(0, args.t_max + 1)\n",
    "\n",
    "    # Learning rate values\n",
    "    lr_values = [\n",
    "        args.eta_max\n",
    "        * cosine_annealing_with_warmup(t, args.t_warmup, args.t_max, args.alpha_f)\n",
    "        for t in steps\n",
    "    ]\n",
    "    if args.t is not None:\n",
    "        lr_at_t = lr_values[args.t]\n",
    "        print(f\"Learning rate at step {args.t}:\", lr_at_t)\n",
    "    else:\n",
    "        lr_at_t = None\n",
    "\n",
    "    # Call the plotting function\n",
    "    plot_scheduler(steps, lr_values, args.t, lr_at_t, args.save)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    # 692431388224 0.0001589897227106072 21450\n",
    "```\n",
    "\n",
    "## Visualization\n",
    "\n",
    "The plot visualizes the learning rate schedule defined by the\n",
    "`CosineAnnealingWithWarmupScheduler`. Here's what the graph illustrates:\n",
    "\n",
    "1. **Warmup Phase**: In the initial phase, the learning rate linearly increases\n",
    "   from 0 to the initial learning rate $\\eta\\_{\\max}$.\n",
    "\n",
    "2. **Cosine Decay Phase**: After the warmup phase, the learning rate follows a\n",
    "   cosine decay pattern, decreasing towards $\\eta\\_{\\min}$.\n",
    "\n",
    "## Summary\n",
    "\n",
    "This combination of initial warmup followed by cosine decay is a common practice\n",
    "in training neural networks. It allows the model to start learning slowly and\n",
    "then follow a smooth decay, which often leads to better convergence. The\n",
    "parameters $\\eta*i$, $\\alpha_f$, $t*{\\text{warmup}}$, and $\n",
    "t\\_{\\text{max}}$\n",
    "provide flexibility in configuring the learning rate schedule according to the\n",
    "specific needs of the training process.\n",
    "\n",
    "## Intuition\n",
    "\n",
    "Cosine annealing modifies the learning rate during training in a way that's\n",
    "inspired by the shape of the cosine function. Initially, the learning rate is\n",
    "set high, enabling rapid exploration of the parameter space. As training\n",
    "progresses, the learning rate is gradually reduced following a cosine curve.\n",
    "This helps the model to find more refined solutions by doing fine-tuning during\n",
    "later stages of training.\n",
    "\n",
    "The **intuition** behind cosine annealing involves two aspects:\n",
    "\n",
    "1. **Exploration vs. Exploitation**: In the initial epochs, a high learning rate\n",
    "   encourages exploring a broad region of the parameter space. As the learning\n",
    "   rate decreases, the optimizer starts to exploit local minima, fine-tuning the\n",
    "   weights.\n",
    "2. **Avoiding Saddle Points and Local Minima**: The cosine annealing schedule\n",
    "   also includes brief moments where the learning rate increases slightly, which\n",
    "   can provide the necessary \"nudge\" to help the model escape non-optimal\n",
    "   solutions.\n",
    "\n",
    "## References and Further Readings\n",
    "\n",
    "-   <https://d2l.ai/chapter_optimization/lr-scheduler.html#>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
