{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inefficient Processing of Large Datasets Using Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator, Iterable, List\n",
    "\n",
    "\n",
    "def process_large_dataset_inefficient(data: Iterable[int]) -> int:\n",
    "    processed: List[int] = [x * 2 for x in data if x > 0]\n",
    "    return sum(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 μs, sys: 0 ns, total: 1 μs\n",
      "Wall time: 3.1 μs\n",
      "9999999900000000\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "large_data = range(10**8)  # 10 million items\n",
    "result = process_large_dataset_inefficient(large_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   [Generator] Use generator instead of list to save memory.\n",
    "-   [Eager Evaluation] List is eager evaluation, means that it will evaluate the\n",
    "    entire list before returning. This implies the entire data structure (list)\n",
    "    is computed and stored in memory all at once.\n",
    "-   [Lazy Evaluation] Generator is lazy evaluation, it will evaluate the item on\n",
    "    the fly.\n",
    "\n",
    "The `process_large_dataset_inefficient` function is designed to process a large\n",
    "dataset by performing the following operations:\n",
    "\n",
    "1. **List Comprehension:** It creates a new list, `processed`, containing\n",
    "   elements from `data` that are greater than 0, each multiplied by 2.\n",
    "2. **Summation:** It then computes the sum of all elements in the `processed`\n",
    "   list.\n",
    "\n",
    "While this approach is straightforward and works well for smaller datasets, it\n",
    "becomes inefficient and potentially problematic when dealing with very large\n",
    "datasets due to the following reasons:\n",
    "\n",
    "-   **High Memory Consumption:** The list comprehension\n",
    "    `[x * 2 for x in data if x > 0]` generates an entire list in memory. For\n",
    "    large datasets, this can consume a significant amount of memory, leading to\n",
    "    increased memory usage or even memory exhaustion.\n",
    "\n",
    "-   **Unnecessary Intermediate Storage:** Storing all processed elements before\n",
    "    summing them is unnecessary when only the cumulative sum is required. This\n",
    "    intermediate storage adds overhead without providing any tangible benefits.\n",
    "\n",
    "-   **Lack of Lazy Evaluation:** The current implementation does not leverage\n",
    "    Python's ability to handle data lazily, which can process elements\n",
    "    on-the-fly without holding the entire dataset in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x110c1c040>\n",
      "<class 'generator'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "squared_gen: Generator[int, None, None] = (x**2 for x in range(10))\n",
    "print(squared_gen)\n",
    "print(type(squared_gen))\n",
    "print(isinstance(squared_gen, Generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_dataset_efficient(data: Iterable[int]) -> int:\n",
    "    processed: Generator[int, None, None] = (x * 2 for x in data if x > 0)\n",
    "    return sum(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   **Generator Expression:** Replaced the list comprehension with a generator\n",
    "    expression: `(x * 2 for x in data if x > 0)`. This change ensures that\n",
    "    elements are processed one at a time, reducing memory footprint.\n",
    "\n",
    "-   **Elimination of Intermediate List:** Removed the `processed` list, thereby\n",
    "    avoiding the storage of all processed elements in memory.\n",
    "\n",
    "-   **Documentation:** Added a docstring to explain the purpose and behavior of\n",
    "    the function, enhancing code readability and maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 μs, sys: 1e+03 ns, total: 2 μs\n",
      "Wall time: 6.91 μs\n",
      "9999999900000000\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "result = process_large_dataset_efficient(large_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Complexity\n",
    "\n",
    "-   _Question:_ What is the time complexity of the original function\n",
    "    compared to the refactored version?\n",
    "-   _Answer:_ Both functions have $\\mathcal{O}(n)$ time complexity, where `n` is\n",
    "    the number of elements in `data`. This is because each function iterates\n",
    "    through the entire dataset once to process and sum the elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space Complexity\n",
    "\n",
    "-   _Question:_ What is the space complexity of the original function\n",
    "    compared to the refactored version?\n",
    "-   _Answer:_ The original function has $\\mathcal{O}(n)$ space complexity due to the\n",
    "    creation of the `processed` list, where `n` is the number of elements in\n",
    "    `data` that satisfy the condition `x > 0`. The refactored version using\n",
    "    a generator expression has $\\mathcal{O}(1)$ space complexity, as it\n",
    "    processes one element at a time without storing the entire list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Potential Interview Questions:**\n",
    "\n",
    "1. **Identify the Inefficiency:**\n",
    "\n",
    "    - What is inefficient about the `process_large_dataset_inefficient` function\n",
    "      when handling large datasets?\n",
    "\n",
    "2. **Impact of the Inefficiency:**\n",
    "\n",
    "    - How does the current implementation affect memory usage and performance\n",
    "      with large inputs?\n",
    "\n",
    "3. **Refactoring for Efficiency:**\n",
    "\n",
    "    - How would you refactor the `process_large_dataset_inefficient` function to\n",
    "      handle large datasets more efficiently?\n",
    "\n",
    "4. **Advantages of the Refactored Approach:**\n",
    "\n",
    "    - What are the benefits of your proposed changes in terms of memory usage\n",
    "      and performance?\n",
    "\n",
    "5. **Trade-offs and Considerations:**\n",
    "    - Are there any trade-offs or considerations to keep in mind when modifying\n",
    "      the function for better efficiency?\n",
    "\n",
    "## **Answers to the Questions:**\n",
    "\n",
    "1. **Identify the Inefficiency:**\n",
    "\n",
    "    - **Answer:** The inefficiency lies in the use of a list comprehension to\n",
    "      create the entire `processed` list in memory before summing its elements.\n",
    "      For large datasets, this results in high memory consumption and can lead\n",
    "      to performance degradation or memory errors.\n",
    "\n",
    "2. **Impact of the Inefficiency:**\n",
    "\n",
    "    - **Answer:** The current implementation's memory usage scales linearly with\n",
    "      the size of the input dataset because it stores all processed elements in\n",
    "      a list. This can cause the program to use excessive memory, slow down due\n",
    "      to memory swapping, or even crash if the system runs out of memory when\n",
    "      dealing with very large datasets.\n",
    "\n",
    "3. **Refactoring for Efficiency:**\n",
    "\n",
    "    - **Answer:** To improve efficiency, the function can be refactored to use\n",
    "      generator expressions instead of list comprehensions. Generator\n",
    "      expressions allow for lazy evaluation, processing one element at a time\n",
    "      without storing the entire list in memory. Here's the refactored function:\n",
    "\n",
    "        ```python\n",
    "        from typing import Iterable\n",
    "\n",
    "        def process_large_dataset_efficient(data: Iterable[int]) -> int:\n",
    "            return sum(x * 2 for x in data if x > 0)\n",
    "\n",
    "        # Example usage:\n",
    "        large_data = range(1, 10000000)\n",
    "        print(process_large_dataset_efficient(large_data))\n",
    "        ```\n",
    "\n",
    "4. **Advantages of the Refactored Approach:**\n",
    "\n",
    "    - **Answer:** The refactored function significantly reduces memory usage by\n",
    "      eliminating the need to store all processed elements simultaneously.\n",
    "      Instead, it processes each element one at a time, allowing the program to\n",
    "      handle much larger datasets without exhausting system memory.\n",
    "      Additionally, it can lead to performance improvements due to reduced\n",
    "      memory overhead.\n",
    "\n",
    "5. **Trade-offs and Considerations:**\n",
    "    - **Answer:** While generator expressions are more memory-efficient, they\n",
    "      can be slightly slower in scenarios where the entire list is needed\n",
    "      multiple times because generators are single-iteration iterables. However,\n",
    "      in this specific case, since the processed data is only needed for\n",
    "      summation, using a generator is ideal. Another consideration is code\n",
    "      readability; some developers might find list comprehensions more readable,\n",
    "      but in cases where memory efficiency is crucial, generator expressions are\n",
    "      preferable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Function: Time = 3.84s, Max Memory = 2805.09MB\n",
      "Refactored Function: Time = 3.05s, Max Memory = 24.95MB\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "\n",
    "def benchmark() -> None:\n",
    "    data = range(10**8)\n",
    "\n",
    "    def run_inefficient() -> int:\n",
    "        return process_large_dataset_inefficient(data)\n",
    "\n",
    "    def run_efficient() -> int:\n",
    "        return process_large_dataset_efficient(data)\n",
    "\n",
    "    mem_inefficient = max(memory_usage(run_inefficient))\n",
    "    time_inefficient = timeit.timeit(run_inefficient, number=1)\n",
    "\n",
    "    mem_efficient = max(memory_usage(run_efficient))\n",
    "    time_efficient = timeit.timeit(run_efficient, number=1)\n",
    "\n",
    "    print(\n",
    "        f\"Original Function: Time = {time_inefficient:.2f}s, Max Memory = {mem_inefficient:.2f}MB\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Refactored Function: Time = {time_efficient:.2f}s, Max Memory = {mem_efficient:.2f}MB\"\n",
    "    )\n",
    "\n",
    "\n",
    "benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More accurate profiling, run in python script instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling inefficient function:\n",
      "ERROR: Could not find file /var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_16711/1902328925.py\n",
      "Result: 9999999900000000\n",
      "         83 function calls in 20.019 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000   20.019   20.019 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:1185(wrapper)\n",
      "        1    0.000    0.000   20.019   20.019 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:759(f)\n",
      "        1    0.630    0.630   20.018   20.018 /var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_16711/1902328925.py:19(run_inefficient)\n",
      "        1    0.001    0.001   19.388   19.388 /var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_16711/1902328925.py:10(process_large_dataset_inefficient)\n",
      "        1   17.807   17.807   17.807   17.807 /var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_16711/1902328925.py:11(<listcomp>)\n",
      "        1    1.580    1.580    1.580    1.580 {built-in method builtins.sum}\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:713(__call__)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:728(add_function)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:645(add)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen genericpath>:16(exists)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method posix.stat}\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/contextlib.py:141(__exit__)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
      "        2    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:740(_count_ctxmgr)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n",
      "        2    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/ipykernel/iostream.py:655(write)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:702(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:782(disable_by_count)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:853(show_results)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:748(wrap_function)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/asyncio/coroutines.py:21(iscoroutinefunction)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/inspect.py:409(iscoroutinefunction)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:849(disable)\n",
      "        2    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/ipykernel/iostream.py:505(parent_header)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/contextlib.py:132(__enter__)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:1201(choose_backend)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:689(items)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/inspect.py:391(_has_code_flag)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:775(enable_by_count)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/contextlib.py:299(helper)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:842(enable)\n",
      "        2    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/ipykernel/iostream.py:550(_is_master_process)\n",
      "        6    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:1215(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/contextlib.py:104(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/inspect.py:300(ismethod)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:640(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method sys.settrace}\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/inspect.py:378(isfunction)\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/functools.py:421(_unwrap_partial)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'get' of '_contextvars.ContextVar' objects}\n",
      "        2    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/ipykernel/iostream.py:577(_schedule_flush)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method sys.gettrace}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Profiling efficient function:\n",
      "ERROR: Could not find file /var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_16711/1902328925.py\n",
      "Result: 9999999900000000\n",
      "         100000082 function calls in 46.804 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000   46.804   46.804 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:1185(wrapper)\n",
      "        1    0.000    0.000   46.803   46.803 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:759(f)\n",
      "        1    0.000    0.000   46.803   46.803 /var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_16711/1902328925.py:25(run_efficient)\n",
      "        1    0.000    0.000   46.803   46.803 /var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_16711/1902328925.py:15(process_large_dataset_efficient)\n",
      "        1   15.457   15.457   46.803   46.803 {built-in method builtins.sum}\n",
      "100000000   31.346    0.000   31.346    0.000 /var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_16711/1902328925.py:16(<genexpr>)\n",
      "        1    0.000    0.000    0.001    0.001 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:713(__call__)\n",
      "        1    0.000    0.000    0.001    0.001 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:728(add_function)\n",
      "        1    0.000    0.000    0.001    0.001 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:645(add)\n",
      "        1    0.000    0.000    0.001    0.001 <frozen genericpath>:16(exists)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method posix.stat}\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:748(wrap_function)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:702(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/asyncio/coroutines.py:21(iscoroutinefunction)\n",
      "        2    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/ipykernel/iostream.py:655(write)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:1201(choose_backend)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/contextlib.py:141(__exit__)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/inspect.py:409(iscoroutinefunction)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/inspect.py:391(_has_code_flag)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/contextlib.py:299(helper)\n",
      "        2    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:740(_count_ctxmgr)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/contextlib.py:104(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:782(disable_by_count)\n",
      "        2    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/ipykernel/iostream.py:550(_is_master_process)\n",
      "        2    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/ipykernel/iostream.py:505(parent_header)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:853(show_results)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/contextlib.py:132(__enter__)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:849(disable)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/inspect.py:378(isfunction)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:775(enable_by_count)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/inspect.py:300(ismethod)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:842(enable)\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:689(items)\n",
      "        6    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:1215(<genexpr>)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method sys.settrace}\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/memory_profiler.py:640(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "        1    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/functools.py:421(_unwrap_partial)\n",
      "        4    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'endswith' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 /opt/homebrew/Caskroom/miniconda/base/envs/cfs/lib/python3.11/site-packages/ipykernel/iostream.py:577(_schedule_flush)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'get' of '_contextvars.ContextVar' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method sys.gettrace}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "import io\n",
    "import pstats\n",
    "from typing import Iterable\n",
    "\n",
    "from memory_profiler import profile\n",
    "\n",
    "\n",
    "def process_large_dataset_inefficient(data: Iterable[int]) -> int:\n",
    "    processed = [x * 2 for x in data if x > 0]\n",
    "    return sum(processed)\n",
    "\n",
    "\n",
    "def process_large_dataset_efficient(data: Iterable[int]) -> int:\n",
    "    processed = (x * 2 for x in data if x > 0)\n",
    "    return sum(processed)\n",
    "\n",
    "\n",
    "@profile\n",
    "def run_inefficient(data_size: int) -> int:\n",
    "    data = range(data_size)\n",
    "    return process_large_dataset_inefficient(data)\n",
    "\n",
    "\n",
    "@profile\n",
    "def run_efficient(data_size: int) -> int:\n",
    "    data = range(data_size)\n",
    "    return process_large_dataset_efficient(data)\n",
    "\n",
    "\n",
    "def profile_function(func, data_size: int) -> None:\n",
    "    pr = cProfile.Profile()\n",
    "    pr.enable()\n",
    "    result = func(data_size)\n",
    "    pr.disable()\n",
    "\n",
    "    s = io.StringIO()\n",
    "    ps = pstats.Stats(pr, stream=s).sort_stats(\"cumulative\")\n",
    "    ps.print_stats()\n",
    "\n",
    "    print(f\"Result: {result}\")\n",
    "    print(s.getvalue())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_size = 10**8  # Adjust as needed\n",
    "\n",
    "    print(\"Profiling inefficient function:\")\n",
    "    profile_function(run_inefficient, data_size)\n",
    "\n",
    "    print(\"\\nProfiling efficient function:\")\n",
    "    profile_function(run_efficient, data_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
