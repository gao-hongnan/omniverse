{"cells":[{"cell_type":"markdown","metadata":{"id":"jtwTIjo7oOuB"},"source":["# Encoder-only transformer model for AG News classification"]},{"cell_type":"markdown","metadata":{},"source":["# TODO\n","\n","1. Add Positional Encoding\n","2. Add LR Scheduler\n","3. Check why need to use `torch.nn.utils.clip_grad_norm_` to clip gradients\n","4. Why unsqueeze mask?\n","5. Can you init weights inside Encoder instead of outside?\n","\n","https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"o5eW3azllzRd"},"source":["In this notebook, I train a encoder-only transformer to do text classification on the AG_NEWS dataset.\n","Text classification seems to be a pretty simple task, and using transformer is probably overkill. But this is my first time implementing the transformer structure from scratch (including the self-attention module), and it was fun :-)"]},{"cell_type":"code","execution_count":346,"metadata":{"executionInfo":{"elapsed":30613,"status":"ok","timestamp":1697792166823,"user":{"displayName":"EE E","userId":"05397898254477422912"},"user_tz":-480},"id":"Mtq3abS2lL_i"},"outputs":[],"source":["# # some commands in th is notebook require torchtext 0.12.0\n","# !pip install torch --upgrade --quiet\n","# !pip install torchtext --upgrade --quiet\n","# !pip install torchdata --quiet\n","# !pip install torchinfo --quiet\n","# !pip install portalocker --quiet"]},{"cell_type":"code","execution_count":347,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4473,"status":"ok","timestamp":1697792171292,"user":{"displayName":"EE E","userId":"05397898254477422912"},"user_tz":-480},"id":"bQEBuaIm5s9R","outputId":"b35e59c0-6b7d-434c-ce1e-72197d01a5ad"},"outputs":[],"source":["import collections\n","import math\n","from dataclasses import dataclass\n","from rich.pretty import pprint\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as functional\n","import torchdata\n","import torchinfo\n","import torchtext\n","from torch.utils.data import DataLoader\n","from torchtext.datasets import AG_NEWS\n","from tqdm import tqdm\n","import os\n","import time\n","import random\n","\n","from typing import Optional"]},{"cell_type":"code","execution_count":348,"metadata":{},"outputs":[],"source":["def seed_all(seed: Optional[int] = 1992, seed_torch: bool = True) -> int:\n","    \"\"\"\n","    Seed all random number generators.\n","\n","    Parameters\n","    ----------\n","    seed : int, optional\n","        Seed number to be used, by default 1992.\n","    seed_torch : bool, optional\n","        Whether to seed PyTorch or not, by default True.\n","    \"\"\"\n","    # fmt: off\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)        # set PYTHONHASHSEED env var at fixed value\n","    np.random.seed(seed)                            # numpy pseudo-random generator\n","    random.seed(seed)                               # python's built-in pseudo-random generator\n","\n","    if seed_torch:\n","        torch.manual_seed(seed)\n","        # torch.manual_seed may call manual_seed_all but calling it again here\n","        # to make sure it gets called at least once\n","        torch.cuda.manual_seed_all(seed)             # pytorch (both CPU and CUDA)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","        torch.backends.cudnn.enabled = False\n","    # fmt: on\n","    return seed"]},{"cell_type":"code","execution_count":349,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["seed_all(42, seed_torch=True)\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"SmM0VcYnCC8M"},"source":["## Data processing"]},{"cell_type":"code","execution_count":350,"metadata":{"executionInfo":{"elapsed":4259,"status":"ok","timestamp":1697792175546,"user":{"displayName":"EE E","userId":"05397898254477422912"},"user_tz":-480},"id":"7UdYsN6J5uke"},"outputs":[],"source":["# One can easily modify the data processing part of this code to accommodate for   other datasets for text classification listed in https://pytorch.org/text/stable/datasets.html#text-classification\n","train_iter, test_iter = AG_NEWS()\n","\n","num_classes = len(set([label for (label, text) in train_iter]))\n","tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"]},{"cell_type":"code","execution_count":351,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1697792175547,"user":{"displayName":"EE E","userId":"05397898254477422912"},"user_tz":-480},"id":"HchVgEXWlqLz","outputId":"b9810607-b8ea-4540-b883-1b1b5a6868ad"},"outputs":[{"data":{"text/plain":["(3,\n"," \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"]},"execution_count":351,"metadata":{},"output_type":"execute_result"}],"source":["# see an example of the dateset\n","next(iter(train_iter))"]},{"cell_type":"code","execution_count":352,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10875,"status":"ok","timestamp":1697792186415,"user":{"displayName":"EE E","userId":"05397898254477422912"},"user_tz":-480},"id":"6DeTWUptkYnG","outputId":"f1fa36f6-96af-4c66-c393-0e7f2c487393","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# fmt: off\n","# convert the labels to be in range(0, num_classes)\n","y_train       = torch.tensor([label - 1 for (label, text) in train_iter])\n","y_test        = torch.tensor([label - 1 for (label, text) in test_iter])\n","\n","# There are many \"\\\\\" in the texts in the AG_news dataset, we get rid of them.\n","train_iter    = ((label, text.replace(\"\\\\\", \" \")) for label, text in train_iter)\n","test_iter     = ((label, text.replace(\"\\\\\", \" \")) for label, text in test_iter)\n","\n","# tokenize the texts, and truncate the number of words in each text to max_seq_len\n","max_seq_len   = 100\n","x_train_texts = [tokenizer(text.lower())[0:max_seq_len] for (label, text) in train_iter]\n","x_test_texts  = [tokenizer(text.lower())[0:max_seq_len] for (label, text) in test_iter]\n","# fmt: on"]},{"cell_type":"code","execution_count":353,"metadata":{"executionInfo":{"elapsed":1498,"status":"ok","timestamp":1697792187909,"user":{"displayName":"EE E","userId":"05397898254477422912"},"user_tz":-480},"id":"MYVE8HSGkYnH","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# build the vocabulary and word-to-integer map\n","counter = collections.Counter()\n","for text in x_train_texts:\n","    counter.update(text)\n","\n","vocab_size = 15000\n","most_common_words = np.array(counter.most_common(vocab_size - 2))\n","vocab = most_common_words[:,0]\n","\n","# indexes for the padding token, and unknown tokens\n","PAD = 0\n","UNK = 1\n","word_to_id = {vocab[i]: i + 2 for i in range(len(vocab))}"]},{"cell_type":"code","execution_count":354,"metadata":{},"outputs":[{"data":{"text/plain":["14998"]},"execution_count":354,"metadata":{},"output_type":"execute_result"}],"source":["len(vocab)"]},{"cell_type":"code","execution_count":355,"metadata":{"executionInfo":{"elapsed":2726,"status":"ok","timestamp":1697792190633,"user":{"displayName":"EE E","userId":"05397898254477422912"},"user_tz":-480},"id":"I7-4KQI8kYnH","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# map the words in the training and test texts to integers\n","x_train = [\n","    torch.tensor([word_to_id.get(word, UNK) for word in text]) for text in x_train_texts\n","]\n","x_test = [\n","    torch.tensor([word_to_id.get(word, UNK) for word in text]) for text in x_test_texts\n","]\n","x_test = torch.nn.utils.rnn.pad_sequence(x_test, batch_first=True, padding_value=PAD)\n"]},{"cell_type":"code","execution_count":356,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1697792190633,"user":{"displayName":"EE E","userId":"05397898254477422912"},"user_tz":-480},"id":"hJe8LAUNkYnI","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# constructing the dataset in order to be compatible with torch.utils.data.Dataloader\n","class AGNewsDataset:\n","    def __init__(self, features, labels):\n","        self.features = features\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, item):\n","        return self.features[item], self.labels[item]\n","\n","\n","train_dataset = AGNewsDataset(x_train, y_train)\n","test_dataset  = AGNewsDataset(x_test, y_test)"]},{"cell_type":"code","execution_count":357,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1697792190634,"user":{"displayName":"EE E","userId":"05397898254477422912"},"user_tz":-480},"id":"ov3tX4sRkYnI","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# collate_fn to be used in torch.utils.data.DataLoader().\n","# It pads the texts in each batch such that they have the same sequence length.\n","def pad_sequence(batch):\n","    texts = [text for text, label in batch]\n","    labels = torch.tensor([label for text, label in batch])\n","    texts_padded = torch.nn.utils.rnn.pad_sequence(\n","        texts, batch_first=True, padding_value=PAD\n","    )\n","    return texts_padded, labels\n","\n","\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=128, shuffle=True, collate_fn=pad_sequence\n",")\n","test_loader = torch.utils.data.DataLoader(\n","    test_dataset, batch_size=128, shuffle=True, collate_fn=pad_sequence\n",")\n"]},{"cell_type":"code","execution_count":358,"metadata":{},"outputs":[],"source":["# # test loader\n","# counter = 0\n","# for batch in train_loader:\n","#     print(batch[0].shape)\n","#     print(batch[1].shape)\n","#     counter += 1\n","#     if counter == 5:\n","#         break\n"]},{"cell_type":"markdown","metadata":{"id":"bj_u800uTHEs"},"source":["# my"]},{"cell_type":"code","execution_count":359,"metadata":{"executionInfo":{"elapsed":485,"status":"ok","timestamp":1697792194387,"user":{"displayName":"EE E","userId":"05397898254477422912"},"user_tz":-480},"id":"-OgYaw-kTH0h"},"outputs":[],"source":["import copy\n","import math\n","import unittest\n","from abc import ABC, abstractmethod\n","from typing import Optional, Tuple\n","\n","import numpy as np\n","import rich\n","import torch\n","# from d2l import torch as d2l\n","from rich.pretty import pprint\n","from torch import nn\n","from dataclasses import dataclass\n","\n","# from src.utils.reproducibility import seed_all\n","\n","\n","class Attention(ABC, nn.Module):\n","    def __init__(self, dropout: float = 0.0) -> None:\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout, inplace=False)\n","\n","    @abstractmethod\n","    def forward(\n","        self,\n","        query: torch.Tensor,\n","        key: torch.Tensor,\n","        value: torch.Tensor,\n","        mask: Optional[torch.Tensor] = None,\n","    ) -> torch.Tensor:\n","        ...\n","\n","\n","class ScaledDotProductAttention(Attention):\n","    \"\"\"\n","    Scaled Dot-Product Attention Class.\n","\n","    This class performs scaled dot-product attention following the equations:\n","\n","    .. math::\n","        \\\\text{Attention}(Q, K, V) = \\\\text{softmax} \\\\left( \\\\frac{QK^T}{\\\\sqrt{d_k}} \\\\right) V\n","\n","    Inherits from Attention class.\n","\n","    Methods\n","    -------\n","    forward(query, key, value, mask)\n","        Forward pass for scaled dot-product attention.\n","    \"\"\"\n","\n","    def forward(\n","        self,\n","        query: torch.Tensor,\n","        key: torch.Tensor,\n","        value: torch.Tensor,\n","        mask: Optional[torch.BoolTensor] = None,\n","    ) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"\n","        Perform the forward pass for scaled dot-product attention.\n","\n","        This function applies the attention mechanism on the input tensors\n","        `query`, `key`, and `value`. It's worth noting that for cross-attention,\n","        the sequence lengths of `query` and `key`/`value` may differ.\n","        This is because :math:`query` is usually projected from the decoder's states,\n","        while :math:`key` and :math:`value` are from the encoder's states.\n","\n","        Notations:\n","\n","        - :math:`D` : Embedding dimension\n","        - :math:`d_k`: Dimension of the keys, queries, and values\n","        - :math:`N`: Batch size\n","        - :math:`T`: Sequence length for `query`\n","        - :math:`S`: Sequence length for `key` and `value`\n","\n","        NOTE: We use :math:`L` in our notes instead of :math:`T` and :math:`S`\n","        since we assume all query, key and value are of same length.\n","\n","        Parameters\n","        ----------\n","        query : torch.Tensor\n","            Tensor containing query vectors for each sequence.\n","            Shape: :math:`(N, T, d_k)`.\n","        key : torch.Tensor\n","            Tensor containing key vectors for each sequence.\n","            Shape: :math:`(N, S, d_k)`.\n","        value : torch.Tensor\n","            Tensor containing value vectors for each sequence.\n","            Shape: :math:`(N, S, d_k)`.\n","        mask : torch.BoolTensor, optional\n","            Optional mask tensor. Used for padding and future masking.\n","            Shape could be :math:`(N, S)` or :math:`(T, T)`, depending on the type\n","            of attention (self-attention or cross-attention).\n","\n","        Returns\n","        -------\n","        Tuple[torch.Tensor, torch.Tensor]\n","            The context vectors and the attention weights. The context vectors are the weighted sum\n","            of the `value` vectors, representing the information to be attended to.\n","            The attention weights represent the attention probabilities.\n","\n","            - Context Vectors shape: :math:`(N, T, d_k)`\n","            - Attention Weights shape: :math:`(N, T, S)`\n","        \"\"\"\n","\n","        # 1. Find the embedding dimension D or d_q from the query feature vector.\n","        #    Q = Z @ W_q \\in R^{L x D}\n","        #    Q_h = Q @ W_q^h \\in R^{L x d_q}\n","        d_q = query.size(dim=-1)\n","\n","        # 2. Compute the dot product of the query feature vector with the key feature vector.\n","        #    Note since key is of dim (batch_size, L, d_k) so we operate the\n","        #    transpose on the last two dimensions, specified by dim0 and dim1.\n","        #    key.transpose(dim0=-2, dim1=-1) means let the second last dimension\n","        #    be the last dimension and let the last dimension be the second last dimension.\n","        # fmt: off\n","        attention_scores = torch.matmul(query, key.transpose(dim0=-2, dim1=-1)) / math.sqrt(d_q)\n","        # fmt: on\n","        torch.testing.assert_close(\n","            attention_scores,\n","            torch.matmul(query, key.transpose(dim0=-2, dim1=-1)) / math.sqrt(d_q),\n","            msg=\"attention scores from bmm and matmul should be the same.\",\n","        )\n","\n","        # 3. Apply mask to the scores if mask is not None.\n","        if mask is not None:\n","            # TODO: give example of shape of mask\n","            #print(f\"mask.shape: {mask.shape}\")\n","            mask = mask.squeeze(2)\n","            attention_scores = attention_scores.masked_fill(mask, float(\"-inf\"))\n","\n","        # 4. Apply softmax to the attention scores to obtain attention weights and context vectors.\n","        attention_weights = attention_scores.softmax(dim=-1)\n","        attention_weights = self.dropout(attention_weights)\n","\n","        context_vector = torch.matmul(attention_weights, value)\n","        torch.testing.assert_close(\n","            context_vector,\n","            torch.matmul(attention_weights, value),\n","            msg=\"context vector from bmm and matmul should be the same.\",\n","        )\n","        return context_vector, attention_weights\n","\n","\n","@dataclass\n","class ModelConfig:\n","    attention: Attention\n","    num_layers: int\n","    vocab_size: int\n","    H: int\n","    d_model: int\n","    d_ff: int\n","    dropout: float\n","    max_seq_len: int\n","    bias: bool = True\n","\n","\n","class MultiHeadedAttention(nn.Module):\n","    __slots__ = [\n","        \"d_model\",\n","        \"d_k\",\n","        \"d_q\",\n","        \"d_v\",\n","        \"H\",\n","        \"W_Q\",\n","        \"W_K\",\n","        \"W_V\",\n","        \"W_O\",\n","        \"attention\",\n","        \"dropout\",\n","    ]\n","\n","    def __init__(\n","        self,\n","        attention: Attention,\n","        H: int,\n","        d_model: int,\n","        dropout: float = 0.1,\n","        bias: bool = False,\n","    ) -> None:\n","        super().__init__()\n","        assert d_model % H == 0\n","\n","        # fmt: off\n","        self.d_model   = d_model       # D\n","        self.d_k       = d_model // H  # stay true to notations\n","        self.d_q       = d_model // H\n","        self.d_v       = d_model // H\n","\n","        self.H         = H             # number of heads\n","\n","        # shadow my notations, actually they are of shape D x D.\n","        self.W_Q       = nn.Linear(self.d_model, self.d_q * self.H, bias=bias)  # D x D\n","        self.W_K       = nn.Linear(self.d_model, self.d_k * self.H, bias=bias)\n","        self.W_V       = nn.Linear(self.d_model, self.d_v * self.H, bias=bias)\n","        self.W_O       = nn.Linear(self.d_model, self.d_model, bias=bias)\n","\n","        self.attention = attention\n","        self.dropout   = nn.Dropout(p=dropout, inplace=False)\n","        # fmt: on\n","\n","    def forward(\n","        self,\n","        query: torch.Tensor,\n","        key: torch.Tensor,\n","        value: torch.Tensor,\n","        mask: Optional[torch.BoolTensor] = None,\n","    ) -> torch.Tensor:\n","\n","        if mask is not None:\n","            mask = mask.unsqueeze(1)\n","\n","        # fmt: off\n","        Q = self.W_Q(query).contiguous() # Z @ W_Q -> LxD @ DxD = LxD\n","        K = self.W_K(key).contiguous()   # Z @ W_K\n","        V = self.W_V(value).contiguous() # Z @ W_V\n","\n","\n","        Q = self.transpose_qkv(Q)        # [B, H, L, D]\n","        K = self.transpose_qkv(K)\n","        V = self.transpose_qkv(V)\n","\n","        # Attention\n","        # same as the other code: x = torch.matmul(p_atten, value)\n","        context_vector, attention_weights = self.attention(Q, K, V, mask)\n","        context_vector_concat = self.reverse_transpose_qkv(context_vector)\n","\n","        # fmt: on\n","        return self.W_O(context_vector_concat)\n","\n","    def transpose_qkv(self, q_or_k_or_v: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Transposition for parallel computation of multiple attention heads.\n","        TODO: Why does transpose allow parallel computation?\n","        \"\"\"\n","        # fmt: off\n","        # 1. q_or_k_or_v is shape (B, L, D)\n","        # 2. aim to make it of shape (B, L, H, D / H = d_qkv)\n","        batch_size, seq_len, _ = q_or_k_or_v.shape\n","        q_or_k_or_v            = q_or_k_or_v.view(batch_size, seq_len, self.H, self.d_model // self.H)\n","\n","        # 3. switch H from 3rd to 2nd dimension, or in python swap 2nd to 1st\n","        q_or_k_or_v            = q_or_k_or_v.permute(0, 2, 1, 3)\n","        # fmt: on\n","        return q_or_k_or_v\n","\n","    def reverse_transpose_qkv(self, q_or_k_or_v: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Reverse the transposition operation for concatenating multiple attention heads.\"\"\"\n","        # fmt: off\n","        # 1. q_or_k_or_v is shape (B, H, L, D / H = d_qkv)\n","        # 2. aim to make it of shape (B, L, H, D / H = d_qkv)\n","        q_or_k_or_v = q_or_k_or_v.permute(0, 2, 1, 3)\n","\n","        # 3. Merge H and d_qkv into D\n","        batch_size, seq_len, _, _ = q_or_k_or_v.shape\n","        q_or_k_or_v = q_or_k_or_v.contiguous().view(batch_size, seq_len, self.d_model)\n","        # fmt: on\n","        return q_or_k_or_v\n","\n","\n","class ResidualConnection(nn.Module):\n","    \"\"\"residual connection: x + dropout(sublayer(layernorm(x)))\"\"\"\n","\n","    def __init__(self, d_model, dropout):\n","        super().__init__()\n","        self.drop = nn.Dropout(dropout)\n","        self.norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, x, sublayer):\n","        return x + self.drop(sublayer(self.norm(x)))\n","\n","\n","# I simply let the model learn the positional embeddings in this notebook, since this\n","# almost produces identital results as using sin/cosin functions embeddings, as claimed\n","# in the original transformer paper. Note also that in the original paper, they multiplied\n","# the token embeddings by a factor of sqrt(d_embed), which I do not do here.\n","\n","\n","class Encoder(nn.Module):\n","    \"\"\"Encoder = token embedding + positional embedding -> a stack of N EncoderBlock -> layer norm\"\"\"\n","\n","    def __init__(self, config: ModelConfig):\n","        super().__init__()\n","        self.d_model = config.d_model\n","        self.tok_embed = nn.Embedding(config.vocab_size, config.d_model)\n","        self.pos_embed = nn.Parameter(\n","            torch.zeros(1, config.max_seq_len, config.d_model)\n","        )\n","        self.encoder_blocks = nn.ModuleList(\n","            [EncoderBlock(config) for _ in range(config.num_layers)]\n","        )\n","        self.dropout = nn.Dropout(config.dropout)\n","        self.norm = nn.LayerNorm(config.d_model)\n","\n","        # self._reset_parameters()\n","\n","    def _reset_parameters(self):\n","        for p in self.parameters():\n","            if p.dim() > 1:\n","                torch.nn.init.xavier_uniform_(p)\n","\n","    def forward(self, input, mask=None):\n","        x = self.tok_embed(input)\n","        x_pos = self.pos_embed[:, : x.size(1), :]\n","        x = self.dropout(x + x_pos)\n","        for layer in self.encoder_blocks:\n","            x = layer(x, mask)\n","        return self.norm(x)\n","\n","\n","class PositionWiseFFN(nn.Module):\n","    \"\"\"The positionwise feed-forward network.\"\"\"\n","\n","    pass\n","\n","\n","class EncoderBlock(nn.Module):\n","    \"\"\"EncoderBlock: self-attention -> position-wise fully connected feed-forward layer\"\"\"\n","\n","    def __init__(self, config: ModelConfig):\n","        super().__init__()\n","        self.mha = MultiHeadedAttention(\n","            config.attention, config.H, config.d_model, config.dropout, config.bias\n","        )\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(config.d_model, config.d_ff),\n","            nn.ReLU(),\n","            nn.Dropout(config.dropout),\n","            nn.Linear(config.d_ff, config.d_model),\n","        )\n","        self.residual1 = ResidualConnection(config.d_model, config.dropout)\n","        self.residual2 = ResidualConnection(config.d_model, config.dropout)\n","\n","    def forward(self, x, mask=None):\n","        # self-attention\n","        x = self.residual1(x, lambda x: self.mha(x, x, x, mask=mask))\n","        # position-wise fully connected feed-forward layer\n","        return self.residual2(x, self.feed_forward)\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(self, config: ModelConfig, num_classes):\n","        super().__init__()\n","        self.encoder = Encoder(config)\n","        self.linear = nn.Linear(config.d_model, num_classes)\n","\n","    def forward(self, x, pad_mask=None):\n","        x = self.encoder(x, pad_mask)\n","        return self.linear(torch.mean(x, -2))\n"]},{"cell_type":"markdown","metadata":{"id":"zNVoCKz0CM3g"},"source":["## Building the encoder-only transformer model for text classification"]},{"cell_type":"code","execution_count":360,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1697792349760,"user":{"displayName":"EE E","userId":"05397898254477422912"},"user_tz":-480},"id":"c4Bbgt4GTpv1"},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ModelConfig</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">attention</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScaledDotProductAttention</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"font-weight: bold\">)</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">num_layers</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">vocab_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15000</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">H</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">d_model</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">d_ff</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">dropout</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">max_seq_len</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n","<span style=\"font-weight: bold\">)</span>\n","</pre>\n"],"text/plain":["\u001b[1;35mModelConfig\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│   \u001b[0m\u001b[33mattention\u001b[0m=\u001b[1;35mScaledDotProductAttention\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[1m)\u001b[0m,\n","\u001b[2;32m│   \u001b[0m\u001b[33mnum_layers\u001b[0m=\u001b[1;36m1\u001b[0m,\n","\u001b[2;32m│   \u001b[0m\u001b[33mvocab_size\u001b[0m=\u001b[1;36m15000\u001b[0m,\n","\u001b[2;32m│   \u001b[0m\u001b[33mH\u001b[0m=\u001b[1;36m1\u001b[0m,\n","\u001b[2;32m│   \u001b[0m\u001b[33md_model\u001b[0m=\u001b[1;36m32\u001b[0m,\n","\u001b[2;32m│   \u001b[0m\u001b[33md_ff\u001b[0m=\u001b[1;36m128\u001b[0m,\n","\u001b[2;32m│   \u001b[0m\u001b[33mdropout\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m,\n","\u001b[2;32m│   \u001b[0m\u001b[33mmax_seq_len\u001b[0m=\u001b[1;36m100\u001b[0m,\n","\u001b[2;32m│   \u001b[0m\u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\n","\u001b[1m)\u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["# config = ModelConfig(\n","#     attention=ScaledDotProductAttention(),\n","#     num_layers=6,\n","#     vocab_size=vocab_size,\n","#     H=8,\n","#     d_model=512,\n","#     d_ff=2048,\n","#     dropout=0.1,\n","#     max_seq_len=max_seq_len,\n","#     bias=True\n","# )\n","\n","config = ModelConfig(\n","    attention=ScaledDotProductAttention(),\n","    num_layers=1,\n","    vocab_size=vocab_size,\n","    H=1,\n","    d_model=32,\n","    d_ff=4*32,\n","    dropout=0.0,\n","    max_seq_len=max_seq_len,\n","    bias=True\n",")\n","pprint(config)\n","\n","model = Transformer(config, num_classes).to(DEVICE)\n","\n","# initialize model parameters\n","# it seems that this initialization is very important!\n","for p in model.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform_(p)"]},{"cell_type":"code","execution_count":361,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":493,"status":"ok","timestamp":1697792371925,"user":{"displayName":"EE E","userId":"05397898254477422912"},"user_tz":-480},"id":"ANYjAr5nT6FC","outputId":"dd38a594-2caf-4449-9b9d-f44d0c12ece9"},"outputs":[{"name":"stdout","output_type":"stream","text":["=====================================================================================\n","Layer (type:depth-idx)                                       Param #\n","=====================================================================================\n","Transformer                                                  --\n","├─Encoder: 1-1                                               3,200\n","│    └─Embedding: 2-1                                        480,000\n","│    └─ModuleList: 2-2                                       --\n","│    │    └─EncoderBlock: 3-1                                12,704\n","│    └─Dropout: 2-3                                          --\n","│    └─LayerNorm: 2-4                                        64\n","├─Linear: 1-2                                                132\n","=====================================================================================\n","Total params: 496,100\n","Trainable params: 496,100\n","Non-trainable params: 0\n","=====================================================================================\n"]}],"source":["print(torchinfo.summary(model))\n","optimizer = torch.optim.Adam(model.parameters())\n","loss_fn = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{"id":"1HdgZPJBoKY2"},"source":["## Train the model"]},{"cell_type":"code","execution_count":362,"metadata":{"executionInfo":{"elapsed":311,"status":"ok","timestamp":1697792358200,"user":{"displayName":"EE E","userId":"05397898254477422912"},"user_tz":-480},"id":"Ydp6IfBrkYnL","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["\n","def train_epoch(model, dataloader):\n","    model.train()\n","    losses, acc, count = [], 0, 0\n","    pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n","    for idx, (x, y) in pbar:\n","        optimizer.zero_grad()\n","        features = x.to(DEVICE)\n","        labels = y.to(DEVICE)\n","        pad_mask = (features == PAD).view(features.size(0), 1, 1, features.size(-1))\n","        pred = model(features, pad_mask)\n","\n","        loss = loss_fn(pred, labels).to(DEVICE)\n","        loss.backward()\n","        optimizer.step()\n","\n","        losses.append(loss.item())\n","        acc += (pred.argmax(1) == labels).sum().item()\n","        count += len(labels)\n","        # report progress\n","        if idx > 0 and idx % 50 == 0:\n","            pbar.set_description(\n","                f\"train loss={loss.item():.4f}, train_acc={acc/count:.4f}\"\n","            )\n","    return np.mean(losses), acc / count\n","\n","\n","def train(model, train_loader, test_loader, epochs):\n","    for ep in range(epochs):\n","        train_loss, train_acc = train_epoch(model, train_loader)\n","        val_loss, val_acc = evaluate(model, test_loader)\n","        print(f\"ep {ep}: val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n","\n","\n","def evaluate(model, dataloader):\n","    model.eval()\n","    losses = []\n","    with torch.no_grad():\n","        pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n","        for x, y in pbar:\n","            features = x_test.to(DEVICE)\n","            labels = y_test.to(DEVICE)\n","            pad_mask = (features == PAD).view(features.size(0), 1, 1, features.size(-1))\n","            pred = model(features, pad_mask)\n","            loss = loss_fn(pred, labels).to(DEVICE)\n","            losses.append(loss.item())\n","            acc = (pred.argmax(1) == labels).sum().item()\n","            count = len(labels)\n","    return np.mean(losses), acc / count\n"]},{"cell_type":"code","execution_count":363,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1697784709494,"user":{"displayName":"EE E","userId":"05397898254477422912"},"user_tz":-480},"id":"tIau66WRlzRm","outputId":"9c29df40-c101-4866-aa85-1090a71d185a","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# config = ModelConfig(encoder_vocab_size = vocab_size,\n","#                      d_embed = 32,\n","#                      d_ff = 4*32,\n","#                      h = 1,\n","#                      N_encoder = 1,\n","#                      max_seq_len = max_seq_len,\n","#                      dropout = 0.1\n","#                      )\n","# model = make_model(config)\n","# print(torchinfo.summary(model))\n","# optimizer = torch.optim.Adam(model.parameters())\n","# loss_fn = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":364,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":392,"status":"error","timestamp":1697792484286,"user":{"displayName":"EE E","userId":"05397898254477422912"},"user_tz":-480},"id":"Qo7RYNx0lzRm","outputId":"d50c2595-82b4-4a40-f207-a043660b12fa","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stderr","output_type":"stream","text":["train loss=0.2568, train_acc=0.8996: 100%|██████████| 938/938 [00:23<00:00, 39.93it/s]\n","100%|██████████| 60/60 [00:46<00:00,  1.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["ep 0: val_loss=0.2333, val_acc=0.9200\n"]},{"name":"stderr","output_type":"stream","text":["train loss=0.0873, train_acc=0.9441: 100%|██████████| 938/938 [00:23<00:00, 39.73it/s]\n","100%|██████████| 60/60 [00:46<00:00,  1.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["ep 1: val_loss=0.2481, val_acc=0.9176\n"]},{"name":"stderr","output_type":"stream","text":["train loss=0.0828, train_acc=0.9593: 100%|██████████| 938/938 [00:23<00:00, 39.80it/s]\n","100%|██████████| 60/60 [00:45<00:00,  1.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["ep 2: val_loss=0.2649, val_acc=0.9208\n"]},{"name":"stderr","output_type":"stream","text":["train loss=0.1102, train_acc=0.9702: 100%|██████████| 938/938 [00:23<00:00, 39.69it/s]\n","100%|██████████| 60/60 [00:45<00:00,  1.31it/s]"]},{"name":"stdout","output_type":"stream","text":["ep 3: val_loss=0.2932, val_acc=0.9171\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["train(model, train_loader, test_loader, epochs=4)"]},{"cell_type":"markdown","metadata":{"id":"XBSr0A9noh2i"},"source":["## News classification example"]},{"cell_type":"code","execution_count":365,"metadata":{"id":"uaAPcoPTkYnM","pycharm":{"name":"#%%\n"}},"outputs":[{"name":"stdout","output_type":"stream","text":["This is a Sci/Tec news\n"]}],"source":["ag_news_label = {1: \"World\",\n","                 2: \"Sports\",\n","                 3: \"Business\",\n","                 4: \"Sci/Tec\"}\n","\n","def classify_news(news):\n","    x_text = tokenizer(news.lower())[0:max_seq_len]\n","    x_int = torch.tensor([[word_to_id.get(word, UNK) for word in x_text]]).to(DEVICE)\n","\n","    model.eval()\n","    with torch.no_grad():\n","        pred = model(x_int).argmax(1).item() + 1\n","    print(f\"This is a {ag_news_label[pred]} news\")\n","\n","# The model correctly classifies a theoretical physics news as Sci/Tec news, :-)\n","news = \"\"\"The conformal bootstrapDavid Poland1,2and David Simmons-Duﬃn2*The conformal bootstrap was\n","proposed in the 1970s as a strategy for calculating the properties of second-order phasetransitions.\n","After spectacular success elucidating two-dimensional systems, little progress was made on systems in\n"," higher dimensions until a recent renaissance beginning in 2008. We report on some of the main results and\n","  ideas from thisrenaissance, focusing on new determinations of critical exponents and correlation\n","  functions in the three-dimensional Ising and O(N) models.\n","\"\"\"\n","classify_news(news)"]},{"cell_type":"code","execution_count":366,"metadata":{"id":"7AA69knJUkwh"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'src'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m/Users/gaohn/gaohn/common-utils/transformer/transformer/encoder.ipynb Cell 28\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gaohn/gaohn/common-utils/transformer/transformer/encoder.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39md2l\u001b[39;00m \u001b[39mimport\u001b[39;00m torch \u001b[39mas\u001b[39;00m d2l\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/gaohn/gaohn/common-utils/transformer/transformer/encoder.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/gaohn/gaohn/common-utils/transformer/transformer/encoder.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreproducibility\u001b[39;00m \u001b[39mimport\u001b[39;00m seed_all\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gaohn/gaohn/common-utils/transformer/transformer/encoder.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPositionalEncoding\u001b[39;00m(ABC, nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gaohn/gaohn/common-utils/transformer/transformer/encoder.ipynb#X35sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, d_model: \u001b[39mint\u001b[39m, dropout: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"]}],"source":["import unittest\n","from abc import ABC, abstractmethod\n","\n","import torch\n","from d2l import torch as d2l\n","from torch import nn\n","\n","from src.utils.reproducibility import seed_all\n","\n","\n","class PositionalEncoding(ABC, nn.Module):\n","    def __init__(self, d_model: int, dropout: float = 0.0) -> None:\n","        super().__init__()\n","        self.d_model = d_model\n","        self.dropout = nn.Dropout(p=dropout, inplace=False)\n","\n","    @abstractmethod\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        ...\n","\n","\n","class Sinusoid(PositionalEncoding):\n","    def __init__(\n","        self, d_model: int, dropout: float = 0.0, max_seq_len: int = 3\n","    ) -> None:\n","        super().__init__(d_model, dropout)\n","        self.max_seq_len = max_seq_len\n","        self.d_model = d_model\n","\n","        P = self._init_positional_encoding()\n","        self.register_buffer(\"P\", P, persistent=True)\n","\n","    def _init_positional_encoding(self) -> torch.Tensor:\n","        \"\"\"Initialize the positional encoding tensor.\"\"\"\n","        P = torch.zeros((1, self.max_seq_len, self.d_model))\n","        position = self._get_position_vector()\n","        div_term = self._get_div_term_vector()\n","        P[:, :, 0::2] = torch.sin(position / div_term)\n","        P[:, :, 1::2] = torch.cos(position / div_term)\n","        return P\n","\n","    def _get_position_vector(self) -> torch.Tensor:\n","        \"\"\"Return a vector representing the position of each token in a sequence.\"\"\"\n","        return torch.arange(self.max_seq_len, dtype=torch.float32).reshape(-1, 1)\n","\n","    def _get_div_term_vector(self) -> torch.Tensor:\n","        \"\"\"Return a vector representing the divisor term for positional encoding.\"\"\"\n","        return torch.pow(\n","            10000,\n","            torch.arange(0, self.d_model, 2, dtype=torch.float32) / self.d_model,\n","        )\n","\n","    def forward(self, Z: torch.Tensor) -> torch.Tensor:\n","        Z = self._add_positional_encoding(Z)\n","        return self.dropout(Z)\n","\n","    def _add_positional_encoding(self, Z: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Add the positional encoding tensor to the input tensor.\"\"\"\n","        return Z + self.P[:, : Z.shape[1], :].to(Z.device)\n","\n","\n","class TestPositionalEncoding(unittest.TestCase):\n","    def setUp(self) -> None:\n","        seed_all(42, seed_torch=True)\n","\n","        # Initialize queries, keys, and values\n","        # fmt: off\n","        self.batch_size    = 1  # B\n","        self.num_heads     = 2  # H\n","        self.seq_len       = 60 # L\n","        self.d_model = 32 # D\n","        self.dropout       = 0.0\n","        self.max_seq_len       = 1000\n","\n","        self.embeddings    = torch.zeros(self.batch_size, self.seq_len, self.d_model) # Z\n","\n","        self.pos_encoding  = Sinusoid(d_model=self.d_model, dropout=self.dropout, max_seq_len=self.max_seq_len)\n","        # fmt: on\n","\n","        # Initialize the attention models\n","        self.pos_encoding_d2l = d2l.PositionalEncoding(\n","            self.d_model, dropout=self.dropout, max_seq_len=self.max_seq_len\n","        )\n","        self.pos_encoding_d2l.eval()\n","\n","    def test_positional_encoding_with_d2l_as_sanity_check(self) -> None:\n","        # fmt: off\n","        # d2l implementation\n","        Z_d2l = self.pos_encoding_d2l(self.embeddings)\n","        P_d2l = self.pos_encoding_d2l.P[:, : Z_d2l.shape[1], :]\n","\n","        # own implementation\n","        Z     = self.pos_encoding(self.embeddings)\n","        P     = self.pos_encoding.P[:, : Z.shape[1], :]\n","        # fmt: on\n","\n","        # Test if both are close\n","        self.assertTrue(torch.allclose(Z, Z_d2l))\n","        self.assertTrue(torch.allclose(P, P_d2l))\n","\n","\n","if __name__ == \"__main__\":\n","    unittest.main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JqwFd-U9UmVp"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"n_8N4ifOUxYD"},"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/hbchen-one/Transformer-Models-from-Scratch/blob/main/Encoder_only_transformer_AG_News_classification.ipynb","timestamp":1697769107746},{"file_id":"1P7oU2EWQ1Qk17N9NutZv9FV1a_hMGUoR","timestamp":1647373127550}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
