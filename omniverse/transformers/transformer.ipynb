{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter, OrderedDict\n",
    "from rich.pretty import pprint\n",
    "from torchtext.vocab import vocab\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Union, Optional, Any, Callable\n",
    "import functools\n",
    "from common_utils.core.common import seed_all\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "seed_all(42, seed_torch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/transformer_from_scratch/transformer_from_scratch.py\n",
    "- https://peterbloem.nl/blog/transformers\n",
    "- Show how self attention linear algebra works with simple examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U torch torchvision torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 knowledge in NLP..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, each pair of sentences represents a single turn in a conversation. The first sentence is the user's input, and the second sentence is the bot's response. \n",
    "\n",
    "We define the following variables:\n",
    "\n",
    "- $\\mathcal{V}$: the vocabulary, which contains all the words that can appear in the sentences.\n",
    "- $N$: the number of sentences in the dataset, or the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n",
    "VOCAB_SIZE = 16\n",
    "VOCAB_SIZE_WITH_SPECIAL_TOKENS = VOCAB_SIZE + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"Hello bot\", \"Hello human\"),\n",
    "    (\"You good\", \"I bot\"),\n",
    "    (\"Tell joke\", \"Sure thing\"),\n",
    "    (\"You are funny\", \"Thanks\"),\n",
    "    (\"Bye bot\", \"Bye human\"),\n",
    "    (\"Bye human\", \"Bye bot\"),\n",
    "    (\"Am I funny.\", \"You funny\"),\n",
    "    (\"I bot\", \"You human\"),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "en_tokenizer = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m20\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20\n",
      "Token-to-index mapping:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;PAD&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;SOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;EOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;UNK&gt;'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'bot'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'human'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'you'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'bye'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'i'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'funny'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'hello'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'good'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'tell'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'joke'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'sure'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'thing'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'are'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'thanks'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'am'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32mPAD\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'<SOS>'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'<EOS>'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m2\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'<UNK\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m: \u001b[1;36m3\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'bot'\u001b[0m: \u001b[1;36m4\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'human'\u001b[0m: \u001b[1;36m5\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'you'\u001b[0m: \u001b[1;36m6\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'bye'\u001b[0m: \u001b[1;36m7\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'i'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'funny'\u001b[0m: \u001b[1;36m9\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'hello'\u001b[0m: \u001b[1;36m10\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'good'\u001b[0m: \u001b[1;36m11\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'tell'\u001b[0m: \u001b[1;36m12\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'joke'\u001b[0m: \u001b[1;36m13\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'sure'\u001b[0m: \u001b[1;36m14\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'thing'\u001b[0m: \u001b[1;36m15\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'are'\u001b[0m: \u001b[1;36m16\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'thanks'\u001b[0m: \u001b[1;36m17\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'am'\u001b[0m: \u001b[1;36m18\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'.'\u001b[0m: \u001b[1;36m19\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Build vocabularies\n",
    "counter = Counter()\n",
    "for (user_input, bot_response) in training_data:\n",
    "    counter.update(tokenizer(user_input))\n",
    "    counter.update(tokenizer(bot_response))\n",
    "\n",
    "\n",
    "# Sort the Counter by frequency and create an OrderedDict\n",
    "sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "# Create a vocab object\n",
    "v = vocab(ordered_dict)\n",
    "\n",
    "# Add special tokens to the counter\n",
    "specials = [\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"]\n",
    "\n",
    "# Add special tokens to the vocab\n",
    "for i, token in enumerate(specials):\n",
    "    v.insert_token(token, i)\n",
    "\n",
    "pprint(len(v))\n",
    "# Print the vocabulary size and the token-to-index and index-to-token mappings\n",
    "# assert len(v) == VOCAB_SIZE_WITH_SPECIAL_TOKENS\n",
    "\n",
    "itos: List[str] = v.get_itos()\n",
    "# assert len(itos) == VOCAB_SIZE_WITH_SPECIAL_TOKENS\n",
    "\n",
    "stoi: Dict[str, int] = v.get_stoi()\n",
    "\n",
    "def sort_stoi_by_index(itos: List[str]) -> Dict[str, int]:\n",
    "    return {token: index for index, token in enumerate(itos)}\n",
    "\n",
    "stoi_sorted_by_index: Dict[str, int] = sort_stoi_by_index(v.get_itos())\n",
    "\n",
    "print(f\"Vocabulary size: {len(v)}\")\n",
    "print(\"Token-to-index mapping:\")\n",
    "pprint(stoi_sorted_by_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]))</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]))</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]))</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]))</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]))</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]))</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]))</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]))</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m10\u001b[0m,  \u001b[1;36m4\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m10\u001b[0m,  \u001b[1;36m5\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m,  \u001b[1;36m6\u001b[0m, \u001b[1;36m11\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m12\u001b[0m, \u001b[1;36m13\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m14\u001b[0m, \u001b[1;36m15\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m,  \u001b[1;36m6\u001b[0m, \u001b[1;36m16\u001b[0m,  \u001b[1;36m9\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m17\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m18\u001b[0m,  \u001b[1;36m8\u001b[0m,  \u001b[1;36m9\u001b[0m, \u001b[1;36m19\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m9\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert to Tensor\n",
    "def data_process(raw_text_iter, tokenizer):\n",
    "    data = [\n",
    "        torch.tensor(\n",
    "            [v[\"<SOS>\"]] + [v[token] for token in tokenizer(item)] + [v[\"<EOS>\"]],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        for item in raw_text_iter\n",
    "    ]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "\n",
    "train_data = []\n",
    "for (user_input, bot_response) in training_data:\n",
    "    user_input_tensor = data_process([user_input], tokenizer)\n",
    "    bot_response_tensor = data_process([bot_response], tokenizer)\n",
    "    train_data.append((user_input_tensor, bot_response_tensor))\n",
    "\n",
    "# Let's check the output\n",
    "pprint(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "Padding is a common technique used in deep learning when dealing with sequences of varying lengths. Here are a few reasons why padding is necessary:\n",
    "\n",
    "1. **Uniform Input Size**: Many deep learning models, such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), require inputs to have a uniform size. For example, in a batch of sentences, each sentence might have a different number of words. By padding the sentences with a special `<PAD>` token, all sentences in the batch can be made to have the same length, allowing them to be processed as a single tensor.\n",
    "\n",
    "2. **Efficiency**: Processing sentences one by one can be inefficient. By padding sentences to the same length and forming a batch, we can process multiple sentences at once, leading to faster training times.\n",
    "\n",
    "3. **Sequence Models**: In sequence-to-sequence models, such as those used in machine translation or chatbot applications, padding is necessary not only to ensure uniform input size, but also to delineate where a sequence starts and ends. Special tokens like `<SOS>` (start of sequence) and `<EOS>` (end of sequence) are often used in conjunction with padding for this purpose.\n",
    "\n",
    "It's important to note that while padding allows for efficient batch processing and uniform input sizes, it can also introduce noise into the data, as the model needs to learn to ignore these padding tokens. This is often handled in practice by using masking techniques, which explicitly tell the model to ignore certain inputs (like the padding tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do we need padding in Transformers?\n",
    "\n",
    "Yes, the Transformer model does require padding, and here's why:\n",
    "\n",
    "1. **Batch Processing**: Just like other neural network architectures, Transformers also benefit from processing data in batches for computational efficiency. Since sequences in a batch may have varying lengths, padding is used to ensure all sequences in a batch have the same length.\n",
    "\n",
    "2. **Masking**: The Transformer uses a mechanism called \"attention\", which involves looking at all words in the sequence simultaneously. However, we don't want the model to pay attention to the padding tokens, as they don't contain any useful information. Therefore, we create a \"mask\" that explicitly tells the model to ignore the padding tokens when computing attention. This mask is a sequence of Boolean values indicating which tokens are padding (True) and which are not (False).\n",
    "\n",
    "3. **Sequence Boundaries**: In sequence-to-sequence tasks (like translation), padding is also used in conjunction with special tokens like `<SOS>` (start of sequence) and `<EOS>` (end of sequence) to indicate the boundaries of the sequence. This helps the model identify when a new sequence starts and when a sequence ends.\n",
    "\n",
    "So, while the Transformer doesn't inherently require sequences to be of the same length, padding is still a necessary step in the data preprocessing pipeline for the reasons mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]))</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]))</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]))</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]))</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]))</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]))</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]))</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]))</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m10\u001b[0m,  \u001b[1;36m4\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m10\u001b[0m,  \u001b[1;36m5\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m,  \u001b[1;36m6\u001b[0m, \u001b[1;36m11\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m12\u001b[0m, \u001b[1;36m13\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m14\u001b[0m, \u001b[1;36m15\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m,  \u001b[1;36m6\u001b[0m, \u001b[1;36m16\u001b[0m,  \u001b[1;36m9\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m17\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m7\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m18\u001b[0m,  \u001b[1;36m8\u001b[0m,  \u001b[1;36m9\u001b[0m, \u001b[1;36m19\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m9\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m6\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pad the sequences\n",
    "train_data_padded = []\n",
    "for (user_input_tensor, bot_response_tensor) in train_data:\n",
    "    user_input_tensor_padded = pad_sequence(\n",
    "        [user_input_tensor], padding_value=v[\"<PAD>\"], batch_first=True\n",
    "    ).squeeze()\n",
    "    bot_response_tensor_padded = pad_sequence(\n",
    "        [bot_response_tensor], padding_value=v[\"<PAD>\"], batch_first=True\n",
    "    ).squeeze()\n",
    "    train_data_padded.append((user_input_tensor_padded, bot_response_tensor_padded))\n",
    "\n",
    "# Let's check the output\n",
    "pprint(train_data_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I do not see any changes\n",
    "The `pad_sequence` function is designed to pad a batch of sequences, not a single sequence. In the code, you're applying `pad_sequence` to each pair of sentences individually, which is why you're not seeing any padding.\n",
    "\n",
    "So each data point is a batch, i.e. batch size = 1, so there's no need for padding\n",
    "and therefore no effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the padding in action, you would need to apply `pad_sequence` to a batch of sentences. Here's an example where we take the first and fourth sentence of\n",
    "different lengths and pad them to have the same length. The batch size is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]))</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m10\u001b[0m,  \u001b[1;36m4\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m10\u001b[0m,  \u001b[1;36m5\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]))</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m,  \u001b[1;36m6\u001b[0m, \u001b[1;36m16\u001b[0m,  \u001b[1;36m9\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m17\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's take the first two pairs of sentences as a batch\n",
    "batch = [train_data[0], train_data[3]]\n",
    "pprint(batch[0]) # (4, 4)\n",
    "pprint(batch[1]) # (5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After padding, we expect the input tensor to be of shape \n",
    "\n",
    "```python\n",
    "(batch_size, max_seq_len) = (2, 5)\n",
    "```\n",
    "\n",
    "and the response tensor to be of shape \n",
    "\n",
    "```python\n",
    "(batch_size, max_seq_len) = (2, 4)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User inputs (padded):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m10\u001b[0m,  \u001b[1;36m4\u001b[0m,  \u001b[1;36m2\u001b[0m,  \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m,  \u001b[1;36m6\u001b[0m, \u001b[1;36m16\u001b[0m,  \u001b[1;36m9\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot responses (padded):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m10\u001b[0m,  \u001b[1;36m5\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m17\u001b[0m,  \u001b[1;36m2\u001b[0m,  \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Separate the user inputs and bot responses\n",
    "user_inputs, bot_responses = zip(*batch)\n",
    "\n",
    "# Pad the user inputs\n",
    "user_inputs_padded = pad_sequence(\n",
    "    user_inputs, padding_value=v[\"<PAD>\"], batch_first=True\n",
    ")\n",
    "\n",
    "# Pad the bot responses\n",
    "bot_responses_padded = pad_sequence(\n",
    "    bot_responses, padding_value=v[\"<PAD>\"], batch_first=True\n",
    ")\n",
    "\n",
    "# Let's check the output\n",
    "print(\"User inputs (padded):\")\n",
    "pprint(user_inputs_padded)\n",
    "print(\"Bot responses (padded):\")\n",
    "pprint(bot_responses_padded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this code, we first create a batch of sentences by taking the first two pairs of sentences from `train_data`. We then separate the English and French sentences, and pad each set of sentences using `pad_sequence`.\n",
    "\n",
    "This should give you padded sentences where all sentences in a batch have the same length. The shorter sentences in the batch are padded with the `<PAD>` token to make them the same length as the longest sentence in the batch.\n",
    "\n",
    "However, in practice, you would typically handle padding in the `collate_fn` function when you create your `DataLoader`, as this allows you to create batches of any size and automatically pad the sentences in each batch to the correct length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "\n",
    "So far, the intuition is simple, to convert words in sentences to numbers,\n",
    "we achieved this through a series of preprocessing steps such as tokenization,\n",
    "padding, and attention mask. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: List[Tuple[str, str]],\n",
    "        tokenizer_type: str = \"basic_english\",\n",
    "        specials: List[str] = [\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"],\n",
    "    ) -> None:\n",
    "        self.data = data\n",
    "        self.tokenizer_type = tokenizer_type\n",
    "        self.tokenizer = get_tokenizer(tokenizer_type)\n",
    "        self.specials = specials\n",
    "        self.vocab = self.build_vocab()\n",
    "        self.padded_data = self.pad_data()\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, target_ids = self.padded_data[index]\n",
    "        item = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": (input_ids != self.vocab[\"<PAD>\"]).type(torch.long),\n",
    "            \"target_ids\": target_ids,\n",
    "        }\n",
    "        return item\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.padded_data)\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def build_vocab(self) -> vocab:\n",
    "        counter = Counter()\n",
    "        for (user_input, bot_response) in self.data:\n",
    "            counter.update(self.tokenizer(user_input))\n",
    "            counter.update(self.tokenizer(bot_response))\n",
    "\n",
    "        sorted_by_freq_tuples = sorted(\n",
    "            counter.items(), key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "        ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "        v = vocab(ordered_dict)\n",
    "        for i, token in enumerate(self.specials):\n",
    "            v.insert_token(token, i)\n",
    "\n",
    "        return v\n",
    "\n",
    "    def pad_data(self) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        train_data_padded = []\n",
    "        for (user_input, bot_response) in self.data:\n",
    "            user_input_tensor = self._text_to_tensor(user_input)\n",
    "            bot_response_tensor = self._text_to_tensor(bot_response)\n",
    "            user_input_tensor_padded = self._pad_sequence(user_input_tensor)\n",
    "            bot_response_tensor_padded = self._pad_sequence(bot_response_tensor)\n",
    "            train_data_padded.append(\n",
    "                (user_input_tensor_padded, bot_response_tensor_padded)\n",
    "            )\n",
    "        return train_data_padded\n",
    "\n",
    "    def _text_to_tensor(self, text: str) -> torch.Tensor:\n",
    "        return torch.tensor(\n",
    "            [self.vocab[\"<SOS>\"]]\n",
    "            + [self.vocab[token] for token in self.tokenizer(text)]\n",
    "            + [self.vocab[\"<EOS>\"]],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "\n",
    "    def _pad_sequence(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        return pad_sequence(\n",
    "            [tensor], padding_value=self.vocab[\"<PAD>\"], batch_first=True\n",
    "        ).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChatDataset(training_data)\n",
    "\n",
    "assert len(dataset) == N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the below code, you get error\n",
    "\n",
    "```python\n",
    "RuntimeError: stack expects each tensor to be equal size, but got [6] at entry 0 and [5] at entry 1\n",
    "```\n",
    "\n",
    "because you have different length of sentences in the batch.\n",
    "So we need to apply padding properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DataLoader\n",
    "# batch_size = 2\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Iterate over data\n",
    "# for i, batch in enumerate(dataloader):\n",
    "#     print(f\"Batch {i+1}\")\n",
    "#     print(f\"Input IDs: {batch['input_ids']}\")\n",
    "#     print(f\"Attention Mask: {batch['attention_mask']}\")\n",
    "#     print(f\"Target IDs: {batch['target_ids']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    target_ids = [item['target_ids'] for item in batch]\n",
    "    \n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=dataset.vocab[\"<PAD>\"])\n",
    "    target_ids = pad_sequence(target_ids, batch_first=True, padding_value=dataset.vocab[\"<PAD>\"])\n",
    "    \n",
    "    attention_mask = (input_ids != dataset.vocab[\"<PAD>\"]).type(torch.long)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids, \n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"target_ids\": target_ids\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set `shuffle=False` because I want to keep the order of the data for easy\n",
    "visualization later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4\n",
      "\n",
      "Input IDs:\n",
      "[[ 1, 18,  8,  9, 19,  2],\n",
      " [ 1,  8,  4,  2,  0,  0]]\n",
      "\n",
      "Attention Mask:\n",
      "[[1, 1, 1, 1, 1, 1],\n",
      " [1, 1, 1, 1, 0, 0]]\n",
      "\n",
      "Target IDs:\n",
      "[[1, 6, 9, 2],\n",
      " [1, 6, 5, 2]]\n"
     ]
    }
   ],
   "source": [
    "# Use DataLoader\n",
    "def create_dataloader(\n",
    "    dataset: Dataset, batch_size: int, shuffle: bool = True, **kwargs: Dict[str, Any]\n",
    ") -> DataLoader:\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, **kwargs)\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "dataloader = create_dataloader(\n",
    "    dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "# Iterate over data\n",
    "for i, batch in enumerate(dataloader):\n",
    "    if i == 3:\n",
    "        print(f\"Batch {i+1}\\n\")\n",
    "        print(\n",
    "            f\"Input IDs:\\n{np.array2string(batch['input_ids'].numpy(), separator=', ')}\\n\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Attention Mask:\\n{np.array2string(batch['attention_mask'].numpy(), separator=', ')}\\n\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Target IDs:\\n{np.array2string(batch['target_ids'].numpy(), separator=', ')}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the last batch and input ids and target ids are easy, but the\n",
    "attention mask means that the model should not pay attention to the\n",
    "padding tokens. As we know they are a bit like dummy tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "(tensor([ 1, 18,  8,  9, 19,  2]), tensor([1, 6, 9, 2])),\n",
    "(tensor([1, 8, 4, 2]), tensor([1, 6, 5, 2]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of transformer models such as BERT or GPT, an attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to these positions.\n",
    "\n",
    "The attention mechanism in transformer models allows them to focus on different words in the input sequence when producing the output. However, when processing the input data, we often pad shorter sentences with special tokens (like \"<PAD>\") to make all sentences the same length. We don't want the model to focus on these padding tokens, as they don't contain any useful information.\n",
    "\n",
    "The attention mask has the same length as the input sequence and has a value of 1 for real tokens and a value of 0 for padding tokens. It tells the model to ignore (by setting to zero) the attention scores of padding tokens so that they don't affect the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First layer - Word embeddings + Positional embeddings\n",
    "\n",
    "In the original \"Attention is All You Need\" paper where Transformers were first introduced, the authors proposed the use of two types of embeddings: token embeddings and position embeddings.\n",
    "\n",
    "1. **Token Embeddings**: These are the embeddings that we commonly refer to when talking about word embeddings. They are learned from the data and represent the individual words (or tokens) from the vocabulary. For example, the words \"queen\" and \"king\" will have their unique vectors in this embedding space.\n",
    "\n",
    "2. **Position Embeddings**: These are an additional type of embedding introduced specifically in Transformers. They represent the position of a word in a sentence. This is important because unlike previous architectures like RNNs, Transformers do not have an inherent understanding of the position or order of words in a sentence. These position embeddings are added to the token embeddings to give the model information about the position of the words.\n",
    "\n",
    "In summary, the first block in the Transformer's architecture is a combination of token and position embeddings. Both are vital for the model to understand the data correctly. If you would like to only focus on token embeddings for now, you can just use the `nn.Embedding` layer in PyTorch, but it's important to remember that this will only get you part way to replicating the Transformer's architecture.\n",
    "\n",
    "We look at the first part, the token embeddings, in more detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer in a transformer is indeed the embedding layer. This layer takes in words as input and outputs corresponding dense vectors. These vectors capture semantic meanings of the words, and words with similar meanings have vectors that are close in the vector space.\n",
    "\n",
    "An embedding layer is essentially a simple lookup table that's learned from data. It is a matrix where each row corresponds to a vector for a particular word in the vocabulary. The dimension of these vectors, also called the embedding dimension, is a hyperparameter. \n",
    "\n",
    "The intuition here is to transform sparse one-hot vectors representing words into dense vectors with lower dimensionality. These dense vectors can capture relationships between words in a more expressive way than the original one-hot vectors.\n",
    "\n",
    "In PyTorch, you can create an embedding layer using `nn.Embedding`. Here's how you can do it for your transformer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int) -> None:\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        nn.init.kaiming_normal_(self.token_emb.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.token_emb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m10\u001b[0m,  \u001b[1;36m4\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m,  \u001b[1;36m6\u001b[0m, \u001b[1;36m11\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2766</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0947</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4780</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2163</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0395</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7271</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.8569</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4931</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4062</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4378</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4284</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1996</span><span style=\"font-weight: bold\">]]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2766</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0947</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4780</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.0490</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3985</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6406</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3346</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1897</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0836</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4378</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4284</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1996</span><span style=\"font-weight: bold\">]]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">EmbeddingBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.2766\u001b[0m, \u001b[1;36m-1.0947\u001b[0m, \u001b[1;36m-0.4780\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.2163\u001b[0m,  \u001b[1;36m1.0395\u001b[0m, \u001b[1;36m-0.7271\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.8569\u001b[0m,  \u001b[1;36m0.4931\u001b[0m, \u001b[1;36m-1.4062\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.4378\u001b[0m,  \u001b[1;36m0.4284\u001b[0m, \u001b[1;36m-1.1996\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.2766\u001b[0m, \u001b[1;36m-1.0947\u001b[0m, \u001b[1;36m-0.4780\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-2.0490\u001b[0m,  \u001b[1;36m0.3985\u001b[0m,  \u001b[1;36m0.6406\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.3346\u001b[0m, \u001b[1;36m-1.1897\u001b[0m, \u001b[1;36m-0.0836\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.4378\u001b[0m,  \u001b[1;36m0.4284\u001b[0m, \u001b[1;36m-1.1996\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mEmbeddingBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usage\n",
    "embedding_dim = 3\n",
    "embedding_layer = TokenEmbedding(len(dataset.vocab), embedding_dim)\n",
    "\n",
    "# After you've loaded your batch from your dataloader\n",
    "for i, batch in enumerate(dataloader):\n",
    "    if i == 0:\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        embedded_sequences = embedding_layer(input_ids)\n",
    "\n",
    "        pprint(input_ids)\n",
    "        pprint(embedded_sequences)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, `nn.Embedding(vocab_size, embedding_dim)` creates an embedding layer with `vocab_size` number of embeddings each of dimension `embedding_dim`. \n",
    "\n",
    "In the `forward` method, we compute token embeddings of the input ids by passing them through the embedding layer. \n",
    "\n",
    "This is a very simple embedding layer for a transformer. In reality, you might want to add more complexity like dropout, layer normalization, and positional encoding instead of simple position embeddings. Also, transformer-based models like BERT use subword tokenization, which adds another layer of complexity to the embedding layer. But this should give you a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the `nn.Embedding` layer?\n",
    "\n",
    "When you use an embedding layer like `nn.Embedding` in PyTorch, what you are effectively doing is learning a kind of word2vec (or similar) model as part of your larger model. The `nn.Embedding` layer is initialized with random weights, and these weights are then adjusted during training through backpropagation. The goal is to learn word embeddings that are useful for the specific task your model is trying to achieve. \n",
    "\n",
    "This is slightly different from using a pre-trained word embedding like word2vec or GloVe. In those cases, the embeddings are trained beforehand on a large corpus of text with the aim of capturing semantic similarities between words. These pre-trained embeddings can be useful when you don't have a lot of training data, or when you want to leverage the semantic knowledge captured by these models.\n",
    "\n",
    "When training a Transformer model from scratch, it's common to learn the embeddings from scratch as well, as part of the model. This allows the model to learn embeddings that are tailored to the specific task it's trying to solve. However, it's also possible to use pre-trained embeddings, especially in cases where the training data is limited.\n",
    "\n",
    "In summary, using `nn.Embedding` in PyTorch doesn't preclude the use of word2vec-like embeddings. It's just a different approach, and the right one to use depends on the specifics of your task and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this embeddings below corresponds to the first two data points (sentences).\n",
    "\n",
    "```python\n",
    "tensor([[[-0.2766, -1.0947, -0.4780],\n",
    "│   │    [ 0.2163,  1.0395, -0.7271],\n",
    "│   │    [-0.8569,  0.4931, -1.4062],\n",
    "│   │    [ 0.4378,  0.4284, -1.1996]],\n",
    "│   │   \n",
    "│   │   [[-0.2766, -1.0947, -0.4780],\n",
    "│   │    [-2.0490,  0.3985,  0.6406],\n",
    "│   │    [ 0.3346, -1.1897, -0.0836],\n",
    "│   │    [ 0.4378,  0.4284, -1.1996]]], grad_fn=<EmbeddingBackward0>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is first and last token always the same?\n",
    "\n",
    "The reason why the first and the last embeddings are the same is due to the use of special tokens `<SOS>` (Start Of Sentence) and `<EOS>` (End Of Sentence) in your sequences. In your pre-processing steps, each sentence is surrounded by these special tokens.\n",
    "\n",
    "When you feed these sequences into the `TokenEmbedding` layer, each unique token gets a unique embedding. Since the `<SOS>` token is always the first token in your sequences, it always gets the same embedding. The same goes for the `<EOS>` token, which is always the last token in your sequences.\n",
    "\n",
    "Hence, for every sequence, the first and the last embeddings are the same because they correspond to the `<SOS>` and `<EOS>` tokens, respectively. The embeddings for these tokens do not change unless the model is trained and the weights of the embedding layer get updated.\n",
    "\n",
    "You might see different weights for the `<SOS>` and `<EOS>` tokens in different batches if you shuffle your dataset between epochs and train your model, because the model is learning to update the weights of these tokens based on the context in which they appear.\n",
    "\n",
    "That being said, note that the purpose of these special tokens is not to carry semantic meaning like the other words in the vocabulary, but to provide structural information to the model. In the case of transformers, they also provide positions for the positional encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Input ID drill down\n",
    "\n",
    "To drill down further, let's just consider the first data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1\u001b[0m, \u001b[1;36m10\u001b[0m,  \u001b[1;36m4\u001b[0m,  \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2766</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0947</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4780</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2163</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0395</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7271</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.8569</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4931</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4062</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4378</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4284</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1996</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">SelectBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.2766\u001b[0m, \u001b[1;36m-1.0947\u001b[0m, \u001b[1;36m-0.4780\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.2163\u001b[0m,  \u001b[1;36m1.0395\u001b[0m, \u001b[1;36m-0.7271\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.8569\u001b[0m,  \u001b[1;36m0.4931\u001b[0m, \u001b[1;36m-1.4062\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.4378\u001b[0m,  \u001b[1;36m0.4284\u001b[0m, \u001b[1;36m-1.1996\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mSelectBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_input = dataset[0][\"input_ids\"]\n",
    "pprint(first_input)\n",
    "\n",
    "first_input_embeddings = embedded_sequences[0]\n",
    "pprint(first_input_embeddings)\n",
    "\n",
    "# equivalent to:\n",
    "assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)\n",
    "assert_equal(first_input_embeddings, embedding_layer.token_emb(first_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the scenario you've outlined:\n",
    "\n",
    "1. The sentence \"Hello bot\" has been preprocessed into the sequence of token IDs `[1, 10, 4, 2]`. \n",
    "\n",
    "2. Your `TokenEmbedding` model is learning a unique vector representation (i.e., an embedding) for each unique token ID in your vocabulary.\n",
    "\n",
    "    ```python\n",
    "    tensor([[-0.2766, -1.0947, -0.4780],\n",
    "    │   │   [ 0.2163,  1.0395, -0.7271],\n",
    "    │   │   [-0.8569,  0.4931, -1.4062],\n",
    "    │   │   [ 0.4378,  0.4284, -1.1996]], grad_fn=<SelectBackward0>)\n",
    "    ```\n",
    "\n",
    "3. When you pass your sequence of token IDs `[1, 10, 4, 2]` into your `TokenEmbedding` model, it looks up the vector representation for each token ID in the sequence, effectively converting your sequence of token IDs into a sequence of embeddings.\n",
    "\n",
    "4. The tensor that is output by the model represents these embeddings. Each row in the tensor corresponds to the embedding for the respective token ID in your sequence.\n",
    "\n",
    "In summary, your understanding is correct: the tensor represents the \"learnt word/token embeddings\" for the input sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and its weights.\n",
    "\n",
    "In the context of neural networks, \"weights\" often refer to the parameters that the model learns during training. These are the values that are updated in each iteration of the optimization process, with the aim to minimize the loss function.\n",
    "\n",
    "The Embedding layer in PyTorch is essentially a lookup table that maps each token in the vocabulary to a vector in a high-dimensional space. The \"weights\" in the Embedding layer are the values of these vectors. Each row in the weight matrix corresponds to the vector for a specific token.\n",
    "\n",
    "For a vocabulary of `vocab_size` and an embedding size of `embedding_dim`, the Embedding layer will have a weight matrix of shape `(vocab_size, embedding_dim)`. Each row of this matrix is the embedding vector for a corresponding token in the vocabulary. For example, the first row of the matrix is the vector for the token with ID 0, the second row is for the token with ID 1, and so on.\n",
    "\n",
    "When the model is initialized, these weights are usually assigned random values. During training, the model learns the appropriate values for these weights based on the training data. The goal is to arrange the vectors in such a way that semantically similar words are located close to each other in the embedding space.\n",
    "\n",
    "To summarize, the weights in the Embedding layer represent the embeddings for each token in the vocabulary. They are the parameters that the model needs to learn from the data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Parameter containing:\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2762</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.3874</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0089</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2766</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0947</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4780</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4378</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4284</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1996</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1702</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6074</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3932</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.8569</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4931</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4062</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6759</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4063</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3876</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-2.0490</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3985</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6406</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0234</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5232</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4762</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1789</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4512</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1513</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6146</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3305</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1457</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2163</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0395</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7271</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3346</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1897</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0836</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4892</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3895</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5929</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0744</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2951</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0447</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0104</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1966</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0598</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6684</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2088</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2816</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1628</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0950</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7942</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7826</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1705</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4626</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3472</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2143</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1750</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4257</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2848</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7900</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">requires_grad</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Parameter containing:\n",
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.2762\u001b[0m,  \u001b[1;36m1.3874\u001b[0m,  \u001b[1;36m0.0089\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.2766\u001b[0m, \u001b[1;36m-1.0947\u001b[0m, \u001b[1;36m-0.4780\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.4378\u001b[0m,  \u001b[1;36m0.4284\u001b[0m, \u001b[1;36m-1.1996\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.1702\u001b[0m,  \u001b[1;36m0.6074\u001b[0m, \u001b[1;36m-0.3932\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.8569\u001b[0m,  \u001b[1;36m0.4931\u001b[0m, \u001b[1;36m-1.4062\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.6759\u001b[0m, \u001b[1;36m-0.4063\u001b[0m,  \u001b[1;36m0.3876\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-2.0490\u001b[0m,  \u001b[1;36m0.3985\u001b[0m,  \u001b[1;36m0.6406\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0234\u001b[0m,  \u001b[1;36m0.5232\u001b[0m,  \u001b[1;36m0.4762\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.1789\u001b[0m,  \u001b[1;36m0.4512\u001b[0m, \u001b[1;36m-0.1513\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.6146\u001b[0m,  \u001b[1;36m0.3305\u001b[0m,  \u001b[1;36m0.1457\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.2163\u001b[0m,  \u001b[1;36m1.0395\u001b[0m, \u001b[1;36m-0.7271\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.3346\u001b[0m, \u001b[1;36m-1.1897\u001b[0m, \u001b[1;36m-0.0836\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.4892\u001b[0m,  \u001b[1;36m0.3895\u001b[0m,  \u001b[1;36m0.5929\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0744\u001b[0m,  \u001b[1;36m3.2951\u001b[0m,  \u001b[1;36m1.0447\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.0104\u001b[0m,  \u001b[1;36m0.1966\u001b[0m,  \u001b[1;36m0.0598\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.6684\u001b[0m,  \u001b[1;36m1.2088\u001b[0m,  \u001b[1;36m0.2816\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-1.1628\u001b[0m, \u001b[1;36m-0.0950\u001b[0m, \u001b[1;36m-0.7942\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.7826\u001b[0m, \u001b[1;36m-1.1705\u001b[0m, \u001b[1;36m-0.4626\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.3472\u001b[0m,  \u001b[1;36m0.2143\u001b[0m, \u001b[1;36m-1.1750\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.4257\u001b[0m,  \u001b[1;36m0.2848\u001b[0m,  \u001b[1;36m0.7900\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mrequires_grad\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token_embeddings_weight = embedding_layer.token_emb.weight\n",
    "pprint(token_embeddings_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4]), torch.Size([4, 3]), torch.Size([20, 3]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_input.shape, first_input_embeddings.shape, token_embeddings_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.<span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.<span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.<span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.<span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m., \u001b[1;36m1\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m.,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m1\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m.,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m1\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m.,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m1\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m.,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2766</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0947</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4780</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2163</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0395</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7271</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.8569</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4931</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4062</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4378</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4284</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1996</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">MmBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.2766\u001b[0m, \u001b[1;36m-1.0947\u001b[0m, \u001b[1;36m-0.4780\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.2163\u001b[0m,  \u001b[1;36m1.0395\u001b[0m, \u001b[1;36m-0.7271\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.8569\u001b[0m,  \u001b[1;36m0.4931\u001b[0m, \u001b[1;36m-1.4062\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.4378\u001b[0m,  \u001b[1;36m0.4284\u001b[0m, \u001b[1;36m-1.1996\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mMmBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the first token ID to a one-hot encoded vector\n",
    "first_input_onehot = F.one_hot(first_input, num_classes=len(dataset.vocab)).float()\n",
    "pprint(first_input_onehot)\n",
    "\n",
    "# Perform a matrix multiplication between the one-hot vector and the embedding weight matrix\n",
    "first_input_embedding_via_matmul = torch.matmul(first_input_onehot, token_embeddings_weight)\n",
    "# Now, first_token_embedding_via_matmul should be the same as first_token_embedding\n",
    "pprint(first_input_embedding_via_matmul)\n",
    "\n",
    "# Now, first_input_embedding_via_matmul should be the same as first_input_embeddings\n",
    "assert_equal(first_input_embeddings, first_input_embedding_via_matmul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuition 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2163</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0395</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7271</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">SelectBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.2163\u001b[0m,  \u001b[1;36m1.0395\u001b[0m, \u001b[1;36m-0.7271\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mSelectBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hello_weight = token_embeddings_weight[stoi_sorted_by_index['hello']]\n",
    "pprint(hello_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The weight is the embedding of the word (token)\n",
    "\n",
    "Yes, exactly! In the context of the `nn.Embedding` layer in PyTorch, the weights are indeed the embeddings. \n",
    "\n",
    "When you have a sentence and you convert each word/token to its corresponding ID, you use these IDs as indices to lookup in the embedding table (weights). The row from the embedding table (weights) corresponding to the ID is the embedding vector for that word/token. \n",
    "\n",
    "So in essence, the weights in the embedding layer are the learned embeddings for each word/token in your vocabulary. \n",
    "\n",
    "This is a good realization to make as it highlights the fact that in machine learning models, weights are not just random numbers, but they often have interpretable meanings. In this case, the weights are the learned vector representations of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tensor([ 0.2163,  1.0395, -0.7271], grad_fn=<SelectBackward0>)` \n",
    "    - is the weight corresponding to the token (word) `hello` in the vocabulary corpus.\n",
    "    - is also the word/token representation (embedding) of the word `hello` in the vocabulary corpus.\n",
    "    - Why so? Because one-hot encoding ensures that the weight corresponding to the token (word) `hello` is 1 and the weights corresponding to all other tokens (words) are 0. So, the dot product of the one-hot encoded vector of the word `hello` with the embedding matrix will be the weight corresponding to the token (word) `hello` in the vocabulary corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_embeddings = hello_weight "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for the first input setence `Hello bot`, the encoded embedding for \n",
    "this sentence is four tokens (including the special tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2766</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.0947</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4780</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2163</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0395</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7271</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.8569</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4931</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4062</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4378</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4284</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.1996</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">SelectBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.2766\u001b[0m, \u001b[1;36m-1.0947\u001b[0m, \u001b[1;36m-0.4780\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.2163\u001b[0m,  \u001b[1;36m1.0395\u001b[0m, \u001b[1;36m-0.7271\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.8569\u001b[0m,  \u001b[1;36m0.4931\u001b[0m, \u001b[1;36m-1.4062\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.4378\u001b[0m,  \u001b[1;36m0.4284\u001b[0m, \u001b[1;36m-1.1996\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mSelectBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(first_input_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And notice the `[ 0.2163,  1.0395, -0.7271]` weight we got corresponding to `hello`\n",
    "is indeed here, in the output embeddings 2nd row, which is indeed `hello` since\n",
    "the full first sentence is `<SOS> hello bot <EOS>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### am i right to say at this stage, the learnt embeddings are on a token level and not on a sentence level\n",
    "\n",
    "At the embedding stage, the model is learning embeddings for individual tokens and not for entire sentences. \n",
    "\n",
    "Each token in your vocabulary is associated with a unique vector in the embedding space, irrespective of the context in which the token appears. This means that the same word has the same embedding regardless of its position in the sentence or the other words in the sentence. \n",
    "\n",
    "However, it's important to note that this is the initial step in the transformer model. The transformer architecture uses these individual token embeddings and, with its self-attention mechanism, generates context-aware embeddings where the representation of a word does consider its context within a sentence. These context-aware embeddings are what is used for downstream tasks like text classification, named entity recognition, etc. \n",
    "\n",
    "So while the initial embeddings are at the token level, the transformer architecture builds on top of this to generate embeddings that capture the context of a token within a sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intuition (To refine)\n",
    "\n",
    "Emphasize again, that this stage is just the embedding space for individual tokens\n",
    "or words, not the sentence, the output of each input sentence here is just a\n",
    "stack of vectors (learnt word embeddings) for each word in the sentence.\n",
    "\n",
    "Semantically related words such as 'King' and 'Queen' are represented in the vector space such that the geometric relationships between these vectors capture the semantic relationships between the corresponding words. So if you visualize in a 2d space,\n",
    "then king and queen are closer than that of men and women.\n",
    "\n",
    "In the context of Word Embeddings:\n",
    "\n",
    "Word embeddings are the result of mapping words or phrases from the vocabulary to vectors of real numbers. They are capable of capturing the context of a word in a document, semantic and syntactic similarity, and other relevant linguistic patterns. \n",
    "\n",
    "Here is an example to give a clear idea of how word embeddings capture relationships:\n",
    "\n",
    "Let's say we have four words: King, Queen, Man, Woman. After training the word embeddings (like Word2Vec, GloVe, etc.) on a large corpus, these words are mapped into a high-dimensional space. \n",
    "\n",
    "The relationships between these words are captured as vectors in this space. For example, if we subtract the vector for 'Man' from 'King' and add the vector for 'Woman', we get a vector that is very close to the vector for 'Queen'.\n",
    "\n",
    "Mathematically: \n",
    "\n",
    "    Vector('King') - Vector('Man') + Vector('Woman') ≈ Vector('Queen')\n",
    "\n",
    "This is because the word embeddings have learned that 'King' is to 'Queen' what 'Man' is to 'Woman' from the co-occurrence statistics in the training corpus. This is known as an analogy relationship in word embeddings.\n",
    "\n",
    "Remember that while this analogy works well in many cases, it's not always perfect and depends largely on the training data and the quality of the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1. Token (Word) Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_simple = [\n",
    "    (\"hello bot\", \"hello human\"),\n",
    "    (\"hello hello human\", \"hello bot\"),\n",
    "]\n",
    "\n",
    "dataset = ChatDataset(\n",
    "    data=training_data_simple,\n",
    "    tokenizer_type=\"basic_english\",\n",
    "    specials=[\"<SOS>\", \"<EOS>\", \"<PAD>\"],\n",
    ")\n",
    "\n",
    "batch_size = 1\n",
    "dataloader = create_dataloader(\n",
    "    dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "\n",
      "Input IDs:\n",
      "[[0, 3, 4, 1]]\n",
      "\n",
      "Attention Mask:\n",
      "[[1, 1, 1, 1]]\n",
      "\n",
      "Target IDs:\n",
      "[[0, 3, 5, 1]]\n",
      "Batch 2\n",
      "\n",
      "Input IDs:\n",
      "[[0, 3, 3, 5, 1]]\n",
      "\n",
      "Attention Mask:\n",
      "[[1, 1, 1, 1, 1]]\n",
      "\n",
      "Target IDs:\n",
      "[[0, 3, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    print(f\"Batch {i+1}\\n\")\n",
    "    print(\n",
    "        f\"Input IDs:\\n{np.array2string(batch['input_ids'].numpy(), separator=', ')}\\n\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Attention Mask:\\n{np.array2string(batch['attention_mask'].numpy(), separator=', ')}\\n\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Target IDs:\\n{np.array2string(batch['target_ids'].numpy(), separator=', ')}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;SOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;EOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;PAD&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'hello'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'bot'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'human'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32mSOS\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'<EOS>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'<PAD\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'hello'\u001b[0m, \u001b[32m'bot'\u001b[0m, \u001b[32m'human'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;SOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;EOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;PAD&gt;'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'hello'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'bot'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'human'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32mSOS\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'<EOS>'\u001b[0m\u001b[39m: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'<PAD\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m: \u001b[1;36m2\u001b[0m, \u001b[32m'hello'\u001b[0m: \u001b[1;36m3\u001b[0m, \u001b[32m'bot'\u001b[0m: \u001b[1;36m4\u001b[0m, \u001b[32m'human'\u001b[0m: \u001b[1;36m5\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "itos: List[str] = dataset.vocab.get_itos()\n",
    "pprint(itos)\n",
    "\n",
    "V_mathcal = itos # our $\\mathcal{V}$\n",
    "V = len(V_mathcal) # our $V = |\\mathcal{V}|$\n",
    "\n",
    "stoi_sorted_by_index: Dict[str, int] = sort_stoi_by_index(itos)\n",
    "pprint(stoi_sorted_by_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'input_ids'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]])</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'attention_mask'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]])</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'target_ids'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]])</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'input_ids'\u001b[0m: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'attention_mask'\u001b[0m: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'target_ids'\u001b[0m: \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m5\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "pprint(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are interested in only the `input_ids` now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = batch[\"input_ids\"] # our $\\mathbf{X}$\n",
    "pprint(X)\n",
    "pprint(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first `input` is the sentence `hello bot`.\n",
    "- The tokenization is `[0, 3, 4, 1]` which correctly maps to the tokens `[<SOS>, hello, bot, <EOS>]` as per the `stoi` mapping.\n",
    "- The shape is `[1, 4]` because we have one sentence with 4 tokens, where the first token is the `<SOS>` token and the last token is the `<EOS>` token.\n",
    "    - In this context, the first dimension of the shape corresponds to the number of sentences (or samples) in the batch, and the second dimension corresponds to the number of tokens in each sentence.\n",
    "\n",
    "---\n",
    "\n",
    "In mathematical terms, we have:\n",
    "\n",
    "- The vocabulary $\\mathcal{V}$ consists of the unique tokens $\\{\\text{<SOS>, <EOS>, <PAD>, hello, bot, human}\\}$ in our text data. The size of the vocabulary is $V = |\\mathcal{V}| = 6$.\n",
    "\n",
    "- We define a function $f_{\\text{stoi}}$ that maps each token in our vocabulary to a unique integer index. \n",
    "  \n",
    "  $$\n",
    "  \\begin{aligned}\n",
    "  f_{\\text{stoi}}: \\mathcal{V} &\\to \\{0, 1, ..., V-1\\} \\\\\n",
    "  v &\\mapsto f_{\\text{stoi}}(v)\n",
    "  \\end{aligned}\n",
    "  $$\n",
    "  \n",
    "  This function represents the `stoi` mapping in the code. For example, $f_{\\text{stoi}}(\\text{<SOS>}) = 0$, $f_{\\text{stoi}}(\\text{hello}) = 3$, etc.\n",
    "\n",
    "- A sequence of text, such as the sentence \"hello bot\", is represented as a sequence of token indices $X=(x_1, x_2, x_3, x_4)$ using the $f_{\\text{stoi}}$ mapping. For the sentence \"hello bot\", we have:\n",
    "  \n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\mathbf{X} &= (f_{\\text{stoi}}(\\text{<SOS>}), f_{\\text{stoi}}(\\text{hello}), f_{{\\text{stoi}}}(\\text{bot}), f_{\\text{stoi}}(\\text{<EOS>})) \\\\\n",
    "    &= (0, 3, 4, 1)\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "- Although $\\mathbf{X}$ is a sequence, we can consider it to be a vector:\n",
    "\n",
    "    $$\n",
    "    \\mathbf{X} = \\begin{bmatrix} 0 & 3 & 4 & 1 \\end{bmatrix} \\in \\mathbb{Z}^{1 \\times 4}\n",
    "    $$\n",
    "\n",
    "    which correctly corresponds to the shape `[1, 4]` in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to map each token to a vector in a high-dimensional space. The vectors we use are called embeddings. The integer tokens, by themselves, do not carry much information. For example, the word `hello` is tokenized to be $3$, which is an arbitrary integer. In a one-dimensional Euclidean space, the word `hello` and the next word `bot`, indexed by $4$, ***would appear to be very close to each other***. However, if we were to extend the vocabulary to say, a size of $10000$, and the word `bot` is instead indexed at $9999$, then the words `hello` and `bot` ***would appear to be very far from each other***. This means that the model would not recognize any relationship between these two words based on their tokenized integers. To address this, we use embedding vectors. While the initial mapping from words to vectors is arbitrary, during training, the model adjusts these vectors so that words used in similar contexts come to have similar vectors. This allows the model to capture semantic relationships between words, improving its ability to understand and generate meaningful text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "One-hot encoding is a common technique used in machine learning to represent categorical variables, such as our tokens. In the context of Natural Language Processing (NLP), one-hot encoding means representing each token as an $V$-dimensional vector. Each token is represented by a vector of all zeros except for a single one at the position corresponding to its index in the vocabulary. Therefore, the token 'hello' which corresponds to the index 3 in the vocabulary would be represented as the vector `[0, 0, 0, 1, 0, 0]`. This step is necessary as it allows us to convert the tokens into a format that can be processed by the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.<span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.<span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.<span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>., <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.<span style=\"font-weight: bold\">]]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m1\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m1\u001b[0m., \u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m., \u001b[1;36m1\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m., \u001b[1;36m0\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m6\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "O = F.one_hot(X, num_classes=len(dataset.vocab)).float() # our one-hot encoding $\\mathbf{O}$\n",
    "pprint(O)\n",
    "pprint(O.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall our tokenized sequence is `[0, 3, 4, 1]`.\n",
    "- Converting it to one-hot encoding, we get:\n",
    "    - `[1, 0, 0, 0, 0, 0]`\n",
    "    - `[0, 0, 0, 1, 0, 0]`\n",
    "    - `[0, 0, 0, 0, 1, 0]`\n",
    "    - `[0, 1, 0, 0, 0, 0]`\n",
    "- The shape has turned from `[1, 4]` to be broadcasted to `[1, 4, 6]` corresponding to the sequence length and the vocabulary size. Or more concretely `[batch_size, sequence_length, vocab_size]`.\n",
    "- For simplicity sake, you can remove the first dimension, the batch index, and just use `[4, 6]`. The `4` is the sequence length and the `6` is the vocabulary size.\n",
    "\n",
    "---\n",
    "\n",
    "In mathematical terms, we have:\n",
    "\n",
    "Using these notations, we can say that our tokenized sequence $\\mathbf{X}$ is converted to a one-hot matrix $O$ of size $[L, V]$ (or more generally $[N, L, V]$ in the presence of batch size $N$). \n",
    "\n",
    "In the given example, $\\mathbf{X} = [0, 3, 4, 1]$, which is of size $[4]$, and after one-hot encoding, we get $O$, a $[4, 6]$ matrix as follows:\n",
    "\n",
    "$$\n",
    "O = \\begin{bmatrix} 1 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 & 0 & 0 \\end{bmatrix} \\in \\mathbb{R}^{4 \\times 6}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $4$ is the sequence length $L$.\n",
    "- $6$ is the vocabulary size $V$.\n",
    "- Each row is a one-hot vector of the token $x_{\\ell} \\in \\mathbb{R}^{V}$ at position $\\ell$.\n",
    "\n",
    "A minute quirk here is that the token $x_{\\ell}$ exists in the ***continuous space***\n",
    "instead of the ***discrete space***. This is because we have to perform the dot product between the one-hot vector and the embedding vector, which is a continuous vector.\n",
    "\n",
    "The reason is that these spaces will be used in computations that involve real-valued numbers, especially when the models are trained with gradient-based optimization algorithms, where the parameters of the model are updated using real-valued gradients.\n",
    "\n",
    "Therefore, in our code, we also converted the one-hot vector to `.float()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer/Matrix\n",
    "\n",
    "Now, let's introduce the concept of an \"embedding\" which is a way to transform our one-hot encoded vectors into a lower, or sometimes higher, dimensional space that is easier to process by the model and, more importantly, contains meaningful representation of the words. Embedding also helps the model understand the semantic relationships between the words. Each unique word in our vocabulary is associated with an embedding, which is a real-valued vector.\n",
    "\n",
    "Usually, it is a lower dimension because our vocabulary size is in the tens\n",
    "of thousands and the embedding dimension are typically $512$ or $768$.\n",
    "\n",
    "Here, our vocabulary size is $V=6$ and the embedding dimension is $D=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Parameter containing:\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1103</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.6898</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.9890</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9580</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.3221</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8172</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7658</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7506</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.3525</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6863</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.3278</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7950</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">requires_grad</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Parameter containing:\n",
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.1103\u001b[0m, \u001b[1;36m-1.6898\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.9890\u001b[0m,  \u001b[1;36m0.9580\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.3221\u001b[0m,  \u001b[1;36m0.8172\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.7658\u001b[0m, \u001b[1;36m-0.7506\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.3525\u001b[0m,  \u001b[1;36m0.6863\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.3278\u001b[0m,  \u001b[1;36m0.7950\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mrequires_grad\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m6\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed_all(42, seed_torch=True)\n",
    "D = embedding_dim = 2\n",
    "token_emb = nn.Embedding(num_embeddings=len(dataset.vocab), embedding_dim=embedding_dim)\n",
    "nn.init.kaiming_normal_(token_emb.weight)\n",
    "\n",
    "E = embedding_weight_matrix = token_emb.weight # E for embedding weights\n",
    "pprint(E)\n",
    "pprint(E.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use PyTorch's embedding layer to create our embeddings. The embedding layer is simply a linear layer that maps the one-hot encoded input to the embedding space. This layer has a weight matrix, often called the embedding matrix, which is a $V \\times D$ matrix where $V$ is the size of the vocabulary and $D$ is the desired embedding dimension.\n",
    "\n",
    "It is of course randomly initialized, and it is learned during the training of the model. \n",
    "\n",
    "So for now, the embedding matrix is:\n",
    "\n",
    "$$\n",
    "\\mathbf{E} = \\begin{bmatrix} 1.1103 & -1.6898 \\\\ -0.9890 & 0.9580 \\\\ 1.3221 & 0.8172 \\\\ -0.7658 & -0.7506 \\\\ 1.3525 & 0.6863 \\\\ -0.3278 & 0.7950 \\end{bmatrix} \\in \\mathbb{R}^{6 \\times 2}\n",
    "$$\n",
    "\n",
    "This is a $6 \\times 2$ matrix where each row represents the embedding vector of a word in the vocabulary. The matrix is in $\\mathbb{R}^{V \\times D}$, where $V$ is the vocabulary size and $D$ is the embedding dimension. In this case, $V = 6$ and $D = 2$.\n",
    "\n",
    "The element $e_{j, d}$ at the $j$-th row and the $d$-th column is the $d$-th element of the embedding vector of the $j$-th word in the vocabulary.\n",
    "\n",
    "We want to associate each token with a $D$ dimensional vector, as mentioned earlier.\n",
    "This is to associate each token with a more meaningful representation.\n",
    "\n",
    "In our example:\n",
    "\n",
    "- `[1.1103, -1.6898]` is the embedding vector of the token `<SOS>`.\n",
    "- `[-0.9890, 0.9580]` is the embedding vector of the token `<EOS>`.\n",
    "- `[1.3221, 0.8172]` is the embedding vector of the token `<PAD>`.\n",
    "- `[-0.7658, -0.7506]` is the embedding vector of the token `hello`.\n",
    "- `[1.3525, 0.6863]` is the embedding vector of the token `bot`.\n",
    "- `[-0.3278, 0.7950]` is the embedding vector of the token `human`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAK9CAYAAADWhvE6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAACXrElEQVR4nOzdeZyN5f/H8feZwcxYZmzD2Pd9yZLsWQtJSQkpS3ZaRIkWSyoJIfsW6huVClHILkLZCpF9yZ5lxs7M3L8/rp9pzjZmmDlnzszr+XicR811X/d9PucY5j33uRabZVmWAAAAAMTw83YBAAAAQHJDSAYAAAAcEJIBAAAAB4RkAAAAwAEhGQAAAHBASAYAAAAcEJIBAAAAB4RkAAAAwAEhGQAAAHBASAYAJJkjR47IZrNp1qxZ3i4lTkuXLlWFChUUGBgom82mS5cuJenz2Ww2vfTSS0n6HADuDyEZSOGeeOIJpU+fXpcvX3bbp23btkqXLp3Onz+fqM/94YcfasGCBYl6zbjcCWR3Hn5+fsqaNauaNGmijRs3eqwOX/PXX39p8ODBOnLkyD1fY86cORozZkyi1eRJ58+f17PPPqugoCBNmDBBX3zxhTJkyODUL/b3VlyPNWvWeP5FAEh0abxdAICk1bZtWy1atEjz589Xu3btnI5fu3ZNCxcuVOPGjZUtW7ZEfe4PP/xQzzzzjJo3b56o172bNm3a6LHHHlNUVJT27duniRMnql69evr9999Vrlw5j9biC/766y8NGTJEdevWVcGCBe/pGnPmzNGuXbvUu3dvu/YCBQro+vXrSps27f0XmkR+//13Xb58WUOHDlXDhg3d9vviiy/svv7888+1fPlyp/ZSpUolSZ0APIuQDKRwTzzxhDJlyqQ5c+a4DMkLFy7U1atX1bZtWy9Ul3BXr151eZcvtkqVKun555+P+bp27dpq0qSJJk2apIkTJyZ1iXbiU29KZrPZFBgY6O0y4nT27FlJUubMmePsF/t7SpI2bdqk5cuXO7UDSBkYbgGkcEFBQWrRooVWrlwZEwZimzNnjjJlyqQnnnhCknTp0iX17t1b+fLlU0BAgIoWLarhw4crOjra7rzo6GiNHTtW5cqVU2BgoEJDQ9W4cWNt2bJFkglHV69e1ezZs2M+hu7QoUPM+du3b1eTJk0UHBysjBkzqkGDBtq0aZPdc8yaNUs2m01r165Vz549lSNHDuXNmzfB70Ht2rUlSQcPHrRrj89rvTOEY+TIkRo9erQKFCigoKAg1alTR7t27bK7XocOHZQxY0YdPHhQjz32mDJlyhTzy0d0dLTGjBmjMmXKKDAwUDlz5lS3bt108eJFu2ts2bJFjRo1Uvbs2RUUFKRChQrpxRdfdHrv43OtggUL6vHHH9f69ev10EMPKTAwUIULF9bnn39u9x63bNlSklSvXj2nIQMLFy5U06ZNlTt3bgUEBKhIkSIaOnSooqKiYq5Rt25d/fjjjzp69GjM+XfuSLsbk7xq1SrVrl1bGTJkUObMmfXkk09qz549dn0GDx4sm82mAwcOqEOHDsqcObNCQkLUsWNHXbt2TfExb948Va5cWUFBQcqePbuef/55nThxwq729u3bS5KqVKni9H2aUFevXlXfvn1jvqdKlCihkSNHyrKsu577/vvvy8/PT+PGjYtpW7JkScz7lClTJjVt2lS7d++2O+/O992JEyfUvHlzZcyYUaGhoXr99dft/pwk6auvvlLlypWVKVMmBQcHq1y5cho7duw9v14gJeNOMpAKtG3bVrNnz9Y333xjN1nowoULWrZsmdq0aaOgoCBdu3ZNderU0YkTJ9StWzflz59fv/76qwYMGKBTp07ZjTnt1KmTZs2apSZNmqhz586KjIzUL7/8ok2bNunBBx/UF198oc6dO+uhhx5S165dJUlFihSRJO3evVu1a9dWcHCw+vXrp7Rp02rKlCmqW7eu1q5dq6pVq9rV37NnT4WGhmrgwIG6evVqgl//nbG2WbJkiWlLyGuVzEfrly9fVq9evXTjxg2NHTtW9evX186dO5UzZ86YfpGRkWrUqJFq1aqlkSNHKn369JKkbt26adasWerYsaNeeeUVHT58WOPHj9f27du1YcMGpU2bVmfPntWjjz6q0NBQ9e/fX5kzZ9aRI0f0/fff29USn2vdceDAAT3zzDPq1KmT2rdvr88++0wdOnRQ5cqVVaZMGT388MN65ZVX9Omnn+qtt96KGSpw57+zZs1SxowZ1adPH2XMmFGrVq3SwIEDFRERoREjRkiS3n77bYWHh+uff/7R6NGjJUkZM2Z0++exYsUKNWnSRIULF9bgwYN1/fp1jRs3TjVr1tS2bduchnw8++yzKlSokIYNG6Zt27Zp+vTpypEjh4YPHx7nn/ud96hKlSoaNmyYzpw5o7Fjx2rDhg3avn27MmfOrLffflslSpTQ1KlT9d5776lQoUIx36cJZVmWnnjiCa1evVqdOnVShQoVtGzZMr3xxhs6ceJEzHvjyjvvvKMPP/xQU6ZMUZcuXSSZ4R3t27dXo0aNNHz4cF27dk2TJk1SrVq1tH37drv3KSoqSo0aNVLVqlU1cuRIrVixQqNGjVKRIkXUo0cPSdLy5cvVpk0bNWjQIOa927NnjzZs2KBXX331nl4zkKJZAFK8yMhIK1euXFb16tXt2idPnmxJspYtW2ZZlmUNHTrUypAhg7Vv3z67fv3797f8/f2tY8eOWZZlWatWrbIkWa+88orTc0VHR8f8f4YMGaz27ds79WnevLmVLl066+DBgzFtJ0+etDJlymQ9/PDDMW0zZ860JFm1atWyIiMj7/o6Dx8+bEmyhgwZYp07d846ffq09csvv1hVqlSxJFnz5s2L6Rvf13rnmkFBQdY///wT02/z5s2WJOu1116LaWvfvr0lyerfv7/dNX/55RdLkvXll1/atS9dutSuff78+ZYk6/fff3f7GuN7LcuyrAIFCliSrHXr1sW0nT171goICLD69u0b0zZv3jxLkrV69Wqn57t27ZpTW7du3az06dNbN27ciGlr2rSpVaBAAae+d96/mTNnxrRVqFDBypEjh3X+/PmYtj/++MPy8/Oz2rVrF9M2aNAgS5L14osv2l3zqaeesrJly+b0XLHdunXLypEjh1W2bFnr+vXrMe2LFy+2JFkDBw6MabvzfRbX++5Kr169rNg/RhcsWGBJst5//327fs8884xls9msAwcOxLRJsnr16mVZlmX17dvX8vPzs2bNmhVz/PLly1bmzJmtLl262F3r9OnTVkhIiF37ne+79957z65vxYoVrcqVK8d8/eqrr1rBwcHx+rsEwLIYbgGkAv7+/mrdurU2btxot4LBnDlzlDNnTjVo0ECS+Wi6du3aypIli/7999+YR8OGDRUVFaV169ZJkr777jvZbDYNGjTI6blsNluctURFRennn39W8+bNVbhw4Zj2XLly6bnnntP69esVERFhd06XLl3k7+8f79c7aNAghYaGKiwsTLVr19aePXs0atQoPfPMMzF94vta72jevLny5MkT8/VDDz2kqlWr6qeffnJ6/jt37mI/V0hIiB555BG756pcubIyZsyo1atXS/pvTOzixYt1+/Ztl68tvte6o3Tp0jHDTSQpNDRUJUqU0KFDh+LxTprhOndcvnxZ//77r2rXrq1r165p79698bpGbKdOndKOHTvUoUMHZc2aNaa9fPnyeuSRR1y+n927d7f7unbt2jp//rzT90lsW7Zs0dmzZ9WzZ0+7MdFNmzZVyZIl9eOPPya49rv56aef5O/vr1deecWuvW/fvrIsS0uWLLFrtyxLL730ksaOHav//e9/McM+JHPX99KlS2rTpo3dn7O/v7+qVq3q9OcsuX6fYv85Z86cWVevXtXy5csT4+UCKR7DLYBUom3btho9erTmzJmjt956S//8849++eUXvfLKKzEBdP/+/frzzz8VGhrq8hp3xjQfPHhQuXPntgs58XXu3Dldu3ZNJUqUcDpWqlQpRUdH6/jx4ypTpkxMe6FChRL0HF27dlXLli1148YNrVq1Sp9++qnT2Mz4vtY7ihUr5tSnePHi+uabb+za0qRJ4zRuev/+/QoPD1eOHDnifK46dero6aef1pAhQzR69GjVrVtXzZs313PPPaeAgIAEXeuO/PnzO/XJkiWL0/hld3bv3q133nlHq1atcgql4eHh8bpGbEePHpUkt3/+y5Ytc5rs6Pga7gybuXjxooKDgxP8PCVLltT69esTXPvdHD16VLlz51amTJns2u8MXblT0x2ff/65rly5okmTJqlNmzZ2x/bv3y9Jql+/vsvncnzdd+YFxOb459yzZ0998803atKkifLkyaNHH31Uzz77rBo3bpyAVwmkHoRkIJWoXLmySpYsqblz5+qtt97S3LlzZVmW3aoW0dHReuSRR9SvXz+X1yhevLinyrUT+25mfBQrVixmKa/HH39c/v7+6t+/v+rVq6cHH3xQUtK91oCAAPn52X9IFx0drRw5cujLL790ec6dcGOz2fTtt99q06ZNWrRokZYtW6YXX3xRo0aN0qZNm5QxY8Z4X+sOd3fgrXhMJLt06ZLq1Kmj4OBgvffeeypSpIgCAwO1bds2vfnmm06TOZPK/byG5KxmzZrasWOHxo8fr2effdbul8477+0XX3yhsLAwp3PTpLH/8R2fT1py5MihHTt2aNmyZVqyZImWLFmimTNnql27dpo9e/Z9vhog5SEkA6lI27Zt9e677+rPP//UnDlzVKxYMVWpUiXmeJEiRXTlypU414q902/ZsmW6cOFCnHeTXQ29CA0NVfr06fX33387Hdu7d6/8/PyUL1++BLyqu3v77bc1bdo0vfPOO1q6dKmk+L/WO+7c2Ytt37598VpXuEiRIlqxYoVq1qwZr8BfrVo1VatWTR988IHmzJmjtm3b6quvvlLnzp0TfK34cDdEZs2aNTp//ry+//57PfzwwzHthw8fjvc1HBUoUECS3P75Z8+ePVGWzIv9PI53Y//++++Y44mpQIECWrFihS5fvmx3N/nOsBTH5yxatKg+/vhj1a1bV40bN9bKlStjzrszeTBHjhzx/h6Nj3Tp0qlZs2Zq1qyZoqOj1bNnT02ZMkXvvvuuihYtmmjPA6QEjEkGUpE7d40HDhyoHTt2OK2N/Oyzz2rjxo1atmyZ07mXLl1SZGSkJOnpp5+WZVkaMmSIU7/Yd/cyZMjgtL2vv7+/Hn30US1cuNBufPSZM2c0Z84c1apVy+1H6Pcqc+bM6tatm5YtW6YdO3ZIiv9rvWPBggV2S4f99ttv2rx5s5o0aXLX53/22WcVFRWloUOHOh2LjIyMeY8uXrzodHe0QoUKkqSbN28m6FoJcSeUuvqzkuz/TG/duuVyrekMGTLEa/hFrly5VKFCBc2ePdvu+Xbt2qWff/5Zjz32WILrd+XBBx9Ujhw5NHny5Jj3TjJLqu3Zs0dNmzZNlOeJ7c4GNuPHj7drHz16tGw2m8vvlfLly+unn37Snj171KxZM12/fl2S1KhRIwUHB+vDDz90OT793LlzCa7PcUdNPz8/lS9fXpLs3iMABneSgVSkUKFCqlGjhhYuXChJTiH5jTfe0A8//KDHH388Zpmwq1evaufOnfr222915MgRZc+eXfXq1dMLL7ygTz/9VPv371fjxo0VHR2tX375RfXq1YtZZq5y5cpasWKFPvnkE+XOnVuFChVS1apV9f7772v58uWqVauWevbsqTRp0mjKlCm6efOmPv744yR57a+++qrGjBmjjz76SF999VW8X+sdRYsWVa1atdSjRw/dvHlTY8aMUbZs2dwO14itTp066tatm4YNG6YdO3bo0UcfVdq0abV//37NmzdPY8eO1TPPPKPZs2dr4sSJeuqpp1SkSBFdvnxZ06ZNU3BwcEx4jO+1EqJChQry9/fX8OHDFR4eroCAANWvX181atRQlixZ1L59e73yyiuy2Wz64osvXA5zqFy5sr7++mv16dNHVapUUcaMGdWsWTOXzzdixAg1adJE1atXV6dOnWKWgAsJCdHgwYMTVLs7adOm1fDhw9WxY0fVqVNHbdq0iVkCrmDBgnrttdcS5Xlia9asmerVq6e3335bR44c0QMPPKCff/5ZCxcuVO/evd0uLVetWjUtXLhQjz32mJ555hktWLBAwcHBmjRpkl544QVVqlRJrVu3VmhoqI4dO6Yff/xRNWvWdArjd9O5c2dduHBB9evXV968eXX06FGNGzdOFSpUYJdAwBVvLasBwDsmTJhgSbIeeughl8cvX75sDRgwwCpatKiVLl06K3v27FaNGjWskSNHWrdu3YrpFxkZaY0YMcIqWbKklS5dOis0NNRq0qSJtXXr1pg+e/futR5++GErKCjIkmS3HNy2bdusRo0aWRkzZrTSp09v1atXz/r111/takno0lx3lhsbMWKEy+MdOnSw/P39Y5biis9rjX3NUaNGWfny5bMCAgKs2rVrW3/88Yfd9du3b29lyJDBbX1Tp061KleubAUFBVmZMmWyypUrZ/Xr1886efJkzHvSpk0bK3/+/FZAQICVI0cO6/HHH7e2bNmS4GtZllkCrmnTpk7n1qlTx6pTp45d27Rp06zChQtb/v7+dsvBbdiwwapWrZoVFBRk5c6d2+rXr5+1bNkypyXjrly5Yj333HNW5syZLUkxy8G5WgLOsixrxYoVVs2aNa2goCArODjYatasmfXXX3/Z9bmzBNy5c+fs2u98Xxw+fNjte33H119/bVWsWNEKCAiwsmbNarVt29ZuKb/Y17vfJeAsy3xPvfbaa1bu3LmttGnTWsWKFbNGjBhhtzSiZdkvAXfHwoULrTRp0litWrWyoqKiLMuyrNWrV1uNGjWyQkJCrMDAQKtIkSJWhw4d7L4n3H3f3Xn/7vj222+tRx991MqRI4eVLl06K3/+/Fa3bt2sU6dOJeh1A6mFzbJ8fOYDACShI0eOqFChQhoxYoRef/11b5cDAPAQxiQDAAAADgjJAAAAgANCMgAAAODAZ0LysGHDVKVKFWXKlEk5cuRQ8+bNXa6z6WjevHkqWbKkAgMDVa5cOZdbngKAOwULFpRlWYxHBoBUxmdC8tq1a9WrVy9t2rRJy5cv1+3bt/Xoo4/q6tWrbs/59ddf1aZNG3Xq1Enbt29X8+bN1bx5c+3atcuDlQMAAMDX+OzqFufOnVOOHDm0du1au52gYmvVqpWuXr2qxYsXx7RVq1ZNFSpU0OTJkz1VKgAAAHyMz24mcmdnp7i2xN24caP69Olj19aoUSMtWLDA7Tk3b96023koOjpaFy5cULZs2eK97SoAAAA8x7IsXb58Wblz55afX+IMlPDJkBwdHa3evXurZs2aKlu2rNt+p0+fVs6cOe3acubMqdOnT7s9Z9iwYS632gUAAEDydvz4ceXNmzdRruWTIblXr17atWuX1q9fn+jXHjBggN3d5/DwcOXPn1/Hjx9XcHBwoj8fAAAA7k9ERITy5cunTJkyJdo1fS4kv/TSS1q8eLHWrVt3198UwsLCdObMGbu2M2fOKCwszO05AQEBCggIcGoPDg4mJAMAACRjiTk01mdWt7AsSy+99JLmz5+vVatWqVChQnc9p3r16lq5cqVd2/Lly1W9evWkKhMAAAApgM/cSe7Vq5fmzJmjhQsXKlOmTDHjikNCQhQUFCRJateunfLkyaNhw4ZJkl599VXVqVNHo0aNUtOmTfXVV19py5Ytmjp1qtdeBwAAAJI/n7mTPGnSJIWHh6tu3brKlStXzOPrr7+O6XPs2DGdOnUq5usaNWpozpw5mjp1qh544AF9++23WrBgQZyT/QAAAACfXSfZUyIiIhQSEqLw8HDGJAMAACRDSZHXfOZOMgAAAOAphGQAAADAASEZAAAAcEBIBgAAABwQkgEAAAAHhGQAAADAASEZAAAAcEBIBgAAABwQkgEAAAAHhGQAAADAASEZAAAAcEBIBgAAABwQkgEAAAAHhGQAAADAASEZAAAAcEBIBgAAABwQkgEAAAAHhGQAAADAASEZAAAAcEBIBgAAABwQkgEAAAAHhGQAAADAASEZAAAAcEBIBgAAABwQkgEAAAAHhGQAAADAASEZAAAAcEBIBgAAABwQkgEAAAAHhGQAAADAASEZAAAAcEBIBgAAABwQkgEAAAAHhGQAAADAASEZAAAAcEBIBgAAABwQkgEAAAAHhGQAAADAASEZAAAAcEBIBgAAABwQkgEAAAAHhGQAAADAASEZAAAAcEBIBgAAABwQkgEAAAAHhGQAAADAASEZAAAAcEBIBgAAABwQkgEAAAAHhGQAAADAASEZAAAAcEBIBgAAABwQkgEAAAAHhGQAAADAASEZAAAAcEBIBgAAABwQkgEAAAAHhGQAAADAASEZAAAAcEBIBgAAABwQkgEAAAAHhGQAAADAASEZAAAAcEBIBgAAABwQkgEAAAAHhGQAAADAgU+F5HXr1qlZs2bKnTu3bDabFixYEGf/NWvWyGazOT1Onz7tmYIBAADgk3wqJF+9elUPPPCAJkyYkKDz/v77b506dSrmkSNHjiSqEAAAAClBGm8XkBBNmjRRkyZNEnxejhw5lDlz5sQvCAAAACmST91JvlcVKlRQrly59Mgjj2jDhg1x9r1586YiIiLsHgAAAEhdUnRIzpUrlyZPnqzvvvtO3333nfLly6e6detq27Ztbs8ZNmyYQkJCYh758uXzYMUAAABIDmyWZVneLuJe2Gw2zZ8/X82bN0/QeXXq1FH+/Pn1xRdfuDx+8+ZN3bx5M+briIgI5cuXT+Hh4QoODr6fkgEAAJAEIiIiFBISkqh5zafGJCeGhx56SOvXr3d7PCAgQAEBAR6sCAAAAMlNih5u4cqOHTuUK1cub5cBAACAZMyn7iRfuXJFBw4ciPn68OHD2rFjh7Jmzar8+fNrwIABOnHihD7//HNJ0pgxY1SoUCGVKVNGN27c0PTp07Vq1Sr9/PPP3noJAAAA8AE+FZK3bNmievXqxXzdp08fSVL79u01a9YsnTp1SseOHYs5fuvWLfXt21cnTpxQ+vTpVb58ea1YscLuGgAAAIAjn5245ylJMRAcAAAAiScp8lqqG5MMAAAA3A0hGQAAAHBASAYAAAAcEJIBAAAAB4RkAAAAwAEhGQAAAHBASAYAAAAcEJIBAAAAB4RkAAAAwAEhGQAAAHBASAYAAAAcEJIBAAAAB4RkAAAAwAEhGQAAAHBASAYAAAAcEJIBAAAAB4RkAAAAwAEhGQAAAHBASAYAAAAcEJIBAAAAB4RkAAAAwAEhGQAAAHBASAYAAAAcEJIBAAAAB4RkAAAAwAEhGQAAAHBASAYAAAAcEJIBAAAAB4RkAAAAwAEhGQAAAHBASAYAAAAcEJIBAAAAB4RkAAAAwAEhGQAAAHBASAYAAAAcEJIBAAAAB4RkAAAAwAEhGQAAAHBASAYAAAAcEJIBAAAAB4RkAAAAwAEhGQAAAHBASAYAAAAcEJIBAAAAB4RkAAAAwAEhGQAAAHBASAYAAAAcEJIBAAAAB4RkAAAAwAEhGQAAAHBASAYAAAAcEJIBAAAAB4RkAAAAwAEhGQAAAHBASAYAAAAcEJIBAAAAB4RkAAAAwAEhGQAAAHBASAYAAAAcEJIBAAAAB4RkAAAAwAEhGQAAAHBASAYAAAAcEJIBAAAAB4RkAAAAwAEhGQAAAHDgUyF53bp1atasmXLnzi2bzaYFCxbc9Zw1a9aoUqVKCggIUNGiRTVr1qwkrxMAAAC+zadC8tWrV/XAAw9owoQJ8ep/+PBhNW3aVPXq1dOOHTvUu3dvde7cWcuWLUviSgHPstlsLh9fffVVTJ+oqCiNHj1a5cqVU2BgoLJkyaImTZpow4YNdteKiorSRx99pJIlSyooKEhZs2ZV1apVNX36dE+/LAAAvCaNtwtIiCZNmqhJkybx7j958mQVKlRIo0aNkiSVKlVK69ev1+jRo9WoUaOkKhPwiIsXLypt2rTKmDGjJGnmzJlq3LixXZ/MmTNLkizLUuvWrbVixQqNGDFCDRo0UEREhCZMmKC6detq3rx5at68uSRpyJAhmjJlisaPH68HH3xQERER2rJliy5evBhz3ZMnTypHjhxKk8an/gkBACDeUvRPuI0bN6phw4Z2bY0aNVLv3r3dnnPz5k3dvHkz5uuIiIikKg9IsMjISC1btkyzZs3SokWLtHnzZj3wwAOSTCAOCwtzed4333yjb7/9Vj/88IOaNWsW0z516lSdP39enTt31iOPPKIMGTLohx9+UM+ePdWyZcuYfnee445p06Zp0qRJev7559W+fXuVK1cuCV4tAADe41PDLRLq9OnTypkzp11bzpw5FRERoevXr7s8Z9iwYQoJCYl55MuXzxOlAnHauXOn+vbtq7x586pdu3YKDQ3V6tWrncKrO3PmzFHx4sXtAvIdffv21fnz57V8+XJJUlhYmFatWqVz5865vd6bb76psWPHas+ePapUqZIqVaqkTz/9NM5zAADwJSk6JN+LAQMGKDw8POZx/Phxb5eEVOr8+fMaO3asKlWqpAcffFCHDh3SxIkTderUKU2cOFHVq1e369+mTRtlzJjR7nHs2DFJ0r59+1SqVCmXz3Onfd++fZKkTz75ROfOnVNYWJjKly+v7t27a8mSJXbnBAYGqlWrVvrxxx914sQJtWvXTrNmzVKePHnUvHlzzZ8/X5GRkYn9lgAA4DEperhFWFiYzpw5Y9d25swZBQcHKygoyOU5AQEBCggI8ER5QJzGjRunIUOGqHbt2jpw4MBdP9UYPXq00/Ci3Llzx/y/ZVnxet7SpUtr165d2rp1qzZs2BCzqkyHDh1cTt7LkSOHevfurd69e2vJkiXq0KGDFi5cqO3bt6tChQrxek4AAJKbFH0nuXr16lq5cqVd2/Lly53uwAHJUdeuXTV06FCdPn1aZcqUUceOHbVq1SpFR0e77B8WFqaiRYvaPe5MrCtevLj27Nnj8rw77cWLF49p8/PzU5UqVdS7d299//33mjVrlmbMmKHDhw87nX/58mXNnDlT9evXV7NmzVS2bFnNnj1bpUuXvt+3AAAAr/GpkHzlyhXt2LFDO3bskGSWeNuxY0fMR8oDBgxQu3btYvp3795dhw4dUr9+/bR3715NnDhR33zzjV577TVvlA8kSO7cufXOO+9o3759Wrp0qdKlS6cWLVqoQIEC6t+/v3bv3h3va7Vu3Vr79+/XokWLnI6NGjVK2bJl0yOPPOL2/DuB9+rVq5LMMnFLlizRc889p5w5c+qjjz5SgwYNdOjQIa1cuVLt2rVTunTpEviKAQBIRiwfsnr1akuS06N9+/aWZVlW+/btrTp16jidU6FCBStdunRW4cKFrZkzZyboOcPDwy1JVnh4eOK8COA+XL9+3Zo7d67VqFEjy9/f3/rzzz8ty7IsSdbMmTOtU6dO2T2uXLliWZZlRUdHW0899ZSVJUsWa/r06dbhw4etP/74w+ratauVJk0aa/78+THP8fTTT1uffPKJtWnTJuvIkSPW6tWrrWrVqlnFixe3bt++bVmWZb333ntWSEiI1bVrV2vDhg0efx8AAIgtKfKazbLiOVAxlYqIiFBISIjCw8MVHBzs7XKQGpw6Ja1eLbVqJfn7u+128uRJZcyYUcHBwbLZbC77DBs2TP3795dklo8bM2aMZs2apf379yswMFDVq1fXu+++q5o1a8acM23aNM2dO1e7du1SeHi4wsLCVL9+fQ0ePFgFChSQJB05ckRhYWEKDAxMxBcOAMC9SYq8Rki+C0IyvKJ1a2nnTum996SnnpL8fGpkFAAAHpUUeY2fvEBy9NFH0sGD0jPPSJUrS4sXS/w+CwCAxxCSgeSoYEHpzgTTHTukZs2k6tWl5csJywAAeAAhGUiuBgyQQkP/+3rzZunRR6W6daV167xWFgAAqQEhGUiugoOloUOd29etk+rUMYF582bP1wUAQCpASAaSs06dpDJlXB9bvlyqVs0Mxdi+3bN1AQCQwhGSgeQsTRpp1Ki4+yxeLFWqZCb5JWCDEQAA4B4hGUjuGjWSmjS5e7/vvpPKlZPatpX270/6ugAASMEIyYAvGDkyzo1FYliWNGeOVKqUGapx5EiSlwYAQEpESAZ8QenSUteu8e8fFSV99plUvLjUs6d04kTS1QYAQApESAZ8xZAhZsWLhLh924xZnj7d/D8AAIgXQjLgK0JDpXfeiV/fLFmkbt2ktWvNkItBg6S0aZO0PADAf2w2W8wjJCRENWvW1KpVq+z6/PPPP0qXLp3Kli1712tkyJBBxYoVU4cOHbR161ZPvIRUj5AM+JKXX5YKFbp7v/LlzTjmhx+W/PhrDgCecPHiRV25ciXm65kzZ+rUqVPasGGDsmfPrscff1yHDh2KOT5r1iw9++yzioiI0GY3697fucbu3bs1YcIEXblyRVWrVtXnn38e0+fGjRs6d+5c0r2wVIqfnoAvCQyUhg+/e7+1a81mI+HhSV8TAKRikZGR+vHHH9WyZUvlypVLBw8ejDmWOXNmhYWFqWzZspo0aZKuX7+u5cuXS5Isy9LMmTP1wgsv6LnnntOMGTNcXv/ONQoWLKhHH31U3377rdq2bauXXnpJFy9elCSdOXNGefLkUfPmzTV//nzdZnhdoiAkA77mmWekGjXu3m/jRqlBA+n8+aSvKRWpW7euevfu7e0yAHjZzp071bdvX+XNm1ft2rVTaGioVq9erQceeMBl/6CgIEnSrVu3JEmrV6/WtWvX1LBhQz3//PP66quvdPXq1Xg992uvvabLly/HBO4CBQpo48aNKlCggLp166ZcuXLplVdeYVjGfSIkA77GZpNGj7ZvS5tWqlXLue/WrVK9etKZM56pDQBSsPPnz2vs2LGqVKmSHnzwQR06dEgTJ07UqVOnNHHiRFWvXt3ledeuXdM777wjf39/1alTR5I0Y8YMtW7dWv7+/ipbtqwKFy6sefPmxauOkiVLSpKOxFrms3Llyho7dqxOnjwZM0SjZs2aKleunEaOHKkz/BxIMEIy4IseeshsGnJHvXrSsmVmiIWjnTulOnVYBg4A7tO4cePUu3dvZcyYUQcOHND8+fPVokULpUuXzmX/Nm3aKGPGjMqUKZO+++47zZgxQ+XLl9elS5f0/fff6/nnn4/p+/zzz7sdcuHIsixJZmKfozRp0qhZs2aaN2+eDh8+rLCwML3xxhsaNmzYPbzi1I2QDPiqDz80Y5QlqXlzKX166YcfpCeecO77999mEh+biySK6Oho9evXT1mzZlVYWJgGDx4sydzVsdls2rFjR0zfS5cuyWazac2aNZKkNWvWyGazadmyZapYsaKCgoJUv359nT17VkuWLFGpUqUUHBys5557TteuXYu5ztKlS1WrVi1lzpxZ2bJl0+OPP2439vHOc3///feqV6+e0qdPrwceeEAbN270xFsCpApdu3bV0KFDdfr0aZUpU0YdO3bUqlWrFB0d7bL/6NGjtWPHDp0+fVqnT59W+/btJUlz5szRjRs3VLVqVaVJk0Zp0qTRm2++qfXr12vfvn13rWPPnj2SpEIuJnJblqV169apS5cuKlWqlA4cOKCBAweqT58+9/HKUydCMuCr8ueX+vY1/38nGAcESN9+K7Vs6dz/0CETlNmy+r7Nnj1bGTJk0ObNm/Xxxx/rvffeixkbGF+DBw/W+PHj9euvv+r48eN69tlnNWbMGM2ZM0c//vijfv75Z40bNy6m/9WrV9WnTx9t2bJFK1eulJ+fn5566imnH85vv/22Xn/9de3YsUPFixdXmzZtFBkZmSivG0jtcufOrXfeeUf79u3T0qVLlS5dOrVo0UIFChRQ//79tXv3brv+YWFhKlq0qEJDQ+3aZ8yYob59+2rHjh0xjz/++EO1a9fWZ599dtc6xowZo+DgYDVs2DCmbd++fXr33XdVuHBhNW3aVJGRkVqwYIEOHTqkIUOGKH/+/InzJqQmFuIUHh5uSbLCw8O9XQrgLCLCsp54wrn99m3LatfOssxG1faPsDDL2r3b87WmEHXq1LFq1apl11alShXrzTfftA4fPmxJsrZv3x5z7OLFi5Yka/Xq1ZZlWdbq1astSdaKFSti+gwbNsySZB08eDCmrVu3blajRo3c1nHu3DlLkrVz507LsqyY554+fXpMn927d1uSrD179tzPSwYQh+vXr1tz5861GjVqZPn7+1t//vmnZVmWJcmaP3++U//t27e7/Xs5ceJEKywszLp9+3bMNWbOnGmdOnXKOnLkiPXzzz9bTz/9tOXv7299+eWXMecdPXrU8vPzs+rXr2/Nnj3bunLlStK82GQsKfIad5IBX5Ypk/TFF87tadJIM2eaDUUcnT5txijHGhKAhClfvrzd17ly5dLZs2fv+Ro5c+ZU+vTpVbhwYbu22Nfcv3+/2rRpo8KFCys4OFgFCxaUJB07dsztdXPlyiVJCa4NgIP/X5HClcDAQLVu3VpLly7VsWPHVKBAgTgvNWPGDJUuXTpm8l1sTz31lM6ePauffvoppq1jx47KlSuXSpYsqR49eihjxoz67bff9Nxzz8X0yZ49uw4fPqyVK1eqXbt2ypAhwz28SDhK4+0CANwnd1tV+/lJkyaZcctjx9of+/ff/yb7PfRQ0teYwqR12L3QZrMpOjpafv+/cYv1/5NqJLldrzT2NWw2m9tr3tGsWTMVKFBA06ZNU+7cuRUdHa2yZcvGLCfl7rqS3I6XBODGzZvSpk3SihVm3fl335UeeeSup+XOnTvm/2P/OxBb7GFUjsLCwhQVFXXXazhKnz49wymSACEZSMnuLBeXPr3kOLP50iWpYUPpxx+l2rW9Ul5Kc2fc4alTp1SxYkVJspvEd6/Onz+vv//+W9OmTVPt//+zWr9+/X1fF8D/i442KwGtWGEe69ZJdybOfvZZvAIyUh5CMpDS2WxmJYz06c3dkNguX5YaN5YWLjSBGfclKChI1apV00cffaRChQrp7Nmzeuedd+77ulmyZFG2bNk0depU5cqVS8eOHVP//v0ToWIgFTtyRFq50oTilSslV9s6Dx0qdezo8dKQPDAmGUgt3nlHGjnSuf3aNenxx80dZcTtzz/v2uWzzz5TZGSkKleurN69e+v999+/76f18/PTV199pa1bt6ps2bJ67bXXNGLEiPu+LpCqnD9vVv/p3l0qWlQqVEjq3Fn66ivXAblbN+nttz1fJ5INmxXfAS+pVEREhEJCQhQeHq5gd2M/AV8ycaLUq5dze9q00ty50tNPe74mX9G4sTRunFSsmLcrAXA3169LGzb8N4Ri2zazxk98PPmk9N13kr9/0taIRJMUeY3hFkBq07OnFBQkdepk/wPj9m2pVStp9mz73fzwn6Agsyb1pk1SSIi3qwEQW1SUCcJ3hlCsX28m4CVU9erSnDkEZBCSgVSpY0ez6sULL5gfLHdERZm2GzdMiIa93LmlBQuk554zuxvyQxRIPvbuNTcBtmy592uUKCEtWmTmcCDVY0wykFq1aSPNm2eGWcRmWWac3vjx3qkrOcuTx/z3p58YqwgkN2XKmE95Pv3UrCGfUGFh0tKlUrZsiV8bfBIhGUjNnnrKrGwRGOh87OWXJSaH2Yu1BqqGDzcfyQJIPvz9zb9de/ZITZrE/7xMmaQlS6T/36QHkAjJAJo0MStbuPp4sV8/aciQ+E92Selih2TJDEm5n492ASQ+yzJjkn//PX7906aV5s+XKlRI0rLgewjJAKT69aWff3b9EeXgwdKAAQRlyTkk37ghNW9utvoG4H27d0t160odOpidReNj1iypQYN7fsq6deuqd+/e93w+ki9CMgCjZk0zKzxLFudjw4dLr75qdqVKzRxDsiSdOCG1aHFvs+gBJI6rV6X+/c3d4HXr4n/eiBFmIq4XzZo1S5kzZ/ZqDXCNkAzgP1WqSKtXS/+/vbKdcePM4vqxV8NIbbJkkQICnNs3bjSz6rnbDnjeDz+YSXvDh0uRka77pEvn3Pbqq1LfvklbG3waIRmAvQcekNaulXLlcj42fbr5GNPdD6KUzmb7b4ULR599Zn6RAOAZx46Z4U5PPikdPeq6T6lS0po1Up069u3PPit98on5O50IIiMj9dJLLykkJETZs2fXu+++qzt7tV28eFHt2rVTlixZlD59ejVp0kT79++XJK1Zs0YdO3ZUeHi4bDabbDabBg8enCg14f4RkgE4K1XKfGSZP7/zsf/9T2rdWrp1y/N1JQeuhlzc0aePGbICIOncvi19/LH5d2rhQtd9goKkYcOkHTtMQM6R479jdeqYTZP8Ei8CzZ49W2nSpNFvv/2msWPH6pNPPtH06dMlSR06dNCWLVv0ww8/aOPGjbIsS4899phu376tGjVqaMyYMQoODtapU6d06tQpvf7664lWF+4Pm4kAcK1oUROUGzSQDh60P/bdd2b76nnzXC8fl5LFFZKjoqSWLc2s+iJFPFcTkFr88ovUo4eZoOfO44+bT3ViL+eWM6f5b9myZkOgRP53K1++fBo9erRsNptKlCihnTt3avTo0apbt65++OEHbdiwQTVq1JAkffnll8qXL58WLFigli1bKiQkRDabTWFhYYlaE+4fd5IBuFeggAnKJUs6H1u8WGrWzEyYSU3iCsmSdPGi+fj38mXP1AOkBv/+K734ovTww+4Dct68Zim3H35wXu84Rw5zfMkSKQkmyVWrVk22WEM3qlevrv379+uvv/5SmjRpVLVq1Zhj2bJlU4kSJbRnz55ErwOJi5AMIG65c5sxyuXLOx9bscKss5yaAuHdQrJkfog//zyrgQD3KzrazIUoUUKaOdN1H39/6fXXzQYizZu7HmdcooTZTS9v3iQtFykLIRnA3eXIYVa9ePBB52O//CI98oi5g5oaxCckS+Zu1sCBSVsLkJL9+adUu7bUpYt04YLrPjVrStu3m6XcMmZ0f63mzc0KGElk8+bNdl9v2rRJxYoVU+nSpRUZGWl3/Pz58/r7779VunRpSVK6dOkUlZpXDUrGCMkA4idrVnPnuGZN52ObN5sNSc6d83xdnuZudQtXPvhA+uabpKsFSImuXDF3hitVkn791XWfbNmkGTPMcLBy5TxbnwvHjh1Tnz599Pfff2vu3LkaN26cXn31VRUrVkxPPvmkunTpovXr1+uPP/7Q888/rzx58ujJJ5+UJBUsWFBXrlzRypUr9e+//+ratWtefjW4g5AMIP5CQsxHlvXrOx/bscPsdHXqlKer8qz43km+o0MHc6cLQNwsS/r+e7NqxahR7tdk79RJ2rvXjFFOxBUq4vTvv9KqVdLYsVLnzmbi8vHjMYfbtWun69ev66GHHlKvXr306quvqmvXrpKkmTNnqnLlynr88cdVvXp1WZaln376SWnTppUk1ahRQ927d1erVq0UGhqqjz/+2DOvCXdlsyxWv49LRESEQkJCFB4eruDgYG+XAyQP16+bHxJLljgfK1bMLIOWL5/n6/KEy5elu/1bUKqU2bzgwgXzyJbNrHgRexkqAP85fFh66SXpp5/c9ylbVpo82fWnWYnlxg0ztnnnTjPc485/Y289X7iw+VStUKGkqwMJlhR5jSXgACRcUJCZRd66tVlOKbb9+80M9JUrzQ+TlCZTJvOIa7JivXrShAn/fX3zJrvxAa7cuiWNHCkNHWoCqivp00tDhpgd8v7/7ut9i442G5Ds3GkfiPfti3tX0dKlpeXLE/6JEnwSd5LvgjvJQBxu35batZO++sr5WJ48JiiXKOH5upJayZLS33+b/8+XTzp71gThOzJmlE6eNGEagGtr1pg1j/fudd+neXMzxMHVxkbxdfHif2H4TiDetSvhq/JUqiQtWyZlz37vtSDJJEVeY0wygHuXNq3Zga9DB+djJ06YO8o7d3q8rCR35y5ScLD5eLh1a/vjV66Y9wWAs7NnzS/X9eq5D8gFCkiLFplPrO41IF+6ZOZPZM1qdtl76SVpyhRp48aEB+SaNc2YZAJyqkJIBnB//P3NLPMePZyPnT1rJvNt2+bxspJUnjxSmjRm58GyZaVevZz7TJjAEAsgtuhoM6a4RAnpiy9c90mTRhowQPrrL7Nz3v3InNkMB2vV6v6u88gj5g5ySMj9XQc+h5AM4P75+ZlQ2KeP87ELF8zdnI0bPV9XUsmdW5o6VWrY0HxdpYrzGtK7d5s1pAGYFV6qVze/TF+65LpPnTrSH39IH35oxiEnhuBgae5ccwc5ICDh5zdvbu5oZ8iQOPXApxCSASQOm81MwHnnHedj4eHmbsyaNR4vK0n07i117Gjf1rOnc7+JEz1SDpBsRUSYCXcPPij99pvrPqGh0uzZZsOi/99gI1HZbFKbNgm/o9y2rVnn/F7CNVIEQjKAxGOzmVnqH3zgfOzqVbOF9bJlnq8rseXK5dzWqpWUJYt923ff2S8dBaQWlmUCZsmS0qefut6i3WaTunUz45LbtXO9nfT9unJFGj7cLNf2+efxP697d9M/sVbTgE8iJANIfG+9JY0e7dx+44b0xBNmy+aUJn16s7lBbJGR0vTp3qkH8JYDB6TGjc0vju42F3rgAbOb3uTJZmJdYrt61WxVXaiQ1L+/dP58/M99/XXzKZCnNipBssV3AICk0bu3+QHo6NYtsxFJStyuuXt357YpU0xYBlK6mzel994zk1l//tl1n4wZzS/QW7ZI1aolfg3Xrpnd+goXlvr1MzvlJcR770kff5w0d7XhcwjJAJJOt27SrFnOd2QiI80YwYR8/OkLihaVGjWyb/vnHzPxB0jJVqyQypWTBg2yXzM8tmeeMUMrevc2q1gkpuvXTfguXNjcCT571n1fd+F89Gjp3XcJyIhBSAaQtNq3N7PLHX8oRkebY1OmeKeupMIEPqQmp06ZX3gfecTstulK4cJmC/t588zyiYnp+nWz2UjhwmZ1nTNn3PetX9+sOPP99/btNps0bZoJ70AshGQASe/ZZ6Vvv5XSpXM+1r27NGaMx0tKMk2bOm9+sGLFfzv0ASlBVJQ0fryZmOdqx03J/H1/912zu13jxon7/DduSOPGSUWKmHAb1wTZunWltWvNDqC1atnfKU6TxvwS37lz4taHFIGQDMAznnzSTNgLDHQ+9tpr0rBhnq8pKfj7ux6bPGmS52sBksKWLVLVqtLLL5sl3lxp0MBs//zee1JQUOI9982b5pOZokWlV15xPzFQMjt+rl5tHg8/7Hw8IMDcVb7fzUaQYhGSAXhOo0bmY1dXC/O/9Za565QSdqnr1Ml56ahZs8yMe8BXXbpkdpd86CFp61bXfXLmlL78Ulq+3Oysl1hu3TITgYsVMzWcOOG+b61a5q7xmjXmLrIjm838G/Tjj1KzZolXI1IcQjIAz6pb1/wADQ52Pvb++9Ibb/h+UM6RQ2rZ0r4tPNx8rAv4GssywbdkSXMX19XfT5vNhNe9e6Xnnku8yW+3bpndLYsVM7v1HT/uvm+NGubflnXrzPhjdzUEBpp+DRokTo1IsQjJADyvenVp1SrX66OOGiW99JLrzQd8iasJfBMm+P4vAEhd/v7bbL/+/PPuJ8VVrixt3mzGKGfOnDjPe/u2WWO8eHGzSs6xY+77Vq0qLV0qrV9var1bQA8JMf8GAXdBSAbgHZUrm49Dc+RwPjZxoplIExXl8bISTY0aZsOE2HbskDZt8ko5QIJcvy4NHCiVL29+oXUlONhMntu8WapSJXGe9/Zt6bPPzFCNLl2ko0fd961SRfrpJ2njRjOUi6XbkMgIyQC8p1w589Goq2WhZs40d69u3/Z8XYnBZmM5OPimpUvNhiBDh5rhDq60aWOGVrz0kpmser8iI824/VKlzJj+w4fd933wQWnxYhPOmzQhHCPJEJIBeFeJEiYoFyzofOyrr8zMc3ebEyR3zz3nPPb6m2+kc+e8Uw8QlxMnzFj6Jk2kQ4dc9ylWzIznnTNHypXr/p8zMtJsKlSqlNSxo3TwoPu+lSqZFXJ++80stUg4RhIjJAPwvsKFTVAuWtT52Pz50lNPmY9/fU3GjGbDlNhu3TIfJwPJRWSk2W2uZEmznrkrAQHSkCFmWbeGDe//OaOipP/9TypTxvwdOXDAfd8KFaQFC8zSc82aEY7hMYRkAMlDvnwmKJcu7XxsyRJz5+jKFc/Xdb9cDbmYNMm3x1sj5di0yQxf6NPH/d+vRo3MhiADB7pe5zwhoqLMXegyZaQXXpD27XPft3x5s47x1q1mnXXCMTyMkAwg+ciVy0zmq1DB+djq1WbXrvBwT1d1f0qWNMtRxXb0qAn+gLdcuGBWjahRQ/rjD9d9cuUyw4OWLHH9KU9CREdLX39t5iG0bRv3DpRly5o72tu3m0+R/Igq8A6+8wAkL6GhZjb9Qw85H9uwwXzUe+GC5+u6H0zgQ3JhWWYMcMmSZv1hV0sS+vlJr75qJua1bHl/d3Cjo6V588xd4datpT173PctU8aE8j/+kJ5+mnAMr/O578AJEyaoYMGCCgwMVNWqVfXbb7+57Ttr1izZbDa7R+D9flQEIOllyWImB9Wu7XxsyxapXj3p7FnP13WvnnhCyp3bvm3p0rgnKQGJ7a+/zN+d9u3dTx596CHzd2zMGNcb/sRXdLT03XfmU6Fnn5V273bft1QpM0n3zz9NKCccI5nwqe/Er7/+Wn369NGgQYO0bds2PfDAA2rUqJHOxvHDMjg4WKdOnYp5HI1rzUUAyUdwsPmY19UkoT//lOrUkU6e9Hxd9yJtWvPRdmyWZbbZBZLatWvSgAFm3e61a133yZzZjJX/9VepYsV7fy7LMpNtK1aUnnlG2rnTfd8SJcxOfjt3mlVsCMdIZnzqO/KTTz5Rly5d1LFjR5UuXVqTJ09W+vTp9VkcM8VtNpvCwsJiHjlz5vRgxQDuS4YM0qJFZtKeo717pYcfjnuzgeSkc2cpTRr7ts8+881VO+A7Fi82k2E/+sisYuHKCy+Yv0/du9/7mseWJS1caJZpa9HC/CLrTrFi0hdfmLvLzz2XOOssA0nAZ0LyrVu3tHXrVjWMdVfJz89PDRs21MaNG92ed+XKFRUoUED58uXTk08+qd1xfeQj6ebNm4qIiLB7APCiwEAzw/3pp52PHTxognJcy0clF7lzm0lIsV24YMZgAont2DHz/dasmftfJEuWNBNiP/9cutcbSJZlfpF98EGpeXOzq6Q7RYtKs2ebYR/PP084RrLnMyH533//VVRUlNOd4Jw5c+r06dMuzylRooQ+++wzLVy4UP/73/8UHR2tGjVq6J9//nH7PMOGDVNISEjMI1++fIn6OgDcg3TpzJjFtm2djx07ZoJyXBOCkgsm8CGp3b4tjRhhxvkuWOC6T2Cg9OGHZoJc3br39jyWJf34oxnD/MQT0rZt7vsWLmx20NyzR2rXzvkTFSCZ8pmQfC+qV6+udu3aqUKFCqpTp46+//57hYaGasqUKW7PGTBggMLDw2Mex48f92DFANxKk8bcherc2fnYqVNmjLK7paySizp1nNeB/u03M1EKuF/r15vhDv36mXHIrjz2mLmTO2CA+eUzoSzLzBWoVk16/PG4v3cLFpRmzDBDOTp0IBzD5/hMSM6ePbv8/f115swZu/YzZ84oLCwsXtdImzatKlasqANxfDQbEBCg4OBguweAZMLfX5oyRXr5Zedj586Zmfu//+75uuLLZuNuMhLfv/9KnTqZ1WB27XLdJ29es9rE4sVSoUIJfw7LkpYtM+sqP/aY+eXOnQIFpGnTzEYhL75oJq4CPshnQnK6dOlUuXJlrVy5MqYtOjpaK1euVPXq1eN1jaioKO3cuVO5EmO/eQDe4ecnjR1r7pY5unhRatDA3FFLrl54wUxIjG3uXN9b+xneFx1t7tSWLOl+q3N/f6lvXzPUoUWLhK95bFnSihVSrVpmM59Nm9z3zZ/f/BK7b5/5xIdwDB/nMyFZkvr06aNp06Zp9uzZ2rNnj3r06KGrV6+qY8eOkqR27dppwIABMf3fe+89/fzzzzp06JC2bdum559/XkePHlVnVx/XAvAdNpuZrT94sPOxy5fNNrqrVnm8rHgJDjZBObYbN6RZs7xSDnzUzp1mLH7nztL586771KhhxgqPHCllzJiw61uW+Tv08MPSI4+YpeHcyZvXLB+3b5/Uteu9DeMAkiGfCsmtWrXSyJEjNXDgQFWoUEE7duzQ0qVLYybzHTt2TKdOnYrpf/HiRXXp0kWlSpXSY489poiICP36668q7TgmEIDvsdmkQYOk4cOdj127Zj4S/uknz9cVH66GXEyaZO4MAnG5ckV64w2zDvGGDa77ZM0qTZ8u/fKL2ekuodasMRP67vapTJ480oQJZnWZ7t2lgICEPxeQjNksy9WelLgjIiJCISEhCg8PZ3wykFyNH+96nHLatNLXXzsvvZYcPPywCTGxLV1q7oIDjizLrFbx6qtSXBPKO3aUPv5Yyp494c+xbp35xXPNmrj75colvfWWuYvNLrZIJpIir/nUnWQAcOmll8xEIcfxlrdvm21u5871Tl1xYQIf4uvwYbPecYsW7gNymTLml67PPkt4QF6/3tw1rlMn7oAcFma2qz540PydIyAjhSMkA0gZOnc2u3g5blAQFWXWV45jZ06vaNFCypHDvm3xYt/ZQRBJ79YtadgwE4B//NF1n/TpzZ3j7dvN5LqE+PVXM964du24x/DnzCl98ol06JC5kx0UlLDnAXwUIRlAytG2rRle4bgeq2WZJbImTPBOXa6kSyd16WLfFh0tTZ3qnXqQvKxdK1WoYIY1uNu6/MknzZrHb7yRsJUkNm82K1XUrGlWrnAnNNRM+jt0SHrtNcIxUh1CMoCU5emnzdhNV5OIXnpJGjXK4yW51a2bWdIutmnTpJs3vVMPvO/sWal9ezNxzt0ukgUKSD/8YL7PCxSI/7V/+81MaK1Wzax57E727Obu9OHDZvm49OkT8gqAFIOQDCDladrUDF1wdefr9deloUPN3WVvy5fPbOkb27lzZtMHpC7R0WaN4ZIlpc8/d90nTRrpzTel3bvNGOX42rLF7I5XtarZLc+dbNnM0oqHD5u7047reQOpDCEZQMrUsKG5W+ZqfdiBA6W3304eQZkJfNixw6xp3L272RDHlYcfNv0++ij+4XXbNvNLWJUq7sc0S2bJuA8/NOH4zTcTvqYykEIRkgGkXLVrmzGXmTM7Hxs2zIyz9HZQbtBAKlbMvm3DBumPP7xTDzzn8mXzPVi5shkn7Er27GajmTVrzAS++NixQ2re3Fx30SL3/bJkkd5/34TjAQOkTJkSVj+QwhGSAaRsVatKq1e7XhZr7Fhz986bm3j4+XE3ObWxLGnePDO0YswY999/XbtKf/9txijHZzvpP/80q6ZUrCgtXOi+X+bM0nvvmXD89ttmF0gATthM5C7YTARIIXbvNkMwTp92PvbCC2aJOMdVMTzl4kWze1nsVQzSp5dOnpRCQrxTE5LGwYNSr15xT5wrX16aPFmqXj1+19y5Uxoy5O5j2YODzZ3r3r1df7oC+DA2EwGAe1WmjNlRLF8+52NffCE995zZfMQbsmQxzx/btWvuJ3DB99y8aSaMlinjPiBnzGjWI966NX4Befdu6dlnTaiOKyBnyiS9+6505Ig0eDABGYgnQjKA1KNYMROUCxVyPjZvnlk+7sYNz9cluR9ywYd9vm/lShNkBw50v7zf00+bJd9ee+3un2js2SO1aSOVK2e+b93JmNEMpzhyxAyvyJLlnl8CkBoRkgGkLgULmu17S5RwPrZokdmg4do1j5elSpXM+OnY9u4146nhm06fNhvcNGwo7dvnuk+hQtJPP0nffivlzRv39f7+21yvTBnpq6/c/wKVIYOZiHfkiJmYlzXrfb0MILUiJANIffLkMTualSvnfOznn82GC5cve76uXr2c25jA53uioszujiVLSnPmuO6TNq30zjtmyESTJnFfb98+M26+dGlzPXfhOH16s4TbkSNmSbds2e7rZQCpHRP37oKJe0AKdv689OijZj1ZR9WqmY0XPDl+88YNczfx/Pn/2vz9paNHTbBH8rdli9Sjh/mvO/XqmV9+SpaM+1oHDphxzP/7X9wrsAQFmV+w3nhDypHj3uoGfBwT9wAgMWXLZsaLupoktWmTVL++9O+/nqsnMFDq1Mm+LSrKbFWN5C083Gx7/tBD7gNyjhwm8K5cGXdAPnhQ6tjxv9333AXkoCCpTx+zlNuIEQRkIJERkgGkbpkzmyEWdes6H9u+3dz1c7VsXFLp3t15TdypU7238gbiZlnS3Lkm0E6Y4HoohM1mJmbeGVPsbs3jw4fNL0klSpgNRKKiXPcLDDTLuB06JI0aJeXMmVivBkAshGQAyJjRbNvbqJHzsV27pDp1pH/+8UwthQqZMdGxnTolLVjgmedH/O3bJz3yiFm+z90vUpUqmU8lJkxwP3TnyBGpSxepeHGzXre7cBwQIL3yignHo0dLYWGJ8SoAuEFIBgDJTHpauNCsbuFo3z7p4YfNnT5PYAe+5O3GDWnQIDPxc+VK130yZZI+/VT67TczBMOVY8ekbt1MOJ4+XYqMdN0vXTozlOPgQbNLZK5cifM6AMSJkAwAdwQEmHVnn33W+djhwyYou1vKKzE1auS8lvOaNdJffyX9cyNuy5ZJZcuadYdv3XLdp1Urs3zfyy+biZeOjh83k/uKFo17KE26dOYXpoMHpXHjmLwJeBghGQBiS5vWLLPVvr3zsX/+MUF5166krcHf34xNdsTdZO85ccL88tS4sQmtrhQtasa3f/WVlDu362u89JLpN3my+3CcNq358z9wwAzTuNv6yQCSBCEZABz5+5uxoa6C6pkzZpLf9u1JW8OLL5o727F9/rl31m9OzSIjzRCHUqXc724XEGC2e96504xRdnTypBlLXKSICb3u7kCnSSN17Srt3y9NmuR6C3UAHkNIBgBX/PzMndvevZ2PnT9vVr3YtCnpnj97dvOxfWyXL0tffpl0zwl7mzdLVaqY7wF3v5w88ogJx4MGmVUnYjt1ypxbpIgZLuFuS+o0aaTOnU04njJFKlAgMV8FgHtESAYAd2w26ZNPpLfecj4WHm4C0rp1Sff87ibwsQdU0rp40YwZrl5d2rHDdZ9cucywimXLpGLF7I+dOWPWLy5c2NyFvnHD9TX8/c0nBn//bdbCLlgwMV8FgPtESAaAuNhs0gcfSO+/73zsyhUzRnX58qR57oceMkuIxbZzp7R+fdI8X2pnWdIXX5g1jydPdv3LiJ+fGTqxZ4+50x97zeOzZ6XXXzeTLkePdh+O/fzMmPe9e6UZM0yYBpDsEJIBID7eftts3ODo+nXp8celRYsS/zltNrPdsCMm8CW+PXvMDovt2pmw60qVKtLvv5u7wyEh/7WfOyf162fC8ahR5nvCFT8/6YUXTDieNctM4AOQbBGSASC++vRxHVBv3ZJatHA/set+tG7tvAnFd995dhfAlOzaNTOc5oEHzDJ7roSEmD/3jRvt7+z/+6/Uv78JxyNGmGu54udndtr76y8z+dJxeAaAZImQDAAJ0aOHNHOmCT6xRUaaQPu//yXu86VPL3XsaN92+7b5mB7358cfpTJlpGHD3C/H9vzzZsxwjx7/rXl8/rwJ1oUKScOHS1evuj7XZpPatJF27zbfFyVKJM3rAJAkCMkAkFAdOphVJhw3ioiONh/XT5uWuM/naim6KVPc79CGuB0/bu78P/642RLalRIlzG56X3wh5cxp2i5ckN55x4TjYcPMmHRXbDYzXnnXLrPmdsmSSfIyACQtQjIA3IvWraVvvzUbP8RmWWat208/TbznKl7cef3d48elxYsT7zlSg9u3pZEjzZrH8+e77hMYaCZp/vGHGaMsSZcumSXeChUykzjjWqu6ZUvpzz/NyhelSyf6SwDgOYRkALhXzZtLP/zgvD6uJL36qvkoPrEwge/+/PqrVLmy9MYb7odHNGlihka8/bbZICQ8XBoyxCzN9t57UkSE++s//bQJx998Y7atBuDzCMkAcD8aNzZjWzNkcD7Wv7+5A5kY6xo3beq8A9vy5dK+ffd/7ZTs/HmzUUfNmmb5PFfy5DGfCvz4o1mOLSJCGjrUhOPBg01Yduepp8xayt9+K5UrlwQvAIC3EJIB4H7Vr282lQgOdj723nvSm2/ef1BOk0bq1s25ffLk+7tuShUdbSZYlijhfpKjv79ZsWTPHnMn+MoVM5yiYEFp4EAzzMKdJ56Qtm2Tvv/erIwBIMWxWRZbN8UlIiJCISEhCg8PV7CrH4AAcMeWLdKjj5od2xz16mXGKTuuipEQZ86Yu8mxV2LInFk6ccKsggFj1y6zGkVcm65Ury5NmmQC7uXL0vjxZrzyhQtxX/vxx83d5cqVE7VkAPcnKfIad5IBILE8+KBZazc01PnYhAlmQl9U1L1fP2dO6Zln7NsuXZLmzr33a6YkV6+aTT0qVnQfkLNkkaZONceLFDHjxgsVMku6xRWQH3tM+u03s2kMARlIFQjJAJCYypeX1q2Tcud2PjZjhlki7n6WbuvZ07ltwoTEGffsyxYuNKtWjBjh/v3t0MGsefzcc9Inn5jxx/37m3HL7jRuLG3aZMYrV6mSJKUDSJ4IyQCQ2EqWNEE5f37nY3PmmDV0b926t2vXrOk8QWz7dnOXMzU6csSMD27e3CyL50rp0tLateaXiS++MOH4jTfMdtLuPPqoWRFjyRKpatWkqBxAMkdIBoCkUKSI9Msv5r+Ovv/erIpw40bCr2uzub+bnJrcumWGSpQubYZAuBIUJH30kQm727aZcNy3r3T2rPvrNmxohmIsW2bGLQNItQjJAJBU8uc3d5RLlXI+9tNPZhKYuzV74/L881KmTPZtX38t/fvvvdXpa9atM+OO+/eXrl933eeJJ8wd9qAg8/6/9pqZ+OhO/frmusuXm7v1AFI9QjIAJKXcuc1kPlfLhK1caca8xrVJhSsZM0rt29u33bolffbZPZfpE86dM+OK69SR/vrLdZ/8+c2GHo88YoLvq69Kp065v2bduubPZ+VKqXbtJCgagK9iCbi7YAk4AIniwgUTiH//3flYlSrS0qVS1qzxv95ff0llyti3FSok7d9v1v9NSaKjpenTzZ1jV8vrSWYd6VdekfLmlUaNMsvixaV2bbObXr16iV8vAI9jCTgA8FVZs0orVki1ajkf+/13c9czrolkjkqXNndBYzt82ITtlOSPP8x71q2b+4Bcs6Y0YIA0b57ZHCSugFyzpvlzWLuWgAwgToRkAPCU4GATYuvXdz72xx9mGEFcQwMc9erl3DZx4r3Xl5xcvmwCb+XK0saNrvtky2aGnRw/braRdre6hWQm4f38s5lM2aCBmQAJAHEgJAOAJ2XIIC1ebDancLRnj/Tww9KxY/G71pNPSrly2bctWSIdOnT/dXqLZUnffmsm240e7X7zldq1zXs5e3bc71fVquYXkw0bzDhlwjGAeCIkA4CnBQVJ8+dLLVo4HztwwATlgwfvfp20ac0ufrFZljRlSuLU6WmHDklNm0otW7ofMpEnj5kM+csvcYfjKlXMCiIbN0qNGhGOASQYIRkAvCFdOrNs23PPOR87etQE5b17736dLl2cJ+rNmHFvazB7y82b0vvvm4mIS5a47pMunRleceKEdPKk+2tVrmzu1G/eLDVpQjgGcM8IyQDgLWnSSJ9/Lr34ovOxkydNUP7zz7ivkSeP2W0utvPnzTJovmDVKrM83rvvug/2GTOaJe7i2j66YkXphx/MJMimTQnHAO4bIRkAvMnfX5o2zfUkvHPnzAoMW7bEfQ1fnMB35ozZFKVBA+nvv133SZvW/PfKFffXeeABM3Rl61apWTPCMYBEQ0gGAG/z85PGjZNef9352IULJkj++qv78+vWlUqWtG/bvNkEx+QmKsoE+BIlpC+/jLvv7dvuj5UrJ333ndluunlzwjGAREdIBoDkwGaTPv5YGjjQ+VhEhPToo9Lq1e7P7dnTuT253U3eutUsxdarlxQefm/XKFvWrIe8Y4eZ+OjHjzEASYN/XQAgubDZzC5ww4Y5H7t61Swb526zkHbtzJJosc2Z434DDk8KDze74T30kOsdB+OjdGkz0fGPP6RnniEcA0hy/CsDAMlN//7S2LHO7TduSE88IS1Y4HwsJMSM8XXsP2tWUlQYP5Zlgm3JkmY4SXR0wq9RsqQ0d66ZwPjss4RjAB7DvzYAkBy98opZ79hxrO3t2+ZO6tdfO5/To4dz26RJ9xZO79f+/WZ94tatpdOnE37+nTHLu3aZazgucwcASYyQDADJVdeuZkc5x7unUVFmfWXHu8QPPCDVrGnftn+/tHJlkpZp58YNafBgM7Fu+fKEn1+smPTFF9Lu3eY1Eo4BeAkhGQCSsxdekL76yqypHFt0tNSxo7lTHJurCXwTJiRdfbH9/LMJx0OGmA1CEqJIEfMLwV9/mWEjhGMAXkZIBoDkrmVL6fvvza5zjnr2lEaP/u/rp5+WcuSw77NoUdxbON+vkyelVq3M8IoDBxJ2bqFC0syZZnfBdu2cfxkAAC8hJAOAL2jWzGy3HBTkfKxPH+mDD8z/BwRInTvbH4+OlqZOTfyaIiOlTz81k+sSusNfwYLS9OlmI5EOHQjHAJIdQjIA+IpHHpGWLDHbNDt65x3p7bfNihJduzqPY542zWztnFh++80s6fbqq9Lly/E/L39+E9j//lvq1Om/XfUAIJkhJAOAL6lTx0yICwlxPvbhh1LfviaIPv64/bGzZ80Odffr0iUzxKNaNWn79vifly+fNHmymUjYpYvroSMAkIwQkgHA11SrJq1aJWXL5nxs9GgTYl0tB3c/O/BZlvS//5ml2SZNMl/HR9685nn375e6dSMcA/AZhGQA8EWVKklr1kg5czofmzzZbMBRtKh9+/r1ZlOOhNq7V2rQwKy0cfZs/M7JnVsaP95M5OvRw4yVBgAfQkgGAF9Vtqy0bp25W+vo889dj112XDIuLtevm7HO5ctLq1fH75xcucxkvoMHpV69CMcAfBYhGQB8WfHiJigXLOh8bMcO5wl8X3whhYff/bo//SSVKWNWzbh9++79w8KkMWNMOH75ZSkwMB7FA0DyRUgGAF9XqJD0yy8mMDty3JL66lUTlN355x+z1nLTptLhw3d/7hw5pE8+MeH41VddL1EHAD6IkAwAKUHevNLatebu791MnOg88S4y0oTdYsXMxiV3ExoqjRxpgvRrr0np099b3QCQTBGSASClCAszk/kqVoy73549JlDfsXGjVKqUWT7uxo24z82eXRo+3ITjvn0JxwBSLEIyAKQk2bOb5eGqVYu734QJ0oUL0lNPSTVq3H076axZpWHDTDju10/KkCHxagaAZCjeIfnkyZNJWUe8TZgwQQULFlRgYKCqVq2q3377Lc7+8+bNU8mSJRUYGKhy5crpp59+8lClAOAlmTNLP/8sPfyw+z7ffWfuPC9YcPdrffCBdOSI1L+/6xUzACAFindILlOmjObMmZOUtdzV119/rT59+mjQoEHatm2bHnjgATVq1Ehn3azb+euvv6pNmzbq1KmTtm/frubNm6t58+batWuXhysHAA/LlMlsYf3II66PW1bcq1ZkyiQNHSodPSq99Zb5GgBSEZtlxW/bpIkTJ+rNN99U48aNNWXKFGXNmjWpa3NStWpVValSRePHj5ckRUdHK1++fHr55ZfVv39/p/6tWrXS1atXtXjx4pi2atWqqUKFCpo8eXK8njMiIkIhISEKDw9XcHBw4rwQAPCUGzekZ5+VFi2KX/+gIOnNN6XevV1vfQ0AyVBS5LV430nu2bOn/vzzT50/f16lS5fWovj+g5tIbt26pa1bt6phw4YxbX5+fmrYsKE2btzo8pyNGzfa9ZekRo0aue0vSTdv3lRERITdAwB8VmCg9O23Up06cfdLm9ZMxDt5Uho0iIAMINVLk5DOhQoV0qpVqzR+/Hi1aNFCpUqVUpo09pfYtm1bohZ4x7///quoqCjldNiCNWfOnNq7d6/Lc06fPu2y/+nTp90+z7BhwzRkyJD7LxgAkos1a6TNm90fb9xYmjNHypLFYyUBQHKXoJAsSUePHtX333+vLFmy6Mknn3QKyb5uwIAB6tOnT8zXERERypcvnxcrAoD78OOPUosW0q1b7vscOcImIADgIEEJd9q0aerbt68aNmyo3bt3KzQ0NKnqcpI9e3b5+/vrzJkzdu1nzpxRWFiYy3PCwsIS1F+SAgICFBAQcP8FA4C3LVhgxiPfbVvpvXulIUPMEm8AAEkJGJPcuHFjvfnmmxo/fry+//57jwZkSUqXLp0qV66slStXxrRFR0dr5cqVql69ustzqlevbtdfkpYvX+62PwCkGPPmSS1bOgdkPz+pUyfn/h9/LP3+u2dqAwAfEO87yVFRUfrzzz+VN2/epKwnTn369FH79u314IMP6qGHHtKYMWN09epVdezYUZLUrl075cmTR8P+/27Iq6++qjp16mjUqFFq2rSpvvrqK23ZskVTp0712msAgCT35ZdSu3ZSdLR9u7+/9L//Sa1aScePm7WU74iOljp2lLZulfg0DQDiH5KXL1+elHXES6tWrXTu3DkNHDhQp0+fVoUKFbR06dKYyXnHjh2Tn99/N8dr1KihOXPm6J133tFbb72lYsWKacGCBSpbtqy3XgIAJK2ZM82dYsfVPdOkkb76Snr6afP1tGlSmTLSlSv/9dm9W3r/fbM+MgCkcvFeJzm1Yp1kAD5jyhSpe3fn9nTpzDJwzZrdvb+/vxl2UbFi0tUJAInMq+skAwCSsXHjXAfkwEBp4ULngCxJXbtK9evbt0VFmWEXca2GAQCpACEZAHzdyJHSK684twcFSYsXm3WQXbHZpOnTpQwZ7Nv/+IOVLgCkeoRkAPBlH3wgvfGGc3vGjNLSpVKDBnGfX6iQ9NFHzu3vvy/9+Wfi1AgAPoiQDAC+yLLM9tHvvON8LDjYrFzx8MPxu1bPns59IyPNsIu7rbEMACkUIRkAfI1lSW+9Jb33nvOxzJmlFSukhKwH7+cnzZjhvOvetm1m/WQASIUIyQDgSyxL6tvX9RCJbNmkVaukKlUSft2iRaUPP3Ruf+89szQcAKQyhGQA8BXR0dLLL0ujRzsfy5FDWr36/pZue/llqWZN+7Zbt8ywi8jIe78uAPggQjIA+ILoaKlbN2nCBOdjuXJJa9ZI5crd33P4+0uffWaWjYvt99+lUaPu79oA4GMIyQCQ3EVFSS++aJZrc5Q3r7R2rVSqVOI8V/HirnfcGzRI2rs3cZ4DAHwAIRkAkrPISOmFF6TZs52PFSggrVsnFSuWuM/52mtStWr2bTdvmmEXUVGJ+1wAkEwRkgEgubp1S2rdWpo71/lYkSImIBcqlPjPe2fYRUCAffumTdLYsYn/fACQDBGSASA5unlTatlS+u4752MlSpghFvnzJ93zlyolDR7s3P7229K+fUn3vACQTBCSASC5uX5deuop6YcfnI+VLm0m6eXJk/R1vP669OCD9m03bkidOpmJhACQghGSASA5uXZNeuIJackS52Ply5uAHBbmmVrSpJFmzpTSprVvX79eGj/eMzUAgJcQkgEgubhyRXrsMbNjnqNKlcxGIaGhnq2pbFlp4EDn9v79pYMHPVsLAHgQIRkAkoPwcKlRIzPW2FHVqtLKlWZHPW94803nTUquX2fYBYAUjZAMAN528aL0yCPSr786H6tVS/r5ZylzZo+XFSNtWjPsIk0a+/a1a6XJk71TEwAkMUIyAHjTv/9KDRqYXe0c1atnxiYHB3u+LkcPPGBWtnDUr590+LDn6wGAJEZIBgBvOXtWql9f2r7d+dgjj0iLF0sZM3q+LnfeestMHozt6lWpSxfJsrxTEwAkEUIyAHjDqVNS3brSzp3Ox5o2Ncu/pU/v8bLilC6dGXbh72/fvnKlNG2ad2oCgCRCSAYAT/vnH6lOHWnPHudjzZtL338vBQZ6vKx4qVTJrGzh6PXXpWPHPF8PACQRQjIAeNLRoyYg79/vfKxlS+mbb8wd2+Ts3XelMmXs2y5fZtgFgBSFkAwAnnLwoPTww9KhQ87H2raV5sxx3rgjOQoIMMMu/Bx+hPz8s2kHgBSAkAwAnvD33+YOsqshCR06SLNnOy+xlpxVqSK98YZze58+ZjgJAPg4QjIAJLW//jIB+cQJ52PdukkzZjhPhvMFgwdLJUvat4WHm9fEsAsAPo6QDABJ6c8/zSoWZ844H3v5ZWnSJOdhC74iMFD67DPJZrNv/+kn6YsvvFMTACQSH/2XGQB8wLZtZkOQc+ecj73+ujR2rHPA9DXVq5shFo5efdUscwcAPoqQDABJYfNms1HIhQvOx956S/r4Y98PyHcMHSoVK2bfdumS1L07wy4A+CxCMgAktg0bzI554eHOx4YMkd5/P+UEZEkKCnI97OKHH6S5c71TEwDcJ0IyACSmNWukRo3MusGOhg2TBg5MWQH5jlq1zBhrRy+/7Ho8NgAkc4RkAEgsK1ZIjz0mXb3qfOyTT1zvVJeSfPihVLiwfduFC1LPngy7AOBzCMkAkBh++kl6/HHp+nXnY+PHS6+95vmaPC1DBrOcnaPvv5fmzfN8PQBwHwjJAHC/Fi6UmjeXbt60b7fZpKlTpV69vFKWV9Sta+4cO+rVy/UqHwCQTBGSAeB+zJsnPfOMdPu2fbufn9miuUsX79TlTcOHSwUL2rf9+6/rMcsAkEwRkgHgXs2ZI7VuLUVG2rf7+0v/+5/Uvr136vK2jBml6dOd27/+2gy9AAAfQEgGgHsxe7b0/PNSdLR9e5o00ldfSW3aeKeu5KJBA6lrV+f2Hj2k8+c9Xw8AJBAhGQASato0qWNH5xUb0qaVvv3WDL+ANGKElC+ffdvZs2Y3PgBI5gjJAJAQEyaYO6SOATkgwEzge/JJ79SVHAUHm18oHH35pdloBACSMUIyAMTXJ59IL73k3B4UJC1eLDVp4vmakrtGjaQXX3Ru795dunjR8/UAQDwRkgEgPoYNk/r2dW7PkEFaskRq2NDzNfmKUaOk3Lnt206dSh1rRwPwWYRkAIiLZUlDhkhvveV8LFMmadkyqU4dz9flSzJnNutFO5o922zCAgDJECEZANyxLOntt6XBg52PZc5stqGuWdPTVfmmpk2ldu2c27t2lcLDPV8PANwFIRkAXLEs6Y03zDALR1mzSitXSg895Pm6fNno0VJYmH3biROuh7EAgJcRkgHAkWWZZcpGjXI+FhoqrV4tVark+bp8Xdas0pQpzu0zZkg//+z5egAgDoRkAIgtOtqsvDBunPOxsDBpzRqpfHmPl5ViPPGE9Nxzzu2dO0sREZ6vBwDcICQDwB1RUVKnTq4nmeXJI61dK5Uu7fm6UppPP5Vy5LBvO35c6tfPO/UAgAuEZACQpMhIM7Fs1iznY/nzm4BcvLjHy0qRsmWTJk50bp8yRVq1yvP1AIALhGQAuH3bDAGYM8f5WOHC0rp1UpEinq8rJXv6aenZZ53bO3WSrlzxfD0A4ICQDCB1u3lTatlSmjfP+Vjx4uYOcoECnq8rNRg/Xsqe3b7tyBGpf3+vlAMAsRGSAaReN25ILVpICxc6Hytd2kzSy5vX42WlGqGhJig7mjDB/HICAF5ESAaQOl27ZlZacLXjW7lyZpm3XLk8X1dq8+yz5hcVR506SVever4eAPh/hGQAqc+VK2YHuOXLnY9VrGgCsuPqC0gaNpuZxJc1q337wYNmt0MA8BJCMoDUJSJCatzYDKVw9NBDZie9bNk8XlaqljOnWRbO0aefShs2eL4eABAhGUBqcumS9OijroNXjRrmznKWLB4vCzKrizzxhH2bZUkdO0rXr3unJgCpGiEZQOpw/rzUoIG0ebPzsTp1pGXLpOBgz9cFw2aTJk+WMme2b9+/X3r3Xa+UBCB1IyQDSPnOnpXq15e2bXM+1rChmbyXMaPn64K9XLmkMWOc20ePljZt8ng5AFI3QjKAlO3UKalePenPP52PNWkiLVokpU/v+brgWrt25s8ltuhoM+zixg3v1AQgVSIkA0i5TpyQ6taV/vrL+diTT0rz50uBgR4vC3Gw2aSpU52HvuzdKw0Z4p2aAKRKhGQAKdPRo9LDD0v79jkfe+YZs8NeQIDn68Ld5c0rffKJc/vHH0u//+75egCkSoRkACnPoUNmMt6hQ87H2rSR5s6V0qb1fF2IvxdfNCuRxHZn2MXNm96pCUCqQkgGkLLs328C8tGjzsfat5e++EJKk8bzdSFhbDZp2jQpUyb79t27pfff905NAFIVQjKAlGPPHjPE4p9/nI916SJ99pnk7+/5unBv8ueXRoxwbh82zPVKJQCQiAjJAFKGnTvNHeTTp52P9epl1uD14588n9O1q1m+L7aoKDPs4tYt79QEIFXgJwYA37d9u1nm7dw552N9+kjjxhGQfZXNJk2fLmXIYN/+55/mjjIAJBF+agDwbb/9Zu40nj/vfGzAAGnkSBO04LsKFZKGD3duf/996Y8/PF8PgFSBkAzAd/36q9kx79Il52ODB0sffEBATil69DDDaWKLjDTDLm7f9k5NAFI0nwnJFy5cUNu2bRUcHKzMmTOrU6dOunLlSpzn1K1bVzabze7RvXt3D1UMIEmtW2eWCLt82fnYBx9IgwYRkFMSPz8z7CIoyL59+3azfjIAJDKfCclt27bV7t27tXz5ci1evFjr1q1T165d73pely5ddOrUqZjHx/xjCvi+lSulxo2lq1edj40cKb31ludrQtIrWtT1OOQhQ6RduzxfD4AUzSdC8p49e7R06VJNnz5dVatWVa1atTRu3Dh99dVXOnnyZJznpk+fXmFhYTGPYMetTgH4lqVLpccfl65fdz726adS376erwme8/LLUs2a9m23b5thF5GR3qkJQIrkEyF548aNypw5sx588MGYtoYNG8rPz0+bN2+O89wvv/xS2bNnV9myZTVgwABdu3Ytzv43b95URESE3QNAMrFokfTkk9KNG87HpkwxAQopm5+fWe86MNC+fcsWadQo79QEIEXyiZB8+vRp5ciRw64tTZo0ypo1q067WhP1/z333HP63//+p9WrV2vAgAH64osv9Pzzz8f5XMOGDVNISEjMI1++fInyGgDcp+++k1q0cF4b12YzoSkew6+QQhQv7nrXvUGDzIYyAJAIvBqS+/fv7zSxzvGxd+/ee75+165d1ahRI5UrV05t27bV559/rvnz5+vgwYNuzxkwYIDCw8NjHsePH7/n5weQSObOlVq1cv443c/PbDPdsaN36oL39O4tVatm33bzpvTii2azEQC4T2m8+eR9+/ZVhw4d4uxTuHBhhYWF6ezZs3btkZGRunDhgsLCwuL9fFWrVpUkHThwQEWKFHHZJyAgQAEBAfG+JoAkNnu2CT7R0fbtadJIc+ZILVt6py54l7+/+QShYkUTju/YtEkaM4ax6QDum1dDcmhoqEJDQ+/ar3r16rp06ZK2bt2qypUrS5JWrVql6OjomOAbHzt27JAk5cqV657qBeBh06ebYRSWZd+eNq00b54Zn4zUq1Qpsx72gAH27e+8IzVrZoZlAMA98okxyaVKlVLjxo3VpUsX/fbbb9qwYYNeeukltW7dWrlz55YknThxQiVLltRvv/0mSTp48KCGDh2qrVu36siRI/rhhx/Url07Pfzwwypfvrw3Xw6A+Jg4UerSxTkgBwRICxYQkGG8/roUa1K3JDOxk2EXAO6TT4RkyaxSUbJkSTVo0ECPPfaYatWqpalTp8Ycv337tv7++++Y1SvSpUunFStW6NFHH1XJkiXVt29fPf3001q0aJG3XgKA+BozRurVy7k9MFD64Qfpscc8XhKSqTRppJkzzacLsW3YII0f752aAKQINstyvE2D2CIiIhQSEqLw8HDWWAY8YfhwqX9/5/b06aXFi6V69TxfE5K/99+X3n3Xvi0oSNq5U3IzBwVAypEUec1n7iQDSAWGDnUdkDNlkpYtIyDDvTffNJP4Yrt+XerUyXnSJwDEAyEZgPdZlplsNXCg87GQEGn5cqlWLc/XBd+RNq0ZdpHGYT762rXSpEneqQmATyMkA/Auy5L69ZM++MD5WNas0sqVUgJWsUEq9sAD0ttvO7e/+aZ0+LDn6wHg0wjJALzHssymECNHOh/Lnl1atUr6/2UfgXh56y3JcQWjq1elzp2dV0oBgDgQkgF4R3S01LOn9Omnzsdy5pTWrDF3BoGESJfODLvw97dvX7VKirUiEgDcDSEZgOdFRZk1kCdPdj6WO7cZR1qmjOfrQspQqZLrCaBvvCEdO+b5egD4JEIyAM+KjJQ6dDBbCjvKl88E5BIlPF4WUph333X+RevyZdcb1ACAC4RkAJ5z+7bUtq30v/85HytUSFq3Tipa1PN1IeUJCDDDLvwcfsz9/LNpB4C7ICQD8Ixbt6RWraRvvnE+VqyYuYNcsKDHy0IKVqWKGWLh6LXXpH/+8Xw9AHwKIRlA0rtxQ2rRQpo/3/lYyZImIOfL5/m6kPINHmy+x2KLiJC6dWPYBYA4EZIBJI71612vRXvtmvTkk9KPPzofK1vWrGKRK1eSl4dUKjDQ9bCLn36SvvjCOzUB8AmEZAD3z7Kkvn2lBQvs269elR5/3IwDdVShgrR6tVnuDUhK1aqZIRaOXn1VOnnS8/UA8AmEZAD375tvpN9+k77//r+2y5elxo1NEHZUpYpZtzZ7ds/ViNRt6FAz9j22S5ekHj0YdgHAJUIygPtz48Z/a9Ju2CCdOWPCx6OPmiEYjqpXl5Yvl7Jk8WiZSOWCgsyygzabffsPP0hz53qnJgDJGiEZwP0ZP146csT8v2WZ8Z8NG0qbNjn3ffhhadkyKSTEoyUCkqRataRXXnFuf/ll6fRpz9cDIFmzWRafM8UlIiJCISEhCg8PV3BwsLfLAZKX8+elIkWk8PC7961f39y1y5Ah6esC3Ll6VSpfXjp0yL69RQvp22+d7zQD8AlJkde4kwzg3r33XvwCcuPG0uLFBGR4X4YM0owZzu3ffy/Nm+f5egAkW4RkAPdm/35p4sS792vWzKx6ERSU5CUB8VK3rtSrl3N7r17SuXMeLwdA8kRIBnBv+veXIiPj7pMmjdS0qXT9umdqAuLro4+cd3j891/ppZe8Ug6A5IeQDCDh1q+3X+7NnchIqXt3sxZy8+bS11+bzUUAb8uYUZo+3bn9m2/i970NIMUjJANImOhos3FIQty6JS1caMaC/v130tQFJFSDBmZ7akc9ephJqQBSNUIygDjVrVtXvXv3/q/hzsYh8TRYUoX06c2mIj//rA5jx6p58+aJXCVwjz7+WMqXz77t7FnXS8UBSFUIyQDiL/bGIfFRoYLUpo3Z6axu3aSqCrh3wcHStGnO7XPmmCULAaRahGQA8TdunHT06N37lShhxh9v3SoVL570dQH3o1Ej6cUXndu7dZMuXPB8PQCSBUIygLuKjo5Wv5dfVtY331SYzBCKOy5J6iwpVFKwzab6JUvqjy+/lJ59VvK7+z8xN2/e1CuvvKIcOXIoMDBQtWrV0u+//54ULwNwb9QoKU8e+7bTp6XXXvNOPQC8jpAM4K5mz56tDL//rs2WpY8lvSdp+f8faynpbLp0WvLGG9q6c6cqNW2qBo0a6UI878D169dP3333nWbPnq1t27apaNGiapSA84FEkTmzNHWqc/vnn0s//ujxcgB4HyEZwF2VL1ZMg7ZuVTFJ7SQ9KGmlpPWZMum3gADNO3VKD378sYqVKaORI0cqc+bM+vbbb+963atXr2rSpEkaMWKEmjRpotKlS2vatGkKCgrSDFe7ogFJ6bHHpHbtnNu7dYvfzpIAUhRCMoC7Kn/hgt3GIbn8/XX2gQf0x7vv6srt28qWP78yZswY8zh8+LAOHjx41+sePHhQt2/fVs2aNWPa0qZNq4ceekh79uxJktcCxGnMGClXLvu2EycSvuwhAJ+XxtsFAEjmwsOV9vBh8/8BAVKvXrL99Zeic+bUleho5cqVS2vWrHE6LXPmzB4tE0gUWbJIkydLTz5p3z5jhtSypZnkByBV4E4yAPeio6WDByWbzXzkfOCAmeAUECBJqlSpkk6fPq00adKoaNGido/s2bPf9fJFihRRunTptGHDhpi227dv6/fff1fp0qWT7GUBcXriCem555zbu3SRIiI8Xw8AryAkA3Dv22+loCCpfXtzdy1vXrvDDRs2VPXq1dW8eXP9/PPPOnLkiH799Ve9/fbb2rJly10vnyFDBvXo0UNvvPGGli5dqr/++ktdunTRtWvX1KlTp6R6VcDdffqplCOHfdvx41K/ft6pB4DHEZIBuNewoVSqlBQS4vKwzWbTTz/9pIcfflgdO3ZU8eLF1bp1ax09elQ5c+aM11N89NFHevrpp/XCCy+oUqVKOnDggJYtW6YsWbIk5isBEiZbNmnSJOf2KVOklSs9Xw8Aj7NZlmV5u4jkLCIiQiEhIQoPD1dwcLC3ywEAeFKrVmYr9tgKFpR27pQyZvRKSQCcJUVe404yAADujB8vOY6vP3IkYduzA/BJhGQAANwJDZUmTHBunzBBcrGqC4CUg5AMAEBcWraUWrRwbu/USbp61fP1APAIQjIAAHGx2aSJE6WsWe3bDx2S3n7bOzUBSHKEZAAA7iZnTmncOOf2Tz+V1q/3fD0AkhwhGQCA+GjTxmw0EptlSS++KF275p2aACQZQjIAAPFhs5lNdRy3XN+/Xxo40CslAUg6hGQAAOIrVy5pzBjn9tGjpY0bPV4OgKRDSAYAICHatZMee8y+LTraDLu4ccM7NQFIdIRkAAASwmYz21M77uq1d680eLBXSgKQ+AjJAAAkVN680iefOLePGCH9/rvn6wGQ6AjJAADcixdflB591L4tOlrq2FG6edM7NQFINIRkAADuhc0mTZsmZcpk3757tzR0qHdqApBoCMkAANyr/PnNEAtHH30kbdvm+XoAJBpCMgAA96NrV6l+ffu2qCgz7OLWLe/UBOC+EZIBALgfNps0fbqUIYN9+59/Sh9+6J2aANw3QjIAAPerUCFp+HDn9g8+kP74w/P1ALhvhGQAABJDjx5SnTr2bZGRZtjF7dveqQnAPSMkAwCQGPz8pBkzpKAg+/bt213fZQaQrBGSAQBILEWKSMOGObe/9560a5fn6wFwzwjJAAAkppdflmrWtG+7fdsMu4iM9E5NABKMkAwAQGLy85M++0wKDLRv37JFGjXKOzUBSDBCMgAAia14cen9953bBw6U9uzxfD0AEoyQDABAUujdW6pWzb7t1i3pxRfNZiMAkjVCMgAAScHf3wy7CAiwb9+0SRozxislAYg/QjIAAEmlVClpyBDn9nfekfbt83w9AOKNkAwAQFLq21eqUsW+7cYNhl0AyRwhGQCApJQmjRl2kTatffuGDdL48d6pCcBdEZIBAEhqZctKgwY5tw8YIB044Pl6ANwVIRkAAE/o10+qWNG+7fp1qVMnKTraOzUBcIuQDACAJ6RNK82caYZfxLZunTRpkndqAuAWIRkAAE954AGzsoWjN9+UDh/2fD0A3CIkAwDgSQMGSOXL27ddvSp17ixZlndqAuDEZ0LyBx98oBo1aih9+vTKnDlzvM6xLEsDBw5Urly5FBQUpIYNG2r//v1JWygAAHFJl84Mu/D3t29ftUqaOtU7NQFw4jMh+datW2rZsqV69OgR73M+/vhjffrpp5o8ebI2b96sDBkyqFGjRrpx40YSVgoAwF1UqiT17+/c/vrr0tGjnq8HgBObZfnWZzuzZs1S7969denSpTj7WZal3Llzq2/fvnr99dclSeHh4cqZM6dmzZql1q1bx+v5IiIiFBISovDwcAUHB99v+QAAGDdvSpUrS7t327c/8oi0bJlks3mnLsAHJUVe85k7yQl1+PBhnT59Wg0bNoxpCwkJUdWqVbVx40a35928eVMRERF2DwAAEl1AgBl24efwo3j5crP5CACvSrEh+fTp05KknDlz2rXnzJkz5pgrw4YNU0hISMwjX758SVonACAVq1JFeuMN5/Y+faR//vF8PQBieDUk9+/fXzabLc7H3r17PVrTgAEDFB4eHvM4fvy4R58fAJDKDB4slSxp3xYRIXXtymoXgBeluXuXpNO3b1916NAhzj6FCxe+p2uHhYVJks6cOaNcuXLFtJ85c0YVKlRwe15AQIACAgLu6TkBAEiwwEAz7KJmTfud95YskT7/XGrf3nu1AamYV0NyaGioQkNDk+TahQoVUlhYmFauXBkTiiMiIrR58+YErZABAECSq1ZNeu01adQo+/bevc1Evty5vVIWkJr5zJjkY8eOaceOHTp27JiioqK0Y8cO7dixQ1euXInpU7JkSc2fP1+SZLPZ1Lt3b73//vv64YcftHPnTrVr1065c+dW8+bNvfQqAABwY+hQqVgx+7ZLl6Tu3Rl2AXiBV+8kJ8TAgQM1e/bsmK8rVqwoSVq9erXq1q0rSfr7778VHh4e06dfv366evWqunbtqkuXLqlWrVpaunSpAgMDPVo7AAB3FRRkVrV4+GH7ULxokTRnjtS2rfdqA1Ihn1sn2dNYJxkA4FG9e0tjx9q3Zc1q1lP+//k2AOyxTjIAACndBx9IjpPWL1yQevZk2AXgQYRkAACSkwwZXG8mMn++9M03nq8HSKUIyQAAJDd16ki9ejm3v/SSdO6c5+sBUiFCMgAAydFHH0kFC9q3/fuvCcoAkhwhGQCA5ChjRmnGDOf2b76RvvvO8/UAqQwhGQCA5Kp+falbN+f2nj3NXWUASYaQDABAcvbxx1K+fPZtZ89Kr77qnXqAVIKQDABAchYcLE2b5tw+Z460cKHn6wFSCUIyAADJXaNGUqdOzu3du5s1lAEkOkIyAAC+YNQoKU8e+7bTp6XXXvNOPUAKR0gGAMAXhIRIU6c6t3/+ufTjj56vB0jhCMkAAPiKxx6T2rd3bu/aVbp0yePlACkZIRkAAF8yerSUK5d928mTUt++3qkHSKEIyQAA+JIsWaTJk53bP/tMWrbM8/UAKRQhGQAAX/PEE9Jzzzm3d+kiRUR4vh4gBSIkAwDgiz79VMqRw77t+HHpjTe8Uw+QwhCSAQDwRdmySZMmObdPnSqtXOn5eoAUhpAMAICvatFCevZZ5/ZOnaTLlz1fD5CCEJIBAPBl48dL2bPbtx09KvXv7516gBSCkAwAgC8LDZUmTHBunzhRWrPG4+UAKQUhGQAAX9eypRl64ahTJ+nqVc/XA6QAhGQAAHydzWbuHGfNat9+6JD01lveqQnwcYRkAABSgpw5pXHjnNvHjZPWr/d8PYCPIyQDAJBStGljNhqJzbKkF1+Url3zTk2AjyIkAwCQUthsZsvqzJnt2/fvlwYO9EpJgK8iJAMAkJLkyiWNHevc/skn0saNnq8H8FGEZAAAUpoXXpAee8y+7c6wixs3vFMT4GMIyQAApDQ2mzRlihQcbN++d680eLBXSgJ8DSEZAICUKG9eafRo5/YRI6TffvN8PYCPISQDAJBSdewoNWpk3xYdbdpv3vROTYCPICQDAJBS2WzS1KlSpkz27X/9JQ0d6p2aAB9BSAYAICXLn98MsXD00UfS1q2erwfwEYRkAABSuq5dpfr17duiosywi1u3vFMTkMwRkgEASOlsNmnGDClDBvv2nTulDz/0Tk1AMkdIBgAgNShYUBo+3Ln9gw+kP/7weDlAckdIBgAgtejRQ6pTx74tMlLq0EG6fdsrJQHJFSEZAIDUws/PDLtIn96+fccO13eZgVSMkAwAQGpSpIjrccjvvSft2uX5eoBkipAMAEBq8/LLUq1a9m23b5thF5GRXikJSG4IyQAApDZ+ftJnn0mBgfbtW7dKI0d6pyYgmSEkAwCQGhUrJr3/vnP7oEFmRz4glSMkAwCQWvXuLVWrZt9265b04otmsxEgFSMkAwCQWvn7m2EXAQH27Zs3S6NHe6cmIJkgJAMAkJqVKiUNGeLc/u670r59nq8HSCYIyQAApHZ9+0pVqti33bjBsAukaoRkAABSuzRpzLCLdOns2zdskMaN805NgJcRkgEAgFS2rDRwoHP7W29JBw54vh7AywjJAADA6NdPqljRvu36dalTJyk62js1AV5CSAYAAEbatNKsWWb4RWzr1kkTJ3qlJMBbCMkAAOA/5ctL77zj3N6/v3T4sOfrAbyEkAwAAOwNGGDCcmxXr0qdO0uW5Z2aAA8jJAMAAHvp0kkzZ5rNRmJbtUqaOtU7NQEeRkgGAADOKlUyd5Qdvf66dPSo5+sBPIyQDAAAXHvnHalMGfu2K1ekLl0YdoEUj5AMAABcCwgwwy78HOLC8uVm8xEgBSMkAwAA96pUMesnO+rTR/rnH8/XA3gIIRkAAMRt0CCpVCn7togIqWtXhl0gxSIkAwCAuAUGmuEVjsMuliyRPv/cOzUBSYyQDAAA7q5aNem115zbe/eWTp70eDlAUiMkAwCA+Bk6VCpe3L7t0iWpe3eGXSDFISQDAID4CQoywy5sNvv2RYukOXO8UxOQRAjJAAAg/mrWlF55xbn9lVek06c9Xw+QRAjJAAAgYT74QCpSxL7twgWpZ0+GXSDFICQDAICEyZBBmjHDuX3+fOmbbzxfD5AECMkAACDh6tSRevVybn/pJensWc/XAyQyQjIAALg3H30kFSxo3/bvvyYoAz7OZ0LyBx98oBo1aih9+vTKnDlzvM7p0KGDbDab3aNx48ZJWygAAKlFxoyuh13Mmyd9953n6wESkc+E5Fu3bqlly5bq0aNHgs5r3LixTp06FfOYO3duElUIAEAqVL++1K2bc3vPnuauMuCj0ni7gPgaMmSIJGnWrFkJOi8gIEBhYWFJUBEAAJAkffyx9NNP0vHj/7WdPWuWhWP9ZPgon7mTfK/WrFmjHDlyqESJEurRo4fOnz8fZ/+bN28qIiLC7gEAAOIQHCxNn+7cPneutHCh5+sBEkGKDsmNGzfW559/rpUrV2r48OFau3atmjRpoqioKLfnDBs2TCEhITGPfPnyebBiAAB81KOPSp06Obd3727WUAZ8jFdDcv/+/Z0m1jk+9u7de8/Xb926tZ544gmVK1dOzZs31+LFi/X7779rzZo1bs8ZMGCAwsPDYx7HY390BAAA3Bs1SsqTx77t9Gnptde8Uw9wH7w6Jrlv377q0KFDnH0KFy6caM9XuHBhZc+eXQcOHFCDBg1c9gkICFBAQECiPScAAKlGSIg0darUtKl9++efS88+69wOJGNeDcmhoaEKDQ312PP9888/On/+vHLlyuWx5wQAIFV57DGpfXtp9mz79q5dpd27pXgu4wp4m8+MST527Jh27NihY8eOKSoqSjt27NCOHTt05cqVmD4lS5bU/PnzJUlXrlzRG2+8oU2bNunIkSNauXKlnnzySRUtWlSNGjXy1ssAACDlGz1acrwhdfKk1Levd+oB7oHPhOSBAweqYsWKGjRokK5cuaKKFSuqYsWK2rJlS0yfv//+W+Hh4ZIkf39//fnnn3riiSdUvHhxderUSZUrV9Yvv/zCcAoAAJJSlizS5MnO7Z99Ji1d6tx+86YUx6R6wBtslmVZ3i4iOYuIiFBISIjCw8MVHBzs7XIAAPAdzz8vffmlfVvevGbYReyfqcOHS7VqSTVrerY+pBhJkdd85k4yAADwMWPHSjlz2rf984/0xhv/fX3qlPT++9KiRZ6tDbgLQjIAAEga2bJJEyc6t0+dKq1YYf7/7belK1cIyUh2CMkAACDptGghtWrl3N65s7R6tTRzpvn6r7+kgwc9WxsQB0IyAABIWuPGSY5Lvh49KjVubN/G3WQkI4RkAACQtEJDpfHjndtv3bL/mpCMZISQDAAAklZ0tFm94qGH4u63bp30/0u5At5GSAYAAIlv61apYUOpaFEpKEjKk0f67be4z4mMdL2OMuAFhGQAAJD4Klc221MfP+48rCIuDLlAMkFIBgAASeOFF8wKFjlyxP+cn34yd5QBLyMkAwCApFOjhhlmUb58/PpfvCht2JC0NQHxQEgGAABJq0ABE3yfeCJ+/RlygWSAkAwAAJJexozS/PnSm2/evS8hGckAIRkAAHiGn5/00UfS7NlSunTu++3bZx6AFxGSAQCAZ7VrZyb0Oe7CF9t93E1eu3at6tevr6xZsyp9+vQqVqyY2rdvr1uxVtmIiorS6NGjVa5cOQUGBipLlixq0qSJNjiMh46KitJHH32kkiVLKigoSFmzZlXVqlU1ffr0e64PvoGQDAAAPK9GDen336Vy5Vwf/+GHBF3u4sWLunLliv766y81btxYDz74oNatW6edO3dq3LhxSpcunaKioiRJlmWpdevWeu+99/Tqq69qz549WrNmjfLly6e6detqwYIFMdcdMmSIRo8eraFDh+qvv/7S6tWr1bVrV126dCmmz8mTJxXJihwpjs2yLMvbRSRnERERCgkJUXh4uIKDg71dDgAAKcvly9LzzzuHYn9/6exZKWtWt6dGRkZq2bJlmjVrlhYtWqTNmzdr9erVGjt2rA4fPuz2vK+//lqtW7fWDz/8oGbNmtkde/rpp7V27VodPXpUGTJkUIUKFfTUU09p0KBBbq83ZMgQTZo0Sc8//7zat2+vcu6CP5JMUuQ17iQDAADvyZRJ+v57qV8/+/aoKGnJEpen7Ny5U3379lXevHnVrl07hYaGavXq1XrggQcUFhamU6dOad26dW6fcs6cOSpevLhTQJakvn376vz581q+fLkkKSwsTKtWrdK5c+fcXu/NN9/U2LFjtWfPHlWqVEmVKlXSp59+Guc5SP4IyQAAwLv8/aXhw6VZs+wn9MUal3z+/HmNHTtWlSpV0oMPPqhDhw5p4sSJOnXqlCZOnKjq1atLklq2bKk2bdqoTp06ypUrl5566imNHz9eERERMdfat2+fSpUq5bKUO+37/n/i4CeffKJz584pLCxM5cuXV/fu3bXEIbwHBgaqVatW+vHHH3XixAm1a9dOs2bNUp48edS8eXPNnz+f4Rg+iJAMAACSh/btpVWr/pvQt3RpzJbW48aNU+/evZUxY0YdOHBA8+fPV4sWLZTOYZUMf39/zZw5U//8848+/vhj5cmTRx9++KHKlCmjU6dOxfSL72jT0qVLa9euXdq0aZNefPFFnT17Vs2aNVPnzp1d9s+RI4d69+6tbdu2aeHChdq4caNatGihXbt23cMbAm8iJAMAgOSjZk2zQ1+5clJ4uPTLL5Kkrl27aujQoTp9+rTKlCmjjh07atWqVYqOjnZ5mTx58uiFF17Q+PHjtXv3bt24cUOTJ0+WJBUvXlx79uxxed6d9uLFi8e0+fn5qUqVKurdu7e+//57zZo1SzNmzHA57vny5cuaOXOm6tevr2bNmqls2bKaPXu2SpcufV9vCzyPkAwAAJKXggXNDn3NmsUMucidO7feeecd7du3T0uXLlW6dOnUokULFShQQP3799fu3bvdXi5LlizKlSuXrl69Kklq3bq19u/fr0UulpkbNWqUsmXLpkceecTt9e4E3jvXi4qK0pIlS/Tcc88pZ86c+uijj9SgQQMdOnRIK1euVLt27ZzueCP5Y3WLu2B1CwAAvCQqSpoxQ+ra1eXhGzduaMGCBZo1a5ZWrFih7du369dff9WOHTv01FNPqUiRIrpx44Y+//xzjRw5UqtWrVKdOnVkWZaefvpprVmzRiNGjFCDBg0UERGhCRMm6LPPPtO8efPUvHlzSdIzzzyjmjVrqkaNGgoLC9Phw4c1YMAAXbhwQbt371aaNGk0dOhQjRo1Sq1atVL79u1Vo0YND75JkJImrxGS74KQDABA8nfy5EllzJhRBw8e1CeffKINGzbEtJUpU0avv/663WoWkZGRGjNmjGbNmqX9+/crMDBQ1atX17vvvquaNWvG9Js2bZrmzp2rXbt2KTw8XGFhYapfv74GDx6sAgUKSJKOHDmisLAwBQYGevx1wyAkewEhGQAAIHljnWQAAADAAwjJAAAAgANCMgAAAOCAkAwAAAA4ICQDAAAADgjJAAAAgANCMgAAAOCAkAwAAAA4ICQDAAAADgjJAAAAgANCMgAAAOCAkAwAAAA4ICQDAAAADgjJAAAAgANCMgAAAOCAkAwAAAA4ICQDAAAADgjJAAAAgANCMgAAAOCAkAwAAAA4ICQDAAAADgjJAAAAgANCMgAAAOCAkAwAAAA4ICQDAAAADgjJAAAAgANCMgAAAOCAkAwAAAA4ICQDAAAADgjJAAAAgANCMgAAAOCAkAwAAAA4ICQDAAAADgjJAAAAgANCMgAAAOCAkAwAAAA4ICQDAAAADgjJAAAAgANCMgAAAOCAkAwAAAA4ICQDAAAADgjJAAAAgAOfCMlHjhxRp06dVKhQIQUFBalIkSIaNGiQbt26Fed5N27cUK9evZQtWzZlzJhRTz/9tM6cOeOhqgEAAOCrfCIk7927V9HR0ZoyZYp2796t0aNHa/LkyXrrrbfiPO+1117TokWLNG/ePK1du1YnT55UixYtPFQ1AAAAfJXNsizL20XcixEjRmjSpEk6dOiQy+Ph4eEKDQ3VnDlz9Mwzz0gyYbtUqVLauHGjqlWrFq/niYiIUEhIiMLDwxUcHJxo9QMAACBxJEVeS5MoV/GC8PBwZc2a1e3xrVu36vbt22rYsGFMW8mSJZU/f/44Q/LNmzd18+ZNu+eRzJsPAACA5OdOTkvMe78+GZIPHDigcePGaeTIkW77nD59WunSpVPmzJnt2nPmzKnTp0+7PW/YsGEaMmSIU3u+fPnuuV4AAAAkvfPnzyskJCRRruXVkNy/f38NHz48zj579uxRyZIlY74+ceKEGjdurJYtW6pLly6JXtOAAQPUp0+fmK8vXbqkAgUK6NixY4n2pqd0ERERypcvn44fP84QlQTgfUs43rN7w/uWcLxn94b3LeF4z+5NeHi48ufPH+cog4Tyakju27evOnToEGefwoULx/z/yZMnVa9ePdWoUUNTp06N87ywsDDdunVLly5dsrubfObMGYWFhbk9LyAgQAEBAU7tISEhfLMmUHBwMO/ZPeB9Szjes3vD+5ZwvGf3hvct4XjP7o2fX+KtSeHVkBwaGqrQ0NB49T1x4oTq1aunypX/r717j6my/uMA/gbkIpkQyc0lCmLYlESPwQ41gSAP6QqSWRJTdOSF0EGmZXYhW80iFpuOhl3EtTUtNy/LLEPk4FRERFBEJTGSgYKFgRdUFD6/P/x55jlc5CDnPAfO+7U9m36f75fzeT778vXjc56LCrm5uQ9Mgkqlgr29PfLz8xEXFwcAqKqqQm1tLdRq9UPHTkRERESD14B4BFx9fT3Cw8Ph4+ODzMxM/PPPP2hoaNC7tri+vh7jx4/HkSNHANw985uUlITly5ejoKAApaWlWLBgAdRqda+fbEFERERE1mlA3LiXl5eH6upqVFdX44knntDbd+8uxtu3b6Oqqgqtra26fVlZWbC1tUVcXBxu3boFjUaDr7/+2qjPdnR0RHp6epeXYFDXmLO+Yd6Mx5z1DfNmPOasb5g34zFnfWOKvA3Y5yQTEREREZnKgLjcgoiIiIjInFgkExEREREZYJFMRERERGSARTIRERERkQEWyQb+/vtvJCUlwdfXF0OHDsXYsWORnp6Otra2HsfdvHkTKSkpePzxxzFs2DDExcWhsbHRTFEr77PPPkNoaCicnZ07vQq8O/Pnz4eNjY3eFh0dbdpALUxf8iYi+Oijj+Dt7Y2hQ4ciKioKZ8+eNW2gFuTy5ctISEjA8OHD4erqiqSkJFy7dq3HMeHh4Z3m2pIlS8wUsTKys7MxZswYODk5ISQkRPd4zO5s3boV48ePh5OTEwIDA7F7924zRWo5jMnZpk2bOs0pJycnM0arvP379+Oll17CyJEjYWNjgx07djxwjFarxZQpU+Do6Ah/f39s2rTJ5HFaGmPzptVqO801GxsbvcfgDnZr167FM888g0cffRQeHh6IjY1FVVXVA8c97LrGItnAmTNn0NHRgQ0bNqCyshJZWVnIycnB6tWrexz31ltv4ZdffsHWrVtRWFiICxcuYNasWWaKWnltbW2YPXs2kpOTjRoXHR2Nixcv6rbNmzebKELL1Je8ZWRkYN26dcjJyUFxcTEeeeQRaDQa3Lx504SRWo6EhARUVlYiLy8Pu3btwv79+7Fo0aIHjlu4cKHeXMvIyDBDtMr46aefsHz5cqSnp+PYsWOYNGkSNBoNLl261GX/Q4cOIT4+HklJSSgrK0NsbCxiY2Nx8uRJM0euHGNzBtx9I9r9c+r8+fNmjFh5169fx6RJk5Cdnd2r/jU1NZg5cyYiIiJQXl6OtLQ0vPHGG9izZ4+JI7UsxubtnqqqKr355uHhYaIILU9hYSFSUlJw+PBh5OXl4fbt25g+fTquX7/e7Zh+WdeEHigjI0N8fX273d/c3Cz29vaydetWXdvp06cFgBQVFZkjRIuRm5srLi4uveqbmJgoMTExJo1noOht3jo6OsTLy0u+/PJLXVtzc7M4OjrK5s2bTRihZTh16pQAkJKSEl3bb7/9JjY2NlJfX9/tuLCwMElNTTVDhJYhODhYUlJSdH9vb2+XkSNHytq1a7vs/+qrr8rMmTP12kJCQmTx4sUmjdOSGJszY9Y6awBAtm/f3mOfd955RyZMmKDX9tprr4lGozFhZJatN3krKCgQAPLff/+ZJaaB4NKlSwJACgsLu+3TH+sazyT3QktLC9zc3LrdX1paitu3byMqKkrXNn78ePj4+KCoqMgcIQ5YWq0WHh4eCAgIQHJyMpqampQOyaLV1NSgoaFBb665uLggJCTEKuZaUVERXF1dMXXqVF1bVFQUbG1tUVxc3OPYH3/8ESNGjMDEiRPx3nvv6b14aDBpa2tDaWmp3hyxtbVFVFRUt3OkqKhIrz8AaDQaq5hTQN9yBgDXrl3D6NGjMWrUKMTExKCystIc4Q5Y1j7PHlZQUBC8vb3xwgsv4ODBg0qHo6iWlhYA6LE264/5NiDeuKek6upqrF+/HpmZmd32aWhogIODQ6drSj09Pa3qmiFjRUdHY9asWfD19cW5c+ewevVqvPjiiygqKoKdnZ3S4Vmke/PJ09NTr91a5lpDQ0OnrxiHDBkCNze3Ho//9ddfx+jRozFy5EicOHEC7777LqqqqrBt2zZTh2x2//77L9rb27ucI2fOnOlyTENDg9XOKaBvOQsICMDGjRvx9NNPo6WlBZmZmQgNDUVlZWWnN8PSXd3NsytXruDGjRsYOnSoQpFZNm9vb+Tk5GDq1Km4desWvvvuO4SHh6O4uBhTpkxROjyz6+joQFpaGp599llMnDix2379sa5ZzZnkVatWdXnh+/2b4WJYX1+P6OhozJ49GwsXLlQocuX0JWfGmDNnDl5++WUEBgYiNjYWu3btQklJCbRabf8dhAJMnbfByNQ5W7RoETQaDQIDA5GQkIAffvgB27dvx7lz5/rxKMiaqNVqzJs3D0FBQQgLC8O2bdvg7u6ODRs2KB0aDTIBAQFYvHgxVCoVQkNDsXHjRoSGhiIrK0vp0BSRkpKCkydPYsuWLSb/LKs5k/z2229j/vz5Pfbx8/PT/fnChQuIiIhAaGgovvnmmx7HeXl5oa2tDc3NzXpnkxsbG+Hl5fUwYSvK2Jw9LD8/P4wYMQLV1dWIjIzst59rbqbM27351NjYCG9vb117Y2MjgoKC+vQzLUFvc+bl5dXpRqo7d+7g8uXLRv2uhYSEALj7TdHYsWONjteSjRgxAnZ2dp2ertPTeuTl5WVU/8GmLzkzZG9vj8mTJ6O6utoUIQ4K3c2z4cOH8yyykYKDg3HgwAGlwzC7pUuX6m7YftA3Nv2xrllNkezu7g53d/de9a2vr0dERARUKhVyc3Nha9vzCXeVSgV7e3vk5+cjLi4OwN27UGtra6FWqx86dqUYk7P+UFdXh6amJr3ibyAyZd58fX3h5eWF/Px8XVF85coVFBcXG/1kEUvS25yp1Wo0NzejtLQUKpUKALBv3z50dHToCt/eKC8vB4ABP9e64uDgAJVKhfz8fMTGxgK4+/Vkfn4+li5d2uUYtVqN/Px8pKWl6dry8vIG9PpljL7kzFB7ezsqKiowY8YME0Y6sKnV6k6P4LKmedafysvLB+X61R0RwbJly7B9+3ZotVr4+vo+cEy/rGt9vbNwsKqrqxN/f3+JjIyUuro6uXjxom67v09AQIAUFxfr2pYsWSI+Pj6yb98+OXr0qKjValGr1UocgiLOnz8vZWVlsmbNGhk2bJiUlZVJWVmZXL16VdcnICBAtm3bJiIiV69elRUrVkhRUZHU1NTI3r17ZcqUKTJu3Di5efOmUodhdsbmTUTk888/F1dXV9m5c6ecOHFCYmJixNfXV27cuKHEIZhddHS0TJ48WYqLi+XAgQMybtw4iY+P1+03/P2srq6WTz75RI4ePSo1NTWyc+dO8fPzk2nTpil1CCa3ZcsWcXR0lE2bNsmpU6dk0aJF4urqKg0NDSIiMnfuXFm1apWu/8GDB2XIkCGSmZkpp0+flvT0dLG3t5eKigqlDsHsjM3ZmjVrZM+ePXLu3DkpLS2VOXPmiJOTk1RWVip1CGZ39epV3ZoFQL766ispKyuT8+fPi4jIqlWrZO7cubr+f/31lzg7O8vKlSvl9OnTkp2dLXZ2dvL7778rdQiKMDZvWVlZsmPHDjl79qxUVFRIamqq2Nrayt69e5U6BLNLTk4WFxcX0Wq1enVZa2urro8p1jUWyQZyc3MFQJfbPTU1NQJACgoKdG03btyQN998Ux577DFxdnaWV155Ra+wHuwSExO7zNn9OQIgubm5IiLS2toq06dPF3d3d7G3t5fRo0fLwoULdf8gWQtj8yZy9zFwH374oXh6eoqjo6NERkZKVVWV+YNXSFNTk8THx8uwYcNk+PDhsmDBAr3/VBj+ftbW1sq0adPEzc1NHB0dxd/fX1auXCktLS0KHYF5rF+/Xnx8fMTBwUGCg4Pl8OHDun1hYWGSmJio1//nn3+WJ598UhwcHGTChAny66+/mjli5RmTs7S0NF1fT09PmTFjhhw7dkyBqJVz79Fkhtu9PCUmJkpYWFinMUFBQeLg4CB+fn56a5u1MDZvX3zxhYwdO1acnJzEzc1NwsPDZd++fcoEr5Du6rL7548p1jWb/384ERERERH9n9U83YKIiIiIqLdYJBMRERERGWCRTERERERkgEUyEREREZEBFslERERERAZYJBMRERERGWCRTERERERkgEUyEREREZEBFslERERERAZYJBMRWYH29naEhoZi1qxZeu0tLS0YNWoU3n//fYUiIyKyTHwtNRGRlfjzzz8RFBSEb7/9FgkJCQCAefPm4fjx4ygpKYGDg4PCERIRWQ4WyUREVmTdunX4+OOPUVlZiSNHjmD27NkoKSnBpEmTlA6NiMiisEgmIrIiIoLnn38ednZ2qKiowLJly/DBBx8oHRYRkcVhkUxEZGXOnDmDp556CoGBgTh27BiGDBmidEhERBaHN+4REVmZjRs3wtnZGTU1Nairq1M6HCIii8QzyUREVuTQoUMICwvDH3/8gU8//RQAsHfvXtjY2CgcGRGRZeGZZCIiK9Ha2or58+cjOTkZERER+P7773HkyBHk5OQoHRoRkcXhmWQiIiuRmpqK3bt34/jx43B2dgYAbNiwAStWrEBFRQXGjBmjbIBERBaERTIRkRUoLCxEZGQktFotnnvuOb19Go0Gd+7c4WUXRET3YZFMRERERGSA1yQTERERERlgkUxEREREZIBFMhERERGRARbJREREREQGWCQTERERERlgkUxEREREZIBFMhERERGRARbJREREREQGWCQTERERERlgkUxEREREZIBFMhERERGRgf8BMEALP1t2m0QAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectors = {\n",
    "    \"<SOS>\": [1.1103, -1.6898],\n",
    "    \"<EOS>\": [-0.9890, 0.9580],\n",
    "    \"<PAD>\": [1.3221, 0.8172],\n",
    "    \"hello\": [-0.7658, -0.7506],\n",
    "    \"bot\": [1.3525, 0.6863],\n",
    "    \"human\": [-0.3278, 0.7950],\n",
    "}\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for word, vec in vectors.items():\n",
    "    ax.quiver(0, 0, vec[0], vec[1], angles='xy', scale_units='xy', scale=1, color='r')\n",
    "    ax.text(vec[0], vec[1], word)\n",
    "\n",
    "ax.set_xlim([-2, 2])\n",
    "ax.set_ylim([-2, 2])\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "plt.title('Vector Representation of Tokens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring how close each vector is semantically can be formalized by cosine\n",
    "similarity, essentially intuition is measuring the angle between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;SOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;EOS&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;PAD&gt;'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'hello'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'bot'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'human'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32mSOS\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'<EOS>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'<PAD\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m, \u001b[32m'hello'\u001b[0m, \u001b[32m'bot'\u001b[0m, \u001b[32m'human'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;SOS&gt;</th>\n",
       "      <th>&lt;EOS&gt;</th>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "      <th>hello</th>\n",
       "      <th>bot</th>\n",
       "      <th>human</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;SOS&gt;</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.975899</td>\n",
       "      <td>0.027705</td>\n",
       "      <td>0.192830</td>\n",
       "      <td>0.111513</td>\n",
       "      <td>-0.981955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;EOS&gt;</th>\n",
       "      <td>-0.975899</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.245178</td>\n",
       "      <td>0.025946</td>\n",
       "      <td>-0.325689</td>\n",
       "      <td>0.917019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;PAD&gt;</th>\n",
       "      <td>0.027705</td>\n",
       "      <td>-0.245178</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.975513</td>\n",
       "      <td>0.996471</td>\n",
       "      <td>0.161838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hello</th>\n",
       "      <td>0.192830</td>\n",
       "      <td>0.025946</td>\n",
       "      <td>-0.975513</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.953609</td>\n",
       "      <td>-0.374917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bot</th>\n",
       "      <td>0.111513</td>\n",
       "      <td>-0.325689</td>\n",
       "      <td>0.996471</td>\n",
       "      <td>-0.953609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.078435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>human</th>\n",
       "      <td>-0.981955</td>\n",
       "      <td>0.917019</td>\n",
       "      <td>0.161838</td>\n",
       "      <td>-0.374917</td>\n",
       "      <td>0.078435</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          <SOS>     <EOS>     <PAD>     hello       bot     human\n",
       "<SOS>  1.000000 -0.975899  0.027705  0.192830  0.111513 -0.981955\n",
       "<EOS> -0.975899  1.000000 -0.245178  0.025946 -0.325689  0.917019\n",
       "<PAD>  0.027705 -0.245178  1.000000 -0.975513  0.996471  0.161838\n",
       "hello  0.192830  0.025946 -0.975513  1.000000 -0.953609 -0.374917\n",
       "bot    0.111513 -0.325689  0.996471 -0.953609  1.000000  0.078435\n",
       "human -0.981955  0.917019  0.161838 -0.374917  0.078435  1.000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate cosine similarity\n",
    "cos_sim = cosine_similarity(E.detach().numpy())\n",
    "\n",
    "# Your vocabulary\n",
    "# V_mathcal = [\"<SOS>\", \"<EOS>\", \"<PAD>\", \"hello\", \"bot\", \"human\"]\n",
    "pprint(V_mathcal)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(cos_sim, index=V_mathcal, columns=V_mathcal)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1103</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.6898</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7658</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7506</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.3525</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6863</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.9890</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9580</span><span style=\"font-weight: bold\">]]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">UnsafeViewBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.1103\u001b[0m, \u001b[1;36m-1.6898\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.7658\u001b[0m, \u001b[1;36m-0.7506\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.3525\u001b[0m,  \u001b[1;36m0.6863\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.9890\u001b[0m,  \u001b[1;36m0.9580\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mUnsafeViewBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Z = torch.matmul(O, E) \n",
    "pprint(Z)\n",
    "pprint(Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our $\\mathbf{Z}$, each row is the embedding vector of a word in the vocabulary. Note that these values are initially random, and they are learned during the training of the model.\n",
    "\n",
    "It is no surprise the dimension is $4 \\times 4$, which is $L \\times D$, the \n",
    "length of the sequence times the dimension of the embedding vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings\n",
    "\n",
    "VISIT after attention.\n",
    "\n",
    "The code below wont work as of now because the `embedding_dim` is odd. It needs to be even.\n",
    "\n",
    "$P$ is not learnable. It is a fixed matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "        self, embedding_dim: int, dropout: float = 0.1, max_len: int = 3\n",
    "    ) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.max_len = max_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.P = self._init_positional_encoding()\n",
    "\n",
    "    def _init_positional_encoding(self):\n",
    "        \"\"\"Initialize the positional encoding tensor.\"\"\"\n",
    "        P = torch.zeros((1, self.max_len, self.embedding_dim))\n",
    "        position = self._get_position_vector()\n",
    "        div_term = self._get_div_term_vector()\n",
    "        P[:, :, 0::2] = torch.sin(position / div_term)\n",
    "        P[:, :, 1::2] = torch.cos(position / div_term)\n",
    "        return P\n",
    "\n",
    "    def _get_position_vector(self):\n",
    "        \"\"\"Return a vector representing the position of each token in a sequence.\"\"\"\n",
    "        return torch.arange(self.max_len, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    def _get_div_term_vector(self):\n",
    "        \"\"\"Return a vector representing the divisor term for positional encoding.\"\"\"\n",
    "        return torch.pow(\n",
    "            10000,\n",
    "            torch.arange(0, self.embedding_dim, 2, dtype=torch.float32)\n",
    "            / self.embedding_dim,\n",
    "        )\n",
    "\n",
    "    def forward(self, Z):\n",
    "        Z = self._add_positional_encoding(Z)\n",
    "        return self.dropout(Z)\n",
    "\n",
    "    def _add_positional_encoding(self, Z):\n",
    "        \"\"\"Add the positional encoding tensor to the input tensor.\"\"\"\n",
    "        return Z + self.P[:, : Z.shape[1], :].to(Z.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `forward` method of the `PositionalEncoding` class, the positional encoding is added to the input `X`:\n",
    "\n",
    "```python\n",
    "def forward(self, X):\n",
    "    X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "    return self.dropout(X)\n",
    "```\n",
    "\n",
    "This method slices the precalculated positional encodings tensor `self.P` to match the sequence length of `X`, adds it to `X`, and then applies dropout. The result, which is the sum of the original embeddings and the positional encodings, is returned. So there's no need to add the positional encodings to `X` outside of this class.\n",
    "\n",
    "So when you call `positional_encoding(Z)`, it adds the positional encodings to `Z` and applies dropout, then returns the result. You could store this result in `Z_with_pos_encodings` or just overwrite it as `Z`:\n",
    "\n",
    "```python\n",
    "Z = positional_encoding(Z)\n",
    "```\n",
    "\n",
    "Now, `Z` contains the original embeddings with the positional encodings added and dropout applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1103</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6898</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0756</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2103</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.2618</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2702</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.8478</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0320</span><span style=\"font-weight: bold\">]]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">AddBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.1103\u001b[0m, \u001b[1;36m-0.6898\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0756\u001b[0m, \u001b[1;36m-0.2103\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m2.2618\u001b[0m,  \u001b[1;36m0.2702\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.8478\u001b[0m, \u001b[1;36m-0.0320\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mAddBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_len = 10\n",
    "dropout = 0\n",
    "D = embedding_dim = 2 # same as earlier\n",
    "\n",
    "positional_encoding = PositionalEncoding(embedding_dim=embedding_dim, dropout=dropout, max_len=max_len)\n",
    "Z = positional_encoding(Z)\n",
    "pprint(Z)\n",
    "pprint(Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of your \"hello bot\" example, the original tensor `Z` represented the word embeddings, where each token in the sequence (i.e., `SOS`, `hello`, `bot`, `EOS`) was converted into a 2-dimensional vector capturing the semantic meaning of each token. After adding positional encoding, the new tensor represents both the semantic and positional information of each token in the sequence.\n",
    "\n",
    "- The first row (`[1.1103, -0.6898]`) now encapsulates both the meaning of the `SOS` token and the information that it's the first token in the sequence.\n",
    "\n",
    "- The second row (`[0.0756, -0.2103]`) is now a representation of the word `hello` that carries not just its semantics (e.g., being a greeting), but also the information that it's the second word in the sentence.\n",
    "\n",
    "- The third row (`[2.2618, 0.2702]`) likewise carries both the semantics of `bot` (likely related to AI or technology), and its position as the third word in the sentence.\n",
    "\n",
    "- The last row (`[-0.8478, -0.0320]`) encapsulates the semantics of `EOS` token, signifying end of a sentence, and the fact that it's the last token in the sentence.\n",
    "\n",
    "The idea here is that in natural language, word order matters. The sentence \"hello bot\" is not the same as \"bot hello\" (okay maybe it is the same in this example, a better one is cat eat mouse isn't the same as mouse eat cat).\n",
    "\n",
    "So, in a language model, we want our representations to capture not just what words mean, but also where they are in a sentence. Positional encoding is a technique to achieve this goal.\n",
    "\n",
    "Again, it's important to note that the exact interpretation of these values is not straightforward. They are the result of complex transformations and learned during the training process to minimize the loss function. They don't have a clear, individual meaning that humans can easily understand. This is a common characteristic of the inner workings of deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Intuition with P added (Pretty bad analogy, to revise)\n",
    "\n",
    "Let's revise the analogy using the sentence \"Mary visited the museum in Paris.\"\n",
    "\n",
    "The original word embeddings (Z) can be thought of as guests at a party, where each guest represents a word: 'Mary,' 'visited,' 'the,' 'museum,' 'in,' and 'Paris.' The personality of each guest represents the semantic meaning of the word they represent. \n",
    "\n",
    "Now, let's think of the positional encoding (P) as the order in which these guests arrived at the party. This captures the positional information in the sentence. 'Mary' was the first to arrive, 'visited' the second, and so on.\n",
    "\n",
    "When we add the positional encoding to the original embeddings (Z = Z + P), we're basically combining the personality of each guest (the meaning of the word) with their arrival order at the party (their position in the sentence). \n",
    "\n",
    "So, for example, 'Mary' isn't just represented as a proper noun referring to a person, but she's also identified as the first person who arrived at the party. Similarly, 'Paris' isn't just a proper noun referring to a city, but it's the last entity that arrived.\n",
    "\n",
    "This way, the combined tensor (Z = Z + P) captures both the semantic meaning and the positional information for each word, allowing the Transformer to understand both the meaning of the words and their order in the sentence. This is crucial because, in many languages, changing the order of words can drastically alter the meaning of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piecing it together \n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "Note if you use the `TokenEmbedding` layer, you do not need to one-hot encode\n",
    "the input $X$ since it will be done automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_all(42, seed_torch=True)\n",
    "# embedding_layer = TokenEmbedding(len(dataset.vocab), embedding_dim=embedding_dim)\n",
    "\n",
    "# Z = embedding_layer(X)\n",
    "# pprint(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "- https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms#:~:text=The%20meaning%20of%20query%2C%20value,the%20same%20tensor%20as%20value.\n",
    "- https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Simple\n",
    "\n",
    "- Query:\n",
    "\n",
    "    The Query vector, like the word embedding, is a learned representation of the word. However, while the word embedding represents the semantic meaning of the word itself, the Query vector represents how the word should be used as a query to attend to other words.\n",
    "\n",
    "    In other words, the Query vector captures the \"asking\" aspect of the word in the context of the sentence. It's used to compute attention scores with the Key vectors of other words. The more similar the Query vector is to a Key vector of another word (as measured by their dot product), the more attention the model should pay to that word.\n",
    "\n",
    "    So, in the case of our example, the Query vector [1.15157688, 1.26007255] for \"cat\" determines how \"cat\" attends to other words in the sentence. For instance, if the Key vector for \"mat\" is very similar to this Query vector, then \"cat\" would pay more attention to \"mat\" in the computation of the output vectors. However, \"cat\" will also attend to all other words in the sentence, with the amount of attention determined by the similarity of the Query vector for \"cat\" to the Key vector of each other word.\n",
    "\n",
    "    Remember, these Query (and Key, and Value) vectors are typically learned during the training process. They are updated iteratively to minimize the prediction error of the model on a set of training examples. As such, they can capture complex patterns in the data and help the model make accurate predictions.\n",
    "\n",
    "    However, it's important to note that the actual values in these vectors can be difficult to interpret directly. They don't necessarily correspond to intuitive features of the words. Instead, they should be understood in relation to each other: the attention mechanism is essentially learning to use these vectors to weigh the importance of different words when processing each individual word in the sentence.\n",
    "\n",
    "  - More intuition: In this example, \"the cat sat on the mat\", the \"Query\" vector for the word \"cat\" can be thought of as posing the queries: \"What is the cat doing?\" and \"Where is the cat?\" As we move forward, we'll see that this \"Query\" vector interacts with the \"Key\" vectors of all other words in the sentence. The goal is for this \"learned query representation\" to align well with the \"Key\" vectors of words that provide meaningful responses to the queries posed. For instance, the word \"mat\" becomes a key part of answering the question \"Where is the cat?\", and so we would want the \"Query\" vector for \"cat\" and the \"Key\" vector for \"mat\" to have a high degree of alignment.\n",
    "\n",
    "- Value, the output of the attention.\n",
    "\n",
    "    The final value vector of a word after the attention mechanism captures not just the word's individual meaning, but also its context within the sentence. It's a more sophisticated representation that captures how the word interacts with all other words in the sentence.\n",
    "\n",
    "    One way to think about this is as a form of feature engineering. The attention mechanism allows the model to automatically learn features that capture important relationships between words. In our example, the final value vector for \"cat\" could capture the fact that \"cat\" is the subject of the sentence, that it's performing an action (\"sat\"), and that this action is happening in a specific location (\"on the mat\"). \n",
    "\n",
    "    To provide an example of how this could be useful, consider a downstream task like sentiment analysis. The sentiment of a sentence often depends on the relationships between words, not just the words themselves. For example, consider the difference between the sentences \"The cat sat on the mat\" and \"The cat did not sit on the mat\". The presence of the word \"not\" changes the meaning of the sentence, and this change is captured by the relationships between the words.\n",
    "\n",
    "    By using the attention mechanism, the model can learn to capture these relationships, and thus it can learn to better perform tasks like sentiment analysis. The final value vector for each word captures a rich set of features that reflect both the word's individual meaning and its context within the sentence, and these features can be very useful for many natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.11515769, 0.12600725],\n",
       "        [1.15157688, 1.26007255],\n",
       "        [0.04948636, 0.08854955],\n",
       "        [0.04948636, 0.08854955],\n",
       "        [1.15157688, 1.26007255]]),\n",
       " array([[0.0861242 , 0.15376671],\n",
       "        [0.86124201, 1.53766711],\n",
       "        [0.04097224, 0.04000152],\n",
       "        [0.04097224, 0.04000152],\n",
       "        [0.86124201, 1.53766711]]),\n",
       " array([[0.17553878, 0.09123364],\n",
       "        [1.7553878 , 0.91233644],\n",
       "        [0.11356005, 0.02379881],\n",
       "        [0.11356005, 0.02379881],\n",
       "        [1.7553878 , 0.91233644]]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word embeddings for \"the\", \"cat\", \"sat\", \"on\", \"mat\"\n",
    "embeddings = np.array([[0.1, 0.1],  # \"the\"\n",
    "                       [1, 1],      # \"cat\"\n",
    "                       [0.2, -0.1], # \"sat\"\n",
    "                       [0.2, -0.1], # \"on\"\n",
    "                       [1, 1]])     # \"mat\"\n",
    "\n",
    "# Initialize the Query, Key, and Value weight matrices\n",
    "np.random.seed(0)\n",
    "Q_weight_matrix = np.random.rand(2, 2)\n",
    "K_weight_matrix = np.random.rand(2, 2)\n",
    "V_weight_matrix = np.random.rand(2, 2)\n",
    "\n",
    "# Compute the Query, Key, and Value vectors for each word\n",
    "Q = np.dot(embeddings, Q_weight_matrix)\n",
    "K = np.dot(embeddings, K_weight_matrix)\n",
    "V = np.dot(embeddings, V_weight_matrix)\n",
    "\n",
    "Q, K, V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02929359, 0.29293585, 0.00975875, 0.00975875, 0.29293585],\n",
       "       [0.29293585, 2.92935851, 0.0975875 , 0.0975875 , 2.92935851],\n",
       "       [0.01787795, 0.17877947, 0.00556968, 0.00556968, 0.17877947],\n",
       "       [0.01787795, 0.17877947, 0.00556968, 0.00556968, 0.17877947],\n",
       "       [0.29293585, 2.92935851, 0.0975875 , 0.0975875 , 2.92935851]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the attention scores\n",
    "attention_scores = Q @ K.T\n",
    "\n",
    "# Print the attention scores\n",
    "attention_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply the softmax function to these scores. This will convert the scores into probabilities that sum to 1. It will also amplify high scores and suppress low scores. The softmax function is applied to each row of the matrix separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17970633, 0.23391698, 0.17622986, 0.17622986, 0.23391698],\n",
       "       [0.03271029, 0.45673907, 0.02690578, 0.02690578, 0.45673907],\n",
       "       [0.18780496, 0.22058994, 0.18550758, 0.18550758, 0.22058994],\n",
       "       [0.18780496, 0.22058994, 0.18550758, 0.18550758, 0.22058994],\n",
       "       [0.03271029, 0.45673907, 0.02690578, 0.02690578, 0.45673907]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute softmax\n",
    "softmax_scores = np.exp(attention_scores) / np.sum(np.exp(attention_scores), axis=1, keepdims=True)\n",
    "\n",
    "# Print the softmax scores\n",
    "softmax_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've computed the softmax scores, which are probabilities indicating how much each word should attend to every other word. You'll notice that now the words \"cat\" and \"mat\" pay most attention to themselves and each other, as their embeddings are the same. The words \"sat\" and \"on\" also pay most attention to themselves and each other, but less to \"cat\" and \"mat\". The word \"the\" is somewhat evenly spread, but with a slightly higher attention to \"cat\" and \"mat\".\n",
    "\n",
    "Finally, let's compute the output vectors for each word. We do this by multiplying the softmax scores by their corresponding Value vectors (element-wise) and summing them up. This produces the output of the self-attention layer at this position (for the initially processed word). This output vector is a weighted sum of all Value vectors, where the weights are the attention scores. This means that words with higher attention scores have a greater influence on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89280079, 0.45160535],\n",
       "       [1.61536116, 0.83766433],\n",
       "       [0.84954134, 0.42846834],\n",
       "       [0.84954134, 0.42846834],\n",
       "       [1.61536116, 0.83766433]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the output vectors\n",
    "output_vectors = softmax_scores @ V\n",
    "\n",
    "# Print the output vectors\n",
    "output_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting matrix contains the output vectors for each word in the sentence: \"the\", \"cat\", \"sat\", \"on\", and \"mat\". These vectors are influenced by all other words in the sentence, with the degree of influence determined by the attention scores.\n",
    "\n",
    "In this particular example, the words \"cat\" and \"mat\" have similar output vectors, which are also influenced by the other words, but mostly by \"cat\" and \"mat\" themselves. This reflects the fact that these words are similar (in our simple representation) and that they should pay attention to each other. Similarly, the words \"sat\" and \"on\" have similar output vectors, indicating that they are related to each other in the context of this sentence.\n",
    "\n",
    "The word \"the\" has a different output vector, which is a result of the attention paid to all other words. This illustrates the power of the attention mechanism: it allows each word to consider the context provided by all other words, with the amount of consideration determined by the attention scores.\n",
    "\n",
    "This ability to consider all parts of the input when processing each word, and to learn how much attention to pay to each part, is what makes the self-attention mechanism so powerful for tasks like language modeling. It allows the model to capture complex dependencies between words, regardless of their distance from each other in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries, Keys and Values\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "First, let's visualize this with human's own brain attention mechanism.\n",
    "\n",
    "Consider the sentence:\n",
    "\n",
    "> Mary visited the museum in Paris.\n",
    "\n",
    "Sure, let's continue with the intuition.\n",
    "\n",
    "As we read the word **Mary**, our brain starts to search for the context or information related to Mary. This is the **query**. \n",
    "\n",
    "Next, we move on to the word **visited**. Now, our brain starts to form a connection between Mary and the action of visiting. The word \"visit\" acts as a **key** to unlock more information about the context.\n",
    "\n",
    "As we continue to read and reach the phrase **the museum in Paris**, our brain recognizes this as the **value** or the object of Mary's visit. \n",
    "\n",
    "So, in this sentence, \"Mary\" is the query, \"visited\" is the key, and \"the museum in Paris\" is the value. Our brain's attention mechanism works by associating queries and keys to extract relevant values or information.\n",
    "\n",
    "> This process happens for each word in the sentence. The attention mechanism is not limited to just one query, key, and value. It can be applied to multiple queries, keys, and values.\n",
    "\n",
    "In the context of the Transformer model, the queries, keys, and values are all vectors that are computed from the input data. The model uses these to determine how much attention should be paid to each part of the input when generating each part of the output. This allows the model to handle long-range dependencies in the data, such as the connection between \"Mary\" and \"the museum in Paris\" in our example sentence.\n",
    "\n",
    "---\n",
    "\n",
    "Let's consider the sentence: \"Mary visited the museum in Paris.\"\n",
    "\n",
    "To understand how attention mechanisms work in a Transformer model, let's imagine we are trying to understand the role of each word in the sentence.\n",
    "\n",
    "Here is a conceptual explanation of what the Queries (Q), Keys (K), and Values (V) might represent:\n",
    "\n",
    "- **Queries (Q):** Each word in the sentence can have a query. For instance, when processing the word \"visited\", the query could be \"Who did the visiting?\" or \"What was visited?\". Similarly, when processing the word \"museum\", the query could be \"Who or what is related to the museum?\".\n",
    "\n",
    "- **Keys (K):** Each word in the sentence also has a key. These keys give us more context about each word. For \"Mary\", the key could represent the idea that Mary is a person. For \"visited\", the key might encapsulate the notion of an action performed by someone. For \"museum\", the key might represent the idea of a place or a destination.\n",
    "\n",
    "- **Values (V):** Values are the actual context or information each word provides. If a key matches a query, the Transformer model will fetch the value related to that key. For example, for the query \"Who did the visiting?\" when processing \"visited\", the key for \"Mary\" would match this query, and thus the value that \"Mary\" is the one who did the visiting is obtained.\n",
    "\n",
    "In the attention mechanism, every word (through its query) interacts with every other word (through their keys), and based on these interactions, attention scores are calculated. These scores decide how much focus should be placed on each word in the sentence while processing a particular word.\n",
    "\n",
    "Again, this is a simplified explanation. In practice, Q, K, and V are all high-dimensional vectors, and these interactions involve complex mathematical operations like dot products and softmax operations.\n",
    "\n",
    "The aim of the Transformer model is to understand the relationship between words in a sentence. In the given example sentence, it helps the model understand the relationship between \"Mary\", \"visited\", \"museum\", and \"Paris\", and that \"Mary\" is the one who did the visiting, the \"museum\" is the object being visited, and the event took place in \"Paris\". \n",
    "\n",
    "I hope this analogy helps to visualize the attention mechanism better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q, K and V\n",
    "\n",
    "In the sentence \"hello bot\", \"hello\" is a greeting and \"bot\" is the entity being greeted. This sentence, in a conversation, might be a user's first interaction with a chatbot. The user is saying hello to the bot. \n",
    "\n",
    "Now, let's break this down in terms of queries (Q), keys (K), and values (V):\n",
    "\n",
    "- The Queries (Q): In the context of the Transformer model trying to understand or generate a response to \"hello bot\", the model might form a **query** for the word \"hello\" as \"What is the intent behind this token?\" and another query for \"bot\" as \"Who is being addressed here?\".\n",
    "\n",
    "- The Keys (K): The model has learned representations (keys) for all the words in its training data. So, it has keys for both \"hello\" and \"bot\". For \"hello\", a key might be its function as a common greeting. For \"bot\", a key might be its role as a term referring to a software application.\n",
    "\n",
    "- The Values (V): The values are the representations that correspond to the keys. If the model's keys for \"hello\" and \"bot\" match with the queries it has formed, it will fetch the corresponding values. These values, in practice, are vector representations carrying the semantic information of the words. The values might tell the model that \"hello\" is a polite way to initiate a conversation and \"bot\" is the addressee here.\n",
    "\n",
    "Remember, this is all abstract and conceptual. In practice, Q, K, and V are all numerical vectors that the model learns to associate with each input token. These vectors are used to calculate the attention scores, which determine how much each word in the sentence contributes to the understanding of every other word in the sentence.\n",
    "\n",
    "The primary role of the transformer is to understand the context of each word in relation to every other word in the sentence, which is a crucial requirement in understanding natural language. For example, it needs to understand that \"bot\" is being greeted, which is an insight it gets by paying attention to both \"hello\" and \"bot\" simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.q_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.k_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.v_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        Q = self.q_linear(x)\n",
    "        K = self.k_linear(x)\n",
    "        V = self.v_linear(x)\n",
    "\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.embedding_dim)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1103, -0.6898],\n",
       "         [ 0.0756, -0.2103],\n",
       "         [ 2.2618,  0.2702],\n",
       "         [-0.8478, -0.0320]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2815</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0562</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5227</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2384</span><span style=\"font-weight: bold\">]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">requires_grad</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.2815\u001b[0m,  \u001b[1;36m0.0562\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.5227\u001b[0m, \u001b[1;36m-0.2384\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mrequires_grad\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0480</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2268</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0886</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0544</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7780</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0626</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.2554</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0400</span><span style=\"font-weight: bold\">]]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">UnsafeViewBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.0480\u001b[0m,  \u001b[1;36m0.2268\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.0886\u001b[0m,  \u001b[1;36m0.0544\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.7780\u001b[0m,  \u001b[1;36m0.0626\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.2554\u001b[0m, \u001b[1;36m-0.0400\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mUnsafeViewBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0495</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0815</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0020</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1135</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1152</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.3875</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0426</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.4696</span><span style=\"font-weight: bold\">]]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">UnsafeViewBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.0495\u001b[0m,  \u001b[1;36m0.0815\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.0020\u001b[0m, \u001b[1;36m-0.1135\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.1152\u001b[0m,  \u001b[1;36m1.3875\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0426\u001b[0m, \u001b[1;36m-0.4696\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mUnsafeViewBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8484</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5723</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2237</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2523</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0267</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.7141</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0804</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7041</span><span style=\"font-weight: bold\">]]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">UnsafeViewBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.8484\u001b[0m,  \u001b[1;36m1.5723\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.2237\u001b[0m,  \u001b[1;36m0.2523\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0267\u001b[0m,  \u001b[1;36m1.7141\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.0804\u001b[0m, \u001b[1;36m-0.7041\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mUnsafeViewBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "H = num_heads = 1\n",
    "d_q = d_k = d_v = D // H\n",
    "pprint(d_q)\n",
    "\n",
    "# W_q = nn.Linear(D, d_q)\n",
    "W_q = torch.randn(D, d_q, requires_grad=True)\n",
    "pprint(W_q)\n",
    "\n",
    "Q = Z @ W_q\n",
    "\n",
    "W_k = torch.randn(D, d_k, requires_grad=True)\n",
    "K = Z @ W_k\n",
    "\n",
    "W_v = torch.randn(D, d_v, requires_grad=True)\n",
    "V = Z @ W_v\n",
    "\n",
    "pprint(Q)\n",
    "pprint(K)\n",
    "pprint(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack the variables `Q`, `K`, and `V` and what their values represent. All these variables are derived from the input matrix `Z` and learned weights `W_q`, `W_k`, and `W_v` for each attention head.\n",
    "\n",
    "1. **Query matrix (`Q`)**: \n",
    "\n",
    "    `Q` is the result of the dot product between the input matrix `Z` and the weight matrix `W_q` for the query. In your case, `Q` has the shape of (1, 4, 2), which represents (batch size, sequence length, feature dimension). \n",
    "\n",
    "    Each entry in `Q` essentially represents a \"question\" or \"request for information\" associated with the corresponding element of the input. These questions will be used to interact with the keys to derive attention scores.\n",
    "\n",
    "    The actual values in the `Q` matrix are computed from the input data and the parameters of the model. They don't necessarily have an intuitive interpretation because they are learned in a data-driven way to achieve the task that the model is trained for (e.g., language modeling, machine translation).\n",
    "\n",
    "2. **Key matrix (`K`)**: \n",
    "\n",
    "    `K` is the result of the dot product between the input matrix `Z` and the weight matrix `W_k` for the key. `K` also has the shape of (1, 4, 2).\n",
    "\n",
    "    Each entry in `K` represents a \"key\" or \"identifier\" associated with the corresponding element of the input. The keys are used in conjunction with the queries to calculate the attention scores. \n",
    "\n",
    "    As with the queries, the values in the `K` matrix are learned from the data and don't necessarily have an intuitive interpretation.\n",
    "\n",
    "3. **Value matrix (`V`)**: \n",
    "\n",
    "    `V` is the result of the dot product between the input matrix `Z` and the weight matrix `W_v` for the value. `V` also has the shape of (1, 4, 2).\n",
    "\n",
    "    Each entry in `V` represents a \"value\" or \"information content\" associated with the corresponding element of the input. Once the model calculates the attention scores (using `Q` and `K`), it uses them to take a weighted sum of the `V` matrix.\n",
    "\n",
    "    Again, the values in the `V` matrix are learned from the data and do not have a direct intuitive interpretation.\n",
    "\n",
    "In summary, the `Q`, `K`, and `V` matrices are transformations of the input data that are used to implement the attention mechanism. The values in these matrices are learned during the training process to enable the model to focus on different parts of the input sequence when producing each element of the output sequence. The attention mechanism is a key component of transformer models and has been found to be very effective for a wide range of tasks in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled Dot-Product Attention\n",
    "\n",
    "From Paper:\n",
    "\n",
    "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.\n",
    "\n",
    "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$. The keys and values are also packed together into matrices $K$ and $V$. We compute the matrix of outputs as:\n",
    "$$\n",
    "\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of $\\frac{1}{\\sqrt{d_k}}$. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
    "\n",
    "While for small values of $d_k$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$ [3]. We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients ${ }^4$. To counteract this effect, we scale the dot products by $\\frac{1}{\\sqrt{d_k}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0209</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0257</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3202</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.1085</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0088</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0060</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0857</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0293</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0334</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0087</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0027</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0037</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0094</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0050</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0261</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0079</span><span style=\"font-weight: bold\">]]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">UnsafeViewBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0209\u001b[0m, \u001b[1;36m-0.0257\u001b[0m,  \u001b[1;36m0.3202\u001b[0m, \u001b[1;36m-0.1085\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0088\u001b[0m, \u001b[1;36m-0.0060\u001b[0m,  \u001b[1;36m0.0857\u001b[0m, \u001b[1;36m-0.0293\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.0334\u001b[0m, \u001b[1;36m-0.0087\u001b[0m, \u001b[1;36m-0.0027\u001b[0m,  \u001b[1;36m0.0037\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0094\u001b[0m,  \u001b[1;36m0.0050\u001b[0m, \u001b[1;36m-0.0261\u001b[0m,  \u001b[1;36m0.0079\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mUnsafeViewBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0147</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0181</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2264</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0767</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0062</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0042</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0606</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0207</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0236</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0061</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0019</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0026</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0066</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0036</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0184</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0056</span><span style=\"font-weight: bold\">]]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">DivBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0147\u001b[0m, \u001b[1;36m-0.0181\u001b[0m,  \u001b[1;36m0.2264\u001b[0m, \u001b[1;36m-0.0767\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0062\u001b[0m, \u001b[1;36m-0.0042\u001b[0m,  \u001b[1;36m0.0606\u001b[0m, \u001b[1;36m-0.0207\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.0236\u001b[0m, \u001b[1;36m-0.0061\u001b[0m, \u001b[1;36m-0.0019\u001b[0m,  \u001b[1;36m0.0026\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0066\u001b[0m,  \u001b[1;36m0.0036\u001b[0m, \u001b[1;36m-0.0184\u001b[0m,  \u001b[1;36m0.0056\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mDivBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2430</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2351</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3002</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2217</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2488</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2462</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2627</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2422</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2459</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2503</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2513</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2525</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2518</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2510</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2456</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2516</span><span style=\"font-weight: bold\">]]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">SoftmaxBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.2430\u001b[0m, \u001b[1;36m0.2351\u001b[0m, \u001b[1;36m0.3002\u001b[0m, \u001b[1;36m0.2217\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.2488\u001b[0m, \u001b[1;36m0.2462\u001b[0m, \u001b[1;36m0.2627\u001b[0m, \u001b[1;36m0.2422\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.2459\u001b[0m, \u001b[1;36m0.2503\u001b[0m, \u001b[1;36m0.2513\u001b[0m, \u001b[1;36m0.2525\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.2518\u001b[0m, \u001b[1;36m0.2510\u001b[0m, \u001b[1;36m0.2456\u001b[0m, \u001b[1;36m0.2516\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mSoftmaxBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2489</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7998</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2538</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7332</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2511</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7029</span><span style=\"font-weight: bold\">]</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2562</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7031</span><span style=\"font-weight: bold\">]]]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">grad_fn</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">UnsafeViewBackward0</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.2489\u001b[0m, \u001b[1;36m0.7998\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.2538\u001b[0m, \u001b[1;36m0.7332\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.2511\u001b[0m, \u001b[1;36m0.7029\u001b[0m\u001b[1m]\u001b[0m,\n",
       "\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0.2562\u001b[0m, \u001b[1;36m0.7031\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mUnsafeViewBackward0\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the attention scores by dot product of Q and K\n",
    "scores = Q @ K.transpose(-2, -1)\n",
    "pprint(scores)\n",
    "\n",
    "# Normalize the scores with sqrt(d_k)\n",
    "scores = scores / math.sqrt(d_k)\n",
    "pprint(scores)\n",
    "\n",
    "# Apply softmax to get the attention weights\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "pprint(weights)\n",
    "\n",
    "# Multiply weights by V to get the output of the attention head\n",
    "output = weights @ V\n",
    "\n",
    "# Note: If multiple heads are used, you'd need to repeat this process for each head,\n",
    "# concatenate the results, and apply a final linear transformation.\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the steps of the self-attention mechanism:\n",
    "\n",
    "1. **Calculating the attention scores**\n",
    "\n",
    "   `scores = Q @ K.transpose(-2, -1)`\n",
    "\n",
    "   This step calculates the raw attention scores by taking the dot product of the query matrix `Q` and the transpose of the key matrix `K`. The transpose operation switches the last two dimensions of `K`, so that the dot product can be properly calculated. The resulting `scores` matrix represents the unnormalized attention scores, with each element indicating the raw score of the query-key pair.\n",
    "\n",
    "2. **Normalizing the scores**\n",
    "\n",
    "   `scores = scores / math.sqrt(d_k)`\n",
    "\n",
    "   This step scales the scores by dividing by the square root of the dimension of the key vectors `d_k`. This normalization is used to ensure that the dot products don't grow too large as the dimension of the keys increases. This prevents the softmax function, which is applied later, from getting squashed into its extreme, where the gradients are very small.\n",
    "\n",
    "3. **Calculating the attention weights**\n",
    "\n",
    "   `weights = F.softmax(scores, dim=-1)`\n",
    "\n",
    "   The softmax function is applied to the scores to convert them into probabilities. The resulting `weights` matrix is the same shape as the `scores` matrix, but its rows now sum up to 1. Each element in `weights` represents the weight (or probability) of the corresponding query-key pair. The softmax function ensures that the attention weights are positive and sum to 1, thus they can be interpreted as probabilities that the model assigns to each key given the query.\n",
    "\n",
    "4. **Calculating the output**\n",
    "\n",
    "   `output = weights @ V`\n",
    "\n",
    "   The final step is to compute the output of the attention mechanism, which is a weighted sum of the values in `V`, where the weights are given by the attention weights. The result, `output`, has the same batch and sequence dimensions as the original input, but its feature dimension is `d_v` (the dimension of the value vectors), rather than the original input dimension.\n",
    "\n",
    "   The idea here is that for each query, the model calculates a weighted average of all the values, where the weight assigned to each value is determined by the corresponding query-key pair.\n",
    "\n",
    "Note: The code snippet provided only calculates the output for a single attention head. If you were implementing multi-head attention, you'd need to repeat this process for each head, concatenate the resulting output tensors, and then apply a final linear transformation to map the concatenated outputs to the desired output dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi Head\n",
    "\n",
    "In multi-head attention, we divide the model's attention into $h$ different \"heads\". Each head learns its own set of parameters, and therefore can potentially learn to focus on different features in the data. The reason we divide the dimensions by the number of heads is to keep the computational cost similar to that of single-head attention, as well as to allow each head to focus on a smaller (and hopefully different) subspace of the data.\n",
    "\n",
    "So in your notation:\n",
    "\n",
    "- $\\mathbf{W}_i^k, \\mathbf{W}_i^q$, and $\\mathbf{W}_i^v$ are the weight matrices for each head in the multi-head attention mechanism. The subscript $i$ denotes the $i$-th head. Each head has its own distinct set of weight matrices.\n",
    "\n",
    "- The dimensions of these weight matrices are smaller compared to the original dimensionality of the model. For instance, if your original model has an embedding dimension $d = 512$ and you have $h = 8$ heads, then each head will have weight matrices of size $d \\times \\frac{d}{h} = 512 \\times \\frac{512}{8} = 512 \\times 64$, if $d_k=d_v=d$.\n",
    "\n",
    "The advantage of this is that each head operates on lower-dimensional data, thereby reducing computational complexity. Moreover, each head learns different features, which enhances the model's power to understand and represent the input data. After each head has processed the data, their outputs are concatenated and transformed to the original dimensionality $d$ using the output weight matrix $\\mathbf{W}^o$.\n",
    "\n",
    "In a nutshell, multi-head attention allows the model to focus on different features in the data at different positions, increasing the expressive power of the model without significantly increasing the computational burden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some notations first:\n",
    "\n",
    "- $\\mathcal{V}$: Vocabulary set. This is a discrete set of tokens that the model is aware of. The size of this set is $V$.\n",
    "\n",
    "- $\\mathbf{X}$: This is the input sentence or text composed of tokens from the vocabulary set. This is a sequence of length $T$, where each token $x_t$ is an element of the vocabulary set, i.e., $x_t \\in \\mathcal{V}$ for all $t$.\n",
    "\n",
    "- $\\mathbf{W}_{emb}$: This is the embedding weight matrix. Each row of this matrix corresponds to the embedding of a token in the vocabulary set. Hence, this matrix is of size $V \\times D$, where $D$ is the dimension of the embedding space.\n",
    "\n",
    "Now, let's define the word/token embedding process:\n",
    "\n",
    "Given an input sequence $\\mathbf{X} = \\{x_1, x_2, ..., x_T\\}$, the goal is to transform each token $x_t$ into a dense vector that represents it in a continuous space.\n",
    "\n",
    "To achieve this, we use an embedding matrix $\\mathbf{W}_{emb}$. The embedding of a token $x_t$ is found by indexing the corresponding row of $\\mathbf{W}_{emb}$. The index is determined by the token's position in the vocabulary $\\mathcal{V}$. Mathematically, this can be written as:\n",
    "\n",
    "$$\n",
    "\\mathbf{e}_t = \\mathbf{W}_{emb}[x_t], \\quad \\text{for } t = 1, 2, ..., T\n",
    "$$\n",
    "\n",
    "where $\\mathbf{e}_t \\in \\mathbb{R}^D$ is the embedding vector for the token $x_t$. Note that in this context, we treat $x_t$ as an index, i.e., it is the position of the token $x_t$ in the vocabulary set $\\mathcal{V}$.\n",
    "\n",
    "Therefore, an input sequence $\\mathbf{X}$ is transformed into a sequence of embedding vectors $\\mathbf{E} = \\{\\mathbf{e}_1, \\mathbf{e}_2, ..., \\mathbf{e}_T\\}$, where each $\\mathbf{e}_t$ is a $D$-dimensional vector. Hence, $\\mathbf{E}$ is a matrix of size $T \\times D$. \n",
    "\n",
    "In implementation, this operation is usually performed as a matrix multiplication with one-hot-encoded vectors for the tokens, but in practice, a lookup operation is performed for efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of natural language processing tasks, each input sample (a sentence\n",
    "or a sequence of tokens) usually doesn't have a fixed length because sentences\n",
    "can be of various lengths. So, it's more common to denote the input as a\n",
    "sequence or a set rather than a fixed-dimension matrix. However, for simplicity,\n",
    "we often pad or truncate sentences to a maximum length, so that we can stack\n",
    "them into a 2D matrix for batch processing.\n",
    "\n",
    "Now, let's revisit the notations and make them more concrete:\n",
    "\n",
    "- $\\mathcal{V}$: Vocabulary set. This is a discrete set of tokens that the\n",
    "    model is aware of. The size of this set is $V$.\n",
    "\n",
    "- $\\mathbf{x}^{(n)}$: The $n$-th input sentence in the training set, composed\n",
    "    of tokens from the vocabulary set. It is a sequence of length $T_n$, where\n",
    "    each token $x_t^{(n)}$ is an element of the vocabulary set, i.e.,\n",
    "    $x_t^{(n)} \\in \\mathcal{V}$ for all $t$. If we have padded or truncated all\n",
    "    sentences to a maximum length $T$, then we can say\n",
    "    $\\mathbf{x}^{(n)} \\in \\mathbb{R}^T$.\n",
    "\n",
    "  - For example, the sentence \"hello human\" can be represented as\n",
    "        $\\mathbf{x}^{(n)} = \\{x_1^{(n)} = \\text{\"hello\"}, x_2^{(n)} = \\text{\"human\"}\\}$,\n",
    "        where $\\mathcal{V} = \\{\\text{\"hello\"}, \\text{\"human\"}\\}$.\n",
    "  - $T_n = 2$ in this case.\n",
    "\n",
    "- $\\mathbf{X}$: The entire training set, composed of $N$ input sentences\n",
    "    $\\{\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, ..., \\mathbf{x}^{(N)}\\}$. If we have\n",
    "    padded or truncated all sentences to a maximum length $T$, then we can say\n",
    "    $\\mathbf{X} \\in \\mathbb{R}^{N \\times T}$.\n",
    "\n",
    "    However, we need to one-hot encode our inputs, something not many talks about.\n",
    "    Let's see.\n",
    "\n",
    "- $\\mathbf{X} \\in \\mathbb{R}^{N \\times T \\times V}$: This is a 3D tensor representing the one-hot encoded representation of all sentences in the entire training set. For each sentence $n$, $\\mathbf{X}^{(n)}$ represents the sequence of one-hot encoded tokens, and each row of $\\mathbf{X}^{(n)}$ corresponds to the one-hot encoding of a token in the $n$-th sentence.\n",
    "\n",
    "- $\\mathbf{W}_{emb}$: This is the embedding weight matrix. Each row of this\n",
    "    matrix corresponds to the embedding of a token in the vocabulary set. Hence,\n",
    "    this matrix is of size $V \\times D$, where $D$ is the dimension of the\n",
    "    embedding space.\n",
    "\n",
    "- $\\mathbf{E} \\in \\mathbb{R}^{N \\times T \\times D}$: This is a 3D tensor representing the embeddings of all sentences in the entire training set. For each sentence $n$, $\\mathbf{E}^{(n)}$ represents the sequence of embedded tokens, and each row of $\\mathbf{E}^{(n)}$ corresponds to the embedding of a token in the $n$-th sentence.\n",
    "\n",
    "The embedding process is then:\n",
    "\n",
    "Given an input sentence\n",
    "$\\mathbf{x}^{(n)} = \\{x_1^{(n)}, x_2^{(n)}, ..., x_{T_n}^{(n)}\\}$, the goal is\n",
    "to transform each token $x_t$ into a dense vector that represents it in a\n",
    "continuous space.\n",
    "\n",
    "To achieve this, we use an embedding matrix $\\mathbf{W}_{emb}$. The embedding of\n",
    "a token $x_t^{(n)}$ is found by using $x_t^{(n)}$ as an index to select the\n",
    "corresponding row of $\\mathbf{W}_{emb}$. The index is determined by the token's\n",
    "position in the vocabulary $\\mathcal{V}$. Mathematically, this can be written\n",
    "as:\n",
    "\n",
    "The embedding of each token $x_t^{(n)}$ is:\n",
    "\n",
    "$$\n",
    "\\mathbf{e}_t^{(n)} = \\mathbf{W}_{emb}[x_t^{(n)}], \\quad \\text{for } t = 1, 2, ..., T_n\n",
    "$$\n",
    "\n",
    "where $\\mathbf{e}_t \\in \\mathbb{R}^D$ is the embedding vector for the token\n",
    "$x_t^{(n)}$. Note that in this context, we treat $x_t^{(n)}$ as an index, i.e.,\n",
    "it is the position of the token $x_t^{(n)}$ in the vocabulary set $\\mathcal{V}$.\n",
    "\n",
    "Therefore, an input sentence $\\mathbf{x}^{(n)}$ is transformed into a sequence\n",
    "of embedding vectors\n",
    "$\\mathbf{E}^{(n)} = \\{\\mathbf{e}_1^{(n)}, \\mathbf{e}_2^{(n)}, ..., \\mathbf{e}_{T_n}^{(n)}\\}$,\n",
    "where each $\\mathbf{e}_t^{(n)}$ is a $D$-dimensional vector obtained by using\n",
    "$x_t^{(n)}$ as an index to select the corresponding row of $\\mathbf{W}_{emb}$.\n",
    "If we have padded or truncated all sentences to a maximum length $T$, then we\n",
    "can say $\\mathbf{E}^{(n)} \\in \\mathbb{R}^{T \\times D}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{P}_{emb}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing from your last point, we now introduce the concept of positional embeddings. In models such as the Transformer, which do not have a built-in sense of word order or position, we often add positional information in the form of additional embeddings. This allows the model to learn and make use of positional relationships between words in a sentence.\n",
    "\n",
    "First, let's define the notation for positional embeddings:\n",
    "\n",
    "- $\\mathbf{W}_{pos}$: This is the positional embedding matrix. Each row of this matrix corresponds to the embedding of a position in a sequence. This matrix is usually of size $T \\times D$, where $T$ is the maximum length of a sequence we allow in the model, and $D$ is the dimension of the embedding space.\n",
    "\n",
    "The process to encode position into the embeddings is:\n",
    "\n",
    "Given an input sequence $\\mathbf{x}^{(n)} = \\{x_1^{(n)}, x_2^{(n)}, ..., x_{T_n}^{(n)}\\}$, we wish to add positional information to each token's embedding. \n",
    "\n",
    "For each position $t$ in the sequence, we select the $t$-th row from the positional embedding matrix $\\mathbf{W}_{pos}$ as the positional embedding. Mathematically, this can be written as:\n",
    "\n",
    "The positional embedding for the $t$-th position is:\n",
    "\n",
    "$$\n",
    "\\mathbf{p}_t = \\mathbf{W}_{pos}[t], \\quad \\text{for } t = 1, 2, ..., T_n\n",
    "$$\n",
    "\n",
    "where $\\mathbf{p}_t \\in \\mathbb{R}^D$ is the positional embedding vector for the $t$-th position.\n",
    "\n",
    "Then, the final embedding for each token in the sequence is obtained by adding the token's embedding vector $\\mathbf{e}_t^{(n)}$ to its positional embedding vector $\\mathbf{p}_t$. This can be written as:\n",
    "\n",
    "$$\n",
    "\\mathbf{e}'_t^{(n)} = \\mathbf{e}_t^{(n)} + \\mathbf{p}_t, \\quad \\text{for } t = 1, 2, ..., T_n\n",
    "$$\n",
    "\n",
    "where $\\mathbf{e}'_t^{(n)} \\in \\mathbb{R}^D$ is the final embedding vector for the $t$-th token in the $n$-th sequence, incorporating both the token's identity and its position.\n",
    "\n",
    "Therefore, an input sequence $\\mathbf{x}^{(n)}$ is transformed into a sequence of embedding vectors with positional information $\\mathbf{E}'^{(n)} = \\{\\mathbf{e}'_1^{(n)}, \\mathbf{e}'_2^{(n)}, ..., \\mathbf{e}'_{T_n}^{(n)}\\}$, where each $\\mathbf{e}'_t^{(n)}$ is a $D$-dimensional vector. If we have padded or truncated all sequences to a maximum length $T$, then we can say $\\mathbf{E}'^{(n)} \\in \\mathbb{R}^{T \\times D}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "galaxy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
