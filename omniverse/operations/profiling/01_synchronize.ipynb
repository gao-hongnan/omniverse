{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Synchronize CUDA To Time CUDA Operations\n","\n","[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n","[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n","[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n","![Tag](https://img.shields.io/badge/Tag-Brain_Dump-red)\n","![Tag](https://img.shields.io/badge/Level-Beginner-green)\n","\n","```{contents}\n",":local:\n","```"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T10:07:09.159212Z","iopub.status.busy":"2024-08-11T10:07:09.158364Z","iopub.status.idle":"2024-08-11T10:07:09.163031Z","shell.execute_reply":"2024-08-11T10:07:09.162107Z","shell.execute_reply.started":"2024-08-11T10:07:09.159180Z"},"trusted":true},"outputs":[],"source":["# %pip install -q omniverse==0.0.62"]},{"cell_type":"markdown","metadata":{},"source":["Timing with cuda operations is tricky, because cuda operations are asynchronous.\n","This means that the time taken to execute a cuda operation is not the time taken\n","to execute the operation, but the time taken to queue the operation. This is\n","because the operation is queued on the GPU, and the CPU continues to execute the\n","next operation. Consider the following code that runs on a P100 GPU (you can\n","test it on kaggle):"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T10:07:29.576479Z","iopub.status.busy":"2024-08-11T10:07:29.575836Z","iopub.status.idle":"2024-08-11T10:07:29.585906Z","shell.execute_reply":"2024-08-11T10:07:29.585018Z","shell.execute_reply.started":"2024-08-11T10:07:29.576446Z"},"trusted":true},"outputs":[],"source":["from __future__ import annotations\n","\n","from timeit import default_timer\n","from typing import Tuple\n","\n","import torch\n","import logging\n","import sys\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n","    handlers=[logging.StreamHandler(sys.stdout)],\n","    force=True,\n",")\n","logger = logging.getLogger(__name__)\n","\n","\n","def perform_matrix_multiplication(x: torch.Tensor, synchronize: bool = False) -> float:\n","    logger.info(\"Starting to time.\")\n","    start_time = default_timer()\n","\n","    _ = torch.matmul(x, x)\n","\n","    if synchronize:\n","        torch.cuda.synchronize()\n","\n","    end_time = default_timer()\n","\n","    time_taken = end_time - start_time\n","    logger.info(\"Ending timer. Time take is %s\", time_taken)\n","    return time_taken\n","\n","\n","def main(\n","    tensor_size: Tuple[int, int] = (20000, 20000), dtype: torch.dtype = torch.float32, synchronize: bool = False\n",") -> float:\n","    if not torch.cuda.is_available():\n","        raise RuntimeError(\"CUDA is not available. This example requires a CUDA-enabled GPU.\")\n","\n","    device: torch.device = torch.device(\"cuda\")\n","\n","    x: torch.Tensor = torch.randn(*tensor_size, device=device, dtype=dtype)\n","\n","    time = perform_matrix_multiplication(x, synchronize=synchronize)\n","    return time"]},{"cell_type":"markdown","metadata":{},"source":["You see that the time taken to complete the matmul `torch.matmul(x, x)` is so\n","fast at just $0.0003$ seconds. Different runs will give different results since\n","variance could be high here, but the point here will be apparent once we run\n","with the `torch.cuda.synchronize()` function."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T09:55:52.386155Z","iopub.status.busy":"2024-08-11T09:55:52.385443Z","iopub.status.idle":"2024-08-11T09:55:52.392266Z","shell.execute_reply":"2024-08-11T09:55:52.391422Z","shell.execute_reply.started":"2024-08-11T09:55:52.386125Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-08-11 09:55:52,387 - __main__ - INFO - Starting to time.\n","2024-08-11 09:55:52,389 - __main__ - INFO - Ending timer. Time take is 0.0003956739992645453\n"]}],"source":["_ = main(synchronize=False)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-11T09:56:05.934162Z","iopub.status.busy":"2024-08-11T09:56:05.933783Z","iopub.status.idle":"2024-08-11T09:56:07.828135Z","shell.execute_reply":"2024-08-11T09:56:07.827231Z","shell.execute_reply.started":"2024-08-11T09:56:05.934134Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-08-11 09:56:05,935 - __main__ - INFO - Starting to time.\n","2024-08-11 09:56:07,824 - __main__ - INFO - Ending timer. Time take is 1.8874175820001255\n"]}],"source":["_ = main(synchronize=True)"]},{"cell_type":"markdown","metadata":{},"source":["Now we see that the operation actually took around $2$ seconds to complete, that\n","is more than $6000$ times slower than the previous result. Why? Because in the\n","first run, you are essentially just measuring the time taken to queue the\n","operation, not the time taken to execute the operation. In other words, and less\n","pedantically since I am not well versed with CUDA on a deep level, once\n","`torch.matmul(x, x)` is called, the CPU queues the operation in the default CUDA\n","stream for execution on the GPU, the CPU does not wait for the operation to\n","complete, and continues to execute the next operation. In our case, if\n","`synchronize` is `False`, it will reach `end_time = default_timer()` and print\n","the time taken to queue the operation, which is very fast. This is possible\n","because GPU operations are asynchrnous and does not block the CPU here.\n","\n","However, if we set `synchronize` to `True`, the CPU is forced to wait until all\n","preceding CUDA operations are completed in the default stream before continuing.\n","\n","Even then, using the default timer is not the best way to measure time taken to\n","execute CUDA operations. One can read more on it such as using\n","`torch.cuda.Event` and other native profiling tools in the torch ecosystem."]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
