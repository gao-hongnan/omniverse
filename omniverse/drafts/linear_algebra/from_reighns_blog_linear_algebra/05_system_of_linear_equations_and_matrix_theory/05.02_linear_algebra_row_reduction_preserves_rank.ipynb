{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "513a59b7-8055-41d8-8258-52b018883495",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\F}{\\mathbb{F}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\v}{\\mathbf{v}}\n",
    "\\newcommand{\\a}{\\mathbf{a}}\n",
    "\\newcommand{\\b}{\\mathbf{b}}\n",
    "\\newcommand{\\c}{\\mathbf{c}}\n",
    "\\newcommand{\\e}{\\mathbf{e}}\n",
    "\\newcommand{\\w}{\\mathbf{w}}\n",
    "\\newcommand{\\u}{\\mathbf{u}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\z}{\\mathbf{z}}\n",
    "\\newcommand{\\0}{\\mathbf{0}}\n",
    "\\newcommand{\\1}{\\mathbf{1}}\n",
    "\\newcommand{\\A}{\\mathbf{A}}\n",
    "\\newcommand{\\B}{\\mathbf{B}}\n",
    "\\newcommand{\\C}{\\mathbf{C}}\n",
    "\\newcommand{\\E}{\\mathbf{E}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b3750-3873-44ee-8a99-8a623672ae83",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "This theorem is important enough to derserve a page of its own. One should note that the **column space** of a matrix $\\A$ is just the subspace spanned by its columns and **row space** is just the subspace spanned by its rows. We will give a formal treatment in the next section. For now, we will prove 2 key theorems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9fac4e-3ed0-4c26-9ff4-7f94eb73bf63",
   "metadata": {},
   "source": [
    "Here we prove that row elimination do not alter the **linear (in)dependence** of the rows and column space. One needs to first realize that any row operation on $\\A$ is equivalent to applying an left multiplcation of elementary matrix $\\E$ on $\\A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a6c65b-259e-4a01-b7e9-ec103ba116e5",
   "metadata": {},
   "source": [
    "## Row Operations Preserves Row Space and Linear In(Depedence)\n",
    "\n",
    "Given a matrix $\\A \\in \\F^{m \\times n}$, if one applies row elimination and row operations to the matrix $\\A$, show that it does not change the row space of $\\A$, and also preserves the linear (in)dependence of the rows of $\\A$, that is equivalent to saying that row elimination/operations do not change the dimension of the row space of $\\A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17092c-3982-45d4-93ca-9a8d55cea483",
   "metadata": {},
   "source": [
    "We prove this theorem first as it is relatively easier to see and visualize[^proof].\n",
    "\n",
    "- We first show that row elimination/operations on $\\A$ preserve the row space. That is, if we represent rows of $\\A$ as $\\v_1, \\v_2, ..., \\v_m$, then it suffices to prove that a row operation $\\E\\A$ preserves the row space: \n",
    "\n",
    "    $$\\textbf{span}(\\v_1, \\v_2, ..., \\v_m) = \\textbf{span}(\\e_1, \\e_2, ..., \\e_m)$$\n",
    "    \n",
    "where $\\v_i$ is the row $i$ of $\\A$ and $\\e_i$ row $i$ of $\\E$.\n",
    "\n",
    "Let $\\v_1,\\ldots,\\v_,$ denote the rows of $A$, and let $V$ be the row-space of $A$. In other words, \n",
    "$$\n",
    "V=\\text{span}\\,\\{\\v_1,\\ldots,\\v_m\\}. \n",
    "$$\n",
    "\n",
    "- Firstly, the easiest type of elementary operations is permutation of rows, which amount to permuting some $\\v_j$ and $\\v_k$ above; such operation will not change the span of $\\v_1,\\ldots,\\v_m$. \n",
    "- Secondly, multiplying a row with a non-zero constant does not change the row space; that is if we multiply any row of $\\A$ by $\\lambda$, then:\n",
    "\n",
    "    $$\\textbf{span}(\\v_1, \\ldots, \\v_i ,\\v_m) = \\textbf{span}(\\v_1, \\ldots, \\lambda_k\\v_k, \\v_m)$$\n",
    "    \n",
    "- Lastly, the row operation amounts to of replacing $\\v_k$ with $\\v_k+\\lambda \\v_j$. In this case, we can write \n",
    "\n",
    "    $$\n",
    "    \\alpha_1\\v_1+\\cdots+\\alpha_n\\v_m=\\alpha_1\\v_1+\\cdots+\\alpha_{k-1}\\v_{k-1}+\\alpha_k(\\v_k+\\lambda \\v_j)+\\alpha_{k+1}\\v_{k+1}+\\cdots (\\alpha_j-\\alpha_k\\lambda)\\v_j+\\cdots+\\alpha_n\\v_m\n",
    "    $$\n",
    "    \n",
    "So every linear combination of $\\v_1,\\ldots,\\v_m$ is also a linear combination of $\\v_1,\\ldots,\\v_m$ with \\$v_k$ replaced by $\\v_k+\\lambda \\v_j$. \n",
    "\n",
    "In summary, after doing any elementary operation to $v_1,\\ldots,v_n$, the span doesn't change. It follows directly that if $A$ and $B$ are row equivalent, since the rows of $B$ can be obtained by elementary operations from the rows of $A$, the spans of their rows are equal. \n",
    "\n",
    "If $A$ is invertible, then it is row equivalent to $I$, and so its row space is $F^n$. Conversely, if the row space of $A$ is $F^n$, then by writing each of $v_1,\\ldots,v_n$ in terms of the canonical basis, we get a recipe to go from $I$ to $A$ by elementary operations, and so $A$ is invertible. \n",
    "\n",
    "[^proof]: https://math.stackexchange.com/questions/1998138/elementary-row-operations-on-a-dont-change-the-row-space-of-a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e24b08-9c7d-4af0-9b8c-0ba37b41c851",
   "metadata": {},
   "source": [
    "- To show that the row operations also preserve the linear (in)depedence, we just need to prove that the dimension of the column space of $\\A$ before and after row operations is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde91433-5c40-4e68-957d-ae05ae538559",
   "metadata": {},
   "source": [
    "## Row Operations Preserve Column Space[^proof]\n",
    "\n",
    "The most important fact is that elementary row operations are realized as multiplication (on the left) by an invertible matrix. So the proof below just suffices to prove that if we have a matrix $\\A \\in \\F^{m \\times n}$, and if we apply a row operation on $\\A$ as $\\E\\A = \\B$, then we need a lemma in between to show that if given a matrix $\\A$, and we do a row operation on $\\A$ as $\\E\\A$ where $\\E$ is naturally invertible, then we suffice to show for the same column indices,  $a_{i_1},a_{i_2},\\dots,a_{i_k}$ are linearly independent if and only if $b_{i_1},b_{i_2},\\dots,b_{i_k}$ are linearly independent. This proof is just proving that the same set of linearly independent (or dependent) columns of $\\A$ coincides with the same set (indexed) columns of $\\B$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532d649d-3573-4a2f-96f8-dae7bf329dd4",
   "metadata": {},
   "source": [
    "Once you know this fact, you can proceed as follows. Suppose $A$ and $B$ are $m\\times n$ and there exists an invertible $m\\times m$ matrix $F$ such that $A=FB$. Denote by $a_1,a_2,\\dots,a_n$ and $b_1,b_2,\\dots,b_n$ the columns of $A$ and $B$ respectively.\n",
    "\n",
    "Consider indices $i_1,i_2,\\dots,i_k$ such that $1\\le i_1<i_2<\\dots<i_k\\le n$. Then the columns $a_{i_1},a_{i_2},\\dots,a_{i_k}$ are linearly independent if and only if $b_{i_1},b_{i_2},\\dots,b_{i_k}$ are linearly independent.\n",
    "\n",
    "It's sufficient to prove one implication, because $B=F^{-1}A$. So, suppose the columns $a_{i_1},a_{i_2},\\dots,a_{i_k}$ are linearly independent and that\n",
    "\n",
    "$$\n",
    "\\alpha_1b_{i_1}+\\alpha_2b_{i_2}+\\dots+\\alpha_kb_{i_k}=0\n",
    "$$\n",
    "\n",
    "Then we can multiply both sides by $F$ and get\n",
    "\n",
    "$$\n",
    "\\alpha_1Fb_{i_1}+\\alpha_2Fb_{i_2}+\\dots+\\alpha_kFb_{i_k}=0\n",
    "$$\n",
    "\n",
    "Since $Fb_i=a_i$, by definition of matrix product, we obtain\n",
    "\n",
    "$$\n",
    "\\alpha_1a_{i_1}+\\alpha_2a_{i_2}+\\dots+\\alpha_ka_{i_k}=0\n",
    "$$\n",
    "\n",
    "so $\\alpha_1=\\alpha_2=\\dots=\\alpha_k=0$.\n",
    "\n",
    "In a similar way, we see that a column $a_i$ of $A$ is a linear combination of the columns $a_{i_1},a_{i_2},\\dots,a_{i_k}$ if and only if $b_i$ is a linear combination of $b_{i_1},b_{i_2},\\dots,b_{i_k}$, *with the same coefficients*.\n",
    "\n",
    "As a consequence, $a_{i_1},a_{i_2},\\dots,a_{i_k}$ is a basis of the column space of $A$ if and only if $b_{i_1},b_{i_2},\\dots,b_{i_k}$ is a basis of the column space of $B$.\n",
    "\n",
    "In particular, the column space of $A$ has the same dimension as the column space of $B$. Therefore $A$ and $B$ have the same *column rank* (the dimension *is* the maximum number of linearly independent columns, of course, because the columns are, by definition, generators of the column space).\n",
    "\n",
    "---\n",
    "\n",
    "This has other important consequences. When you find a row echelon form $U$ for $A$, it's easy to see that the pivot columns of $U$ form a basis of the column space of $U$. Therefore, the columns of $A$ corresponding to the pivot columns form a basis of the column space of $A$. This provides an algorithm for extracting a basis from the columns of $A$.\n",
    "\n",
    "Not only this. If $U$ is the *reduced* row echelon form, we see that a nonpivot column is the linear combination of the pivot columns with lower column index and the coefficients in the nonpivot column are exactly those needed to write it as a linear combination.\n",
    "\n",
    "Thus the same coefficients can be used to express the columns of $A$ corresponding to nonpivot columns as linear combination of the already found basis for the column space of $A$. Thus the reduced row echelon form of $A$ is unique, because its entries only depend on the linear relations between the columns of $A$.\n",
    "\n",
    "---\n",
    "\n",
    "Elementary row operations also preserve the row rank (dimension of the row space or maximum number of linearly independent rows). This is easier, because the row space is unchanged by elementary row operations.\n",
    "\n",
    "This is obvious if the operation is swapping two rows. If the operation is multiplying a row by a nonzero constant, then the original row is a multiple of the new row, and conversely.\n",
    "\n",
    "If the operation is of the form $r_i+kr_j$, then $r_i=(r_i+kr_j)-kr_j$, and conversely.\n",
    "\n",
    "[^proof]: https://math.stackexchange.com/questions/3760618/how-prove-that-the-elementary-operations-dont-change-the-rank-of-a-matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5287d3f-672d-4098-b1e3-ff265db2e677",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Row Operations do not change null space and row space.\n",
    "- For null space $\\A\\x = \\0$, row eliminations also do not change the set of solutions\n",
    "- rank of matrix $\\A$ is the number of pivots after elimination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d89aef6-15cf-4838-85f2-fdd7116809e4",
   "metadata": {},
   "source": [
    "Yes, the Gaussian elimination does not preserve the column space (it does preserve the row space though). However, each time you perform an elementary operation, you basically multiply your original matrix by an invertible matrix. In the language of linear transformations, you act by an isomorphisms (linear invertible operators) and the isomorphisms keep linearly independent columns linearly independent and linearly dependent columns to be linearly dependent. Note that the columns with pivots are linearly independent, and all other columns in the row reduced echelon form can be represented as linear combinations of columns with pivots. Hence the conclusion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78cd0cb-da50-4c95-b2d2-c2d99e718a10",
   "metadata": {},
   "source": [
    "Consider matrix $\\A = \\begin{bmatrix} \\a_1 \\\\ \\a_2 \\\\ \\vdots \\\\ \\a_m \\end{bmatrix}$ where $\\a_i$ is the row of $\\A$.\n",
    "\n",
    "Then one might need to think intuitively why row reduction such as Gaussian Elimination will tell us the number of linearly independent rows by counting all the non-zero rows?\n",
    "\n",
    "First, a toy example, assume that out of $m$ rows, there exist $m-2$ linearly independent rows $\\a_1, \\cdots, \\a_{m-2}$ and the two linearly dependent rows are\n",
    "\n",
    "$$\\a_{m-1} = \\a_1 + \\cdots + \\a_{m-2} \\quad \\a_m = \\a_1 + \\cdots + \\a_{m-1}$$\n",
    "\n",
    "We can work out how row operations can kill (zero) out these linearly dependent rows. \n",
    "\n",
    "Firstly, if we kill $\\a_{m-1}$ first by doing the row operation of $\\a_{m-1} - (\\a_1 + \\cdots + \\a_{m-2})$.\n",
    "\n",
    "Then, $\\a_{m}$ will still be killed even though $\\a_{m-1}$ is reduced to the zero vector because we already know $\\a_{m-1} = \\a_1 + \\cdots + \\a_{m-2}$ and thus $\\a_m - \\left[(\\a_1 + \\cdots + a_{m-2}) + (\\a_1 + \\cdots + \\a_{m-2})\\right]$. \n",
    "\n",
    "How do we ensure that no other (linearly independent) rows are zeroed out? Suppose any other linearly independent rows can be zeroed out by other rows in $\\A$, then a contradiction occurred since they are linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9f7ef-7ec5-4fb8-a0b5-b260e3db8bb3",
   "metadata": {},
   "source": [
    "- https://math.stackexchange.com/questions/2078943/row-operations-do-not-change-the-dependency-relationships-among-columns\n",
    "- https://math.stackexchange.com/questions/354362/why-do-elementary-row-operations-preserve-linear-dependence-between-matrix-colum?rq=1\n",
    "- https://math.stackexchange.com/questions/3760618/how-prove-that-the-elementary-operations-dont-change-the-rank-of-a-matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
