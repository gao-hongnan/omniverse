{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeding and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Seed Number 1992\n"
     ]
    }
   ],
   "source": [
    "def seed_all(seed: int = 1930):\n",
    "    \"\"\"Seed all random number generators.\"\"\"\n",
    "    print(\"Using Seed Number {}\".format(seed))\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(\n",
    "        seed\n",
    "    )  # set PYTHONHASHSEED env var at fixed value\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)  # pytorch (both CPU and CUDA)\n",
    "    np.random.seed(seed)  # for numpy pseudo-random generator\n",
    "    random.seed(\n",
    "        seed\n",
    "    )  # set fixed value for python built-in pseudo-random generator\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n",
    "\n",
    "def seed_worker(_worker_id):\n",
    "    \"\"\"Seed a worker with the given ID.\"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "_ = seed_all(1992)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "> Extracted from Kaggle \n",
    "\n",
    "Serious complications can occur as a result of malpositioned lines and tubes in patients. Doctors and nurses frequently use checklists for placement of lifesaving equipment to ensure they follow protocol in managing patients. Yet, these steps can be time consuming and are still prone to human error, especially in stressful situations when hospitals are at capacity.\n",
    "\n",
    "Hospital patients can have catheters and lines inserted during the course of their admission and serious complications can arise if they are positioned incorrectly. Nasogastric tube malpositioning into the airways has been reported in up to 3% of cases, with up to 40% of these cases demonstrating complications [1-3]. Airway tube malposition in adult patients intubated outside the operating room is seen in up to 25% of cases [4,5]. The likelihood of complication is directly related to both the experience level and specialty of the proceduralist. Early recognition of malpositioned tubes is the key to preventing risky complications (even death), even more so now that millions of COVID-19 patients are in need of these tubes and lines.\n",
    "\n",
    "The gold standard for the confirmation of line and tube positions are chest radiographs. However, a physician or radiologist must manually check these chest x-rays to verify that the lines and tubes are in the optimal position. Not only does this leave room for human error, but delays are also common as radiologists can be busy reporting other scans. Deep learning algorithms may be able to automatically detect malpositioned catheters and lines. Once alerted, clinicians can reposition or remove them to avoid life-threatening complications.\n",
    "\n",
    "The Royal Australian and New Zealand College of Radiologists (RANZCR) is a not-for-profit professional organisation for clinical radiologists and radiation oncologists in Australia, New Zealand, and Singapore. The group is one of many medical organisations around the world (including the NHS) that recognizes malpositioned tubes and lines as preventable. RANZCR is helping design safety systems where such errors will be caught.\n",
    "\n",
    "In this competition, you’ll detect the presence and position of catheters and lines on chest x-rays. Use machine learning to train and test your model on 40,000 images to categorize a tube that is poorly placed.\n",
    "\n",
    "The dataset has been labelled with a set of definitions to ensure consistency with labelling. The normal category includes lines that were appropriately positioned and did not require repositioning. The borderline category includes lines that would ideally require some repositioning but would in most cases still function adequately in their current position. The abnormal category included lines that required immediate repositioning.\n",
    "\n",
    "If successful, your efforts may help clinicians save lives. Earlier detection of malpositioned catheters and lines is even more important as COVID-19 cases continue to surge. Many hospitals are at capacity and more patients are in need of these tubes and lines. Quick feedback on catheter and line placement could help clinicians better treat these patients. Beyond COVID-19, detection of line and tube position will ALWAYS be a requirement in many ill hospital patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Understanding our Data\n",
    "\n",
    "We will go through the data that we were given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Catalog/Description\n",
    "\n",
    "In this competition, you'll detect the presence and position of catheters and lines on chest x-rays. Use machine learning to train and test your model on $40,000$ images to categorize a tube that is poorly placed.\n",
    "\n",
    "- Train Set: $30083$ images\n",
    "- Public Test Set: $3582$ images\n",
    "- Private Test Set: ~$14000$ images\n",
    "- `train.csv`: containing **image IDs, labels and patient IDs**.\n",
    "- `sample_submission.csv`: a sample submission file in the correct format\n",
    "- `test/`: test images\n",
    "- `train/`: training images\n",
    "\n",
    "In particular the train csv has these columns:\n",
    "\n",
    "- StudyInstanceUID - unique ID for each image\n",
    "- ETT - Abnormal - endotracheal tube placement abnormal\n",
    "- ETT - Borderline - endotracheal tube placement borderline abnormal\n",
    "- ETT - Normal - endotracheal tube placement normal\n",
    "- NGT - Abnormal - nasogastric tube placement abnormal\n",
    "- NGT - Borderline - nasogastric tube placement borderline abnormal\n",
    "- NGT - Incompletely Imaged - nasogastric tube placement inconclusive due to imaging\n",
    "- NGT - Normal - nasogastric tube placement borderline normal\n",
    "- CVC - Abnormal - central venous catheter placement abnormal\n",
    "- CVC - Borderline - central venous catheter placement borderline abnormal\n",
    "- CVC - Normal - central venous catheter placement normal\n",
    "- Swan Ganz Catheter Present\n",
    "- PatientID - unique ID for each patient in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: Multi-Label Binary Classification\n",
    "\n",
    "In general, a patient's single chest X-ray could present multiple medical conditions.\n",
    "\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/images/patient_id_323464123.PNG\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Fig 1: Patient ID 323464123; By Hongnan G.</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Patient 323464123's first row corresponds to an unique image defined by the study instance UID.\n",
    "- Noticed the 1's under the columns `ETT-Normal`, `NGT - Incompletely Images`, `CVC-Borderline` and `CVC-Normal` and 0's elsewhere.\n",
    "- Unlike multi-class classification, where **classes are mutually exclusive**, **multi-label** is not.\n",
    "- For example, a patient's X-ray scan of the lungs can show up **pneumonia and covid-19** (both are conditions), as a result the *class labels* are not mutually exclusive (unlike multi-class). The same logic is applied in this setting, where the tube can be labelled differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics: Establish Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Macro-Averaged AUROC\n",
    "\n",
    "Then Macro-Average AUROC is calculated for each individual class, and then averaged over the total number of classes.\n",
    "\n",
    "$$\\text{Macro-Average AUROC} = \\frac{1}{\\text{num_class}}\\sum_{i=1}^{\\text{num_class}}R_{i}$$\n",
    "\n",
    "where $R_i$ is the AUROC score for each individual class.\n",
    "\n",
    "A small example clears up the air:\n",
    "\n",
    "- Consider 3 classes of apple, banana and carrot as a multi-label problem with class index $[0, 1, 2]$.\n",
    "- `y_true`: is a one-hot encoded matrix of 4 rows and 3 columns. The rows means 4 samples, and columns mean the class where 1 means positive and 0 negative. \n",
    "            It is not a surprise that each row can hold multiple 1's since it is a multi-label problem. `[0, 1, 1]` just means that the \"image data\" contains both banana and carrot;\n",
    "- `y_pred`: the predicted matrix, and is the same shape as `y_true`.\n",
    "\n",
    "We now treat the problem running multiple binary ROC computation:\n",
    "\n",
    "- Calculate the ROC score between column $i$ of `y_true` and `y_pred` respectively and call them $R_i$;\n",
    "- Sum $R_i$ and divide by the number of classes and get the Macro-Averaged AUROC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_true = np.asarray([[0, 1, 1],\n",
    "                     [0, 0, 1],\n",
    "                     [1, 1, 0],\n",
    "                     [1, 1, 1]])\n",
    "\n",
    "y_pred = np.asarray([[0, 1, 0],\n",
    "                     [1, 0, 0],\n",
    "                     [1, 1, 0],\n",
    "                     [1, 0, 1]])\n",
    "\n",
    "macro_auroc = np.mean([roc_auc_score(y_true[:, i], y_pred[:, i]) for i in range(3)])\n",
    "macro_auroc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! tip\n",
    "    For a more wholesome treatment of metrics, see my Melanoma write-up and [blog](https://reighns92.github.io/reighns-ml-blog/reighns_ml_journey/machine_learning_and_deep_learning/metrics/classification_metrics/roc_pr_curve/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and Resampling: Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How should we split out data into folds?\n",
    "\n",
    "We should examine the data for a few factors:\n",
    "\n",
    "1. Is the data $\\mathcal{X}$ imbalanced?\n",
    "2. Is the data $\\mathcal{X}$ generated in a **i.i.d.** manner, more specifically, if I split $\\mathcal{X}$ to $\\mathcal{X}_{train}$ and $\\mathcal{X}_{val}$, can we ensure that $\\mathcal{X}_{val}$ has no dependency on $\\mathcal{X}_{train}$?\n",
    "\n",
    "We came to the conclusion:\n",
    "\n",
    "1. Yes, there is quite some imbalanced distribution, in particular, **CVC - Normal**, **ETT - Normal** and **CVC - Borderline** are significantly more than the rest of the classes. Therefore, a stratified cross validation is reasonable. Stratified KFold ensures that relative class frequencies is approximately preserved in each train and validation fold. More concretely, we will not experience the scenario where $X_{train}$ has $m^{+}$ and $m^{-}$ positive and negative samples, but $X_{val}$ has only $p^{+}$ positive samples only and 0 negative samples, simply due to the scarcity of negative samples\n",
    "\n",
    "2. In medical imaging, it is a well known fact that most of the data contains patient level repeatedly. To put it bluntly, if I have 100 samples, and according to **PatientID**, we see that the id 123456 (John Doe) appeared 20 times, this is normal as a patient can undergo multiple settings of say, X-rays. If we allow John Doe's data to appear in both train and validation set, then this poses a problem of information leakage, in which the data is no longer **i.i.d.**. One can think of each patient has an \"unique, underlying features\" which are highly correlated across their different samples. As a result, it is paramount to ensure that amongst this 3255 unique patients, we need to ensure that each unique patients' images **DO NOT** appear in the validation fold. That is to say, if patient John Doe has 100 X-ray images, but during our 5-fold splits, he has 70 images in Fold 1-4, while 30 images are in Fold 5, then if we were to train on Fold 1-4 and validate on Fold 5, there may be potential leakage and the model will predict with confidence for John Doe's images. This is under the assumption that John Doe's data does not fulfill the i.i.d process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StratifiedGroupKFold\n",
    "\n",
    "With the above consideration, we will use `StratifiedGroupKFold` where $K = 5$ splits. There wasn't this splitting function in scikit-learn at the time of competition and as a result, we used a custom written (by someone else) `RepeatedStratifiedGroupKFold` function and just set `n_splits = 1` to get **StratifiedGroupKFold** (yes we cannot afford to do repeated sample, so setting the split to be 1 will collapse the repeated function to just the normal stratified group kfold). However, as of 2022, this function is readily available in the **Scikit-Learn** library.\n",
    "\n",
    "To recap, we applied stratified logic such that each train and validation set has an **equal** weightage of positive and negative samples. We also grouped the patients in the process such that patient $i$ will not appear in both training and validation set.\n",
    "\n",
    "> Data leakage can cause you to have blind confidence on your model. We are also guilty of committing one since we trained our models with the NiH pretrained weights, without taking into consideration if the weights overlap with the training and validation folds information. In other words, we did not check properly if the weights trained on the NiH dataset has information in our RANZCR dataset. Take note this is different from training altogether on the NiH dataset, we are merely using the weights instead of the imagenet weights, which brings to the next point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referring to figure 1, patient 323464123 has 6 images uniquely recorded on different visits. We should absolutely put them all in either train or validation set, as if say 2 images are in train and 4 in validation, there might be data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Cross-Validation Workflow\n",
    "\n",
    "To recap, we have the following:\n",
    "\n",
    "- **Training Set ($X_{\\text{train}}$)**: This will be further split into K validation sets during our cross-validation. This set is used to fit a particular hypothesis $h \\in \\mathcal{H}$.\n",
    "- **Validation Set ($X_{\\text{val}}$)**: This is split from our $X_{\\text{train}}$ during cross-validation. This set is used for model selection (i.e. find best hyperparameters, attempt to produce a best hypothesis $g \\in \\mathcal{H}$).\n",
    "- **Test Set ($X_{\\text{test}}$)**: This is an unseen test set, and we will only use it after we finish tuning our model/hypothesis. Suppose we have a final best model $g$, we will use $g$ to predict on the test set to get an estimate of the generalization error (also called out-of-sample error).\n",
    "\n",
    "<figure>\n",
    "<img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/aiap-coronary-artery-disease/data/images/cv.PNG' width=\"500\"/>\n",
    "<figcaption align = \"center\"><b>Pipeline.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "---\n",
    "\n",
    "<figure>\n",
    "<img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/images/grid_search_workflow.png' width=\"500\"/>\n",
    "<figcaption align = \"center\"><b>Courtesy of scikit-learn on a typical Cross-Validation workflow.</b></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning: The core of Deep Learning\n",
    "\n",
    "As we all know, if we train on `imagenet` weights, we may take quite a while to converge, even if we finetune it. The intuition is simple, `imagenet` were trained on many common items in life, and none of them resemble closely to the **image structures of X-rays**. Therefore, we have a few options.\n",
    "\n",
    "- Freeze earlier layers but unfreeze the later Conv Layers: this is intuitive as earlier layers detect shapes and colors, all low level details that is very useful even for such **dissimilar tasks**, and unfreeze the later conv layers which is what we call the \"abstract feature layers\", where it is more important for the model to learn from scratch. \n",
    "- Fine-Tuning;\n",
    "- Feature Extraction;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning\n",
    "\n",
    "Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. This is what we will be doing and we managed to find a set of `pretrained` weights trained specifically on this dataset as a starting point. The weights can be found **[here](https://www.kaggle.com/c/ranzcr-clip-catheter-line-classification/discussion/215910)**.\n",
    "\n",
    "We used a few models and found out that `resnet200d` has the best results on this set of training images. The reason we used this is mostly empirical, but using `gradcam` we can see how the model sees the images. \n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained. This is less relevant to us as we aren't using it for feature extractions.\n",
    "\n",
    "Therefore, the model may have a hard time detecting abstract features such shapes and details from the X-rays. We can of course unfreeze all the layers and retrain them from scratch, using various backbones, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Most preprocessing techniques we do in an image recognition competition is mostly as follows:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and Standard Deviation\n",
    "\n",
    "- Perform **mean and std** for the dataset given to us. Note that this step may make sense on paper, but empirically, using imagenet's default mean std will always work as well, if not better. Nevertheless, here are the stats:\n",
    "    - Imagenet:\n",
    "        - `mean = [0.485, 0.456, 0.406]` and;\n",
    "        - `std = [0.229, 0.224, 0.225]`\n",
    "        \n",
    "    - RANZCR:\n",
    "        - `mean = [0.4887381077884414, ...]` and;\n",
    "        - `std = [0.23064819430546407, ...]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Distribution\n",
    "\n",
    "This is usually done to check for \"surprises\". More specifically, I remember vividly when participating in Singapore Airline challenge where the classifier recognize weird objects as luggages. After plotting the pixel histogram, we observed that the luggages colors are all of a non-normal distribution, in fact, it is quite scattered. Then it dawned upon us that the classifier is learning the \"color\" too much, instead of the shape of the luggage. When we grayed out the images, the classifier starts to ignore the noise in the colors, and instead focus on other features like shapes. So this removes signal to noise in a way - using the objection detection example such as detecting a strawberry in a tree full of green leaves, then color is important, but if we detect leaves in a tree full of green leaves, we do not wish to incorporate color here as anything green might suggest that it is a leaf.\n",
    "\n",
    "We found, and as mentioned also by [Rueben Schmidt](https://www.kaggle.com/reubenschmidt) in [this post](https://www.kaggle.com/c/ranzcr-clip-catheter-line-classification/discussion/224146), there are some images that have black borders around them. I experimented by removed them during both the training process. There was no significant increase on the LB score, even if there was, it is in the 3-4th decimal places, but I noticed my local cv increased, so I think that some noise are removed locally, but not reflected in the test set. Therefore, during inference, I also removed the black borders, which should be the correct approach (learning from mistakes!). In conclusion, there is a small boost in score, if I keep this consistent in both training and inference, I reckon that no surprise factor would pop out.\n",
    "\n",
    "Here is the code:\n",
    "\n",
    "```python\n",
    "image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "mask  = image > 0\n",
    "image = image[np.ix_(mask.any(1), mask.any(0))]\n",
    "image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "```\n",
    "\n",
    "Notice that the code removes any pixel that is > 0, where black pixel is 0.\n",
    "\n",
    "> On hindsight for the Singapore Airline project, I now know there is **GradCam**, where we can see how the model is learning, as it will highlight the areas on which the model is focusing on in an image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Gray to RGB\n",
    "\n",
    "We know that **X-rays** are **Grayscale** images so converting a grayscale image to RGB is just setting `R=G=B=Grayscale` pixel for all channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256)\n",
      "(256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread(\"../images/1.2.826.0.1.3680043.8.498.10000428974990117276582711948006105617.png\", cv2.IMREAD_GRAYSCALE)\n",
    "print(image.shape)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations\n",
    "\n",
    "We know that augmentation is central in an image competition, as essentially we are adding more data into the training process, effectively **reducing overfitting and improve generalization**.\n",
    "\n",
    "Heavy augmentations are used during Train-Time-Augmentation. But during Test-Time-Augmentation, we used the same set of training augmentations to inference with $100\\%$ probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Time Augmentation\n",
    "\n",
    "The typicals!\n",
    "\n",
    "```python\n",
    "train_augmentations = [\n",
    "    albumentations.RandomResizedCrop(\n",
    "        height=config.image_size, width=config.image_size\n",
    "    ),\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "    albumentations.ShiftScaleRotate(p=0.5),\n",
    "    albumentations.HueSaturationValue(\n",
    "        hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5\n",
    "    ),\n",
    "    albumentations.RandomBrightnessContrast(\n",
    "        brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5\n",
    "    ),\n",
    "    albumentations.CoarseDropout(p=0.5),\n",
    "    albumentations.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "        max_pixel_value=255.0,\n",
    "        p=1.0,\n",
    "    ),\n",
    "    ToTensorV2(p=1.0),\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-Time Augmentation\n",
    "\n",
    "The exact same set of augmentations were used in inference. Not all TTAs provided a increase in score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architectures, Training Parameters & Tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "\n",
    "A brief overview of our model architecture is shown in fig 2.1 and 2.2:\n",
    "\n",
    "\n",
    "<div class=\"container\" style=\"display: inline-block;\">  \n",
    "  <figure>\n",
    "  <div style=\"float: left; padding: 10px;\">\n",
    "    <img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/images/single_head.png' width=\"666\" height=\"666\" align=\"center\"/>\n",
    "    <figcaption align=\"center\"><b>Fig 2.1; Single-Head Approach<br> courtesy of Tawara</b></figcaption>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"float: right; padding: 10px;\">\n",
    "    <img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/images/multi_heads.png' width=\"666\" height=\"666\" align=\"center\"/>\n",
    "    <figcaption align=\"center\"><b>Fig 2.2; Multi-Head Approach<br> courtesy of Tawara</b></figcaption>\n",
    "  </div>\n",
    "  </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backbone\n",
    "\n",
    "We used both `resnet200d` and `seresnet152d` but will focus more on the first model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used `resnet200d`, a ResNet variant model as our main backbone. \n",
    "\n",
    "`ResNet-D` is a modification on the ResNet architecture that utilises an average pooling tweak for downsampling. The motivation is that in the unmodified ResNet, the 1×1 convolution for the downsampling block ignores 3/4 of input feature maps, so this is modified so no information will be ignored.\n",
    "\n",
    "!!! info\n",
    "    So 1x1 convolutional reduces the feature maps depth but not the width or height while pooling reduces the width or height but not the depth. I think the results are better with the latter.\n",
    "    \n",
    "In our case, we did the following:\n",
    "\n",
    "- Create the model with: `model = timm.create_model('resnet200d', pretrained=True, num_classes=1000)`;\n",
    "- Load the pretrained weights from [NiH trained](https://www.kaggle.com/ammarali32/startingpointschestx);\n",
    "- Reset Classifier Head with Global Average Pooling: `self.model.reset_classifier(num_classes=0, global_pool=\"avg\")`;\n",
    "- Attach our own Classifier Head with 11 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "from torchinfo import summary\n",
    "\n",
    "model = timm.create_model('resnet18', pretrained=True, num_classes=1000)\n",
    "batch_size = 2\n",
    "image_shape = (3, 224, 224)\n",
    "input_image_tensor = torch.rand(size=(batch_size, *image_shape))\n",
    "\n",
    "\n",
    "# print(summary(model, (2, 3, 224, 224)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `model.reset_classifier(num_classes=0, global_pool=\"\")` means we **do not want global average pooling** and thus the shape at the last conv layer (penultimate layer) is $(2, 512, 7, 7)$\n",
    "- `model.reset_classifier(num_classes=0, global_pool=\"avg\")` means we **do want global average pooling** and thus the shape at the last conv layer (penultimate layer) is $(2, 512)$\n",
    "   whereby for each and every of the 512 feature maps $f_i$, we average $f_i$ across all pixels (i.e. if $f_i$ is 3 by 3 then average means add all $3 \\times 3 = 9$ pixels and average) and concat to become one $512$ vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([2, 1000])\n",
      "Unpooled shape: torch.Size([2, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('resnet18', pretrained=True, num_classes=1000)\n",
    "\n",
    "o = model(input_image_tensor)\n",
    "print(f'Original shape: {o.shape}')\n",
    "\n",
    "model.reset_classifier(num_classes=0, global_pool=\"\")\n",
    "\n",
    "o = model(input_image_tensor)\n",
    "print(f'Unpooled shape: {o.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([2, 1000])\n",
      "Pooled shape: torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('resnet18', pretrained=True, num_classes=1000)\n",
    "\n",
    "o = model(input_image_tensor)\n",
    "print(f'Original shape: {o.shape}')\n",
    "\n",
    "model.reset_classifier(num_classes=0, global_pool=\"avg\")\n",
    "\n",
    "o = model(input_image_tensor)\n",
    "print(f'Pooled shape: {o.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "Empirically, we realized the `ResNet200D` works very well for this particular task. We all asked ourselves why, and it was also discussed by many, but we all agreed that through various experiments, this model seems to consistently outperform their other SOTA counterparts. However, the closest possible paper on [Revisiting ResNets: Improved Training and Scaling Strategies](https://arxiv.org/abs/2103.07579).\n",
    "\n",
    "Of course, to add diversity to our final predictions, we trained one more `SeResNet152d` as well. In general, ensembling models with vastly different architectures may result in a more robust solution. As an example, you can think of each model as a \"average learner\", and if their structure is different, it may very well so learn information that the other model might miss, hence ensembling them will average out such differences. Later on I will touch upon an ensembling technique called **Forward Ensembling/Selection** in this task, it has since worked well for other similar competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Heads - Multi-Head Approach\n",
    "\n",
    "We will focus a bit more on the multi-heads we used.\n",
    "\n",
    "- [Reading on self attention in X-ray](https://towardsdatascience.com/self-attention-in-computer-vision-2782727021f6)\n",
    "\n",
    "- [What is a multi-headed model? And what exactly is a 'head' in a model?](https://stackoverflow.com/questions/56004483/what-is-a-multi-headed-model-and-what-exactly-is-a-head-in-a-model/56004582)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intuition\n",
    "\n",
    "The usage of multi-heads is not uncommon, let us detail a simple example in Object Detection.\n",
    "\n",
    "The image below shows the general architecture:\n",
    "\n",
    "![alt](../images/object_detection_head.png) \n",
    "\n",
    "- In object detection, we want to predict two things, the **image class label** and its **bounding box coordinates**;\n",
    "- The backbone network (\"convolution and pooling\") is responsible for extracting a feature map from the image that contains higher level summarized information. Each head uses this feature map as input to predict its desired outcome. \n",
    "- The main intuition why feature maps of the last few layers (last layer usually) are important is one needs to recognize the earlier conv layer's feature maps find simple features like shapes, sizes, edges from an image, while the deep conv layers will be of more abstract features in an image. As a result, we really just want the abstract feature maps as they are **more class specific to the image** instead of the earlier layers which gives **generic shapes**.\n",
    "- Let us say you used a **ResNet** as the backbone, then:\n",
    "    - Remove the classifier head, or rather just take the backbone which is all layers up to the last conv layer: say at the last layer. the output feature maps has a shape of (512, 7, 7);\n",
    "    - These 512 feature maps of 7 by 7 are high level information of the image encoded;\n",
    "    - We then connect a classification head to the backbone to predict the image's class given the true class labels;\n",
    "    - We then connect a regression head to the backbone to predict the image's bounding box coordinates given the true bounding box coordinates;\n",
    "- We can create two heads, one responsible for **classification** for the class label and the other **regression** to work on the localization of the bounding boxes;\n",
    "- Thus here we have 2 heads, one for classifying what class the image object is, the other is to localize where the image object is.\n",
    "- The loss that you optimize for during training is usually a weighted sum of the individual losses for each prediction head. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-Head for Multi-Label\n",
    "\n",
    "Since we have 11 targets in this competition and they can be divided into 4 distinct groups: \n",
    "\n",
    "- `ETT`,\n",
    "- `NGT`,\n",
    "- `CVC` and\n",
    "- `Swan`. \n",
    "\n",
    "We envision that different groups have different areas in images to focus on. One possible way to leverage this idea is a **multi-head approach** we talked about. Multipe groups can share one single CNN backbone but have independent **classifier** heads.\n",
    "\n",
    "<div class=\"container\" style=\"display: inline-block;\">  \n",
    "  <figure>\n",
    "  <div style=\"float: left; padding: 10px;\">\n",
    "    <img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/images/single_head.png' width=\"666\" height=\"666\" align=\"center\"/>\n",
    "    <figcaption align=\"center\"><b>Fig 2.1; Single-Head Approach<br> courtesy of Tawara</b></figcaption>\n",
    "  </div>\n",
    "\n",
    "  <div style=\"float: right; padding: 10px;\">\n",
    "    <img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/RANZCR%20CLiP%20-%20Catheter%20and%20Line%20Position%20Challenge/images/multi_heads.png' width=\"666\" height=\"666\" align=\"center\"/>\n",
    "    <figcaption align=\"center\"><b>Fig 2.2; Multi-Head Approach<br> courtesy of Tawara</b></figcaption>\n",
    "  </div>\n",
    "  </figure>\n",
    "</div>\n",
    " With this in mind, let us move on:\n",
    "\n",
    "**Multi-Label**\n",
    "\n",
    "This is a multi-label classification problem. The section on the activation functions fully explained the single head version of using sigmoid layer. In fact, it is not uncommon to train N number of heads on a N-class Multi-Label problem. One thing to note is that if your classification head is `Linear` layer only (with BCE loss), then the back gradient propagation is the same whether you train one head, or multiple heads. However, we have non-linear layers in the head, including the `SpatialAttentionBlock`! At the time of writing, I won't say I fully grasp of all the inner workings of an Attention Module across various use cases, but an analogy to aid my understanding is as follows:\n",
    "\n",
    "> Having taken Learning From Data from Professor Yaser, the inner joke is about the Hypothesis Space. Let me elaborate, given a resnet200D as our hypothesis space $\\mathcal{H}$, we aim to find a $h \\in \\mathcal{H}$ that best represents our true function $f$. Now suppose our learning algorithm $\\mathcal{A}$ does a good job in helping us to find such a optimal $h$, it may take time, maybe say 100 epochs before finding it. Now if I break down the problem into 4 parts, each corresponding to a group, and we \"aid\" the learning algorithm by giving more attention to 4 focused areas, then we might find both a good $h$ that estimate the $f$ well, and may even be faster!\n",
    "\n",
    "> If the above is too meh for understanding, imagine you are taking an exam in Machine Learning, as we all know, this field is a rabbit hole with never ending topics, let us say that there are 20 topics for you to study for the exam, you are dilligent and does that. But you have limited time and you decided to devote equal time to each topic, the consequence is you may not perform well for the exam due to limited understanding of each topic. Now, if I were to tell you that, hey, out of the 20 topics, can you study these 4 topics, as I think they have a higher chance of coming out, you will likely do better in the exam given that you devoted much more time on those \"focused (attention!)\" topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code explains this methodology with reference to the above images.\n",
    "\n",
    "We first note to the readers that typically, if we use a single head approach, where if we were given a problem set $\\mathcal{D} = X \\times y$, a hypothesis space $\\mathcal{H}$ we learn from a learning algorithm $\\mathcal{A}$, producing a final hypothesis $g$ (or h, depends on your notation), that predicts as such $g(X_{val}) = y_{\\text{val_pred}}$, where each element in $y_{\\text{val_pred}}$ corresponds to the class. Think of the basic MNIST example, our prediction vector's first element corresponds to the probability of it being an 0, and so on and so forth (assuming we use soft labels here). \n",
    "\n",
    "The change here is after the feature extraction layer (i.e. the feature logits after backbone), instead of just connecting it to a linear head for classification, we instead split the 11 outputs to 4 distinct groups. Each group will go through the head independent of the others, and this may prompt the model to put more **attention** on the independent groups. Finally, we `torch.cat(..,axis=1)` the outputs after they gone through their respective heads to recover the 11 outputs.\n",
    "\n",
    "```python\n",
    "model = CustomModel(\n",
    "    config,\n",
    "    pretrained=True,\n",
    "    load_weight=True,\n",
    "    load_url=False,\n",
    "    out_dim_heads=[3, 4, 3, 1],\n",
    ")\n",
    "# Multi Head\n",
    "for i, out_dim in enumerate(self.out_dim_heads):\n",
    "    layer_name = f\"head_{i}\"\n",
    "    layer = torch.nn.Sequential(\n",
    "        SpatialAttentionBlock(in_features, [64, 32, 16, 1]),\n",
    "        torch.nn.AdaptiveAvgPool2d(output_size=1),\n",
    "        torch.nn.Flatten(start_dim=1),\n",
    "        torch.nn.Linear(in_features, in_features),\n",
    "        self.activation,\n",
    "        torch.nn.Dropout(0.3),\n",
    "        torch.nn.Linear(in_features, out_dim),\n",
    "    )\n",
    "    setattr(self, layer_name, layer)\n",
    "\n",
    "def forward(self, input_neurons):\n",
    "    \"\"\"Define the computation performed at every call.\"\"\"\n",
    "    if self.use_custom_layers is False:\n",
    "        output_predictions = self.model(input_neurons)\n",
    "    else:\n",
    "        if len(self.out_dim_heads) > 1:\n",
    "            output_logits_backbone = self.architecture[\"backbone\"](input_neurons)\n",
    "            multi_outputs = [\n",
    "                getattr(self, f\"head_{i}\")(output_logits_backbone)\n",
    "                for i in range(self.num_heads)\n",
    "            ]\n",
    "            output_predictions = torch.cat(multi_outputs, axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We built-upon fellow Kaggler **Tawara’s** Multi-head model for our best scoring models. In particular, we experimented with the activation functions and dropout rates. We found models with `Swish` activation in the `multi-head` component of the network to perform > best in our experiments. Our best scoring single model is a multi-head model with a `resnet200d` backbone. In particular, one single fold of `resnet200d` gives a private score of 0.970. \n",
    "> Another very interesting approach is 3-4 stage training. We did not have time to experiment with the 3-4 stage training as we joined the competition late.\n",
    "\n",
    "**Model Architectures:**\n",
    "                    layer = torch.nn.Sequential(\n",
    "                        SpatialAttentionBlock(in_features, [64, 32, 16, 1]),\n",
    "                        torch.nn.AdaptiveAvgPool2d(output_size=1),\n",
    "                        torch.nn.Flatten(start_dim=1),\n",
    "                        torch.nn.Linear(in_features, in_features),\n",
    "                        self.activation,\n",
    "                        torch.nn.Dropout(0.3),\n",
    "                        torch.nn.Linear(in_features, out_dim),\n",
    "                    )\n",
    "- **Backbone**: `ResNet200D` and `SeResNet152d`\n",
    "- **Classifier Head:** Separated and Independent **Spatial-Attention Module** and the typical Multi-Layer Perceptron for Target Group (ETT(3), NGT(4), CVC(3), and Swan(1)).\n",
    "    - **Spatial-Attention Module:** `SpatialAttentionBlock(in_features, [64, 32, 16, 1])`\n",
    "    - **MLP:**: `Linear -> Swish -> Dropout -> Linear`; It is worth noting after the `Linear` layer, there is a `Sigmoid` layer in this particular setup as we are using `BCEWITHLOGITSLOSS` from PyTorch for numerical stability.\n",
    "- **Activation:** One thing to note is we used `Swish` in our Classifier Head. Swish is a smooth and non-monotonic function, the latter contrasts when compared to many other activations. I will explain a bit in the next section.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "As we all know, activation functions are used to transform a neurons' linearity to non-linearity and decide whether to \"fire\" a neuron or not.\n",
    "\n",
    "We chose **Swish** as our main activation function in the classifier head layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Swish\n",
    "\n",
    "When we design or choose an activation function, we need to ensure the follows:\n",
    "\n",
    "- (Smoothness) Differentiable and Continuous: For example, the sigmoid function is continuous and hence differentiable. If the property is not fulfilled, we might face issues as backpropagation may not be performed properly since we cannot differentiate it.If you notice, the heaviside function is not. We cant perform GD using the HF as we cannot compute gradients but for the logistic function we can. The gradient of sigmoid function g is g(1-g) conveniently\n",
    "\n",
    "- Monotonic: This helps the model to converge faster. But spoiler alert, Swish is not monotonic.\n",
    "\n",
    "The properties of Swish are as follows:\n",
    "\n",
    "- Bounded below: It is claimed in the paper it serves as a strong regularization.\n",
    "- Smoothness: More smooth than ReLU which allows the model to optimize better, the error landscape, when smoothed, is easier to traverse in order to find a minima. An intuitive idea is the hill again, imagine you traverse down Bukit Timah Hill, vs traversing down Mount Himalaya LOL!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how swish looks like when plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=[-10.          -7.77777778  -5.55555556  -3.33333333  -1.11111111\n",
      "   1.11111111   3.33333333   5.55555556   7.77777778  10.        ]\n",
      "\n",
      "z=swish(x)=[-4.53978687e-04 -3.25707421e-03 -2.13946242e-02 -1.14817319e-01\n",
      " -2.75182001e-01  8.35929110e-01  3.21851601e+00  5.53416093e+00\n",
      "  7.77452070e+00  9.99954602e+00]\n",
      "\n",
      "min z = -0.27518200126563513\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def swish(x):\n",
    "    sigmoid = 1 / (1 + np.exp(-x))\n",
    "    swish = x * sigmoid\n",
    "    return swish\n",
    "\n",
    "\n",
    "epsilon = 1e-20\n",
    "x = np.linspace(-10, 10, 10)\n",
    "z = swish(x)\n",
    "print(f\"x={x}\")\n",
    "print(f\"\\nz=swish(x)={z}\")\n",
    "print(f\"\\nmin z = {min(z)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, z)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Swish(X)\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture: Final Activation Layer\n",
    "\n",
    "> [Sigmoid vs Softmax](https://stats.stackexchange.com/questions/233658/softmax-vs-sigmoid-function-in-logistic-classifier) I've noticed people often get directed to this question when searching whether to use sigmoid vs softmax in neural networks. If you are one of those people building a neural network classifier, here is how to decide whether to apply sigmoid or softmax to the raw output values from your network:\n",
    "\n",
    "- If you have a multi-label classification problem = there is more than one \"right answer\" = the outputs are NOT mutually exclusive, then use a sigmoid function on each raw output independently. The sigmoid will allow you to have high probability for all of your classes, some of them, or none of them. Example: classifying diseases in a chest x-ray image. The image might contain pneumonia, emphysema, and/or cancer, or none of those findings.\n",
    "\n",
    "---\n",
    "\n",
    "- If you have a multi-class classification problem = there is only one \"right answer\" = the outputs are mutually exclusive, then use a softmax function. The softmax will enforce that the sum of the probabilities of your output classes are equal to one, so in order to increase the probability of a particular class, your model must correspondingly decrease the probability of at least one of the other classes. Example: classifying images from the MNIST data set of handwritten digits. A single picture of a digit has only one true identity - the picture cannot be a 7 and an 8 at the same time.\n",
    "\n",
    "---\n",
    "\n",
    "In the below code we understand that our model's `forward()` call gives us a output `output_logits` of shape (8, 11) if the batch size is 8, and the 11 represents each logit for each of the class. \n",
    "\n",
    "- If we apply `Softmax` to this function on `dimension=1`, it simply means we are applying the function each row, from row 1 to 8. Take row 1 for example, the softmax function will squash all the 11 values into a 0-1 range, you can say this is a probability calibration, and the `output_predictions` is also of shape `(8, 11)` but all sums up to 1.\n",
    "\n",
    "- If we apply `Sigmoid` to this function on `dimension=1`, although PyTorch does not specifiy this because it automatically assumes we are applying sigmoid elementwise, that is to say you cannot simply pass an array of 11 elements to sigmoid function and but we are applying the sigmoid function each row as well. There is a lot of nuance and intricacies here. We take the first row as an example, the first element corresponds to the class **ETT-Abnormal**, when we apply `sigmoid` to this element 0.0762, we get 0.5190, and for the second element class **ETT-Borderline**, we have 0.0877 and when we apply sigmoid, we get 0.5219, so on and so forth for the first row. You should by now observe that they do not sum to 1. This is because each time sigmoid is applied, it is in a **one-vs-all** scenario. Meaning to say, the 0.519 for **ETT-Abnormal** means that **ETT-Abnormal** is treated as the positive class, and the remaining 10 classes are treated as negative class 0. In other words, with 11 elements and sigmoid, we are essentially performing 11 binary classification on the said 11 classes. So 0.519 actually means that the probability of it being class 1 (**ETT-Abnormal**) is 0.519, and the probability of it being NOT class 1 (ALL other classes) is 0.481. The same logic applies to each of the element in the first row. One thing worth noting is that the predictions for row 1 is not **mutually exclusive**, meaning that from the 11 classes, we can have say, **ETT-Abnormal, NGH-Abnormal, CVC-Abnormal** to all have say probability score of 0.9, meaning to say, it is highly likely to be all 3 conditions! This is okay and common in X-Ray imaging.\n",
    "\n",
    "    - In the table dataframe below, I put them into a dataframe for easy visualization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"ETT - Abnormal\",\n",
    "    \"ETT - Borderline\",\n",
    "    \"ETT - Normal\",\n",
    "    \n",
    "    \"NGT - Abnormal\",\n",
    "    \"NGT - Borderline\",\n",
    "    \"NGT - Incompletely Imaged\",\n",
    "    \"NGT - Normal\",\n",
    "    \n",
    "    \"CVC - Abnormal\",\n",
    "    \"CVC - Borderline\",\n",
    "    \"CVC - Normal\",\n",
    "    \n",
    "    \"Swan Ganz Catheter Present\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "output_logits = torch.tensor([  [ 0.0762,  0.0877,  0.1205, -0.0615, -0.0054,  0.0661,  0.1567, -0.0978, 0.0248, -0.0350,  0.0084],\n",
    "                                [-0.0196, -0.0729,  0.0534,  0.0307, -0.0428, -0.0016,  0.0013, -0.0247, -0.0094, -0.0424,  0.0192],\n",
    "                                [-0.0125, -0.0310,  0.0118, -0.1301,  0.0418,  0.0229,  0.0139, -0.0526, 0.0870, -0.0681, -0.0068],\n",
    "                                [-0.0259, -0.0544, -0.0262,  0.0018,  0.0161, -0.0369, -0.0370, -0.0157, 0.0036, -0.0592,  0.0107],\n",
    "                                [-0.0366, -0.0695,  0.0740, -0.0353, -0.0363, -0.0019,  0.0085, -0.0144, 0.0129, -0.0470,  0.0043],\n",
    "                                [-0.0445, -0.0822,  0.0487, -0.0851,  0.0269, -0.0809, -0.0434,  0.0110, -0.0631, -0.0733, -0.0188],\n",
    "                                [-0.0304,  0.0012,  0.0233, -0.0121, -0.0406, -0.0459, -0.0363,  0.0089,-0.0009, -0.0797, -0.0017],\n",
    "                                [-0.0415,  0.0787,  0.0283, -0.0617, -0.0526, -0.0016, -0.0409, -0.0481, 0.0583, -0.0810, -0.0050]],\n",
    "                                dtype=torch.float64, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5190, 0.5219, 0.5301, 0.4846, 0.4987, 0.5165, 0.5391, 0.4756, 0.5062,\n",
      "         0.4913, 0.5021],\n",
      "        [0.4951, 0.4818, 0.5133, 0.5077, 0.4893, 0.4996, 0.5003, 0.4938, 0.4977,\n",
      "         0.4894, 0.5048],\n",
      "        [0.4969, 0.4923, 0.5029, 0.4675, 0.5104, 0.5057, 0.5035, 0.4869, 0.5217,\n",
      "         0.4830, 0.4983],\n",
      "        [0.4935, 0.4864, 0.4935, 0.5004, 0.5040, 0.4908, 0.4908, 0.4961, 0.5009,\n",
      "         0.4852, 0.5027],\n",
      "        [0.4909, 0.4826, 0.5185, 0.4912, 0.4909, 0.4995, 0.5021, 0.4964, 0.5032,\n",
      "         0.4883, 0.5011],\n",
      "        [0.4889, 0.4795, 0.5122, 0.4787, 0.5067, 0.4798, 0.4892, 0.5027, 0.4842,\n",
      "         0.4817, 0.4953],\n",
      "        [0.4924, 0.5003, 0.5058, 0.4970, 0.4899, 0.4885, 0.4909, 0.5022, 0.4998,\n",
      "         0.4801, 0.4996],\n",
      "        [0.4896, 0.5197, 0.5071, 0.4846, 0.4869, 0.4996, 0.4898, 0.4880, 0.5146,\n",
      "         0.4798, 0.4988]], dtype=torch.float64)\n",
      "tensor([[0.0948, 0.0959, 0.0991, 0.0826, 0.0874, 0.0939, 0.1028, 0.0797, 0.0901,\n",
      "         0.0849, 0.0886],\n",
      "        [0.0900, 0.0853, 0.0968, 0.0946, 0.0879, 0.0916, 0.0919, 0.0895, 0.0909,\n",
      "         0.0879, 0.0935],\n",
      "        [0.0907, 0.0890, 0.0929, 0.0806, 0.0957, 0.0939, 0.0931, 0.0871, 0.1001,\n",
      "         0.0858, 0.0912],\n",
      "        [0.0904, 0.0878, 0.0903, 0.0929, 0.0942, 0.0894, 0.0894, 0.0913, 0.0931,\n",
      "         0.0874, 0.0937],\n",
      "        [0.0887, 0.0858, 0.0991, 0.0888, 0.0887, 0.0918, 0.0928, 0.0907, 0.0932,\n",
      "         0.0878, 0.0924],\n",
      "        [0.0901, 0.0868, 0.0989, 0.0865, 0.0968, 0.0869, 0.0902, 0.0953, 0.0885,\n",
      "         0.0876, 0.0925],\n",
      "        [0.0899, 0.0928, 0.0948, 0.0915, 0.0890, 0.0885, 0.0894, 0.0935, 0.0926,\n",
      "         0.0856, 0.0925],\n",
      "        [0.0884, 0.0997, 0.0948, 0.0867, 0.0875, 0.0920, 0.0885, 0.0879, 0.0977,\n",
      "         0.0850, 0.0917]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "sigmoid = torch.nn.Sigmoid()\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "output_predictions_sigmoid = sigmoid(output_logits)\n",
    "output_predictions_softmax = softmax(output_logits)\n",
    "print(output_predictions_sigmoid)\n",
    "print(output_predictions_softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ETT - Abnormal</th>\n",
       "      <th>ETT - Borderline</th>\n",
       "      <th>ETT - Normal</th>\n",
       "      <th>NGT - Abnormal</th>\n",
       "      <th>NGT - Borderline</th>\n",
       "      <th>NGT - Incompletely Imaged</th>\n",
       "      <th>NGT - Normal</th>\n",
       "      <th>CVC - Abnormal</th>\n",
       "      <th>CVC - Borderline</th>\n",
       "      <th>CVC - Normal</th>\n",
       "      <th>Swan Ganz Catheter Present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.519041</td>\n",
       "      <td>0.521911</td>\n",
       "      <td>0.530089</td>\n",
       "      <td>0.484630</td>\n",
       "      <td>0.498650</td>\n",
       "      <td>0.516519</td>\n",
       "      <td>0.539095</td>\n",
       "      <td>0.475569</td>\n",
       "      <td>0.506200</td>\n",
       "      <td>0.491251</td>\n",
       "      <td>0.502100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.495100</td>\n",
       "      <td>0.481783</td>\n",
       "      <td>0.513347</td>\n",
       "      <td>0.507674</td>\n",
       "      <td>0.489302</td>\n",
       "      <td>0.499600</td>\n",
       "      <td>0.500325</td>\n",
       "      <td>0.493825</td>\n",
       "      <td>0.497650</td>\n",
       "      <td>0.489402</td>\n",
       "      <td>0.504800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.496875</td>\n",
       "      <td>0.492251</td>\n",
       "      <td>0.502950</td>\n",
       "      <td>0.467521</td>\n",
       "      <td>0.510448</td>\n",
       "      <td>0.505725</td>\n",
       "      <td>0.503475</td>\n",
       "      <td>0.486853</td>\n",
       "      <td>0.521736</td>\n",
       "      <td>0.482982</td>\n",
       "      <td>0.498300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.493525</td>\n",
       "      <td>0.486403</td>\n",
       "      <td>0.493450</td>\n",
       "      <td>0.500450</td>\n",
       "      <td>0.504025</td>\n",
       "      <td>0.490776</td>\n",
       "      <td>0.490751</td>\n",
       "      <td>0.496075</td>\n",
       "      <td>0.500900</td>\n",
       "      <td>0.485204</td>\n",
       "      <td>0.502675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.490851</td>\n",
       "      <td>0.482632</td>\n",
       "      <td>0.518492</td>\n",
       "      <td>0.491176</td>\n",
       "      <td>0.490926</td>\n",
       "      <td>0.499525</td>\n",
       "      <td>0.502125</td>\n",
       "      <td>0.496400</td>\n",
       "      <td>0.503225</td>\n",
       "      <td>0.488252</td>\n",
       "      <td>0.501075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.488877</td>\n",
       "      <td>0.479462</td>\n",
       "      <td>0.512173</td>\n",
       "      <td>0.478738</td>\n",
       "      <td>0.506725</td>\n",
       "      <td>0.479786</td>\n",
       "      <td>0.489152</td>\n",
       "      <td>0.502750</td>\n",
       "      <td>0.484230</td>\n",
       "      <td>0.481683</td>\n",
       "      <td>0.495300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.492401</td>\n",
       "      <td>0.500300</td>\n",
       "      <td>0.505825</td>\n",
       "      <td>0.496975</td>\n",
       "      <td>0.489851</td>\n",
       "      <td>0.488527</td>\n",
       "      <td>0.490926</td>\n",
       "      <td>0.502225</td>\n",
       "      <td>0.499775</td>\n",
       "      <td>0.480086</td>\n",
       "      <td>0.499575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.489626</td>\n",
       "      <td>0.519665</td>\n",
       "      <td>0.507075</td>\n",
       "      <td>0.484580</td>\n",
       "      <td>0.486853</td>\n",
       "      <td>0.499600</td>\n",
       "      <td>0.489776</td>\n",
       "      <td>0.487977</td>\n",
       "      <td>0.514571</td>\n",
       "      <td>0.479761</td>\n",
       "      <td>0.498750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ETT - Abnormal  ETT - Borderline  ETT - Normal  NGT - Abnormal  \\\n",
       "0        0.519041          0.521911      0.530089        0.484630   \n",
       "1        0.495100          0.481783      0.513347        0.507674   \n",
       "2        0.496875          0.492251      0.502950        0.467521   \n",
       "3        0.493525          0.486403      0.493450        0.500450   \n",
       "4        0.490851          0.482632      0.518492        0.491176   \n",
       "5        0.488877          0.479462      0.512173        0.478738   \n",
       "6        0.492401          0.500300      0.505825        0.496975   \n",
       "7        0.489626          0.519665      0.507075        0.484580   \n",
       "\n",
       "   NGT - Borderline  NGT - Incompletely Imaged  NGT - Normal  CVC - Abnormal  \\\n",
       "0          0.498650                   0.516519      0.539095        0.475569   \n",
       "1          0.489302                   0.499600      0.500325        0.493825   \n",
       "2          0.510448                   0.505725      0.503475        0.486853   \n",
       "3          0.504025                   0.490776      0.490751        0.496075   \n",
       "4          0.490926                   0.499525      0.502125        0.496400   \n",
       "5          0.506725                   0.479786      0.489152        0.502750   \n",
       "6          0.489851                   0.488527      0.490926        0.502225   \n",
       "7          0.486853                   0.499600      0.489776        0.487977   \n",
       "\n",
       "   CVC - Borderline  CVC - Normal  Swan Ganz Catheter Present  \n",
       "0          0.506200      0.491251                    0.502100  \n",
       "1          0.497650      0.489402                    0.504800  \n",
       "2          0.521736      0.482982                    0.498300  \n",
       "3          0.500900      0.485204                    0.502675  \n",
       "4          0.503225      0.488252                    0.501075  \n",
       "5          0.484230      0.481683                    0.495300  \n",
       "6          0.499775      0.480086                    0.499575  \n",
       "7          0.514571      0.479761                    0.498750  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(data =output_predictions_sigmoid.detach().cpu().numpy(), columns=classes)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size and Tricks\n",
    "\n",
    "Due to hardware limitation, we can barely fit in anything more than a `batch_size` of 8.\n",
    "\n",
    "Quoting from [here](https://arxiv.org/abs/1609.04836):\n",
    "\n",
    "> It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize [...]\n",
    "\n",
    "> large-batch methods tend to converge to sharp minimizers of the training and testing functions—and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation.\n",
    "\n",
    "The above shows that large batch size may `fit` the model too well, as the model will learn features of the dataset in less iterations, and may memorize this particular dataset's features, leading to overfitting and poor generalization. However, too small a batch size causes our convergence to go too slow, empirically, we take 32 or 64 as the ideal batch size in this competition. \n",
    "\n",
    "We used both `torch.amp` and `gradient accumulation` to be able to fit more batch sizes. We did not freeze the `batch_norm` layers, which still yielded great results. What we should have done is to experiment more on how to freeze the batch norm layers properly, as I believe that it may help. In the end, we used a batch size of 8 and fit 4 iterations using `gradient accumulation`  and trained a total number of 20 epochs to get a local CV score of roughly 0.969."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer, Scheduler and Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheduler\n",
    "\n",
    "The configuration can be seen here. But note that we incorporated `GradualWarmUpScheduler` along with `CosineAnnealingLR`, we also experimented with `CosineAnnealingWarmRestarts`, the results are similar.\n",
    "\n",
    "From the paper [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677), we learnt about the warmup technique. Although the context of the paper was training under large batch size, we find it helpful even in small batches for the training to converge. \n",
    "\n",
    "The basic algorithm is as follows:\n",
    "\n",
    "1. Set a base_lr or initial lr for what you want in a model, say 1e-4.\n",
    "2. If we set our warmup epoch to be 10, then we will start from with 1e-4/10 in the first epoch, and take equal steps each time to converge to 1e-4 in the 10th epoch.\n",
    "3. After the 10th epoch, warmup ends, we start applying our scheduler's normal steps.\n",
    "\n",
    "\n",
    "However, I took quite some time to understand the idea of gradual warmup, I made my understanding [here](https://github.com/reigHns/reighns-pytorch-gradual-warmup-scheduler/blob/master/src/run.py).\n",
    "\n",
    "We should try `OneCyclePolicy` as detailed by fastai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "num_epochs = 20\n",
    "\n",
    "CosineAnnealingLR:\n",
    "    T_max: num_epochs - 1\n",
    "    eta_min: 1.0e-07\n",
    "    last_epoch: -1\n",
    "    verbose: true\n",
    "```\n",
    "\n",
    "Notice in my configuration above, we set the parameter `T_max` to be the 19, which is like a one-shot training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "model = torch.nn.Linear(2, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=100)\n",
    "cosine_annealing_lr_scheduler_one_shot = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=19, eta_min=1e-7)\n",
    "\n",
    "cosine_annealing_lr_one_shot = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    optimizer.step()\n",
    "    cosine_annealing_lr_one_shot.append(optimizer.param_groups[0][\"lr\"])\n",
    "    \n",
    "#     print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"])\n",
    "    cosine_annealing_lr_scheduler_one_shot.step()\n",
    "\n",
    "plt.plot(cosine_annealing_lr_one_shot);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x260cde7e7c0>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "model = torch.nn.Linear(2, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=100)\n",
    "\n",
    "cosine_annealing_lr_scheduler_normal= torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2, eta_min=1e-7)\n",
    "cosine_annealing_lr_normal = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    optimizer.step()\n",
    "    cosine_annealing_lr_normal.append(optimizer.param_groups[0][\"lr\"])\n",
    "    \n",
    "#     print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"])\n",
    "    cosine_annealing_lr_scheduler_normal.step()\n",
    "\n",
    "plt.plot(cosine_annealing_lr_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss\n",
    "\n",
    "We should also experiment with `Focal Loss` but seeing negative results from fellow Kagglers, on top with limited resources, we did not try it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Ensembling\n",
    "\n",
    "We made use of the [Forward Ensembling](https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/175614) idea from Chris in SIIM-ISIC Melanoma Classification back in August 2020, I modified the code for this specific task. A simple description is as follows, modified from Chris, with more mathematical notations.\n",
    "\n",
    "1. We start off with a dataset $\\mathcal{D} = X \\times y$ where it is sampled from the true population $\\mathcal{X} \\times \\mathcal{Y}$.\n",
    "2. We apply KFold (5 splits) to the dataset, as illustrated in the diagram. \n",
    "3. We can now train five different hypothesis $h_{F1}, h_{F2},...,h_{F5}$, where $h_{F1}$ is trained on Fold 2 to Fold 5 and predict on Fold 1, $h_{F2}$ is trained on Fold 1,3,4,5 and predict on Fold 2. The logic follows for all 5 hypothesis.\n",
    "4. Notice that in the five models, we are predicting on a unique validation fold, and as a result, after we trained all 5 folds, we will have the predictions made on the whole training set (F1-F5). This predictions is called the Out-of-Fold predictions.\n",
    "5. We then go a step further and calculate the AUC score with the OOF predictions with the ground truth to get the OOF AUC. We save it to a csv or dataframe called **oof_1.csv**, subsequent oof trained on different hypothesis space should be named **oof_i.csv** where $i \\in [2,3,...]$.\n",
    "6. After we trained all 5 folds, we will use $h_{1}$ to predict on $X_{test}$ and obtain predictions $Y_{\\text{h1 preds}}$, we then use $h_{2}$ to predict on $X_{test}$ and obtain predictions $Y_{\\text{h2 preds}}$, we do this for all five folds and finally $Y_{\\text{final preds}} = \\dfrac{1}{5}\\sum_{i=1}^{5}Y_{\\text{hi preds}}$. This is a typical pipeline in most machine learning problems. We save this final predictions as **sub_1.csv**, subsequence predictions trained on different hypothesis space should be named **sub_i.csv** where $i \\in [2,3,...]$.\n",
    "7. Now if we train another model, a completely different hypothesis space is used, to be more pedantic, we denote the previous model to be taken from the hypothesis space $\\mathcal{H}_{1}$, and now we move on to $\\mathcal{H}_{2}$. We repeat step 1-6 on this new model (Note that you are essentially training 10 \"models\" now since we are doing KFold twice, and oh, please set the seed of KFold to be the same, it should never be the case that both model comes from different splitting seed for apparent reasons).\n",
    "\n",
    "---\n",
    "\n",
    "Here is the key (given the above setup with 2 different models trained on 5 folds):\n",
    "\n",
    "1. Normally, most people do a simple mean ensemble, that is $\\dfrac{Y_{\\text{final preds H1}} + Y_{\\text{final preds H2}}}{2}$. This works well most of the time as we trust both model holds equal importance in the final predictions.\n",
    "2. One issue may be that certain models should be weighted more than the rest, we should not simply take Leaderboard feedback score to judge the weight assignment. A general heuristic here is called Forward Selection.\n",
    "3. (Extract from Chris) Now say that you build 2 models (that means that you did 5 KFold twice). You now have oof_1.csv, oof_2.csv, sub_1.csv, and sub_2.csv. How do we blend the two models? We find the weight w such that `w * oof_1.predictions + (1-w) * oof_2.predictions` has the largest AUC.\n",
    "\n",
    "```python\n",
    "all = []\n",
    "for w in [0.00, 0.01, 0.02, ..., 0.98, 0.99, 1.00]:\n",
    "    ensemble_pred = w * oof_1.predictions + (1-w) * oof_2.predictions\n",
    "    ensemble_auc = roc_auc_score( oof.target , ensemble_pred )\n",
    "    all.append( ensemble_auc )\n",
    "best_weight = np.argmax( all ) / 100.\n",
    "```\n",
    "\n",
    "Then we can assign the best weight like:\n",
    "\n",
    "```python\n",
    "final_ensemble_pred = best_weight * sub_1.target + (1-best_weight) * sub_2.target\n",
    "```\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=12Mpa_9pTdNYizDCVxq_VxX5qx5j84Y1X' width=\"500\"/>\n",
    "Coutersy of Chris\n",
    "\n",
    "---\n",
    "\n",
    "In this competition, there are two approaches, either maximize the average of the macro AUC score of all the classes, or maximize each column/class separately. It turns out that maximizing the columns separately led to disastrous results (it could be my code and idea is wrong, as ROC is a ranking metric). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "What we could have done better:\n",
    "\n",
    "- Use more variety of `classifier head` like `GeM`.\n",
    "- Use more variety of `backbone`.\n",
    "- Use [Neptune.ai](http://neptune.ai) to log our experiments as soon things start to get messy. Basically MLOps is important!\n",
    "- Experiment on 3-4 stage training.\n",
    "- Pseudo Labelling.\n",
    "- Knowledge Distillation.\n",
    "- Experiment more on maximizing AUC during ensembles. `rank_pct` etc.\n",
    "\n",
    "- **See novel three stage training: https://www.kaggle.com/yasufuminakama/ranzcr-resnet200d-3-stage-training-step1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Multi-Head Deep Learning Model with Multi-Label Classification](https://debuggercafe.com/multi-head-deep-learning-models-for-multi-label-classification/)\n",
    "\n",
    "- [AUC Metric on Multi-Label](https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff)\n",
    "\n",
    "- [Sigmoid and Softmax for Multi-Label](https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/)\n",
    "\n",
    "- [Multi-Label Classification Tutorial](https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff)\n",
    "\n",
    "- [Why we should use Multi-Head in Multi-Label Classification](https://debuggercafe.com/multi-head-deep-learning-models-for-multi-label-classification/)\n",
    "    - [Follow Up 1](https://debuggercafe.com/multi-label-image-classification-with-pytorch-and-deep-learning/)\n",
    "    - [Follow Up 2](https://debuggercafe.com/multi-label-fashion-item-classification-using-deep-learning-and-pytorch/)\n",
    "    - [Follow Up 3](https://debuggercafe.com/deep-learning-architectures-for-multi-label-classification-using-pytorch/)\n",
    "\n",
    "- [Sigmoid is Binary Cross Entropy](https://stats.stackexchange.com/questions/485551/1-neuron-bce-loss-vs-2-neurons-ce-loss)\n",
    "\n",
    "- [Attention Blocks in Computer Vision](https://towardsdatascience.com/attention-in-computer-vision-fd289a5bd7ad)\n",
    "\n",
    "- [Spatial Attention Blocks](https://medium.com/visionwizard/understanding-attention-modules-cbam-and-bam-a-quick-read-ca8678d1c671)\n",
    "\n",
    "- [Spatial Attention Module](https://paperswithcode.com/method/spatial-attention-module)\n",
    "\n",
    "- [Convolutional Block Attention Module](https://arxiv.org/abs/1807.06521)\n",
    "\n",
    "- [Dive Into Deep Learning - Chapter 10: Attention Mechanisms]\n",
    "\n",
    "- [Gradual Warmup: Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60a592695384e68e81c96d019436b4c6d949e7549cb1292bfa72c05e2c5f0fcb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
