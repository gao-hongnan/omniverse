{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"Stage 3 - Feature Engineering.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"piZ1nsyYiZut"},"source":["<div align=\"center\">\n","<h1>Stage 3: Feature Engineering</a></h1>\n","by Hongnan Gao\n","<br>\n","</div>"],"id":"piZ1nsyYiZut"},{"cell_type":"markdown","metadata":{"id":"fe19181f-d930-43fb-a298-794bb2261b99"},"source":["<a id=\"top\"></a>\n","\n","<a id = '1.0'></a>\n","<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal;background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px;padding:5px;text-align:center; font-weight: bold\" >Quick Navigation</h1>\n","\n","    \n","* [Dependencies and Configuration](#1)\n","* [Stage 3: Feature Engineering/Feature Selection](#2)\n","    * [Multicollinearity and Feature Selection](#31)\n","        * [Target Distribution](#31)\n","        * [Using Statsmodels Variance Inflation Factor](#31)\n","        * [Oh Dear, we have a Multicollinearity Problem](#31)\n","    * [Save the Data](#31)"],"id":"fe19181f-d930-43fb-a298-794bb2261b99"},{"cell_type":"markdown","metadata":{"tags":[],"id":"471b74a6-768a-4074-9fd1-b9a6808a97b8"},"source":["## Dependencies and Configuration"],"id":"471b74a6-768a-4074-9fd1-b9a6808a97b8"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e4e21348-94cc-444a-9026-436b9550ff55","executionInfo":{"status":"ok","timestamp":1636794288074,"user_tz":-480,"elapsed":1093,"user":{"displayName":"Hongnan Gao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04730584788169020152"}},"outputId":"12080619-e3a1-48db-db09-337a62a9a389"},"source":["import random\n","from collections import defaultdict\n","from dataclasses import dataclass, field\n","from typing import Dict, List, Union\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from sklearn import (base, decomposition, linear_model, manifold, metrics,\n","                     preprocessing)\n","from statsmodels.stats.outliers_influence import variance_inflation_factor"],"id":"e4e21348-94cc-444a-9026-436b9550ff55","execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"]}]},{"cell_type":"code","metadata":{"id":"016e67c9-594a-44c0-99d9-6829672c9a38","executionInfo":{"status":"ok","timestamp":1636794313691,"user_tz":-480,"elapsed":290,"user":{"displayName":"Hongnan Gao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04730584788169020152"}}},"source":["@dataclass\n","class config:\n","    raw_data: str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\"\n","    processed_data: str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\"\n","    train_size: float = 0.9\n","    seed: int = 1992\n","    num_folds: int = 5\n","    cv_schema: str = \"StratifiedKFold\"\n","    classification_type: str = \"binary\"\n","\n","    target_col: List[str] = field(default_factory=lambda: [\"diagnosis\"])\n","    unwanted_cols: List[str] = field(default_factory=lambda: [\"id\", \"Unnamed: 32\"])\n","\n","    # Plotting\n","    colors: List[str] = field(\n","        default_factory=lambda: [\"#fe4a49\", \"#2ab7ca\", \"#fed766\", \"#59981A\"]\n","    )\n","    cmap_reversed = plt.cm.get_cmap(\"mako_r\")\n","\n","    def to_dict(self) -> Dict:\n","        \"\"\"Convert the config object to a dictionary.\n","\n","        Returns:\n","            Dict: The config object as a dictionary.\n","        \"\"\"\n","        return {\n","            \"raw_data\": self.raw_data,\n","            \"processed_data\": self.processed_data,\n","            \"train_size\": self.train_size,\n","            \"seed\": self.seed,\n","            \"num_folds\": self.num_folds,\n","            \"cv_schema\": self.cv_schema,\n","            \"classification_type\": self.classification_type,\n","            \"target_col\": self.target_col,\n","            \"unwanted_cols\": self.unwanted_cols,\n","            \"colors\": self.colors,\n","            \"cmap_reversed\": self.cmap_reversed,\n","        }"],"id":"016e67c9-594a-44c0-99d9-6829672c9a38","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"sF9XxCmyixka","executionInfo":{"status":"ok","timestamp":1636794314808,"user_tz":-480,"elapsed":3,"user":{"displayName":"Hongnan Gao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04730584788169020152"}}},"source":["def set_seeds(seed: int = 1234) -> None:\n","    \"\"\"Set seeds for reproducibility.\"\"\"\n","    np.random.seed(seed)\n","    random.seed(seed)"],"id":"sF9XxCmyixka","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ecaa7fe-c355-4380-a603-356d1ba12a25","executionInfo":{"status":"ok","timestamp":1636794326104,"user_tz":-480,"elapsed":1213,"user":{"displayName":"Hongnan Gao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04730584788169020152"}}},"source":["config = config()\n","\n","# set seeding for reproducibility\n","_ = set_seeds(seed = config.seed)\n","\n","# read data\n","df = pd.read_csv(config.processed_data)"],"id":"9ecaa7fe-c355-4380-a603-356d1ba12a25","execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9d9dc733-229b-4909-babc-006eab62c1d6"},"source":["# Stage 3: Feature Engineering/Feature Selection"],"id":"9d9dc733-229b-4909-babc-006eab62c1d6"},{"cell_type":"markdown","metadata":{"id":"cbc33543-0b55-49a0-932c-9e72f39e8a03"},"source":["!!! danger \"Foreward on Data Leakage\"\n","    We are fully aware that oftentimes practitioner may accidentally cause data leakage during preprocessing, for example, a subtle yet often mistake is to standardize the whole dataset prior to splitting, or performing feature selection prior to modelling using the information of our response/target variable. \n","    However, this does not mean we should not do any preprocessing before modelling, instead, in the case of removing multicollinearity features, we can still screen predictors for multicollinearity during EDA phase and have a good intuition on which predictors are highly correlated - subsequently, we will incorporate feature selection techniques in our modelling pipeline.\n"],"id":"cbc33543-0b55-49a0-932c-9e72f39e8a03"},{"cell_type":"markdown","metadata":{"id":"823a6c4f-c54a-4223-806b-aac3be4ebb50"},"source":["### Multicollinearity and Feature Selection\n","\n","!!! note \"Why Feature Selection?\"\n","    We need feature selection in certain problems for the following reasons:\n","    - Well, one would definitely have heard of the dreaded **Curse of Dimensionality** in the journey of learning Machine Learning where having too many predictor/features can lead to overfitting; on the other hand, too many dimensions can cause distance between observations to appear equidistance from one another. The equidistance phenomenon causes observations to become harder to cluster, thereby clogging the model's ability to cluster data points (imagine the horror if you use KNN on 1000 dimensions, all the points will be almost the same distance from each other, poor KNN will not know how to predict now).\n","    - In case you have access to Google's GPU clusters, you likely want to train your model faster. Reducing the number predictors can aid this process.\n","    - Reducing uninformative features may aid in model's performance, the idea is to remove unnecessary noise from the dataset.\n","\n","---\n","\n","!!! danger \"Multi-Collinearity\"\n","    Looking back at our dataset, it is clear to me that there are quite a number of features that are correlated with each other, causing multi-collinearity. Multi-Collinearity is an issue in the history of Linear Models, as quoted[^Is there an intuitive explanation why multicollinearity is a problem in linear regression?]\n","    \n","    > Consider the simplest case where Y is regressed against X and Z and where X and Z are highly positively correlated. Then the effect of X on Y is hard to distinguish from the effect of Z on Y because any increase in X tends to be associated with an increase in Z.\n","\n","    We also note that multi-collinearity is not that big of a problem for non-parametric models such as Decision Tree or Random Forests, however, I will attempt to show that it is still best to avoid in this problem setting.\n","\n","[^Is there an intuitive explanation why multicollinearity is a problem in linear regression?]: https://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-r"],"id":"823a6c4f-c54a-4223-806b-aac3be4ebb50"},{"cell_type":"markdown","metadata":{"id":"a80b4ba6-3922-4caf-96ff-31dc50c0b94f"},"source":["!!! alert \"Feature Selection Methods\"\n","    There are many methods to perform feature selection. Scikit-Learn offers some of the following:\n","    - Univariate feature selection.\n","    - Recursive feature elimination.\n","    - Backward Elimination of features using Hypothesis Testing.  \n","    We need to be careful when selecting features before cross-validation. It is therefore, recommended to include feature selection in cross-validation to avoid any \"bias\" introduced before model selection phase! I decided to use the good old Variance Inflation Factor (VIF) as a way to reduce multicollinearity. Unfortunately, there is no out-of-the-box function to integrate into the `Pipeline` of scikit-learn. Thus, I heavily modified an existing code in order achieve what I want below.\n"],"id":"a80b4ba6-3922-4caf-96ff-31dc50c0b94f"},{"cell_type":"markdown","metadata":{"id":"EAMKDaP8k2w1"},"source":["### Variance Inflation Factor\n","\n","A classical way to check for multicollinearity amongst predictors is to calculate the Variable Inflation Factor (VIF). It is simply done by regressing each predictor $\\mathrm{x}_i$ against all other predictors $\\mathrm{x}_j, j \\neq i$. In other words, the VIF for a predictor variable $i$ is given by:\n","\n","$$\\text{VIF}_i = \\dfrac{1}{1 - R^{2}_{i}}$$\n","\n","where $R^{2}_{i}$ is, by definition, the proportion of the variation in the \"dependent variable\" $\\mathrm{x}_i$ that is predictable from the indepedent predictors $\\mathrm{x}_j, j \\neq i$. Consequently, the higher the $R^2_i$ of a predictor, the higher the VIF, and this indicates there is linear dependence among predictors."],"id":"EAMKDaP8k2w1"},{"cell_type":"markdown","metadata":{"id":"d2f7fea1-5183-4f07-816f-77545f88386e"},"source":["#### Using Statsmodels Variance Inflation Factor\n","\n","Note that we need to perform scaling first before fitting our `ReduceVIF` to get the exact same result as the previous version. In this version, I manually added a hard threshold for the number of features remaining to be 15. This hard coded number can be turned into a parameter (hyperparameter) in our pipeline."],"id":"d2f7fea1-5183-4f07-816f-77545f88386e"},{"cell_type":"code","metadata":{"id":"97d87d34-a4e4-47a9-b797-6a5e20ae2147","executionInfo":{"status":"ok","timestamp":1636795306429,"user_tz":-480,"elapsed":320,"user":{"displayName":"Hongnan Gao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04730584788169020152"}}},"source":["import numpy as np\n","import pandas as pd\n","from sklearn import base\n","from statsmodels.regression.linear_model import OLS\n","\n","def variance_inflation_factor(exog, idx_kept, vif_idx):\n","    \"\"\"Compute VIF for one feature.\n","    \n","    Args:\n","        exog (np.ndarray): Observations\n","        idx_kept (List[int]): Indices of features to consider\n","        vif_idx (int): Index of feature for which to compute VIF\n","    \n","    Returns:\n","        float: VIF for the selected feature\n","    \"\"\"\n","    exog = np.asarray(exog)\n","    \n","    x_i = exog[:, vif_idx]\n","    mask = [col for col in idx_kept if col != vif_idx]\n","    x_noti = exog[:, mask]\n","    \n","    r_squared_i = OLS(x_i, x_noti).fit().rsquared\n","    vif = 1. / (1. - r_squared_i)\n","    \n","    return vif\n","\n","class ReduceVIF(base.BaseEstimator, base.TransformerMixin):\n","    \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class;\n","    I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method.\n","    \"\"\"\n","\n","    def __init__(self, thresh=10, max_drop=20):\n","        self.thresh = thresh\n","        self.max_drop = max_drop\n","        self.column_indices_kept_ = []\n","        self.feature_names_kept_ = None\n","\n","    def reset(self):\n","        \"\"\"Resets the state of predictor columns after each fold.\"\"\"\n","\n","        self.column_indices_kept_ = []\n","        self.feature_names_kept_ = None\n","\n","    def fit(self, X, y=None):\n","        \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names\n","\n","        Args:\n","            X ([type]): [description]\n","            y ([type], optional): [description]. Defaults to None.\n","\n","        Returns:\n","            [type]: [description]\n","        \"\"\"\n","        \n","        self.column_indices_kept_, self.feature_names_kept_ = self.calculate_vif(X)\n","        \n","        return self\n","\n","    def transform(self, X, y=None):\n","        \"\"\"Transforms the Validation Set according to the selected feature names.\n","\n","        Args:\n","            X ([type]): [description]\n","            y ([type], optional): [description]. Defaults to None.\n","\n","        Returns:\n","            [type]: [description]\n","        \"\"\"\n","\n","        return X[:, self.column_indices_kept_]\n","\n","    def calculate_vif(self, X: Union[np.ndarray, pd.DataFrame]):\n","        \"\"\"Implements a VIF function that recursively eliminates features.\n","\n","        Args:\n","            X (Union[np.ndarray, pd.DataFrame]): [description]\n","\n","        Returns:\n","            [type]: [description]\n","        \"\"\"\n","        feature_names = None\n","        column_indices_kept = list(range(X.shape[1]))\n","        \n","        if isinstance(X, pd.DataFrame):\n","            feature_names = X.columns\n","\n","        dropped = True\n","        count = 0\n","        \n","        while dropped and count <= self.max_drop:\n","            dropped = False\n","            \n","            max_vif, max_vif_col = None, None\n","            \n","            for col in column_indices_kept:\n","                \n","                vif = variance_inflation_factor(X, column_indices_kept, col)\n","                \n","                if max_vif is None or vif > max_vif:\n","                    max_vif = vif\n","                    max_vif_col = col\n","            \n","            if max_vif > self.thresh:\n","                print(f\"Droppingggggg {max_vif_col} with vif={max_vif}\")\n","                column_indices_kept.remove(max_vif_col)\n","                \n","                if feature_names is not None:\n","                    feature_names.pop(max_vif_col)\n","                    \n","                dropped = True\n","                count += 1\n","                \n","        return column_indices_kept, feature_names\n"],"id":"97d87d34-a4e4-47a9-b797-6a5e20ae2147","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6d6a6e78-c72f-4dfe-a17d-cd9ed0a00193"},"source":["We do a sanity check if this coincides with the previous defined class, and the results are the same."],"id":"6d6a6e78-c72f-4dfe-a17d-cd9ed0a00193"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e7eb8eae-5f4a-445b-aca5-ea34edde4ecf","executionInfo":{"status":"ok","timestamp":1636795313517,"user_tz":-480,"elapsed":1176,"user":{"displayName":"Hongnan Gao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04730584788169020152"}},"outputId":"1d0a977d-89f7-4921-825f-ab582019ef7f"},"source":["predictor_cols = df.columns[1:]\n","transformer = ReduceVIF()\n","scaler = preprocessing.StandardScaler()\n","X = scaler.fit_transform(df[predictor_cols])\n","# Only use 10 columns for speed in this example\n","X = transformer.fit_transform(X)\n","\n","print(f\"Remaining Features: {transformer.column_indices_kept_}\")"],"id":"e7eb8eae-5f4a-445b-aca5-ea34edde4ecf","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Droppingggggg 0 with vif=3806.1152963979675\n","Droppingggggg 20 with vif=616.3508614719424\n","Droppingggggg 2 with vif=325.64131198187516\n","Droppingggggg 22 with vif=123.25781086343038\n","Droppingggggg 6 with vif=64.65479584770004\n","Droppingggggg 10 with vif=35.61751844352034\n","Droppingggggg 25 with vif=33.96063880508537\n","Droppingggggg 27 with vif=30.596655364834078\n","Droppingggggg 3 with vif=25.387829695531458\n","Droppingggggg 5 with vif=18.843208489973282\n","Droppingggggg 21 with vif=17.232376192128665\n","Droppingggggg 13 with vif=16.333806476471736\n","Droppingggggg 26 with vif=15.510661467365699\n","Remaining Features: [1, 4, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 19, 23, 24, 28, 29]\n"]}]},{"cell_type":"markdown","metadata":{"id":"568996cb-733f-4391-b7be-dd25b01acafd"},"source":["We have the remaining indices, and therefore simply use numpy to subset the column indices to get back the original column names that are kept."],"id":"568996cb-733f-4391-b7be-dd25b01acafd"},{"cell_type":"code","metadata":{"id":"2634f944-4d36-4a27-b621-999b2787eb25","executionInfo":{"status":"ok","timestamp":1636795350849,"user_tz":-480,"elapsed":587,"user":{"displayName":"Hongnan Gao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04730584788169020152"}}},"source":["vif_df = pd.DataFrame({'Predictors': predictor_cols[transformer.column_indices_kept_]})"],"id":"2634f944-4d36-4a27-b621-999b2787eb25","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":576},"id":"72adeed8-b69f-4371-b736-422f74a8790b","executionInfo":{"status":"ok","timestamp":1636795351376,"user_tz":-480,"elapsed":529,"user":{"displayName":"Hongnan Gao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04730584788169020152"}},"outputId":"5852435c-9007-46be-a5db-fca24530abf9"},"source":["display(vif_df)"],"id":"72adeed8-b69f-4371-b736-422f74a8790b","execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Predictors</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>texture_mean</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>smoothness_mean</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>concave points_mean</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>symmetry_mean</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>fractal_dimension_mean</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>texture_se</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>perimeter_se</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>smoothness_se</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>compactness_se</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>concavity_se</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>concave points_se</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>symmetry_se</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>fractal_dimension_se</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>area_worst</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>smoothness_worst</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>symmetry_worst</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>fractal_dimension_worst</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 Predictors\n","0              texture_mean\n","1           smoothness_mean\n","2       concave points_mean\n","3             symmetry_mean\n","4    fractal_dimension_mean\n","5                texture_se\n","6              perimeter_se\n","7             smoothness_se\n","8            compactness_se\n","9              concavity_se\n","10        concave points_se\n","11              symmetry_se\n","12     fractal_dimension_se\n","13               area_worst\n","14         smoothness_worst\n","15           symmetry_worst\n","16  fractal_dimension_worst"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"7b44d6e3-bd6a-4a55-8780-b10dab085e1f"},"source":["#### Oh Dear, we have a Multicollinearity Problem\n","\n","!!! info \"Using VIF in Modelling Pipeline\"\n","    At this step, we are just showing how we can remove multicollinear features using VIF; but we will not remove them at this point in time. We will incorporate this feature selection technique in our Cross-Validation pipeline in order to avoid data leakage.\n"],"id":"7b44d6e3-bd6a-4a55-8780-b10dab085e1f"},{"cell_type":"markdown","metadata":{"id":"44a45b17-5801-4a53-baaf-e4d10a346e8d"},"source":["## Save the Data\n","\n","In this phase, we did not make any changes to the predictor or target columns. So there is nothing to save."],"id":"44a45b17-5801-4a53-baaf-e4d10a346e8d"}]}