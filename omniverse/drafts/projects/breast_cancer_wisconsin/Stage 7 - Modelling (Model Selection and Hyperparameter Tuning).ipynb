{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Stage 7 - Modelling (Model Selection and Hyperparameter Tuning).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhMhkG8YnRiw"
      },
      "source": [
        "<div align=\"center\">\n",
        "<h1>Stage 7: Hyperparameter Tuning</a></h1>\n",
        "by Hongnan Gao\n",
        "<br>\n",
        "</div>"
      ],
      "id": "qhMhkG8YnRiw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd2fd28b-c218-495e-9b60-0c37d5060ba2"
      },
      "source": [
        "## Dependencies and Configuration"
      ],
      "id": "fd2fd28b-c218-495e-9b60-0c37d5060ba2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiiP3yfJ78F_"
      },
      "source": [
        "%%capture\n",
        "!pip install -q wandb\n",
        "# !pip install -q shap\n",
        "!pip install -q mlxtend==0.19.0\n",
        "!pip install -q statsmodels==0.13.1\n",
        "# !pip install gcloud == 0.18.3"
      ],
      "id": "NiiP3yfJ78F_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfZ8D6OV7_oM",
        "outputId": "87bec8d4-ded5-4f02-fba3-3bf542035c12"
      },
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "id": "GfZ8D6OV7_oM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d078e09b-5833-4510-b8a2-644087d79510"
      },
      "source": [
        "import copy\n",
        "import csv\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "from dataclasses import asdict, dataclass, field\n",
        "from functools import wraps\n",
        "from pathlib import Path\n",
        "from time import time\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import mlxtend\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from joblib import dump, load\n",
        "from mlxtend.evaluate import bias_variance_decomp, paired_ttest_5x2cv\n",
        "from scipy import stats\n",
        "from sklearn import (base, decomposition, dummy, ensemble, feature_selection,\n",
        "                     linear_model, metrics, model_selection, neighbors,\n",
        "                     pipeline, preprocessing, svm, tree)\n",
        "from statsmodels.regression.linear_model import OLS"
      ],
      "id": "d078e09b-5833-4510-b8a2-644087d79510",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwVYLLr1AKD-"
      },
      "source": [
        "## Utils and Configurations"
      ],
      "id": "XwVYLLr1AKD-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79b04142-7779-4b78-bab7-f02eddb6a3e6"
      },
      "source": [
        "@dataclass\n",
        "class config:\n",
        "    raw_data: str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/raw/data.csv\"\n",
        "    processed_data: str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/processed.csv\"\n",
        "    df_folds: str = \"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/processed/df_folds.csv\"\n",
        "    train_size: float = 0.9\n",
        "    seed: int = 1992\n",
        "    num_folds: int = 5\n",
        "    cv_schema: str = \"StratifiedKFold\"\n",
        "    classification_type: str = \"binary\"\n",
        "    \n",
        "    target_col: List[str] = field(default_factory = lambda: [\"diagnosis\"])\n",
        "    unwanted_cols : List[str] =  field(default_factory = lambda: [\"id\", \"Unnamed: 32\"])\n",
        "    \n",
        "    # Plotting\n",
        "    colors : List[str] =field(default_factory = lambda: [\"#fe4a49\", \"#2ab7ca\", \"#fed766\", \"#59981A\"])\n",
        "    cmap_reversed = plt.cm.get_cmap('mako_r')\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"Convert the config object to a dictionary.\n",
        "\n",
        "        Returns:\n",
        "            Dict: The config object as a dictionary.\n",
        "        \"\"\"\n",
        "\n",
        "        return asdict(self)\n",
        "\n",
        "\n",
        "    \n",
        "#     spot_checking_boxplot = \"../data/images/spot_checking_boxplot.png\"\n",
        "#     oof_confusion_matrix = \"../data/images/oof_confusion_matrix.png\"\n",
        "#     final_train_confusion_matrix = \"../data/images/final_train_confusion_matrix.png\"\n",
        "#     precision_recall_threshold_plot = \"../data/images/precision_recall_threshold_plot.png\"\n",
        "#     roc_plot = \"../data/images/roc_plot.png\"\n",
        "#     feature_importance = \"../data/images/feature_importance.png\""
      ],
      "id": "79b04142-7779-4b78-bab7-f02eddb6a3e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45f2d69f-a76a-47aa-a3a8-e26c5b96591d"
      },
      "source": [
        "def set_seeds(seed: int = 1234) -> None:\n",
        "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    \n",
        "def init_logger(log_file: str = \"info.log\"):\n",
        "    \"\"\"\n",
        "    Initialize logger.\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.setLevel(logging.INFO)\n",
        "    stream_handler = logging.StreamHandler()\n",
        "    stream_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\", datefmt= \"%Y-%m-%d,%H:%M:%S\"))\n",
        "    file_handler = logging.FileHandler(filename=log_file)\n",
        "    file_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\",  datefmt= \"%Y-%m-%d,%H:%M:%S\"))\n",
        "    logger.addHandler(stream_handler)\n",
        "    logger.addHandler(file_handler)\n",
        "    return logger"
      ],
      "id": "45f2d69f-a76a-47aa-a3a8-e26c5b96591d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nopDpYZMxdPx"
      },
      "source": [
        "# Utils functions that we need\n",
        "\n",
        "def variance_inflation_factor(exog, idx_kept, vif_idx):\n",
        "    \"\"\"Compute VIF for one feature.\n",
        "    \n",
        "    Args:\n",
        "        exog (np.ndarray): Observations\n",
        "        idx_kept (List[int]): Indices of features to consider\n",
        "        vif_idx (int): Index of feature for which to compute VIF\n",
        "    \n",
        "    Returns:\n",
        "        float: VIF for the selected feature\n",
        "    \"\"\"\n",
        "    exog = np.asarray(exog)\n",
        "    \n",
        "    x_i = exog[:, vif_idx]\n",
        "    mask = [col for col in idx_kept if col != vif_idx]\n",
        "    x_noti = exog[:, mask]\n",
        "    \n",
        "    r_squared_i = OLS(x_i, x_noti).fit().rsquared\n",
        "    vif = 1. / (1. - r_squared_i)\n",
        "    \n",
        "    return vif\n",
        "\n",
        "class ReduceVIF(base.BaseEstimator, base.TransformerMixin):\n",
        "    \"\"\"The base of the class structure is implemented in https://www.kaggle.com/ffisegydd/sklearn-multicollinearity-class;\n",
        "    I heavily modified the class such that it can take in numpy arrays and correctly implemented the fit and transform method.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, thresh=10, max_drop=20):\n",
        "        self.thresh = thresh\n",
        "        self.max_drop = max_drop\n",
        "        self.column_indices_kept_ = []\n",
        "        self.feature_names_kept_ = None\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the state of predictor columns after each fold.\"\"\"\n",
        "\n",
        "        self.column_indices_kept_ = []\n",
        "        self.feature_names_kept_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"Fits the Recursive VIF on the training folds and save the selected feature names in self.feature_names\n",
        "\n",
        "        Args:\n",
        "            X ([type]): [description]\n",
        "            y ([type], optional): [description]. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            [type]: [description]\n",
        "        \"\"\"\n",
        "        \n",
        "        self.column_indices_kept_, self.feature_names_kept_ = self.calculate_vif(X)     \n",
        "        \n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Transforms the Validation Set according to the selected feature names.\n",
        "\n",
        "        Args:\n",
        "            X ([type]): [description]\n",
        "            y ([type], optional): [description]. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            [type]: [description]\n",
        "        \"\"\"\n",
        "\n",
        "        return X[:, self.column_indices_kept_]\n",
        "\n",
        "    def calculate_vif(self, X: Union[np.ndarray, pd.DataFrame]):\n",
        "        \"\"\"Implements a VIF function that recursively eliminates features.\n",
        "\n",
        "        Args:\n",
        "            X (Union[np.ndarray, pd.DataFrame]): [description]\n",
        "\n",
        "        Returns:\n",
        "            [type]: [description]\n",
        "        \"\"\"\n",
        "        feature_names = None\n",
        "        column_indices_kept = list(range(X.shape[1]))\n",
        "        \n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            feature_names = X.columns\n",
        "\n",
        "        dropped = True\n",
        "        count = 0\n",
        "        \n",
        "        while dropped and count <= self.max_drop:\n",
        "            dropped = False\n",
        "            \n",
        "            max_vif, max_vif_col = None, None\n",
        "            \n",
        "            for col in column_indices_kept:\n",
        "                \n",
        "                vif = variance_inflation_factor(X, column_indices_kept, col)\n",
        "                \n",
        "                if max_vif is None or vif > max_vif:\n",
        "                    max_vif = vif\n",
        "                    max_vif_col = col\n",
        "            \n",
        "            if max_vif > self.thresh:\n",
        "                # print(f\"Dropping {max_vif_col} with vif={max_vif}\")\n",
        "                column_indices_kept.remove(max_vif_col)\n",
        "                \n",
        "                if feature_names is not None:\n",
        "                    feature_names.pop(max_vif_col)\n",
        "                    \n",
        "                dropped = True\n",
        "                count += 1\n",
        "                \n",
        "        return column_indices_kept, feature_names\n",
        "\n",
        "\n",
        "def prepare_y(y: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Prepare the target variable for the model.\n",
        "\n",
        "    If Binary Classification, we need to ravel the array to 1d.\n",
        "\n",
        "    Args:\n",
        "        y (np.ndarray): Target variable.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Transformed Target variable.\n",
        "    \"\"\"\n",
        "    return y.ravel() if config.classification_type == \"binary\" else y"
      ],
      "id": "nopDpYZMxdPx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXHu28Cm7AUf"
      },
      "source": [
        "config = config()\n",
        "\n",
        "basic_config: Dict = config.to_dict()\n",
        "# train_config: Dict = Train().to_dict()\n",
        "\n",
        "global_config: Dict = dict(basic=basic_config)\n",
        "\n",
        "# We can log multiple dict under global_config - in wandb UI, it will show as basic. and train. to show which dict it is referring to.\n",
        "run = wandb.init(project=\"bcw\", name=\"classification\", config=global_config)"
      ],
      "id": "QXHu28Cm7AUf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36d10cc4-01dd-4c2e-b419-3389c9dcd896"
      },
      "source": [
        "# set logger\n",
        "logger = init_logger()\n",
        "\n",
        "# set seeding for reproducibility\n",
        "_ = set_seeds(seed = config.seed)\n",
        "\n",
        "# read data\n",
        "df_folds = pd.read_csv(config.df_folds)"
      ],
      "id": "36d10cc4-01dd-4c2e-b419-3389c9dcd896",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54-mjFK4wMDu"
      },
      "source": [
        "# Assign predictors and target accordingly\n",
        "predictor_cols = df_folds.columns.to_list()[:-2]\n",
        "target_col = config.target_col"
      ],
      "id": "54-mjFK4wMDu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4a9dcf7-9de6-4508-9f83-d97b7aa724f0"
      },
      "source": [
        "## Model Selection: Hyperparameter Tuning with GridSearchCV"
      ],
      "id": "e4a9dcf7-9de6-4508-9f83-d97b7aa724f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c06510a3-7aaf-4b45-982f-ce6703c3e602"
      },
      "source": [
        "!!! success \"Hyperparameter Tuning\"\n",
        "    We have done a quick spot checking on algorithms and realized that `LogisticRegression` is doing well for this task. For this purpose, I will just perform hyperparameter tuning on this single algorithm. However, in practice and if resources are allowed, I will also tune other models such as `RandomForest()`, or gradient boosting algorithms such as `XGBoost`, as I believe they will perform no worse than our Logistic Regression model given the right hyperparameters.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "!!! info \"Grid Search is the Gwei?\"\n",
        "    Meh! We will use an old-fashioned way to search for hyperparameters, which is brute force method. The time complexity of Grid Search is high and if you have many hyperparameters to tune, I recommend trying out <b>Random Grid Search</b> or libraries like <b>Optuna</b> that uses Bayesian Optimization.\n",
        "\n",
        "---\n",
        "\n",
        "!!! note \"TODO\"\n",
        "    Try to code up your own `GridSearchCV` to have maximum flexibility."
      ],
      "id": "c06510a3-7aaf-4b45-982f-ce6703c3e602"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AorZg7j9oNR5"
      },
      "source": [
        "### Make Finetuning Pipeline\n",
        "\n",
        "The following `make_finetuning_pipeline` does exactly the same thing is as `make_pipeline` earlier. The only difference is we can pass in flexible list of steps to the pipeline from outside."
      ],
      "id": "AorZg7j9oNR5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9666a82-1983-45ba-904f-fd292785265a"
      },
      "source": [
        "def make_finetuning_pipeline(\n",
        "    model: Callable, steps: List[Tuple[str, Callable]]\n",
        ") -> pipeline.Pipeline:\n",
        "    \"\"\"Return a pipeline that can be used for finetuning.\n",
        "\n",
        "    Args:\n",
        "        model (Callable): A model with default parameters.\n",
        "        steps (List[Tuple[str, Callable]]): A list of preprocessing steps to pass in Pipeline object.\n",
        "\n",
        "    Returns:\n",
        "        Pipeline: Returns a pipeline that can be used for finetuning.\n",
        "    \"\"\"\n",
        "    return pipeline.Pipeline([*steps, (\"model\", model)])\n",
        "\n",
        "# TODO: Make a class to hold pipelines?\n",
        "# class MakePipeline:\n",
        "    \n",
        "#     def __init__(self, estimator: Callable, steps: List[Callable]):\n",
        "#         pass\n",
        "    \n",
        "#     def spot_checking_pipeline():\n",
        "#         pass\n",
        "    \n",
        "#     def fine_tuning_pipeline():\n",
        "#         pass"
      ],
      "id": "b9666a82-1983-45ba-904f-fd292785265a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs8PD26EYt3S"
      },
      "source": [
        "finetuning_pipeline_steps = [\n",
        "    # standardization\n",
        "    (\"standardize\", preprocessing.StandardScaler()),\n",
        "    # reduce VIF\n",
        "    (\"remove_multicollinearity\", ReduceVIF(thresh=10))\n",
        "]"
      ],
      "id": "rs8PD26EYt3S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm68c9y8u8Ii"
      },
      "source": [
        "### Search Space"
      ],
      "id": "Vm68c9y8u8Ii"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81de67f0-d18c-4962-b040-12abfd64ed22"
      },
      "source": [
        "Run our hyperparameter search with cross-validation. For example, our `param_grid` has $2 \\times 10 = 20$ combinations, and our cross validation has 5 folds, then there will be a total of 100 fits.\n",
        "\n",
        "---\n",
        "\n",
        "Below details the pseudo code of what happens under the hood:\n",
        "\n",
        "- Define $G$ as the set of combination of hyperparamters. Define number of splits to be $K$.\n",
        "- For each set of hyperparameter $z \\in Z$:\n",
        "    - for fold $j$ in K:\n",
        "        - Set $F_{\\text{train}}=\\bigcup\\limits_{i\\neq k}^{K} F_{i}$\n",
        "        - Set $F_{\\text{val}} = F_{j}$ as the validation set\n",
        "        - Perform Standard Scaling on $F_{\\text{train}}$ and find the mean and std\n",
        "        - Perform VIF recursively on $F_{\\text{train}}$ and find the selected features\n",
        "        - Transform $F_{\\text{val}}$ using the mean and std found using $F_{\\text{train}}$\n",
        "        - Transform $F_{\\text{val}}$ to have only the selected features from $F_{\\text{train}}$\n",
        "        - Train and fit on $F_{\\text{train}}$ \n",
        "    - Evaluate the fitted parameters on $F_{\\text{val}}$ to obtain $\\mathcal{M}$\n"
      ],
      "id": "81de67f0-d18c-4962-b040-12abfd64ed22"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8whgRLBpGVl"
      },
      "source": [
        "@dataclass\n",
        "class ModelForTuning:\n",
        "    model: Callable\n",
        "    param_grid: Dict"
      ],
      "id": "m8whgRLBpGVl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "335144e9-ae4f-44c5-a6e0-dd6eac1a28c6"
      },
      "source": [
        "Define our search space for the hyperparameters:\n",
        "\n",
        "```python\n",
        "logistic_r_param_grid = {model__penalty=[\"l1\", \"l2\"],\n",
        "              model__C=np.logspace(-4, 4, 10)}\n",
        "```\n",
        "\n",
        "We conveniently use `dataclass` to act as a medium so we can pass in model and param_grid independently for each model. We then collate them into a list of `ModelForTuning` object."
      ],
      "id": "335144e9-ae4f-44c5-a6e0-dd6eac1a28c6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW1S6x5xpMr2"
      },
      "source": [
        "models_list = [\n",
        "    ModelForTuning(\n",
        "        model=linear_model.LogisticRegression(\n",
        "            solver=\"saga\",\n",
        "            random_state=config.seed,\n",
        "            max_iter=10000,\n",
        "            n_jobs=-1,\n",
        "            fit_intercept=True,\n",
        "        ),\n",
        "        param_grid=dict(\n",
        "            model__penalty=[\"l1\", \"l2\"],\n",
        "            model__C=np.logspace(-4, 4, 10),\n",
        "        ),\n",
        "    ),\n",
        "    ModelForTuning(\n",
        "        model=tree.DecisionTreeClassifier(random_state=config.seed),\n",
        "        param_grid=dict(\n",
        "            model__max_depth=[2, 3, 5, 10, 20],\n",
        "            model__min_samples_leaf=[5, 10, 20, 50, 100],\n",
        "            model__criterion=[\"gini\", \"entropy\"],\n",
        "        ),\n",
        "    ),\n",
        "    ModelForTuning(\n",
        "        model=ensemble.GradientBoostingClassifier(n_estimators=100),\n",
        "        param_grid=dict(\n",
        "            model__max_depth=[3, 6],\n",
        "            model__learning_rate=[0.1, 0.05],\n",
        "            model__subsample=[\n",
        "                1,\n",
        "                0.5,\n",
        "            ],\n",
        "        ),\n",
        "    ),\n",
        "]"
      ],
      "id": "nW1S6x5xpMr2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfwxrz6EZ0GG"
      },
      "source": [
        "def optimize_models(\n",
        "    models_list: List[ModelForTuning],\n",
        "    X_train: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    scorer: Union[str, Callable],\n",
        "    steps: List[Tuple[str, Callable]],\n",
        ") -> List[Callable]:\n",
        "    \"\"\"Optimize models in models_list using X_train and y_train.\n",
        "    We are using GridSearchCV to find the best parameters for each model.\n",
        "    Consider using Optuna for hyperparameter optimization (or wandb for hyperparameter optimization).\n",
        "\n",
        "    Args:\n",
        "        models_list (List[ModelForTuning]): List of models to optimize.\n",
        "        X_train (np.ndarray): X_train data.\n",
        "        y_train (np.ndarray): y_train data.\n",
        "\n",
        "    Returns:\n",
        "        grids (List[Callable]): List of optimized models.\n",
        "    \"\"\"\n",
        "    # @ TODO: make a scoring list to pass in so we can evaluate multiple metrics.\n",
        "    grids = [\n",
        "        model_selection.GridSearchCV(\n",
        "            make_finetuning_pipeline(model.model, steps),\n",
        "            param_grid=model.param_grid,\n",
        "            cv=5,\n",
        "            refit=True,\n",
        "            verbose=1,\n",
        "            scoring=scorer,\n",
        "            n_jobs=-1,\n",
        "        )\n",
        "        for model in models_list\n",
        "    ]\n",
        "\n",
        "    for grid in grids:\n",
        "        grid.fit(X_train, y_train)\n",
        "\n",
        "    return grids"
      ],
      "id": "bfwxrz6EZ0GG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM62Og0JaGD3"
      },
      "source": [
        "roc_auc_scorer = \"roc_auc_ovr\" \n",
        "# Unsure why this gives much lower score - to investigate\n",
        "# metrics.make_scorer(metrics.roc_auc_score, average=\"macro\", multi_class='ovr')"
      ],
      "id": "oM62Og0JaGD3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRdOUY8GtaJQ",
        "outputId": "eefea711-c91e-4957-a9c9-1c5eeb470753"
      },
      "source": [
        "X_train, y_train = df_folds[predictor_cols].values, df_folds[target_col].values\n",
        "y_train = prepare_y(y_train)\n",
        "grids = optimize_models(models_list, X_train, y_train, scorer=roc_auc_scorer, steps=finetuning_pipeline_steps)"
      ],
      "id": "dRdOUY8GtaJQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd695e89-a26e-4014-898a-c918f53eb76f"
      },
      "source": [
        "# The above optimize code is equivalent to the below, for better readability\n",
        "# pipeline_logistic = make_finetuning_pipeline(\n",
        "#     linear_model.LogisticRegression(\n",
        "#         solver=\"saga\", random_state=config.seed, max_iter=10000, n_jobs=None, fit_intercept=True\n",
        "#     ), steps=steps\n",
        "# )\n",
        "\n",
        "# param_grid = dict(\n",
        "#     model__penalty=[\"l1\", \"l2\"],\n",
        "#     model__C=np.logspace(-4, 4, 10),\n",
        "# )\n",
        "\n",
        "# grid = model_selection.GridSearchCV(pipeline_logistic, param_grid=param_grid, cv=5, refit=True, verbose=3, scoring = \"roc_auc\")\n",
        "# _ = grid.fit(X_train, y_train)"
      ],
      "id": "dd695e89-a26e-4014-898a-c918f53eb76f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca21239e-1404-44c8-9812-3e30d433df56"
      },
      "source": [
        "We can save our results in a dataframe, we will also look at the top performing hyperparameter by querying the below:\n",
        "\n",
        "```python\n",
        "grid_cv_df = pd.DataFrame(grid.cv_results_)\n",
        "grid_cv_df.loc[grid_cv_df['rank_test_score']==1]\n",
        "```"
      ],
      "id": "ca21239e-1404-44c8-9812-3e30d433df56"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "iTr2R-Roa1Jy",
        "outputId": "a98b32ec-194c-4f06-88a2-d4ab56205dfd"
      },
      "source": [
        "# For example, we can see Logistic Regression's GridSearchCV\n",
        "# results like this.\n",
        "grid_cv_df = pd.DataFrame(grids[0].cv_results_)\n",
        "display(grid_cv_df.loc[grid_cv_df['rank_test_score']==1])"
      ],
      "id": "iTr2R-Roa1Jy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_model__C</th>\n",
              "      <th>param_model__penalty</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split3_test_score</th>\n",
              "      <th>split4_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.931891</td>\n",
              "      <td>0.058794</td>\n",
              "      <td>0.002457</td>\n",
              "      <td>0.000387</td>\n",
              "      <td>0.359381</td>\n",
              "      <td>l1</td>\n",
              "      <td>{'model__C': 0.3593813663804626, 'model__penal...</td>\n",
              "      <td>0.997997</td>\n",
              "      <td>0.995547</td>\n",
              "      <td>0.997944</td>\n",
              "      <td>0.990132</td>\n",
              "      <td>0.995477</td>\n",
              "      <td>0.995419</td>\n",
              "      <td>0.002863</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean_fit_time  std_fit_time  ...  std_test_score  rank_test_score\n",
              "8       0.931891      0.058794  ...        0.002863                1\n",
              "\n",
              "[1 rows x 15 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cet9xc1Zya8q"
      },
      "source": [
        "def return_grid_df(\n",
        "    grids: List[model_selection.GridSearchCV],\n",
        ") -> Union[pd.DataFrame, List[model_selection.GridSearchCV]]:\n",
        "    \"\"\"Return a dataframe of the grids with shorted names.\n",
        "\n",
        "    Args:\n",
        "        grids (List[model_selection.GridSearchCV]): A list of GridSearchCV models that are tuned.\n",
        "\n",
        "    Returns:\n",
        "        grid_df, grids (Union[pd.DataFrame, List[model_selection.GridSearchCV]]): A dataframe of the grids with shorted names.\n",
        "    \"\"\"\n",
        "\n",
        "    def shorten_param(param_name):\n",
        "        if \"__\" in param_name:\n",
        "            return param_name.rsplit(\"__\", 1)[1]\n",
        "        return param_name\n",
        "\n",
        "    grid_df = []\n",
        "    for grid in grids:\n",
        "        model_name = grid.estimator[\"model\"].__class__.__name__\n",
        "        cv_results = pd.DataFrame(grid.cv_results_).sort_values(\n",
        "            \"mean_test_score\", ascending=False\n",
        "        )\n",
        "\n",
        "        # get the parameter names\n",
        "        column_results = [f\"param_{name}\" for name in grid.param_grid.keys()]\n",
        "        column_results += [\n",
        "            \"mean_test_score\",\n",
        "            \"std_test_score\",\n",
        "            \"rank_test_score\",\n",
        "        ]\n",
        "        cv_results = cv_results[column_results]\n",
        "        cv_results = cv_results.rename(shorten_param, axis=1)\n",
        "        cv_results[\"model_name\"] = model_name\n",
        "        grid_df.append(cv_results)\n",
        "\n",
        "    return grid_df, grids"
      ],
      "id": "cet9xc1Zya8q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwgLdb0PN4LJ"
      },
      "source": [
        "# grid_df and grids should necessarily be in the same sequence.\n",
        "# grid_df[0] == grids[0] in terms of model information, in this\n",
        "# case, the first index of both should be logistic regression.\n",
        "grid_df, grids = return_grid_df(grids)"
      ],
      "id": "fwgLdb0PN4LJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aYd48OtzlIe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "c82a894b-8898-4e47-8709-8cecb76d124a"
      },
      "source": [
        "for model_df, grid in zip(grid_df, grids):\n",
        "    best_hyperparams_df = model_df.iloc[[0]]\n",
        "    model_name = best_hyperparams_df.model_name.unique()[0]\n",
        "    logger.info(f\"Best hyperparameters found for {model_name} is as follows:\\n{grid.best_params_}\")\n",
        "    display(best_hyperparams_df)\n",
        "    print()"
      ],
      "id": "-aYd48OtzlIe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-11-16,09:19:36 - Best hyperparameters found for LogisticRegression is as follows:\n",
            "{'model__C': 0.3593813663804626, 'model__penalty': 'l1'}\n",
            "2021-11-16,09:19:36 - Best hyperparameters found for LogisticRegression is as follows:\n",
            "{'model__C': 0.3593813663804626, 'model__penalty': 'l1'}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>penalty</th>\n",
              "      <th>C</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "      <th>model_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>l1</td>\n",
              "      <td>0.359381</td>\n",
              "      <td>0.995419</td>\n",
              "      <td>0.002863</td>\n",
              "      <td>1</td>\n",
              "      <td>LogisticRegression</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  penalty         C  ...  rank_test_score          model_name\n",
              "8      l1  0.359381  ...                1  LogisticRegression\n",
              "\n",
              "[1 rows x 6 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-11-16,09:19:36 - Best hyperparameters found for DecisionTreeClassifier is as follows:\n",
            "{'model__criterion': 'entropy', 'model__max_depth': 10, 'model__min_samples_leaf': 10}\n",
            "2021-11-16,09:19:36 - Best hyperparameters found for DecisionTreeClassifier is as follows:\n",
            "{'model__criterion': 'entropy', 'model__max_depth': 10, 'model__min_samples_leaf': 10}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>max_depth</th>\n",
              "      <th>min_samples_leaf</th>\n",
              "      <th>criterion</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "      <th>model_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>entropy</td>\n",
              "      <td>0.954515</td>\n",
              "      <td>0.015913</td>\n",
              "      <td>1</td>\n",
              "      <td>DecisionTreeClassifier</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   max_depth min_samples_leaf  ... rank_test_score              model_name\n",
              "41        10               10  ...               1  DecisionTreeClassifier\n",
              "\n",
              "[1 rows x 7 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-11-16,09:19:37 - Best hyperparameters found for GradientBoostingClassifier is as follows:\n",
            "{'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__subsample': 0.5}\n",
            "2021-11-16,09:19:37 - Best hyperparameters found for GradientBoostingClassifier is as follows:\n",
            "{'model__learning_rate': 0.1, 'model__max_depth': 3, 'model__subsample': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>max_depth</th>\n",
              "      <th>learning_rate</th>\n",
              "      <th>subsample</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "      <th>model_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.991031</td>\n",
              "      <td>0.005869</td>\n",
              "      <td>1</td>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  max_depth learning_rate  ... rank_test_score                  model_name\n",
              "1         3           0.1  ...               1  GradientBoostingClassifier\n",
              "\n",
              "[1 rows x 7 columns]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dde751c-61f0-4c77-a510-e14a3f2f8d57"
      },
      "source": [
        "!!! success\n",
        "    Our best performing set of hyperparameters for Logistic Regression `{'model__C': 0.3593813663804626, 'model__penalty': 'l1'}` gives rise to a mean cross validation score of $0.995419$, which is higher than the model with default hyperparameter scoring, $0.995$ by a small margin. Not too surprising for Logistic Regression here since there aren't many things to tune, and should not see major improvements, but for Decesion Tree, it has increased from 0.907 to around 0.95, seeing quite a big jump with tuned params.\n",
        "\n",
        "!!! danger \"DANGERRRRRRRRRRRRR\"\n",
        "    I am being a bit hand wavy in terms of comparison here, I assumed THAT `GridSearchCV` used the exact same splitting strategy (yes it uses `StratifiedKFold` here) with the exact **SEED/RANDOM_STATE**, which I cannot promise as of now. Thus, a different splitting will, unfortunately, result in different results, although, I don't expect by a huge margin - so I think it is a no-go to compare like this.\n",
        "    We can probably pass in a cv function into `GridSearchCV` to ensure seeding. \n",
        "    This also highlights a problem that even K-fold splitting does not guarantee the reduction in variance. "
      ],
      "id": "5dde751c-61f0-4c77-a510-e14a3f2f8d57"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f1e7164-5449-426b-a3fb-8699255183cb"
      },
      "source": [
        "!!! sucess \"Room for Improvement\"\n",
        "    Apart from the other methods to search for the optimal hyperparameters, we can also include preprocessing step as a tunable hyperparameter. More specifically, in our `ReduceVIF()` step, we hard coded two manual criterion in which the algorithm will stop; if the threshold reaches 10, or if the number of features removed hit 20; we can include them in the search space so we do not need to worry about how many features to remove!"
      ],
      "id": "3f1e7164-5449-426b-a3fb-8699255183cb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSO-10S7dHwo"
      },
      "source": [
        "## Model Persistence (Saving Models)\n",
        "\n",
        "[Model Persistence](https://scikit-learn.org/stable/modules/model_persistence.html)\n",
        "\n",
        "We save our models using `joblib` and we can load it back any time. \n",
        "\n",
        "!!! note\n",
        "    Save it to wandb or GCP storage to store models for better consistency."
      ],
      "id": "oSO-10S7dHwo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRqmOF-Ygmbw"
      },
      "source": [
        "model_path = \"/content/\"\n",
        "\n",
        "def save_model(grids: List[Callable], path: str):\n",
        "    \"\"\"Save a model to a file\"\"\"\n",
        "    for grid in grids:\n",
        "        model_name = grid.best_estimator_[\"model\"].__class__.__name__\n",
        "        path_to_save = Path(path, f\"{model_name}_grid.joblib\")\n",
        "        # Dump to local path\n",
        "        dump(grid, Path(path, path_to_save))\n",
        "        # Dump to wandb cloud\n",
        "        # \"model.h5\" is saved in wandb.run.dir & will be uploaded at the end of training\n",
        "        wandb.save(os.path.join(wandb.run.dir, path_to_save))"
      ],
      "id": "mRqmOF-Ygmbw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxVTcaOD3GMf"
      },
      "source": [
        "Save the model!"
      ],
      "id": "IxVTcaOD3GMf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuFv_36GBXYT"
      },
      "source": [
        "### Wandb\n",
        "\n",
        "We first see how we save and load using wandb."
      ],
      "id": "FuFv_36GBXYT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuCa8MZMYAQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52f8cbd8-0d86-4253-b1b8-6f2d8e7187bf"
      },
      "source": [
        "save_model(grids, model_path)"
      ],
      "id": "zuCa8MZMYAQY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNzqw4bb-fpz"
      },
      "source": [
        "logistic_path = \"LogisticRegression_grid.joblib\""
      ],
      "id": "BNzqw4bb-fpz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2lmYkxw9Joh"
      },
      "source": [
        "# restore the model file \"model.h5\" from a specific run by user \"lavanyashukla\"\n",
        "# in project \"save_and_restore\" from run \"10pr4joa\"\n",
        "best_model = wandb.restore(logistic_path)\n",
        "\n",
        "\n",
        "# use the \"name\" attribute of the returned object\n",
        "# if your framework expects a filename, e.g. as in Keras\n",
        "# model.load_weights(best_model.name)"
      ],
      "id": "h2lmYkxw9Joh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdH-otf2BgLm"
      },
      "source": [
        "### Joblib\n",
        "\n",
        "We see how we use `joblib` to save and load."
      ],
      "id": "TdH-otf2BgLm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjzF-_Wf3IZX"
      },
      "source": [
        "Load the model, and we can test it now if our loaded models is predicting correctly!"
      ],
      "id": "tjzF-_Wf3IZX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6LoPzQcYVE2"
      },
      "source": [
        "logistic_grid = load(\"/content/LogisticRegression_grid.joblib\")"
      ],
      "id": "K6LoPzQcYVE2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIaB9fjw3UNp"
      },
      "source": [
        "Great it seems to work!"
      ],
      "id": "jIaB9fjw3UNp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aXAiYeIBlRT"
      },
      "source": [
        "### Sanity Check"
      ],
      "id": "3aXAiYeIBlRT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4D6AC-aBwhv"
      },
      "source": [
        "!!! note\n",
        "    We just make sure our loaded weight from path is the same as the one we trained. We can easily compare predictions (or coefficients) by the following."
      ],
      "id": "M4D6AC-aBwhv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhd340x93P9b",
        "outputId": "2eb121d1-3be9-468b-f89e-eb0b7dd1f4c0"
      },
      "source": [
        "load(best_model.name).predict(X_train).all() == logistic_grid.predict(\n",
        "    X_train\n",
        ").all() == grids[0].predict(X_train).all()"
      ],
      "id": "xhd340x93P9b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9GAZsxFYZba",
        "outputId": "c71260a1-583d-46ac-8576-4263111da2c5"
      },
      "source": [
        "metrics.roc_auc_score(\n",
        "    y_train, logistic_grid.predict_proba(X_train)[:, 1]\n",
        ") == metrics.roc_auc_score(\n",
        "    y_train, grids[0].predict_proba(X_train)[:, 1]\n",
        ") == metrics.roc_auc_score(\n",
        "    y_train, load(best_model.name).predict_proba(X_train)[:, 1]\n",
        ")\n"
      ],
      "id": "M9GAZsxFYZba",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE9dZekKB6Op"
      },
      "source": [
        "Seems like the save and load method works perfectly."
      ],
      "id": "UE9dZekKB6Op"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGYIKrSZ1xyl"
      },
      "source": [
        "!!! warning\n",
        "    Do not call this directly.\n",
        "    ```python\n",
        "    grids[0].best_estimator_[\"model\"].predict(X_train)\n",
        "    ```\n",
        "\n",
        "    This is because `grids[0].best_estimator_[\"model\"]` is only referring to the Logistic Regression Model WITHOUT the pipeline (preprocessing) steps. And hence will raise error if the preprocessing steps has feature selection. But the main idea is, be careful when using the above."
      ],
      "id": "eGYIKrSZ1xyl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3Gwl1sS0YMx"
      },
      "source": [
        "# grids[0].best_estimator_[\"model\"].predict(X_train)"
      ],
      "id": "K3Gwl1sS0YMx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9_5PYUiRNqf"
      },
      "source": [
        "## Retrain using Hyperparameters\n",
        "\n",
        "\n",
        "!!! info \"Retraining Methods\"\n",
        "    From the discussion[^cpmp], my doubts are cleared. Quoting verbatim from the discussion, we have:\n",
        "\n",
        "K-folds cross validation was devised as a way to assess model performance using training data. A great paper on this from Sebastian Raschka is a must read https://arxiv.org/abs/1811.12808. You use K-folds cv to tune you model, then retrain on all training data with best hyperparamters found.\n",
        "\n",
        "However, once you have run K-fold cv, you get $K$ trained models. Kagglers quickly found that ensembling these models was giving good results at zero computation cost, rather than having to retrain a model on full data. It soon became a very common practice.\n",
        "\n",
        "---\n",
        "\n",
        "!!! note \"Takeway\"\n",
        "    For small-medium datasets, after finding the best hyperparameters $G$, we use $G$ in our model $h$ to train on the whole dataset $\\mathcal{X}$ again to get the fitted parameters of $h$. Then you use the newly gained fitted parameters to then evaluate on the **Test Set**.\n",
        "    For large and computationally expensive datasets, when you finished your K-folds, say 5 folds, you get 5 \"different\" models, $h_{i}, i \\in {1, 2, 3, 4, 5}$, what you can do is to save the weights (or in normal ML, weights refer to the parameters gained), and evaluate on the test set for each of the five models, you then get 5 different test predictions, and a common practice is the do a simple mean of these 5 set of predictions. \n",
        "\n",
        "\n",
        "\n",
        "[^cpmp]: https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/275883"
      ],
      "id": "F9_5PYUiRNqf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCpYRWuuP2cs"
      },
      "source": [
        "### Retrain on K-Folds\n",
        "\n",
        "TODO: This should be easy for me as I dabbled more in Kaggle comp and are more familiar with this methodology."
      ],
      "id": "JCpYRWuuP2cs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4a626cc-c72e-4666-8fd8-5120f3554e6d"
      },
      "source": [
        "### Retrain on the whole training set\n",
        "\n",
        "A common practice after the hyperparameter tuning phase is to retrain the model on the whole dataset $X_{\\text{train}}$ where we will get the estimator's coefficients obtained from the retraining. This is actually already done as the scikit-learn's `GridSearchCV` has a parameter `refit`; if we select it to be true, then after the model selection process is done (i.e. getting the best hyperparameters after cross validation with grid search), the grid search object will retrain on the whole $X_{\\text{train}}$ with the best hyperparameters internally, and return us back an object in which we can call `predict` etc."
      ],
      "id": "e4a626cc-c72e-4666-8fd8-5120f3554e6d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUGGk42T443s"
      },
      "source": [
        "!!! warning \"Paranoia Alert\"\n",
        "\n",
        "    However, to be extra careful, we can retrain manually using the best hyperparameters and check if scikit-learn is true to its documentation. We will just reconstruct the pipeline using the grid's best hyper parameters. We will then test if the retrained model's coefficients coincide with the grid's best estimator's coefficients. If there difference is 0, this means they are trained under the same circumstances and we can be sure that the refit parameter is behaving true to its words.\n",
        "\n",
        "    ```python\n",
        "    grid_best_hyperparams = grid.best_params_\n",
        "    print(grid_best_hyperparams) ->\n",
        "    {'model__C': 0.3593813663804626, 'model__penalty': 'l1'}\n",
        "    ```"
      ],
      "id": "TUGGk42T443s"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17df6f72-e15f-4f28-83e2-e1ad08dbf1f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0feec88-1949-466a-d705-dbbd56534232"
      },
      "source": [
        "retrain_logistic_pipeline = pipeline.Pipeline(\n",
        "    [\n",
        "        (\"standardize\", preprocessing.StandardScaler()),\n",
        "        (\"remove_multicollinearity\", ReduceVIF(thresh=10)),\n",
        "        (\n",
        "            \"model\",\n",
        "            linear_model.LogisticRegression(\n",
        "                C=0.3593813663804626,\n",
        "                max_iter=10000,\n",
        "                random_state=1992,\n",
        "                solver=\"saga\",\n",
        "                penalty=\"l1\",\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "_ = retrain_logistic_pipeline.fit(X_train, y_train)\n",
        "logistic_grid = grids[0]\n",
        "coef_diff = (\n",
        "    retrain_logistic_pipeline[\"model\"].coef_\n",
        "    - logistic_grid.best_estimator_[\"model\"].coef_\n",
        ")\n",
        "\n",
        "print(\"...\")\n",
        "assert np.all(coef_diff == 0) == True\n",
        "logger.info(\"Retraining Assertion Passed!\")"
      ],
      "id": "17df6f72-e15f-4f28-83e2-e1ad08dbf1f0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-11-16,09:19:38 - Retraining Assertion Passed!\n",
            "2021-11-16,09:19:38 - Retraining Assertion Passed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "...\n"
          ]
        }
      ]
    }
  ]
}