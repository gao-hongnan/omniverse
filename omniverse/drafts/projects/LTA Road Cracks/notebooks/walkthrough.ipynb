{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "798d39a6-843e-4158-9211-0353a7bb7451",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\M}{\\mathcal{M}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\yhat}{\\mathbf{\\hat{y}}}\n",
    "\\newcommand{\\iou}{\\textbf{IOU}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f99e8-2f8f-47c5-a02f-9cdca2a7082f",
   "metadata": {},
   "source": [
    "## Data Collection and Label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623b7c8a-36e9-4fb6-af1f-07105d56f060",
   "metadata": {},
   "source": [
    "### Original Dataset\n",
    "\n",
    "- 500 images of size $4032 \\times 3024$ provided by LTA:\n",
    "    - 150 positive class 1 where 1 means defective;\n",
    "    - 350 negative class 0 where 0 means non-defective;\n",
    "    - Class labels are subjected to changes depending on model used (i.e. F-RCNN treats background as 0 while Yolo does not care)\n",
    "\n",
    "- There is some slight class imbalance:\n",
    "    - We checked if we can garner more data;\n",
    "    - We were given the green light;\n",
    "    - We collected about 1500 more photos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cf8fd4-b0a2-4043-b57a-33481cdd23b7",
   "metadata": {},
   "source": [
    "### Labelling\n",
    "\n",
    "- The total number of images is $2000$;\n",
    "- We need to label all of them as labels were not provided;\n",
    "- Shortage of manpower prompted us to think of ways to reduce manual labour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67feba70-0e31-45ef-8d68-c3e3577e8f60",
   "metadata": {},
   "source": [
    "#### Semi-Supervised Learning\n",
    "\n",
    "> **Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data). It is a special instance of weak supervision.**\n",
    "\n",
    "**High-level overview**\n",
    "\n",
    "- We manually labour $500$ images of 50-50 ratio with class balanced;\n",
    "- We train on the $500$ images using an Object Detection Model $\\M_1$ until convergence (i.e. an acceptable result say MAP > 0.8, but more importantly, we **do not care** the class accuracy, rather, we want the localization error to be low);\n",
    "- We use $\\M_1$ and perform inference on the remaining $1500$ images so that $\\M_1$ can return us each image's bounding box coordinates, during this process we can discard bounding boxes with low confidences if we feel that an image won't contain more than say, 5 tactile tiles;\n",
    "- Note that we emphasized on a low localization error from $\\M_1$ and therefore expect that at the very least the bounding boxes of the tactile tiles are accurate as drawing bounding boxes manually is more time consuming than labelling whether a given bounding box is of class 0 or 1;\n",
    "- We then run through manually over the predicted $1500$ images to see if the predicted bounding boxes by $\\M_1$ makes sense, and also to correct them if need be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cc64c9-d7e2-49e8-8a37-1640f337295e",
   "metadata": {},
   "source": [
    "## Establish Metrics\n",
    "\n",
    "After understanding the problem better, we should probably define a metric to optimize. We adopt the commonly used metrics for object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe229d3-f115-4bdf-a272-308e4dbef376",
   "metadata": {},
   "source": [
    "### IOU\n",
    "\n",
    "The concept of **Intersection over Union (IOU)**, also known as the **Jaccard Index** is used to decide whether the predicted bounding box is giving us a good outcome. \n",
    "\n",
    "It is defined by the formula: $$J(A,B) = \\dfrac{|(A \\cap B)|}{|(A \\cup B)|}$$\n",
    "\n",
    "where A and B are the area of the ground truth bounding box and predicted ground truth bounding box respectively as shown in figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356e41ec-28c3-4186-884c-49dd51f0b5e4",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/LTA/images/lta_2.PNG\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e19276b-40e1-4318-9c84-40fbff55a2c6",
   "metadata": {},
   "source": [
    "### The Typical Brothers (TP, FP, TN, FN)\n",
    "\n",
    "Object Detection at its core is a complicated algorithm comprising of both regression and classification. We first define the 4 good brothers that we see in a typical **classification problem**.\n",
    "\n",
    "Note in usual classification problem, we will have a probabality logit at the end of a softmax/sigmoid layer, which aids us in determining whether a classification belongs to TP, FP, TN or FN.\n",
    "In object detection, we use the **IOU** to determine if a given a bounding box prediction belongs to the four brothers.\n",
    "\n",
    "Let us define:\n",
    "\n",
    "- $\\y$: 1 single ground truth bbox;\n",
    "- $\\yhat$: 1 single predicted bbox;\n",
    "- $\\iou(\\y, \\yhat)$: The IOU between them.\n",
    "- $t$: The threshold for IOU to cross for it to be a positive, defaults to $0.5$.\n",
    "\n",
    "Then:\n",
    "\n",
    "- TP: The model classified it as positive and it indeed is positive; $\\yhat$ is TP iff $\\iou(\\y, \\yhat) > t$; both the predictor and the ground truth are in agreement;\n",
    "- FP: The model classified it as positive but it is negative; $\\yhat$ is FP iff $\\iou(\\y, \\yhat) < t$; the predictor raised a false alarm that there is an object but there actually isn't;\n",
    "- FN: The model classified it as negative but it is positive; $\\yhat$ is FN iff there exists a ground truth $\\y$ that is not detected by our model;\n",
    "- TN: This is usually ignored since it means that both the predictor and the annotator ground truth did not have a bounding box; it is often termed as **correct rejections** because there exist infinite places on an image where there are no bounding boxes from both the predictor and the annotator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab6dc0b-5459-4d21-ad1d-f2d402bcde27",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/LTA/images/lta_1.PNG\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e3f1cf-b81c-4464-a055-b29012f40420",
   "metadata": {},
   "source": [
    "### The Iconic DUO (Precision and Recall)\n",
    "\n",
    "As with any other classification problem, we will see this iconic duo. Let us recap these 2 metrics in the settings of object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8490e67b-2b7d-4251-b2c6-4a95199f2b82",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "> **Precision measures how many of the samples predicted as positive are actually positive. Mathematically, it is expressed as:**\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\dfrac{\\text{TP}}{\\text{TP} + \\text{FP}}=P(Y=1 | \\hat{Y} = 1)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "> A Probablistic Interpretation\n",
    "    Notice that the above definition has a probabilitic interpretation $P(Y = 1 | \\hat{Y} = 1)$, where $Y$ and $\\hat{Y}$ refers to the actual label and predicted labels respectively. We interpreted precision and recall not as ratios but as [estimations of probabilities](https://en.wikipedia.org/wiki/Precision_and_recall).\n",
    "    Precision is then the estimated probability that a random point selected from the samples are positive. This might be a tough pill to swallow as someone who was never good in statistics but it is just conditional probability. If you try to think a bit further, you can form an intuition as follows:\n",
    "    > If your classifier $h$ is trained and the last layer is say, sigmoid, which in binary classification, calibrates the logits and turn them into probabilities. Then it can be interpretated that given a randomly chosen point $x \\in X_{train}$, what is the probability of this point $x$ to be positive given that it is predicted as positive by the classifer?\n",
    "\n",
    "---\n",
    "\n",
    "Informally, precision answers the question **what proportion of positive predictions was actually correct**? In other words, out of all the positive predictions made by the model, how many of those positive predictions were actually positive when compared to the ground truth?\n",
    "\n",
    "> **In object detection, precision can be throught of as the fraction of correct object predictions among all objects detected by the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917de2b4-4e4e-49b0-a00d-3c01ff576040",
   "metadata": {},
   "source": [
    "### Recall\n",
    "\n",
    "> **Recall measures the following: out of all the actual positives (say, the real cancer patients), how many of them were identified correctly by the classifier? Mathematically, it is expressed as:**\n",
    "    \n",
    "$$\n",
    "\\text{Recall}= \\dfrac{\\text{TP}}{\\text{TP} + \\text{FN}}= P(\\hat{Y}=1 | Y = 1)=1-FNR\n",
    "$$\n",
    "\n",
    "From the formula, we see the denominator to be defined as TP + FN, which is unsurprising as this gives you the actual number of positives. The dynamics is also similar to the one in precision.\n",
    "\n",
    "> **In object detection, recall can be thought of as the fraction of ground truth objects that are correctly detected by the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3841c0b-996c-4c56-9a7e-c049df4f336d",
   "metadata": {},
   "source": [
    "### Recall vs Precision\n",
    "\n",
    "The tug of war between this duo is still present, that is to say, as precision goes down, recall might go up and vice versa.\n",
    "\n",
    "Note that precision and recall are parametrized by the IOU threshold $t$, that means, for each threshold $t$, we can calculate the pair of precision and recall score.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/LTA/images/lta_3.PNG\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b464d6-0ed9-4a50-a4ee-fba162c955ea",
   "metadata": {},
   "source": [
    "### The Objectness Confidence Score\n",
    "\n",
    "Before we proceed to the behemoth **Mean Average Precision**, we should take a step back and recall that an usual object detection model is parametrized by:\n",
    "\n",
    "- predicted bbox: [[...], [...]] 2 predictions;\n",
    "- associated objectness confidence score: [0.2, 0.9] how confident that the predicted bbox really do contains an object;\n",
    "- predicted labels: [1, 2]\n",
    "- associated labels confidence score: [0.3, 0.8] how confident that the predicted label is correct, usually this is the probability logits.\n",
    "\n",
    "Before we even get to IOU calculation, we have another threshold to worry about, which is the objectness confidence score $\\tau$; that is, if the predicted bboxes has confidence score below $\\tau$, then we immediately discard the predictions and do not even consider it a valid prediction. So if you are have a high $\\tau$ threshold, then you may have a high precision at the cost of low recall. Let us breakdown why:\n",
    "\n",
    "- If you raise $\\tau$, then we have a stringent requirement such that more objects might be missed by the model. A high FN ensues.\n",
    "- If you decrease $\\tau$, then we may have many predictions, say a ground truth image has 2 gt bbox, but since we loosen $\\tau$, we have 10 predicted bboxes, this causes a lot of FPs.\n",
    "\n",
    "> **So a precision recall curve is a plot of precision vs recall at various thresholds $\\tau$. Note very carefully that the pr-curve is parametrized by $\\tau$ and not $t$ (the IOU threshold).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc832f-b7fc-4c89-bae2-91ef6d1c24bc",
   "metadata": {},
   "source": [
    "### Average Precision\n",
    "\n",
    "> **The big picture, precison-recall curve can be understood as y = f(x) where y is precision and recall x. More simply, it is just plotting pairs of precision-recall for each threshold $\\tau$ and summing the area under the curve.**\n",
    "\n",
    "> At each confidence level (threshold), we ask what is the precision-recall score of the predictions of **all bounding boxes at a specific IOU** while discarding off those below the threshold, then average them over all thresholds? That's MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a68f26f-509b-4f0f-8b9a-af7a905f009f",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f86dbe-dcb7-4b33-ad2d-88f4132bdaf1",
   "metadata": {},
   "source": [
    "### RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f9050a-f7b4-4cbe-8e9d-b84079505ce8",
   "metadata": {},
   "source": [
    "#### Basic Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df1d5e3-aab1-4ab9-b417-4d9cabfcf289",
   "metadata": {},
   "source": [
    "1. Perform **selective search** to extract multiple high-quality region proposals on the input image. These proposed regions are usually selected at multiple scales with different shapes and sizes. Each region proposal will be labeled with a class and a ground-truth bounding box. *I believe out of the 2000 proposals, some of them won't even enclose any ground truth and so they should be discarded*.\n",
    "2. Choose a pretrained CNN and truncate it before the output layer. Resize each region proposal to the input size required by the network, and output the extracted features for the region proposal through forward propagation. \n",
    "3. Take the extracted features and labeled class of each region proposal as an example. Train multiple support vector machines to classify objects, where each support vector machine individually determines whether the example contains a specific class.\n",
    "4. Take the extracted features and labeled bounding box of each region proposal as an example. Train a linear regression model to predict the ground-truth bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f9e0c1-bbe4-4c73-886e-2680aaca15ee",
   "metadata": {},
   "source": [
    "![The R-CNN model.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/r-cnn.svg?raw=1)\n",
    "<div>\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/LTA/images/rcnn.PNG\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5fb5ec-ae94-4f52-8a0f-fc990bdef58c",
   "metadata": {},
   "source": [
    "> **Own words:**\n",
    "\n",
    "- **Propose say 2000 regions (anchor boxes) then for each of the 2000 regions, we discard the ones with no IOU with ground truth;**\n",
    "- **Run a CNN on each the remaining region proposals and take the output and:**\n",
    "    - Feed into SVMs to classify the region;\n",
    "    - A linear regressor to regress the bboxes;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330ed434-ac4c-41cf-97b5-0b93ddba82c8",
   "metadata": {},
   "source": [
    "#### Pros and Cons\n",
    "\n",
    "**Cons**:\n",
    "- Too slow as if 2000 proposals need to run 2000 CNNs on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193630ad-59bf-4586-9f50-05085cea783e",
   "metadata": {},
   "source": [
    "### Fast-RCNN\n",
    "\n",
    "To resolve the 2000 proposals = 2000 CNNs issue, one can envision that we just use 1 CNN for the image and using this 1 CNN we \"inference on the 2000 proposals\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec74826-66bd-4225-827d-8bf26d56edce",
   "metadata": {},
   "source": [
    "#### Basic Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6932dc35-7014-462d-b122-32e4c06880cb",
   "metadata": {},
   "source": [
    "1. Compared with the R-CNN, in the fast R-CNN the input of the CNN for feature extraction is the entire image, rather than individual region proposals. Moreover, this CNN is trainable. Given an input image, let the shape of the CNN output be $1 \\times c \\times h_1  \\times w_1$.\n",
    "1. Suppose that selective search generates $n$ region proposals. These region proposals (of different shapes) mark regions of interest (of different shapes) on the CNN output. Then these regions of interest further extract features of the same shape (say height $h_2$ and width $w_2$ are specified) in order to be easily concatenated. To achieve this, the fast R-CNN introduces the *region of interest (RoI) pooling* layer: the CNN output and region proposals are input into this layer, outputting concatenated features of shape $n \\times c \\times h_2 \\times w_2$ that are further extracted for all the region proposals.\n",
    "1. Using a fully-connected layer, transform the concatenated features into an output of shape $n \\times d$, where $d$ depends on the model design.\n",
    "1. Predict the class and bounding box for each of the $n$ region proposals. More concretely, in class and bounding box prediction, transform the fully-connected layer output into an output of shape $n \\times q$ ($q$ is the number of classes) and an output of shape $n \\times 4$, respectively. The class prediction uses softmax regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4401cb9b-8896-4005-b879-0012adc8e8b7",
   "metadata": {},
   "source": [
    "![The fast R-CNN model.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/fast-rcnn.svg?raw=1)\n",
    "\n",
    "<div>\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/LTA/images/fast_rcnn.PNG\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d4512-4b5d-44a3-a7d8-9ce74e041a57",
   "metadata": {},
   "source": [
    "> **Own words:**\n",
    "\n",
    "- **Decide on a CNN and truncate head**\n",
    "- **Here say the CNN output feature map is shape (1, 128, 7, 7) with 128 filters of 7 by 7**\n",
    "- **Use proposal method to get say n=200 regions**, these regions can be mapped to the CNN output in previous step.\n",
    "- **Each region can be shaped differently, so we use ROI pooling to shape all n of them into same width and height say 3 by 3. So now our shape is $(200, 128, 3, 3)$ where 200 is stacked region proposals.**\n",
    "- **Then use the traditional FC with 2 heads to predict**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7eb33a-8eec-4706-ae33-9ef3ce2d1890",
   "metadata": {},
   "source": [
    "#### Pros and Cons\n",
    "\n",
    "**Pros**:\n",
    "- Solved 2000 CNN issue.\n",
    "\n",
    "**Cons**:\n",
    "- Selective search still slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed0a08-877b-4ab9-bba8-b3e7d0ec85aa",
   "metadata": {},
   "source": [
    "### Faster-RCNN\n",
    "\n",
    "Basically solved selective search with RPN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d58e1-62bc-4d4f-9591-c0afd67cfa4f",
   "metadata": {},
   "source": [
    "#### Basic Architecture\n",
    "\n",
    "\n",
    "1. Use a $3\\times 3$ convolutional layer with padding of 1 to transform the CNN output to a new output with $c$ channels. In this way, each unit along the spatial dimensions of the CNN-extracted feature maps gets a new feature vector of length $c$.\n",
    "1. Centered on each pixel of the feature maps, generate multiple anchor boxes of different scales and aspect ratios and label them.\n",
    "1. Using the length-$c$ feature vector at the center of each anchor box, predict the binary class (background or objects) and bounding box for this anchor box.\n",
    "1. Consider those predicted bounding boxes whose  predicted classes are objects. Remove overlapped results using non-maximum suppression. The remaining  predicted bounding boxes for objects are the region proposals required by the region of interest pooling layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b07558-eef1-4510-8b9d-76b7e25b8424",
   "metadata": {},
   "source": [
    "![The faster R-CNN model.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/faster-rcnn.svg?raw=1)\n",
    "\n",
    "<div>\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/LTA/images/faster_rcnn.PNG\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505e0fb0-ea9f-4993-bfe4-adbc8ee57c29",
   "metadata": {},
   "source": [
    "RPN like this, basically take feature map and connect to 2k and 4k mappings and compare with gt anchor boxes, can be learned so it is powerful and can predict better proposals.\n",
    "\n",
    "```python\n",
    "in_channels = 512  # depends on the output feature map. in vgg 16 it is equal to 512\n",
    "\n",
    "mid_channels = 512\n",
    "n_anchor = 9  # Number of anchors at each location\n",
    "\n",
    "conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1).to(device)\n",
    "conv1.weight.data.normal_(0, 0.01)\n",
    "conv1.bias.data.zero_()\n",
    "\n",
    "reg_layer = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0).to(device)\n",
    "reg_layer.weight.data.normal_(0, 0.01)\n",
    "reg_layer.bias.data.zero_()\n",
    "\n",
    "cls_layer = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0).to(device)  # I will be going to use softmax here. you can equally use sigmoid if u replace 2 with 1.\n",
    "cls_layer.weight.data.normal_(0, 0.01)\n",
    "cls_layer.bias.data.zero_();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e1cad-7499-467d-9d97-e074978e99df",
   "metadata": {},
   "source": [
    "It is worth noting that, as part of the faster R-CNN model, the region proposal network is jointly trained with the rest of the model. In other words, the objective function of the faster R-CNN includes not only the class and bounding box prediction in object detection, but also the binary class and bounding box prediction of anchor boxes in the region proposal network. As a result of the end-to-end training, the region proposal network learns how to generate high-quality region proposals, so as to stay accurate in object detection with a reduced number of region proposals that are learned from data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993e2b29-a6a8-4548-86be-dadc704517b4",
   "metadata": {},
   "source": [
    "#### Pros and Cons\n",
    "\n",
    "- Still considered 2-stage, accurate but slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cecf50-9b85-49a0-ad80-01c7849debca",
   "metadata": {},
   "source": [
    "### Yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631555f-7a85-4325-bd2e-d2aeba169d3d",
   "metadata": {},
   "source": [
    "#### Pros and Cons VS faster rcnn\n",
    "\n",
    "**Cons**\n",
    "- Yolo v1 7 by 7 grid, each grid 2 bbox so total 98 bounding boxes.\n",
    "    - In each grid cell one image is detected even though 2 bounding boxes proposed.\n",
    "    - This means if objects in an image lie closely in a grid cell then will have issue detecting all of them.\n",
    "    - It doesn’t generalize well when objects in the image show rare aspects of ratio.\n",
    "\n",
    "- Faster RCNN on the other hand, do detect small objects well since it has nine anchors in a single grid, however it fails to do real-time detection with its two step architecture cause too slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1708a-90e0-4f8a-bb8b-fadfd66e2a97",
   "metadata": {},
   "source": [
    "#### Yolo V3\n",
    "\n",
    "While YOLOv2 uses the DarkNet-19 as the model architecture, YOLOv3 uses a much more complex DarkNet-53 as the model backbone— a 106 layer neural network complete with residual blocks and upsampling networks.\n",
    "\n",
    "YOLOv3’s architectural novelty allows it to predict at 3 different scales, with the feature maps being extracted at layers 82, 94, and 106 for these predictions..\n",
    "\n",
    "By detecting features at 3 different scales, YOLOv3 makes up for the shortcomings of YOLOv2 and YOLO, particularly in the detection of smaller objects. With the architecture allowing the concatenation of the upsampled layer outputs with the features from previous layers, the fine-grained features that have been extracted are preserved thus making the detection of smaller objects easier.\n",
    "\n",
    "YOLOv3 only predicts 3 bounding boxes per cell (compared to 5 in YOLOv2) but it makes three predictions at different scales, totaling up to 9 anchor boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846da1a7-a691-4415-9950-667ee3a0114b",
   "metadata": {},
   "source": [
    "#### Screenshots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dd083d-0708-48d3-a0d2-9be9c445329c",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/LTA/images/yolov1_vs_fasterrcnn.PNG\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/LTA/images/yolov123_comparison.PNG\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edca31d-8258-449a-8b99-77ea266b03f4",
   "metadata": {},
   "source": [
    "## Augmentations\n",
    "\n",
    "- Augmentation techniques. This helps artificially expanding the dataset, intuition is in each training iteration (batch), the model sees a slightly different version of the original image, and thus helps to generalize.\n",
    "        - Normal flips and rotational geometrical transformation, that is a given since photos can be upside down, left-right flipped.\n",
    "        - Hue, Saturation and Color to match dusk, noon and dawn.\n",
    "        - Random Eraser, similar to dropout, we mask certain parts of the image to force the model to learn with less information, a regularization to overfitting and memorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795baf3-e9b0-4566-aa7c-adbe3009be90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
