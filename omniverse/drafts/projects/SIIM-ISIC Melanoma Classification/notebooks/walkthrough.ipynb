{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b7c26b-7fe6-4b5b-9fec-564c56246438",
   "metadata": {},
   "source": [
    "## SIIM-ISIC Melanoma Classification\n",
    "\n",
    "This competition is hosted on Kaggle and the [description and overview is stated below](https://www.kaggle.com/c/siim-isic-melanoma-classification/overview).\n",
    "\n",
    "Skin cancer is the most prevalent type of cancer. Melanoma, specifically, is responsible for 75% of skin cancer deaths, despite being the least common skin cancer. The American Cancer Society estimates over 100,000 new melanoma cases will be diagnosed in 2020. It's also expected that almost 7,000 people will die from the disease. As with other cancers, early and accurate detection—potentially aided by data science—can make treatment more effective.\n",
    "\n",
    "Currently, dermatologists evaluate every one of a patient's moles to identify outlier lesions or “ugly ducklings” that are most likely to be melanoma. Existing AI approaches have not adequately considered this clinical frame of reference. Dermatologists could enhance their diagnostic accuracy if detection algorithms take into account “contextual” images within the same patient to determine which images represent a melanoma. If successful, classifiers would be more accurate and could better support dermatological clinic work.\n",
    "\n",
    "As the leading healthcare organization for informatics in medical imaging, the Society for Imaging Informatics in Medicine (SIIM)'s mission is to advance medical imaging informatics through education, research, and innovation in a multi-disciplinary community. SIIM is joined by the International Skin Imaging Collaboration (ISIC), an international effort to improve melanoma diagnosis. The ISIC Archive contains the largest publicly available collection of quality-controlled dermoscopic images of skin lesions.\n",
    "\n",
    "In this competition, you’ll identify melanoma in images of skin lesions. In particular, you’ll use images within the same patient and determine which are likely to represent a melanoma. Using patient-level contextual information may help the development of image analysis tools, which could better support clinical dermatologists.\n",
    "\n",
    "Melanoma is a deadly disease, but if caught early, most melanomas can be cured with minor surgery. Image analysis tools that automate the diagnosis of melanoma will improve dermatologists' diagnostic accuracy. Better detection of melanoma has the opportunity to positively impact millions of people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf367a8-7a56-4f24-8c0d-eaa6a770ddf7",
   "metadata": {},
   "source": [
    "## Establish Metrics\n",
    "\n",
    "After understanding the problem better, we should probably define a metric to optimize. As usual, this step should be closely tied to business problem. We were already given a metric score by the competition host and let us understand it better.\n",
    "\n",
    "Recall that we wish to have a well-calibrated model, the intuition is that a high performance model may not output meaningful probabilities, even if they can have extremely good performance score.\n",
    "\n",
    "Consider a model that outputs logits of $0.51$ when y_true is 1 and $0.49$ otherwise, then a decision threshold of $0.5$ guarantees an accuracy of $100\\%$, we have no complaints here if we have no issue with our threshold if our only goal is to have a high scoring model. However, if in medical case, where doctor wants to understand \"probablistically\" the survival of a patient, then we might want to turn into logits probs. But apparently the example here holds almost no meaning, when compared to a \"well calibrated model\", more concretely.\n",
    "\n",
    "```python\n",
    "y_true = [0, 0, 1, 1]\n",
    "y_prob_uncalibrated = [0.49, 0.49, 0.51, 0.51]\n",
    "y_prob_calibrated = [0.1, 0.45, 0.99, 0.6]\n",
    "```\n",
    "\n",
    "both models give $100\\%$ accuracy, but the latter (assuming calibrated), can give us a laymen idea that ok this patient has 0.99 chance and the other patient 0.6 chance of surviving etc.\n",
    "\n",
    "### Benefit Structure\n",
    "\n",
    "One can introduce a **benefit structure** with relevant cost-benefit assignment.\n",
    "\n",
    "- TP: + 100\n",
    "- FN: -1000\n",
    "- FP: -10\n",
    "- TP+FP: -1 (screening for example)\n",
    "\n",
    "With each TP, we net a profit of 100, and with each FN, we lose -1000, FP loses -10 and whenever the patient get predicted to die (1), send for further screening -1. So towards the end, we can have:\n",
    "\n",
    "$$\n",
    "cost = 100*TP - 1000 * FN - 10 * FP - 1 * (TP+FP)\n",
    "$$\n",
    "\n",
    "This structure helps us decide which metrics to choose.\n",
    "\n",
    "### ROC\n",
    "\n",
    "Definition: The basic (non-probablistic intepretation) of ROC is graph that plots the True Positive Rate on the y-axis and False Positive Rate on the x-axis parametrized by a threshold vector `t`. We then look at the area under the ROC curve (AUROC) to get an overall performance measure. Note that TPR is `recall`.\n",
    "\n",
    "- TPR (recall) = TP / (TP + FN)\n",
    "- FPR = FP / (FP + TN)\n",
    "- Threshold invariant\n",
    "    - The ROC looks at the performance of a model hypothesis at all thresholds. This is better than just optimizing **recall** which only looks at a fixed threshold.\n",
    "- Scale Invariant\n",
    "    - Not necessarily a good thing in this context, as this makes ROC a semi-proper scoring metric, that is, it takes in non-calibrated scores and perform well. **The below code shows that as long as the order is preserved, `y2` and `y4` make zero difference in the outcome. In this case, the doctor may not be able to have a “confidence” level of how likely the patient is going to survive.**\n",
    "        \n",
    "        ```python\n",
    "        y1 = [1,0,1,0]\n",
    "        y2 = [0.52,0.51,0.52,0.51]\n",
    "        y3 = [52,51,52,51]\n",
    "        y4 = [0.99, 0.51, 0.98, 0.51]\n",
    "        uncalibrated_roc = roc(y1,y2) == roc(y1,y3) == roc(y1, y4)\n",
    "        print(f\"{uncalibrated_roc}\") -> 1.0\n",
    "        ```\n",
    "        \n",
    "    - This brings us to the next point.\n",
    "\n",
    "More info in notebook.\n",
    "\n",
    "### Brier Score Loss\n",
    "\n",
    "Brier Score computes the squared difference between the probability of a prediction and its actual outcome. Intuitively, this score punishes “unconfident and neutral” probability logits. If a model consistently spits out probability that is near 0.5, then this score will be large. \n",
    "\n",
    "- Proper scoring\n",
    "    - Tells us if the scores output are well calibrated.\n",
    "    - If not well calibrated, prompt us to either use a different model that calibrated well, or to perform calibration on the model itself.\n",
    "    - Logistic regression produces natural well calibrated probabilities since it optimizes the log-loss (ce loss), in fact, I think MLE models should always produce well calibrated probabilities since behind the scene it is minimizing KL divergence between ground truth distribution P and estimated distribution Q.\n",
    "    - It follows that models like DT do not produce well calibrated probabilities.\n",
    "\n",
    "More info in notebook.\n",
    "\n",
    "### What can we explore?\n",
    "\n",
    "- Did not provide insight if Precision-recall curve and if it may be well posed for this problem than ROC since there is some class imbalance.\n",
    "- Did not go into details on calibration methods, in fact, models like RF are not well calibrated by construction. [https://scikit-learn.org/stable/modules/calibration.html](https://scikit-learn.org/stable/modules/calibration.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fccc2c8-33ac-4f71-bc90-25219c91ce0e",
   "metadata": {},
   "source": [
    "## Validation and Resampling Strategy\n",
    "\n",
    "### How should we split out data into folds?\n",
    "\n",
    "We should examine the data for a few factors:\n",
    "\n",
    "1. Is the data $\\mathcal{X}$ imbalanced?\n",
    "2. Is the data $\\mathcal{X}$ generated in a **i.i.d.** manner, more specifically, if I split $\\mathcal{X}$ to $\\mathcal{X}_{train}$ and $\\mathcal{X}_{val}$, can we ensure that $\\mathcal{X}_{val}$ has no dependency on $\\mathcal{X}_{train}$?\n",
    "\n",
    "We came to the conclusion:\n",
    "\n",
    "1. Yes, the data is severely imbalanced in which there are only around $2\\%$ of positive (malignant) samples. Therefore, a stratified cross validation is reasonable. `StratifiedKFold` ensures that relative class frequencies is approximately preserved in each train and validation fold. More concretely, we will not experience the scenario where $X_{train}$ has $m^{+}$ and $m^{-}$ positive and negative samples, but $X_{val}$ has only $p^{+}$ positive samples only and 0 negative samples, simply due to the scarcity of negative samples.\n",
    "\n",
    "2. In medical imaging, it is a well known fact that most of the data contains patient level repeatedly. To put it bluntly, if I have 100 samples, and according to **PatientID**, we see that the id 123456 (John Doe) appeared 20 times, this is normal as a patient can undergo multiple settings of say, X-rays. If we allow John Doe's data to appear in both train and validation set, then this poses a problem of information leakage, in which the data is no longer **i.i.d.**. One can think of each patient has an \"unique, underlying features\" which are highly correlated across their different samples. As a result, it is paramount to ensure that amongst this 3255 unique patients, we need to ensure that each unique patients' images **DO NOT** appear in the validation fold. That is to say, if patient John Doe has 100 X-ray images, but during our 5-fold splits, he has 70 images in Fold 1-4, while 30 images are in Fold 5, then if we were to train on Fold 1-4 and validate on Fold 5, there may be potential leakage and the model will predict with confidence for John Doe's images. This is under the assumption that John Doe's data does not fulfill the i.i.d proces\n",
    "\n",
    "---\n",
    "\n",
    "With the above consideration, we will use `StratifiedGroupKFold` where $K = 5$ splits. There wasn't this splitting function in scikit-learn at the time of competition and as a result, we used a custom written (by someone else) `RepeatedStratifiedGroupKFold` function and just set `n_splits = 1` to get **StratifiedGroupKFold** (yes we cannot afford to repeated sample, so setting the split to be 1 will collapse the repeated function to just the normal stratified group kfold). However, as of 2022, this function is readily available in the **Scikit-Learn** library.\n",
    "\n",
    "To recap, we applied stratified logic such that each train and validation set has an **equal** weightage of positive and negative samples. We also grouped the patients in the process such that patient $i$ will not appear in both training and validation set.\n",
    "\n",
    "---\n",
    "\n",
    "> It is worth mentioning the famous Kaggler Chris Deotte went one step further to **Triple Stratify** the data where he balanced patient count distribution. One can read more [here](https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/165526).\n",
    "\n",
    "---\n",
    "\n",
    "### Cross-Validation Workflow\n",
    "\n",
    "To recap, we have the following:\n",
    "\n",
    "- **Training Set ($X_{\\text{train}}$)**: This will be further split into K validation sets during our cross-validation. This set is used to fit a particular hypothesis $h \\in \\mathcal{H}$.\n",
    "- **Validation Set ($X_{\\text{val}}$)**: This is split from our $X_{\\text{train}}$ during cross-validation. This set is used for model selection (i.e. find best hyperparameters, attempt to produce a best hypothesis $g \\in \\mathcal{H}$).\n",
    "- **Test Set ($X_{\\text{test}}$)**: This is an unseen test set, and we will only use it after we finish tuning our model/hypothesis. Suppose we have a final best model $g$, we will use $g$ to predict on the test set to get an estimate of the generalization error (also called out-of-sample error).\n",
    "\n",
    "<figure>\n",
    "<img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/aiap-coronary-artery-disease/data/images/cv.PNG' width=\"500\"/>\n",
    "<figcaption align = \"center\"><b>Pipeline.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "---\n",
    "\n",
    "<figure>\n",
    "<img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/supervised_learning/classification/breast-cancer-wisconsin/data/images/grid_search_workflow.png' width=\"500\"/>\n",
    "<figcaption align = \"center\"><b>Courtesy of scikit-learn on a typical Cross-Validation workflow.</b></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa407932-4f93-48d6-93b2-25ebcc2389d5",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Traditionally, training on **ImageNet** weights is a good choice to start. In the event that our training set has a very different distribution of what's inside **ImageNet**, the model may take a while to converge, even if we finetune it. The intuition is simple, **ImageNet** was trained on many common items in life, and none of them resemble closely to the image structures of **Melanoma Images**. Consequently, the model may have a hard time detecting shapes and details from these medical images.\n",
    "\n",
    "We can of course unfreeze all the layers and retrain them from scratch, using various backbones, however, due to limited hardware, we decided it is best to first check if **ImageNet** yields good results, if not, we can explore weights that were originally trained on skin cancer images. \n",
    "\n",
    "The community used a few models and found out that the **EfficientNet** variants yielded the best results on this set of training images using **ImageNet** and hence we adopt the **EfficientNet** family moving forward. Examining the Grad-CAM of the models revealed that this family of models not only focus on the center nucleus of the skin image but also corners, perhaps they capture something other models don't? We will compare them briefly later.\n",
    "\n",
    "### Fine-Tuning\n",
    "\n",
    "Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. This is what we will be doing. References below.\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df0815f-88f6-4467-b341-6e786d3fa9e9",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Most preprocessing techniques we do in an image recognition competition is mostly as follows:\n",
    "\n",
    "### Mean and Standard Deviation\n",
    "\n",
    "- Perform **mean and std** for the dataset given to us. Note that this step may make sense on paper, but empirically, using imagenet's default mean std will always work as well, if not better. You can read my [blog post here](https://reighns92.github.io/reighns-ml-blog/reighns_ml_journey/deep_learning/computer_vision/general/image_normalization/Image_Normalization_and_Standardization/)\"\n",
    "    - Imagenet on RGB: mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "### Channel Distribution\n",
    "\n",
    "This is usually done to check for \"surprises\". More specifically, I remembered once that someone trained a CNN on the blood cells dataset (red, white blood cells etc), as a beginner who just came out from MNIST, he/she grayscaled the images and yielded poor results. This is because one distinct way for the model to differentiate these cells might be because of the colors of the cells.\n",
    "\n",
    "### Let the Model tell you where went wrong!\n",
    "\n",
    "Alternatively, the issues are not obvious and we can use tools like Grad-CAM to see where our model is looking to deduce why the model is performing poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c50759-8330-469f-a98e-2062a2da8770",
   "metadata": {},
   "source": [
    "## Augmentations\n",
    "\n",
    "We know that augmentation is central in an image competition, as essentially we are adding more data into the training process, effectively reducing overfitting.\n",
    "\n",
    "Heavy augmentations are used during Train-Time-Augmentation. But during Test-Time-Augmentation, we used the same set of training augmentations to inference with $100\\%$ probability.\n",
    "\n",
    "### Train-Time Augmentation\n",
    "\n",
    "Community power. We made use of some innovative augmentations:\n",
    "\n",
    "- [AdvancedHairAugmentation](https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/159176) where hairs were randomly added to the image and\n",
    "- [Microscope](https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/159476) where images were made to look as if they were taken from a microscope.\n",
    "\n",
    "Both of these augmentations provided a steady increase in CV and LB.\n",
    "\n",
    "```python\n",
    "albumentations.Compose(\n",
    "    [\n",
    "        AdvancedHairAugmentation(\n",
    "            hairs_folder=pipeline_config.transforms.hairs_folder\n",
    "        ),\n",
    "        albumentations.RandomResizedCrop(\n",
    "            height=pipeline_config.transforms.image_size,\n",
    "            width=pipeline_config.transforms.image_size,\n",
    "            scale=(0.8, 1.0),\n",
    "            ratio=(0.75, 1.3333333333333333),\n",
    "            p=1.0,\n",
    "        ),\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.Cutout(\n",
    "            max_h_size=int(pipeline_config.transforms.image_size * 0.375),\n",
    "            max_w_size=int(pipeline_config.transforms.image_size * 0.375),\n",
    "            num_holes=1,\n",
    "            p=0.3,\n",
    "        ),\n",
    "        Microscope(p=0.5),\n",
    "        albumentations.Normalize(\n",
    "            mean=pipeline_config.transforms.mean,\n",
    "            std=pipeline_config.transforms.std,\n",
    "            max_pixel_value=255.0,\n",
    "            p=1.0,\n",
    "        ),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### Test-Time Augmentation\n",
    "\n",
    "The exact same set of augmentations were used in inference. Not all TTAs provided a increase in score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69ed3c5-e2fb-4541-b000-ce50dccdf0b1",
   "metadata": {},
   "source": [
    "## Optimizer, Scheduler and Loss\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "We used good old `AdamW` keeping in mind the rule of thumb that if batch size increase by a factor of 2, learning rate should increase by a factor of 2 as well.\n",
    "\n",
    "```python\n",
    "optimizer_name: str = \"AdamW\"\n",
    "optimizer_params: Dict[str, Any] = field(\n",
    "    default_factory=lambda: {\n",
    "        \"lr\": 1e-4,\n",
    "        \"betas\": (0.9, 0.999),\n",
    "        \"amsgrad\": False,\n",
    "        \"weight_decay\": 1e-6,\n",
    "        \"eps\": 1e-08,\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### Scheduler\n",
    "\n",
    "We used the following settings:\n",
    "\n",
    "```python\n",
    "scheduler_name: str = \"CosineAnnealingWarmRestarts\"  # Debug\n",
    "if scheduler_name == \"CosineAnnealingWarmRestarts\":\n",
    "    scheduler_params: Dict[str, Any] = field(\n",
    "        default_factory=lambda: {\n",
    "            \"T_0\": 10,\n",
    "            \"T_mult\": 1,\n",
    "            \"eta_min\": 1e-6,\n",
    "            \"last_epoch\": -1,\n",
    "        }\n",
    "    )\n",
    "```\n",
    "\n",
    "One should note that `OneCycleLR` is very popular and yields good results with shorter convergence time.\n",
    "\n",
    "### Loss\n",
    "\n",
    "We used `CrossEntropyLoss` loss with default parameters. Read more in my [blog post](https://reighns92.github.io/reighns-ml-blog/reighns_ml_journey/deep_learning/fundamentals/loss_functions/cross_entropy_loss/cross_entropy_loss_from_scratch/).\n",
    "\n",
    "```python\n",
    "train_criterion_name: str = \"CrossEntropyLoss\"\n",
    "train_criterion_params: Dict[str, Any] = field(\n",
    "    default_factory=lambda: {\n",
    "        \"weight\": None,\n",
    "        \"size_average\": None,\n",
    "        \"ignore_index\": -100,\n",
    "        \"reduce\": None,\n",
    "        \"reduction\": \"mean\",\n",
    "        \"label_smoothing\": 0.0,\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e92c25-f29d-4610-8d99-ea5e460221b2",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61503e3f-820d-4663-ae56-46163a142502",
   "metadata": {},
   "source": [
    "## Model Architectures, Training Parameters\n",
    "\n",
    "### No Meta Data Model Architecture\n",
    "\n",
    "For models that did not make use of meta data, we have the following architecture.\n",
    "\n",
    "<figure>\n",
    "<img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/SIIM-ISIC%20Melanoma%20Classification/images/no_meta_model_architecure.svg' width=\"800\"/>\n",
    "<figcaption align = \"center\"><b>No Meta Data Model Architecture.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "### Meta Data Model Architecture\n",
    "\n",
    "For models that did made use of meta data, we have the following architecture.\n",
    "\n",
    "<figure>\n",
    "<img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/SIIM-ISIC%20Melanoma%20Classification/images/meta_model_architecture.svg' width=\"800\"/>\n",
    "<figcaption align = \"center\"><b>Meta Data Model Architecture.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "We concat the flattened feature maps with the meta features: \n",
    "\n",
    "```python\n",
    "Meta Features: ['sex', 'age_approx', 'site_head/neck', 'site_lower extremity', 'site_oral/genital', 'site_palms/soles', 'site_torso', 'site_upper extremity', 'site_nan']\n",
    "```\n",
    "\n",
    "and the meta features has its own sequential layers as ANN:\n",
    "\n",
    "```python\n",
    "OrderedDict(\n",
    "    [\n",
    "        (\n",
    "            \"fc1\",\n",
    "            torch.nn.Linear(self.num_meta_features, 512),\n",
    "        ),\n",
    "        (\n",
    "            \"bn1\",\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "        ),\n",
    "        (\n",
    "            \"swish1\",\n",
    "            torch.nn.SiLU(),\n",
    "        ),\n",
    "        (\n",
    "            \"dropout1\",\n",
    "            torch.nn.Dropout(p=0.3),\n",
    "        ),\n",
    "        (\n",
    "            \"fc2\",\n",
    "            torch.nn.Linear(512, 128),\n",
    "        ),\n",
    "        (\n",
    "            \"bn2\",\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "        ),\n",
    "        (\n",
    "            \"swish2\",\n",
    "            torch.nn.SiLU(),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "- image shape: $[32, 3, 256, 256]$\n",
    "- meta_inputs shape: $[32, 9]$ we have 9 features.\n",
    "- feature_logits shape: $[32, 1280]$ flattened feature maps at the last conv layer.\n",
    "- meta_logits shape: $[32, 128]$ where we passed in a small sequential ANN for the meta data.\n",
    "- concat_logits shape: $[32, 1280 + 128]$\n",
    "\n",
    "```python\n",
    "if self.use_meta:\n",
    "    # from cnn images\n",
    "    feature_logits = self.extract_features(image)\n",
    "\n",
    "    # from meta features\n",
    "    meta_logits = self.meta_layer(meta_inputs)\n",
    "\n",
    "    # concatenate\n",
    "    concat_logits = torch.cat((feature_logits, meta_logits), dim=1)\n",
    "\n",
    "    # classifier head\n",
    "    classifier_logits = self.architecture[\"head\"](concat_logits)\n",
    "```\n",
    "\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "As we all know, activation functions are used to transform a neurons' linearity to non-linearity and decide whether to \"fire\" a neuron or not.\n",
    "\n",
    "When we design or choose an activation function, we need to ensure the follows:\n",
    "\n",
    "- (Smoothness) Differentiable and Continuous: For example, the sigmoid function is continuous and hence differentiable. If the property is not fulfilled, we might face issues as backpropagation may not be performed properly since we cannot differentiate it.If you notice, the heaviside function is not. We cant perform GD using the HF as we cannot compute gradients but for the logistic function we can. The gradient of sigmoid function g is g(1-g) conveniently\n",
    "\n",
    "- Monotonic: This helps the model to converge faster. But spoiler alert, Swish is not monotonic.\n",
    "\n",
    "The properties of Swish are as follows:\n",
    "\n",
    "- Bounded below: It is claimed in the paper it serves as a strong regularization.\n",
    "- Smoothness: More smooth than ReLU which allows the model to optimize better, the error landscape, when smoothed, is easier to traverse in order to find a minima. An intuitive idea is the hill again, imagine you traverse down your neighbourhood hill, vs traversing down Mount Himalaya.\n",
    "\n",
    "```python\n",
    "# Import matplotlib, numpy and math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def swish(x):\n",
    "    sigmoid =  1/(1 + np.exp(-x))\n",
    "    swish = x * sigmoid\n",
    "    return swish\n",
    "\n",
    "epsilon = 1e-20\n",
    "x = np.linspace(-100,100, 100)\n",
    "z = swish(x)\n",
    "print(z)\n",
    "print(min(z))\n",
    "\n",
    "plt.plot(x, z)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Swish(X)\")\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc82c5c-10d2-4d6b-8424-fdfd05fa5c20",
   "metadata": {},
   "source": [
    "## Ensemble Theory\n",
    "\n",
    "### Mean Blending\n",
    "\n",
    "This is just simple mean blending.\n",
    "\n",
    "### Forward Ensembling\n",
    "\n",
    "We made use of the [Forward Ensembling](https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/175614) idea from Chris in SIIM-ISIC Melanoma Classification back in August 2020, I modified the code for this specific task. A simple description is as follows, modified from Chris, with more mathematical notations.\n",
    "\n",
    "1. We start off with a dataset $\\mathcal{D} = X \\times y$ where it is sampled from the true population $\\mathcal{X} \\times \\mathcal{Y}$.\n",
    "2. We apply KFold (5 splits) to the dataset, as illustrated in the diagram. \n",
    "3. We can now train five different hypothesis $h_{F1}, h_{F2},...,h_{F5}$, where $h_{F1}$ is trained on Fold 2 to Fold 5 and predict on Fold 1, $h_{F2}$ is trained on Fold 1,3,4,5 and predict on Fold 2. The logic follows for all 5 hypothesis.\n",
    "4. Notice that in the five models, we are predicting on a unique validation fold, and as a result, after we trained all 5 folds, we will have the predictions made on the whole training set (F1-F5). This predictions is called the Out-of-Fold predictions.\n",
    "5. We then go a step further and calculate the AUC score with the OOF predictions with the ground truth to get the OOF AUC. We save it to a csv or dataframe called **oof_1.csv**, subsequent oof trained on different hypothesis space should be named **oof_i.csv** where $i \\in [2,3,...]$.\n",
    "6. After we trained all 5 folds, we will use $h_{1}$ to predict on $X_{test}$ and obtain predictions $Y_{\\text{h1 preds}}$, we then use $h_{2}$ to predict on $X_{test}$ and obtain predictions $Y_{\\text{h2 preds}}$, we do this for all five folds and finally $Y_{\\text{final preds}} = \\dfrac{1}{5}\\sum_{i=1}^{5}Y_{\\text{hi preds}}$. This is a typical pipeline in most machine learning problems. We save this final predictions as **sub_1.csv**, subsequence predictions trained on different hypothesis space should be named **sub_i.csv** where $i \\in [2,3,...]$.\n",
    "7. Now if we train another model, a completely different hypothesis space is used, to be more pedantic, we denote the previous model to be taken from the hypothesis space $\\mathcal{H}_{1}$, and now we move on to $\\mathcal{H}_{2}$. We repeat step 1-6 on this new model (Note that you are essentially training 10 \"models\" now since we are doing KFold twice, and oh, please set the seed of KFold to be the same, it should never be the case that both model comes from different splitting seed for apparent reasons).\n",
    "\n",
    "---\n",
    "\n",
    "Here is the key (given the above setup with 2 different models trained on 5 folds):\n",
    "\n",
    "1. Normally, most people do a simple mean ensemble, that is $\\dfrac{Y_{\\text{final preds H1}} + Y_{\\text{final preds H2}}}{2}$. This works well most of the time as we trust both model holds equal importance in the final predictions.\n",
    "2. One issue may be that certain models should be weighted more than the rest, we should not simply take Leaderboard feedback score to judge the weight assignment. A general heuristic here is called Forward Selection.\n",
    "3. (Extracted from Chris) Now say that you build 2 models (that means that you did 5 KFold twice). You now have oof_1.csv, oof_2.csv, sub_1.csv, and sub_2.csv. How do we blend the two models? We find the weight w such that `w * oof_1.predictions + (1-w) * oof_2.predictions` has the largest AUC.\n",
    "\n",
    "```python\n",
    "all = []\n",
    "for w in [0.00, 0.01, 0.02, ..., 0.98, 0.99, 1.00]:\n",
    "    ensemble_pred = w * oof_1.predictions + (1-w) * oof_2.predictions\n",
    "    ensemble_auc = roc_auc_score( oof.target , ensemble_pred )\n",
    "    all.append( ensemble_auc )\n",
    "best_weight = np.argmax( all ) / 100.\n",
    "```\n",
    "\n",
    "Then we can assign the best weight like:\n",
    "\n",
    "```python\n",
    "final_ensemble_pred = best_weight * sub_1.target + (1-best_weight) * sub_2.target\n",
    "```\n",
    "\n",
    "Read more from my blog post in references below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc87811-fa14-4c06-ac68-6ce0839febf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.6.3-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.6.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\reighns\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf1f4a58-e411-4667-95ca-f68baaffb4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Union\n",
    "import torchinfo\n",
    "import torch\n",
    "# Utility functions.\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path, PurePath\n",
    "from typing import Dict, Union, List\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb4b6a08-b3bc-49b9-8406-224e1448b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed: int = 1992) -> None:\n",
    "    \"\"\"Seed all random number generators.\"\"\"\n",
    "    print(f\"Using Seed Number {seed}\")\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(\n",
    "        seed\n",
    "    )  # set PYTHONHASHSEED env var at fixed value\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)  # pytorch (both CPU and CUDA)\n",
    "    np.random.seed(seed)  # for numpy pseudo-random generator\n",
    "    # set fixed value for python built-in pseudo-random generator\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d2c5e95-d13b-4a82-af7d-096a17f144a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Seed Number 1992\n"
     ]
    }
   ],
   "source": [
    "seed_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8254f5d2-108f-44aa-a02c-18818ada76f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelParams:\n",
    "    \"\"\"A class to track model parameters.\n",
    "\n",
    "    model_name (str): name of the model.\n",
    "    pretrained (bool): If True, use pretrained model.\n",
    "    input_channels (int): RGB image - 3 channels or Grayscale 1 channel\n",
    "    output_dimension (int): Final output neuron.\n",
    "                      It is the number of classes in classification.\n",
    "                      Caution: If you use sigmoid layer for Binary, then it is 1.\n",
    "    classification_type (str): classification type.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str = \"resnet50d\"  # resnet50d resnext50_32x4d \"tf_efficientnet_b0_ns\"  # Debug use tf_efficientnet_b0_ns else tf_efficientnet_b4_ns vgg16\n",
    "\n",
    "    pretrained: bool = True\n",
    "    input_channels: int = 3\n",
    "    output_dimension: int = 2\n",
    "    classification_type: str = \"multiclass\"\n",
    "    use_meta: bool = False\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary.\"\"\"\n",
    "        return asdict(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e847667-43c1-4d87-9e3b-aa5b689658bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PARAMS = ModelParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f08ce98-ac10-4224-ab22-af6122bf0f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNeuralNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = MODEL_PARAMS.model_name,\n",
    "        out_features: int = MODEL_PARAMS.output_dimension,\n",
    "        in_channels: int = MODEL_PARAMS.input_channels,\n",
    "        pretrained: bool = MODEL_PARAMS.pretrained,\n",
    "        use_meta: bool = MODEL_PARAMS.use_meta,\n",
    "    ):\n",
    "        \"\"\"Construct a new model.\n",
    "\n",
    "        Args:\n",
    "            model_name ([type], str): The name of the model to use. Defaults to MODEL_PARAMS.model_name.\n",
    "            out_features ([type], int): The number of output features, this is usually the number of classes, but if you use sigmoid, then the output is 1. Defaults to MODEL_PARAMS.output_dimension.\n",
    "            in_channels ([type], int): The number of input channels; RGB = 3, Grayscale = 1. Defaults to MODEL_PARAMS.input_channels.\n",
    "            pretrained ([type], bool): If True, use pretrained model. Defaults to MODEL_PARAMS.pretrained.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.pretrained = pretrained\n",
    "        self.use_meta = use_meta\n",
    "\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name, pretrained=self.pretrained, in_chans=self.in_channels\n",
    "        )\n",
    "\n",
    "        # removes head from backbone: # TODO: Global pool = \"avg\" vs \"\" behaves differently in shape, caution!\n",
    "        self.backbone.reset_classifier(num_classes=0, global_pool=\"avg\")\n",
    "\n",
    "        # get the last layer's number of features in backbone (feature map)\n",
    "        self.in_features = self.backbone.num_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Custom Head\n",
    "        self.single_head_fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.in_features, self.out_features),\n",
    "        )\n",
    "\n",
    "        self.architecture: Dict[str, Callable] = {\n",
    "            \"backbone\": self.backbone,\n",
    "            \"bottleneck\": None,\n",
    "            \"head\": self.single_head_fc,\n",
    "        }\n",
    "\n",
    "    def extract_features(self, image: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Extract the features mapping logits from the model.\n",
    "        This is the output from the backbone of a CNN.\n",
    "\n",
    "        Args:\n",
    "            image (torch.FloatTensor): The input image.\n",
    "\n",
    "        Returns:\n",
    "            feature_logits (torch.FloatTensor): The features logits.\n",
    "        \"\"\"\n",
    "        # TODO: To rename feature_logits to image embeddings, also find out what is image embedding.\n",
    "        feature_logits = self.architecture[\"backbone\"](image)\n",
    "        print(f\"feature logits shape = {feature_logits.shape}\")\n",
    "        return feature_logits\n",
    "\n",
    "    def forward(self, image: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"The forward call of the model.\n",
    "\n",
    "        Args:\n",
    "            image (torch.FloatTensor): The input image.\n",
    "\n",
    "        Returns:\n",
    "            classifier_logits (torch.FloatTensor): The output logits of the classifier head.\n",
    "        \"\"\"\n",
    "\n",
    "        feature_logits = self.extract_features(image)\n",
    "        classifier_logits = self.architecture[\"head\"](feature_logits)\n",
    "        print(f\"classifier_logits shape = {classifier_logits.shape}\")\n",
    "\n",
    "        return classifier_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5b37fd8-04ec-4733-815b-48d2e74e5a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7ceb040b-8d89-4336-b393-3a240c74ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, channel, height, width = 8, 3, 256, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9fb1e8b7-e76c-46f2-9892-227dd1c8b62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature logits shape = torch.Size([8, 2048])\n",
      "classifier_logits shape = torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn((batch_size, channel, height, width))\n",
    "y = model(image=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "19c05bb5-18f4-4f4a-9e64-09fc8427c8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2048])\n",
      "torch.Size([8, 2])\n",
      "==========================================================================================================================================================================\n",
      "Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
      "==========================================================================================================================================================================\n",
      "CustomNeuralNet                               --                        --                        --                        --                        --\n",
      "├─ResNet: 1-1                                 [8, 3, 256, 256]          [8, 2048]                 --                        --                        --\n",
      "│    └─Sequential: 2-1                        [8, 3, 256, 256]          [8, 64, 128, 128]         --                        --                        --\n",
      "│    │    └─Conv2d: 3-1                       [8, 3, 256, 256]          [8, 32, 128, 128]         864                       [3, 32, 3, 3]             113,246,208\n",
      "│    │    └─BatchNorm2d: 3-2                  [8, 32, 128, 128]         [8, 32, 128, 128]         64                        [32]                      512\n",
      "│    │    └─ReLU: 3-3                         [8, 32, 128, 128]         [8, 32, 128, 128]         --                        --                        --\n",
      "│    │    └─Conv2d: 3-4                       [8, 32, 128, 128]         [8, 32, 128, 128]         9,216                     [32, 32, 3, 3]            1,207,959,552\n",
      "│    │    └─BatchNorm2d: 3-5                  [8, 32, 128, 128]         [8, 32, 128, 128]         64                        [32]                      512\n",
      "│    │    └─ReLU: 3-6                         [8, 32, 128, 128]         [8, 32, 128, 128]         --                        --                        --\n",
      "│    │    └─Conv2d: 3-7                       [8, 32, 128, 128]         [8, 64, 128, 128]         18,432                    [32, 64, 3, 3]            2,415,919,104\n",
      "│    └─BatchNorm2d: 2-2                       [8, 64, 128, 128]         [8, 64, 128, 128]         128                       [64]                      1,024\n",
      "│    └─ReLU: 2-3                              [8, 64, 128, 128]         [8, 64, 128, 128]         --                        --                        --\n",
      "│    └─MaxPool2d: 2-4                         [8, 64, 128, 128]         [8, 64, 64, 64]           --                        --                        --\n",
      "│    └─Sequential: 2-5                        [8, 64, 64, 64]           [8, 256, 64, 64]          --                        --                        --\n",
      "│    │    └─Bottleneck: 3-8                   [8, 64, 64, 64]           [8, 256, 64, 64]          75,008                    --                        2,415,929,344\n",
      "│    │    └─Bottleneck: 3-9                   [8, 256, 64, 64]          [8, 256, 64, 64]          70,400                    --                        2,281,707,520\n",
      "│    │    └─Bottleneck: 3-10                  [8, 256, 64, 64]          [8, 256, 64, 64]          70,400                    --                        2,281,707,520\n",
      "│    └─Sequential: 2-6                        [8, 256, 64, 64]          [8, 512, 32, 32]          --                        --                        --\n",
      "│    │    └─Bottleneck: 3-11                  [8, 256, 64, 64]          [8, 512, 32, 32]          379,392                   --                        3,892,334,592\n",
      "│    │    └─Bottleneck: 3-12                  [8, 512, 32, 32]          [8, 512, 32, 32]          280,064                   --                        2,281,713,664\n",
      "│    │    └─Bottleneck: 3-13                  [8, 512, 32, 32]          [8, 512, 32, 32]          280,064                   --                        2,281,713,664\n",
      "│    │    └─Bottleneck: 3-14                  [8, 512, 32, 32]          [8, 512, 32, 32]          280,064                   --                        2,281,713,664\n",
      "│    └─Sequential: 2-7                        [8, 512, 32, 32]          [8, 1024, 16, 16]         --                        --                        --\n",
      "│    │    └─Bottleneck: 3-15                  [8, 512, 32, 32]          [8, 1024, 16, 16]         1,512,448                 --                        3,892,355,072\n",
      "│    │    └─Bottleneck: 3-16                  [8, 1024, 16, 16]         [8, 1024, 16, 16]         1,117,184                 --                        2,281,725,952\n",
      "│    │    └─Bottleneck: 3-17                  [8, 1024, 16, 16]         [8, 1024, 16, 16]         1,117,184                 --                        2,281,725,952\n",
      "│    │    └─Bottleneck: 3-18                  [8, 1024, 16, 16]         [8, 1024, 16, 16]         1,117,184                 --                        2,281,725,952\n",
      "│    │    └─Bottleneck: 3-19                  [8, 1024, 16, 16]         [8, 1024, 16, 16]         1,117,184                 --                        2,281,725,952\n",
      "│    │    └─Bottleneck: 3-20                  [8, 1024, 16, 16]         [8, 1024, 16, 16]         1,117,184                 --                        2,281,725,952\n",
      "│    └─Sequential: 2-8                        [8, 1024, 16, 16]         [8, 2048, 8, 8]           --                        --                        --\n",
      "│    │    └─Bottleneck: 3-21                  [8, 1024, 16, 16]         [8, 2048, 8, 8]           6,039,552                 --                        3,892,396,032\n",
      "│    │    └─Bottleneck: 3-22                  [8, 2048, 8, 8]           [8, 2048, 8, 8]           4,462,592                 --                        2,281,750,528\n",
      "│    │    └─Bottleneck: 3-23                  [8, 2048, 8, 8]           [8, 2048, 8, 8]           4,462,592                 --                        2,281,750,528\n",
      "│    └─SelectAdaptivePool2d: 2-9              [8, 2048, 8, 8]           [8, 2048]                 --                        --                        --\n",
      "│    │    └─AdaptiveAvgPool2d: 3-24           [8, 2048, 8, 8]           [8, 2048, 1, 1]           --                        --                        --\n",
      "│    │    └─Flatten: 3-25                     [8, 2048, 1, 1]           [8, 2048]                 --                        --                        --\n",
      "│    └─Identity: 2-10                         [8, 2048]                 [8, 2048]                 --                        --                        --\n",
      "├─Sequential: 1-2                             [8, 2048]                 [8, 2]                    --                        --                        --\n",
      "│    └─Linear: 2-11                           [8, 2048]                 [8, 2]                    4,098                     [2048, 2]                 32,784\n",
      "==========================================================================================================================================================================\n",
      "Total params: 23,531,362\n",
      "Trainable params: 23,531,362\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 45.21\n",
      "==========================================================================================================================================================================\n",
      "Input size (MB): 6.29\n",
      "Forward/backward pass size (MB): 1992.29\n",
      "Params size (MB): 94.13\n",
      "Estimated Total Size (MB): 2092.71\n",
      "==========================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "_ = torchinfo.summary(\n",
    "    model,\n",
    "    (batch_size, channel, height, width),\n",
    "    col_names=[\n",
    "        \"input_size\",\n",
    "        \"output_size\",\n",
    "        \"num_params\",\n",
    "        \"kernel_size\",\n",
    "        \"mult_adds\",\n",
    "    ],\n",
    "    depth=3,\n",
    "    verbose=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786de061-2065-4904-8675-a242aced2e23",
   "metadata": {},
   "source": [
    "This model architechure means that if I pass in a batch of $8$ images of size $(3, 256, 256)$, the model statistics will tell us a lot of information. Let us give some examples with a naive **ResNet50d**.\n",
    "\n",
    "- Input Shape: $[8, 3, 256, 256]$ passing through the first **Sequential Layer's Conv2d (3-1)** with kernel size of\n",
    "- Kernel Shape: $[3, 32, 3, 3]$ which means $[\\textbf{in_channels, out_channels, kernel_size, kernel_size}]$ will yield an output shape of\n",
    "- Output Shape: $[8, 32, 128, 128]$ indicating that the each input images are now transformed into 32 kernels of size 256 by 256. \n",
    "- Params: The **Params** column calculates the number of parameters in this layer at 864 learnable parameters.\n",
    "\n",
    "---\n",
    "\n",
    "Once we know how to interpret the table, we can also see that our `CustomNeuralnet()` has `extract_features` which outputs the input at the last convolutional layer, in this example, it is at **SelectAdaptivePool2d: 2-9** where it first went through **AdaptiveAvgPool2d: 3-24** to squash the feature maps to $[8, 2048, 1, 1]$ and subsequently a **Flatten: 3-25** layer to flatten out the last 2 dimensions to become $[8, 2048]$ so we can pass on to the dense layers.\n",
    "\n",
    "We can verify this by\n",
    "\n",
    "```python\n",
    "X = torch.randn((batch_size, channel, height, width))\n",
    "y = model(image=X)\n",
    "```\n",
    "\n",
    "yielding\n",
    "\n",
    "```python\n",
    "feature logits shape = torch.Size([8, 2048])\n",
    "classifier_logits shape = torch.Size([8, 2])\n",
    "```\n",
    "\n",
    "where the latter is the final shape of the input after passing through all the dense layers at $[8, 2]$, where one can envision it as 2 output neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2afad0-9894-493c-a7f0-252e6c604629",
   "metadata": {},
   "source": [
    "## Error Analysis using Grad-CAM\n",
    "\n",
    "There is some distinct difference when Grad-CAM is applied to different models, which can help us do error analysis.\n",
    "\n",
    "<figure>\n",
    "<img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/SIIM-ISIC%20Melanoma%20Classification/images/resnet50d_image.PNG' width=\"800\"/>\n",
    "<figcaption align = \"center\"><b>Grad-CAM of ResNet50d</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/SIIM-ISIC%20Melanoma%20Classification/images/tf_efficientnet_b1_ns_image.PNG' width=\"800\"/>\n",
    "<figcaption align = \"center\"><b>Grad-CAM of EfficietNet</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "For more info on [Grad-CAM](https://reighns92.github.io/reighns-ml-blog/reighns_ml_journey/deep_learning/computer_vision/general/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/), see my blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc924d0e-a457-4eb8-a054-7660af8be705",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- MLOps (Weights & Biases for experiment tracking)\n",
    "- Model Persistence\n",
    "- Benefit Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8a822f-a71f-4b9f-81aa-e5ade4462cb7",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Image Normalization](https://reighns92.github.io/reighns-ml-blog/reighns_ml_journey/deep_learning/computer_vision/general/image_normalization/Image_Normalization_and_Standardization/)\n",
    "- [Triple Stratified Leak-Free KFold CV](https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/165526)\n",
    "- [Transfer Learning PyTorch](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "- [Transfer Learning TensorFlow](https://www.tensorflow.org/tutorials/images/transfer_learning)\n",
    "- [Cross-Entropy Loss](https://reighns92.github.io/reighns-ml-blog/reighns_ml_journey/deep_learning/fundamentals/loss_functions/cross_entropy_loss/cross_entropy_loss_from_scratch/)\n",
    "- [Forward Ensemble](https://reighns92.github.io/reighns-ml-blog/reighns_ml_journey/deep_learning/ensemble_theory/forward_ensemble/)\n",
    "- [Forward Ensemble Discussion](https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/175614)\n",
    "- [Grad-CAM](https://reighns92.github.io/reighns-ml-blog/reighns_ml_journey/deep_learning/computer_vision/general/neural_network_interpretation/05_gradcam_and_variants/gradcam_explained/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
