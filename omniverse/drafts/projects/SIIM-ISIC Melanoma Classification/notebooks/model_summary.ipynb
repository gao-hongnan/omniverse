{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61503e3f-820d-4663-ae56-46163a142502",
   "metadata": {},
   "source": [
    "## Model Architectures, Training Parameters\n",
    "\n",
    "### No Meta Data Model Architecture\n",
    "\n",
    "For models that did not make use of meta data, we have the following architecture.\n",
    "\n",
    "<figure>\n",
    "<img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/SIIM-ISIC%20Melanoma%20Classification/images/no_meta_model_architecure.svg' width=\"800\"/>\n",
    "<figcaption align = \"center\"><b>No Meta Data Model Architecture.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "### Meta Data Model Architecture\n",
    "\n",
    "For models that did made use of meta data, we have the following architecture.\n",
    "\n",
    "<figure>\n",
    "<img src='https://storage.googleapis.com/reighns/reighns_ml_projects/docs/projects/SIIM-ISIC%20Melanoma%20Classification/images/meta_model_architecture.svg' width=\"800\"/>\n",
    "<figcaption align = \"center\"><b>Meta Data Model Architecture.</b></figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "We concat the flattened feature maps with the meta features: \n",
    "\n",
    "```python\n",
    "Meta Features: ['sex', 'age_approx', 'site_head/neck', 'site_lower extremity', 'site_oral/genital', 'site_palms/soles', 'site_torso', 'site_upper extremity', 'site_nan']\n",
    "```\n",
    "\n",
    "and the meta features has its own sequential layers as ANN:\n",
    "\n",
    "```python\n",
    "OrderedDict(\n",
    "    [\n",
    "        (\n",
    "            \"fc1\",\n",
    "            torch.nn.Linear(self.num_meta_features, 512),\n",
    "        ),\n",
    "        (\n",
    "            \"bn1\",\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "        ),\n",
    "        (\n",
    "            \"swish1\",\n",
    "            torch.nn.SiLU(),\n",
    "        ),\n",
    "        (\n",
    "            \"dropout1\",\n",
    "            torch.nn.Dropout(p=0.3),\n",
    "        ),\n",
    "        (\n",
    "            \"fc2\",\n",
    "            torch.nn.Linear(512, 128),\n",
    "        ),\n",
    "        (\n",
    "            \"bn2\",\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "        ),\n",
    "        (\n",
    "            \"swish2\",\n",
    "            torch.nn.SiLU(),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "- image shape: $[32, 3, 256, 256]$\n",
    "- meta_inputs shape: $[32, 9]$ we have 9 features.\n",
    "- feature_logits shape: $[32, 1280]$ flattened feature maps at the last conv layer.\n",
    "- meta_logits shape: $[32, 128]$ where we passed in a small sequential ANN for the meta data.\n",
    "- concat_logits shape: $[32, 1280 + 128]$\n",
    "\n",
    "```python\n",
    "if self.use_meta:\n",
    "    # from cnn images\n",
    "    feature_logits = self.extract_features(image)\n",
    "\n",
    "    # from meta features\n",
    "    meta_logits = self.meta_layer(meta_inputs)\n",
    "\n",
    "    # concatenate\n",
    "    concat_logits = torch.cat((feature_logits, meta_logits), dim=1)\n",
    "\n",
    "    # classifier head\n",
    "    classifier_logits = self.architecture[\"head\"](concat_logits)\n",
    "```\n",
    "\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "As we all know, activation functions are used to transform a neurons' linearity to non-linearity and decide whether to \"fire\" a neuron or not.\n",
    "\n",
    "When we design or choose an activation function, we need to ensure the follows:\n",
    "\n",
    "- (Smoothness) Differentiable and Continuous: For example, the sigmoid function is continuous and hence differentiable. If the property is not fulfilled, we might face issues as backpropagation may not be performed properly since we cannot differentiate it.If you notice, the heaviside function is not. We cant perform GD using the HF as we cannot compute gradients but for the logistic function we can. The gradient of sigmoid function g is g(1-g) conveniently\n",
    "\n",
    "- Monotonic: This helps the model to converge faster. But spoiler alert, Swish is not monotonic.\n",
    "\n",
    "The properties of Swish are as follows:\n",
    "\n",
    "- Bounded below: It is claimed in the paper it serves as a strong regularization.\n",
    "- Smoothness: More smooth than ReLU which allows the model to optimize better, the error landscape, when smoothed, is easier to traverse in order to find a minima. An intuitive idea is the hill again, imagine you traverse down your neighbourhood hill, vs traversing down Mount Himalaya.\n",
    "\n",
    "```python\n",
    "# Import matplotlib, numpy and math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def swish(x):\n",
    "    sigmoid =  1/(1 + np.exp(-x))\n",
    "    swish = x * sigmoid\n",
    "    return swish\n",
    "\n",
    "epsilon = 1e-20\n",
    "x = np.linspace(-100,100, 100)\n",
    "z = swish(x)\n",
    "print(z)\n",
    "print(min(z))\n",
    "\n",
    "plt.plot(x, z)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Swish(X)\")\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc87811-fa14-4c06-ac68-6ce0839febf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.6.3-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.6.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\reighns\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf1f4a58-e411-4667-95ca-f68baaffb4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Union\n",
    "import torchinfo\n",
    "import torch\n",
    "# Utility functions.\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path, PurePath\n",
    "from typing import Dict, Union, List\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb4b6a08-b3bc-49b9-8406-224e1448b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed: int = 1992) -> None:\n",
    "    \"\"\"Seed all random number generators.\"\"\"\n",
    "    print(f\"Using Seed Number {seed}\")\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(\n",
    "        seed\n",
    "    )  # set PYTHONHASHSEED env var at fixed value\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)  # pytorch (both CPU and CUDA)\n",
    "    np.random.seed(seed)  # for numpy pseudo-random generator\n",
    "    # set fixed value for python built-in pseudo-random generator\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d2c5e95-d13b-4a82-af7d-096a17f144a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Seed Number 1992\n"
     ]
    }
   ],
   "source": [
    "seed_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8254f5d2-108f-44aa-a02c-18818ada76f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelParams:\n",
    "    \"\"\"A class to track model parameters.\n",
    "\n",
    "    model_name (str): name of the model.\n",
    "    pretrained (bool): If True, use pretrained model.\n",
    "    input_channels (int): RGB image - 3 channels or Grayscale 1 channel\n",
    "    output_dimension (int): Final output neuron.\n",
    "                      It is the number of classes in classification.\n",
    "                      Caution: If you use sigmoid layer for Binary, then it is 1.\n",
    "    classification_type (str): classification type.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str = \"resnet50d\"  # resnet50d resnext50_32x4d \"tf_efficientnet_b0_ns\"  # Debug use tf_efficientnet_b0_ns else tf_efficientnet_b4_ns vgg16\n",
    "\n",
    "    pretrained: bool = True\n",
    "    input_channels: int = 3\n",
    "    output_dimension: int = 2\n",
    "    classification_type: str = \"multiclass\"\n",
    "    use_meta: bool = False\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert to dictionary.\"\"\"\n",
    "        return asdict(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e847667-43c1-4d87-9e3b-aa5b689658bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PARAMS = ModelParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f08ce98-ac10-4224-ab22-af6122bf0f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNeuralNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = MODEL_PARAMS.model_name,\n",
    "        out_features: int = MODEL_PARAMS.output_dimension,\n",
    "        in_channels: int = MODEL_PARAMS.input_channels,\n",
    "        pretrained: bool = MODEL_PARAMS.pretrained,\n",
    "        use_meta: bool = MODEL_PARAMS.use_meta,\n",
    "    ):\n",
    "        \"\"\"Construct a new model.\n",
    "\n",
    "        Args:\n",
    "            model_name ([type], str): The name of the model to use. Defaults to MODEL_PARAMS.model_name.\n",
    "            out_features ([type], int): The number of output features, this is usually the number of classes, but if you use sigmoid, then the output is 1. Defaults to MODEL_PARAMS.output_dimension.\n",
    "            in_channels ([type], int): The number of input channels; RGB = 3, Grayscale = 1. Defaults to MODEL_PARAMS.input_channels.\n",
    "            pretrained ([type], bool): If True, use pretrained model. Defaults to MODEL_PARAMS.pretrained.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.pretrained = pretrained\n",
    "        self.use_meta = use_meta\n",
    "\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name, pretrained=self.pretrained, in_chans=self.in_channels\n",
    "        )\n",
    "\n",
    "        # removes head from backbone: # TODO: Global pool = \"avg\" vs \"\" behaves differently in shape, caution!\n",
    "        self.backbone.reset_classifier(num_classes=0, global_pool=\"avg\")\n",
    "\n",
    "        # get the last layer's number of features in backbone (feature map)\n",
    "        self.in_features = self.backbone.num_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Custom Head\n",
    "        self.single_head_fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.in_features, self.out_features),\n",
    "        )\n",
    "\n",
    "        self.architecture: Dict[str, Callable] = {\n",
    "            \"backbone\": self.backbone,\n",
    "            \"bottleneck\": None,\n",
    "            \"head\": self.single_head_fc,\n",
    "        }\n",
    "\n",
    "    def extract_features(self, image: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Extract the features mapping logits from the model.\n",
    "        This is the output from the backbone of a CNN.\n",
    "\n",
    "        Args:\n",
    "            image (torch.FloatTensor): The input image.\n",
    "\n",
    "        Returns:\n",
    "            feature_logits (torch.FloatTensor): The features logits.\n",
    "        \"\"\"\n",
    "        # TODO: To rename feature_logits to image embeddings, also find out what is image embedding.\n",
    "        feature_logits = self.architecture[\"backbone\"](image)\n",
    "        print(f\"feature logits shape = {feature_logits.shape}\")\n",
    "        return feature_logits\n",
    "\n",
    "    def forward(self, image: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"The forward call of the model.\n",
    "\n",
    "        Args:\n",
    "            image (torch.FloatTensor): The input image.\n",
    "\n",
    "        Returns:\n",
    "            classifier_logits (torch.FloatTensor): The output logits of the classifier head.\n",
    "        \"\"\"\n",
    "\n",
    "        feature_logits = self.extract_features(image)\n",
    "        classifier_logits = self.architecture[\"head\"](feature_logits)\n",
    "        print(f\"classifier_logits shape = {classifier_logits.shape}\")\n",
    "\n",
    "        return classifier_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5b37fd8-04ec-4733-815b-48d2e74e5a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7ceb040b-8d89-4336-b393-3a240c74ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, channel, height, width = 8, 3, 256, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9fb1e8b7-e76c-46f2-9892-227dd1c8b62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature logits shape = torch.Size([8, 2048])\n",
      "classifier_logits shape = torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn((batch_size, channel, height, width))\n",
    "y = model(image=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "19c05bb5-18f4-4f4a-9e64-09fc8427c8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2048])\n",
      "torch.Size([8, 2])\n",
      "==========================================================================================================================================================================\n",
      "Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
      "==========================================================================================================================================================================\n",
      "CustomNeuralNet                               --                        --                        --                        --                        --\n",
      "├─ResNet: 1-1                                 [8, 3, 256, 256]          [8, 2048]                 --                        --                        --\n",
      "│    └─Sequential: 2-1                        [8, 3, 256, 256]          [8, 64, 128, 128]         --                        --                        --\n",
      "│    │    └─Conv2d: 3-1                       [8, 3, 256, 256]          [8, 32, 128, 128]         864                       [3, 32, 3, 3]             113,246,208\n",
      "│    │    └─BatchNorm2d: 3-2                  [8, 32, 128, 128]         [8, 32, 128, 128]         64                        [32]                      512\n",
      "│    │    └─ReLU: 3-3                         [8, 32, 128, 128]         [8, 32, 128, 128]         --                        --                        --\n",
      "│    │    └─Conv2d: 3-4                       [8, 32, 128, 128]         [8, 32, 128, 128]         9,216                     [32, 32, 3, 3]            1,207,959,552\n",
      "│    │    └─BatchNorm2d: 3-5                  [8, 32, 128, 128]         [8, 32, 128, 128]         64                        [32]                      512\n",
      "│    │    └─ReLU: 3-6                         [8, 32, 128, 128]         [8, 32, 128, 128]         --                        --                        --\n",
      "│    │    └─Conv2d: 3-7                       [8, 32, 128, 128]         [8, 64, 128, 128]         18,432                    [32, 64, 3, 3]            2,415,919,104\n",
      "│    └─BatchNorm2d: 2-2                       [8, 64, 128, 128]         [8, 64, 128, 128]         128                       [64]                      1,024\n",
      "│    └─ReLU: 2-3                              [8, 64, 128, 128]         [8, 64, 128, 128]         --                        --                        --\n",
      "│    └─MaxPool2d: 2-4                         [8, 64, 128, 128]         [8, 64, 64, 64]           --                        --                        --\n",
      "│    └─Sequential: 2-5                        [8, 64, 64, 64]           [8, 256, 64, 64]          --                        --                        --\n",
      "│    │    └─Bottleneck: 3-8                   [8, 64, 64, 64]           [8, 256, 64, 64]          75,008                    --                        2,415,929,344\n",
      "│    │    └─Bottleneck: 3-9                   [8, 256, 64, 64]          [8, 256, 64, 64]          70,400                    --                        2,281,707,520\n",
      "│    │    └─Bottleneck: 3-10                  [8, 256, 64, 64]          [8, 256, 64, 64]          70,400                    --                        2,281,707,520\n",
      "│    └─Sequential: 2-6                        [8, 256, 64, 64]          [8, 512, 32, 32]          --                        --                        --\n",
      "│    │    └─Bottleneck: 3-11                  [8, 256, 64, 64]          [8, 512, 32, 32]          379,392                   --                        3,892,334,592\n",
      "│    │    └─Bottleneck: 3-12                  [8, 512, 32, 32]          [8, 512, 32, 32]          280,064                   --                        2,281,713,664\n",
      "│    │    └─Bottleneck: 3-13                  [8, 512, 32, 32]          [8, 512, 32, 32]          280,064                   --                        2,281,713,664\n",
      "│    │    └─Bottleneck: 3-14                  [8, 512, 32, 32]          [8, 512, 32, 32]          280,064                   --                        2,281,713,664\n",
      "│    └─Sequential: 2-7                        [8, 512, 32, 32]          [8, 1024, 16, 16]         --                        --                        --\n",
      "│    │    └─Bottleneck: 3-15                  [8, 512, 32, 32]          [8, 1024, 16, 16]         1,512,448                 --                        3,892,355,072\n",
      "│    │    └─Bottleneck: 3-16                  [8, 1024, 16, 16]         [8, 1024, 16, 16]         1,117,184                 --                        2,281,725,952\n",
      "│    │    └─Bottleneck: 3-17                  [8, 1024, 16, 16]         [8, 1024, 16, 16]         1,117,184                 --                        2,281,725,952\n",
      "│    │    └─Bottleneck: 3-18                  [8, 1024, 16, 16]         [8, 1024, 16, 16]         1,117,184                 --                        2,281,725,952\n",
      "│    │    └─Bottleneck: 3-19                  [8, 1024, 16, 16]         [8, 1024, 16, 16]         1,117,184                 --                        2,281,725,952\n",
      "│    │    └─Bottleneck: 3-20                  [8, 1024, 16, 16]         [8, 1024, 16, 16]         1,117,184                 --                        2,281,725,952\n",
      "│    └─Sequential: 2-8                        [8, 1024, 16, 16]         [8, 2048, 8, 8]           --                        --                        --\n",
      "│    │    └─Bottleneck: 3-21                  [8, 1024, 16, 16]         [8, 2048, 8, 8]           6,039,552                 --                        3,892,396,032\n",
      "│    │    └─Bottleneck: 3-22                  [8, 2048, 8, 8]           [8, 2048, 8, 8]           4,462,592                 --                        2,281,750,528\n",
      "│    │    └─Bottleneck: 3-23                  [8, 2048, 8, 8]           [8, 2048, 8, 8]           4,462,592                 --                        2,281,750,528\n",
      "│    └─SelectAdaptivePool2d: 2-9              [8, 2048, 8, 8]           [8, 2048]                 --                        --                        --\n",
      "│    │    └─AdaptiveAvgPool2d: 3-24           [8, 2048, 8, 8]           [8, 2048, 1, 1]           --                        --                        --\n",
      "│    │    └─Flatten: 3-25                     [8, 2048, 1, 1]           [8, 2048]                 --                        --                        --\n",
      "│    └─Identity: 2-10                         [8, 2048]                 [8, 2048]                 --                        --                        --\n",
      "├─Sequential: 1-2                             [8, 2048]                 [8, 2]                    --                        --                        --\n",
      "│    └─Linear: 2-11                           [8, 2048]                 [8, 2]                    4,098                     [2048, 2]                 32,784\n",
      "==========================================================================================================================================================================\n",
      "Total params: 23,531,362\n",
      "Trainable params: 23,531,362\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 45.21\n",
      "==========================================================================================================================================================================\n",
      "Input size (MB): 6.29\n",
      "Forward/backward pass size (MB): 1992.29\n",
      "Params size (MB): 94.13\n",
      "Estimated Total Size (MB): 2092.71\n",
      "==========================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "_ = torchinfo.summary(\n",
    "    model,\n",
    "    (batch_size, channel, height, width),\n",
    "    col_names=[\n",
    "        \"input_size\",\n",
    "        \"output_size\",\n",
    "        \"num_params\",\n",
    "        \"kernel_size\",\n",
    "        \"mult_adds\",\n",
    "    ],\n",
    "    depth=3,\n",
    "    verbose=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786de061-2065-4904-8675-a242aced2e23",
   "metadata": {},
   "source": [
    "This model architechure means that if I pass in a batch of $8$ images of size $(3, 256, 256)$, the model statistics will tell us a lot of information. Let us give some examples with a naive **ResNet50d**.\n",
    "\n",
    "- Input Shape: $[8, 3, 256, 256]$ passing through the first **Sequential Layer's Conv2d (3-1)** with kernel size of\n",
    "- Kernel Shape: $[3, 32, 3, 3]$ which means $[\\textbf{in_channels, out_channels, kernel_size, kernel_size}]$ will yield an output shape of\n",
    "- Output Shape: $[8, 32, 128, 128]$ indicating that the each input images are now transformed into 32 kernels of size 256 by 256. \n",
    "- Params: The **Params** column calculates the number of parameters in this layer at 864 learnable parameters.\n",
    "\n",
    "---\n",
    "\n",
    "Once we know how to interpret the table, we can also see that our `CustomNeuralnet()` has `extract_features` which outputs the input at the last convolutional layer, in this example, it is at **SelectAdaptivePool2d: 2-9** where it first went through **AdaptiveAvgPool2d: 3-24** to squash the feature maps to $[8, 2048, 1, 1]$ and subsequently a **Flatten: 3-25** layer to flatten out the last 2 dimensions to become $[8, 2048]$ so we can pass on to the dense layers.\n",
    "\n",
    "We can verify this by\n",
    "\n",
    "```python\n",
    "X = torch.randn((batch_size, channel, height, width))\n",
    "y = model(image=X)\n",
    "```\n",
    "\n",
    "yielding\n",
    "\n",
    "```python\n",
    "feature logits shape = torch.Size([8, 2048])\n",
    "classifier_logits shape = torch.Size([8, 2])\n",
    "```\n",
    "\n",
    "where the latter is the final shape of the input after passing through all the dense layers at $[8, 2]$, where one can envision it as 2 output neurons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
