{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "suFMtUKLaX5Q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "from PIL import Image, ImageFile\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2y85rM4h2j6P"
   },
   "outputs": [],
   "source": [
    "def seed_all(seed: int = 1930):\n",
    "    \"\"\"Seed all random number generators.\"\"\"\n",
    "    print(\"Using Seed Number {}\".format(seed))\n",
    "    # set PYTHONHASHSEED env var at fixed value\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)  # pytorch (both CPU and CUDA)\n",
    "    np.random.seed(seed)  # for numpy pseudo-random generator\n",
    "    random.seed(seed)  # set fixed value for python built-in pseudo-random generator\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5C7AM8w1Pbwl",
    "outputId": "c5e57cd3-17cb-4e30-f76e-d851681f5ce2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Seed Number 1930\n"
     ]
    }
   ],
   "source": [
    "seed_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyIIsBk3afdF"
   },
   "source": [
    "## Disclaimer\n",
    "\n",
    "Note that the following method is not the **most efficient way**, but it is good for learning as the steps in the codes are laid out **sequentially** so that it is easy to follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxZnc-PKTmCu"
   },
   "source": [
    "!!! info\n",
    "    There are a few pre-preprocessing techniques for image data. Here we discuss the most common one that I encounter, Normalization across channels.\n",
    "\n",
    "    [1^]: Extracted from [CS231n](https://cs231n.github.io/neural-networks-2/#datapre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lugUJ-nwu8ql"
   },
   "source": [
    "!!! warning\n",
    "    Data leakage will occur if you apply this pre-processing step prior to your train-valid-test split. We should apply normalization on the training step, obtaining the mean and std metrics for $X_{\\text{train}}$ and apply them to validation set during model selection, and to test set during model evaluation.\n",
    "    In our examples below, I apply mean and std calculation on the training set (which includes the validation set), in reality, we should further split the training set into training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4R1Ek3ZMmodc"
   },
   "source": [
    "## General Steps to Normalize\n",
    "\n",
    "!!! warning \"Important Warning\"\n",
    "    **Important: Most of the times we resize the images, so different image size may result in different mean and std. So remember to resize first then calcuate.**\n",
    "\n",
    "1. RGB image with 3 channels. We assume it is channels first, if not convert from channels last to channels first. As I am using PyTorch primarily, this is more natural to me. See CIFAR-10 for such example.\n",
    "\n",
    "    - Load the data into disk using either `cv2` or `PIL`. Divide by 255 across all images first to normalize it.\n",
    "    - Then find the image's mean and std per channel. \n",
    "        - For example, if we want to find the mean of the red channel of a batch of images, and assume we have 10 images of size $(100, 100, 3)$ each. Then each image has 3 channels, each channel has $100 \\times 100$ pixels, and therefore 10 such images will have $10 \\times 100 \\times 100 = 100000$ pixels. We `flatten()` all these 10 images' red channel and take the average (i.e. sum all $100000$ red pixels, and divide by $1000000$). We do the same for all the other channels.\n",
    "\n",
    "2. Grayscale image with 1 channel.\n",
    "    - This is just average the values in one channel.\n",
    "\n",
    "3. Audio/Spectrograms like SETI etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV8whFQCW97U"
   },
   "source": [
    "## CIFAR-10 (RGB)\n",
    "\n",
    "We first see an example of calculating the mean and standard deviation of cifar10, which is of RGB channels.\n",
    "\n",
    "```python\n",
    "Mean: {\"R\": 0.49139968 \"G\": 0.48215827 \"B\": 0.44653124}\n",
    "Standard Deviation: {\"R\": 0.24703233 \"G\": 0.24348505 \"B\": 0.26158768}\n",
    "```\n",
    "\n",
    "We will code a function to calculate the mean and standard deviation of a batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pfphQ4VzQu1T"
   },
   "outputs": [],
   "source": [
    "TRANSFORMS = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aB8R7YLakjNV",
    "outputId": "e1c6fe93-a8d4-4d74-d698-264afe353bd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset_cifar10 = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=TRANSFORMS\n",
    ")\n",
    "testset_cifar10 = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=TRANSFORMS\n",
    ")\n",
    "train_images_cifar10 = np.asarray(trainset_cifar10.data)  # (50000, 32, 32, 3)\n",
    "test_images_cifar10 = np.asarray(testset_cifar10.data)  # (10000, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_dnRL7JGX35u"
   },
   "outputs": [],
   "source": [
    "def calcMeanStd(images: np.ndarray) -> Dict[str, Tuple[float]]:\n",
    "    \"\"\"Take in an numpy array of images and returns mean and std per channel.\n",
    "    This function assumes for a start, your array is loaded into disk.\n",
    "\n",
    "    Args:\n",
    "        images (np.ndarray): [num_images, channel, height, width] or [num_images, height, width, channel]\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Tuple[float]]: {\"mean\": (mean_r, mean_g, mean_b), \"std\": (std_r, std_g, std_b)}\n",
    "    \"\"\"\n",
    "    \n",
    "    images = np.asarray(images) # good way to test if images is passed in the correct dtype\n",
    "    images = images / 255.      # min-max and divide by 255\n",
    "\n",
    "    if images.ndim == 4:                            # RGB\n",
    "        if images.shape[1] != 3:                    # if channel is not first, make it so, assume channels last\n",
    "            images = images.transpose(0, 3, 1, 2)   # if tensor use permute instead\n",
    "                                                    # permutation applies the following mapping\n",
    "                                                    # axis0 -> axis0\n",
    "                                                    # axis1 -> axis3\n",
    "                                                    # axis2 -> axis1\n",
    "                                                    # axis3 -> axis2\n",
    "        \n",
    "        b, c, w, h = images.shape\n",
    "\n",
    "        r_channel, g_channel, b_channel = images[:, 0, :, :], images[:, 1, :, :], images[:, 2, :, :]      # get rgb channels individually\n",
    "        r_channel, g_channel, b_channel = r_channel.flatten(), g_channel.flatten(), b_channel.flatten()   # flatten each channel into one array\n",
    "        mean_r = r_channel.mean(axis=None) # since we are averaging per channel, we get the first channel's mean by r_channel.mean\n",
    "        mean_g = g_channel.mean(axis=None) # same as above\n",
    "        mean_b = b_channel.mean(axis=None) # same as above\n",
    "        \n",
    "        # calculate std over each channel (r,g,b)\n",
    "        std_r = r_channel.std(axis=None)\n",
    "        std_g = g_channel.std(axis=None)\n",
    "        std_b = b_channel.std(axis=None)\n",
    "\n",
    "        return {'mean': (mean_r, mean_g, mean_b), 'std': (std_r, std_g, std_b)}\n",
    "    \n",
    "    elif images.ndim == 3:              # grayscale\n",
    "        gray_channel = images.flatten() # flatten directly since only 1 channel\n",
    "        mean = gray_channel.mean(axis=None)\n",
    "        std = gray_channel.std(axis=None)\n",
    "        \n",
    "        return {\"mean\": (mean,), \"std\": (std, )}\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"passed error is not of the right shape!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PoopXKkTaypR",
    "outputId": "6cb19d93-c546-4ad0-bda1-c825a713e086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': (0.49139967861519745, 0.4821584083946076, 0.44653091444546616), 'std': (0.2470322324632823, 0.24348512800005553, 0.2615878417279641)}\n"
     ]
    }
   ],
   "source": [
    "mean_std_cifar = calcMeanStd(train_images_cifar10)\n",
    "print(mean_std_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NbQvvm7GzbPW",
    "outputId": "866b1e87-5549-43a6-c7ee-8b53162a087d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "[0.49139968 0.48215841 0.44653091]\n",
      "[0.24703223 0.24348513 0.26158784]\n"
     ]
    }
   ],
   "source": [
    "# alternate way to do this.\n",
    "print(trainset_cifar10.data.shape)\n",
    "print(trainset_cifar10.data.mean(axis=(0,1,2))/255)\n",
    "print(trainset_cifar10.data.std(axis=(0,1,2))/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pBuK9hXRGc3"
   },
   "source": [
    "Depending on your use case, we can normalize the test/validation set with the parameters found on the train set, though in practice, for image recognition problems, we use the same normalization parameters on both the train and validation set, and apply it to test set.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "1. Calculate the mean and std using the method above.\n",
    "2. Divide the training/validation/test set by 255.\n",
    "3. Normalize it using the values found. \n",
    "\n",
    "Note step 2 can be skipped **if the normalization method in the library does a division of 255 internally.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Qrz0S7x6bPKw"
   },
   "outputs": [],
   "source": [
    "TRANSFORMS_with_normalization = transforms.Compose(\n",
    "    [\n",
    "        transforms.Normalize(\n",
    "            mean=mean_std_cifar[\"mean\"], std=mean_std_cifar[\"std\"]\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8lpAWBQa_rp",
    "outputId": "8f93cbc8-da00-4829-f918-c3031c7804a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset_cifar10 = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=TRANSFORMS_with_normalization\n",
    ")\n",
    "testset_cifar10 = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=TRANSFORMS_with_normalization\n",
    ")\n",
    "train_images_cifar10 = np.asarray(trainset_cifar10.data)  # (50000, 32, 32, 3)\n",
    "test_images_cifar10 = np.asarray(testset_cifar10.data)  # (10000, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvmP8l3Jz5i4"
   },
   "source": [
    "## MNIST (Grayscale)\n",
    "\n",
    "We next see an example of calculating the mean and standard deviation of MNIST, which is of one channel (grayscale).\n",
    "\n",
    "```python\n",
    "Mean: 0.1307\n",
    "Standard Deviation: 0.3081\n",
    "```\n",
    "\n",
    "We will code a function to calculate the mean and standard deviation of a batch of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ztva7BK6c6Yv"
   },
   "outputs": [],
   "source": [
    "# mnist\n",
    "trainset_mnist = datasets.MNIST(\n",
    "    root=\"./data/\", train=True, download=True, transform=TRANSFORMS\n",
    ")\n",
    "testset_mnist = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=TRANSFORMS\n",
    ")\n",
    "train_images_mnist = np.asarray(trainset_mnist.data)  # (60000, 28, 28)\n",
    "test_images_mnist = np.asarray(testset_mnist.data)  # (10000, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXF8NbqVcQI0",
    "outputId": "24c075f2-15b4-4816-bc11-a00b17aa83f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean': (0.1306604762738429,), 'std': (0.3081078038564622,)}\n"
     ]
    }
   ],
   "source": [
    "mean_std_mnist = calcMeanStd(train_images_mnist)\n",
    "print(mean_std_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EM-FX2WKb4zx",
    "outputId": "5a46b801-c5b1-4f98-dc96-58e55153b16e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1307)\n",
      "tensor(0.3081)\n"
     ]
    }
   ],
   "source": [
    "print(trainset_mnist.data.float().mean() / 255)\n",
    "print(trainset_mnist.data.float().std() / 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-K1LaZHbZ3x"
   },
   "source": [
    "## References\n",
    "\n",
    "To read up more on how others do it **efficiently**, please have a read below.\n",
    "\n",
    "- https://www.kaggle.com/kozodoi/seti-mean-and-std-of-new-data/notebook\n",
    "- https://www.kaggle.com/kozodoi/computing-dataset-mean-and-std\n",
    "- https://forums.fast.ai/t/calculating-our-own-image-stats-imagenet-stats-cifar-stats-etc/40355/3\n",
    "- https://github.com/JoshVarty/CancerDetection/blob/master/01_ImageStats.ipynb\n",
    "- https://forums.fast.ai/t/calculating-new-stats/31214\n",
    "- https://forums.fast.ai/t/calcuating-the-mean-and-standard-deviation-for-normalize/62883/13\n",
    "- https://www.kaggle.com/c/ranzcr-clip-catheter-line-classification/discussion/211039\n",
    "- https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2\n",
    "- https://stackoverflow.com/questions/65699020/calculate-standard-deviation-for-grayscale-imagenet-pixel-values-with-rotation-m/65717887#65717887\n",
    "- https://stackoverflow.com/questions/66678052/how-to-calculate-the-mean-and-the-std-of-cifar10-data\n",
    "- https://drive.google.com/drive/u/1/folders/1Gum3vsRsKKRSFZ1hyKaPTiVs1AUAmdKD\n",
    "- https://stackoverflow.com/questions/50710493/cifar-10-meaningless-normalization-values\n",
    "- https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/6\n",
    "- https://github.com/kuangliu/pytorch-cifar/issues/19\n",
    "- https://github.com/Armour/pytorch-nn-practice/blob/master/utils/meanstd.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Image Normalization and Standardization.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
