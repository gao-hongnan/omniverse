{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ae642c-598b-460a-a85d-da9842cf6c14",
   "metadata": {},
   "source": [
    "## Interpretating what Convolutional Neural Networks Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d01a6e-e827-4b59-9ee0-fe1e01d3b277",
   "metadata": {},
   "source": [
    "### On the Importance of Model Interpretability in Healthcare\n",
    "\n",
    "When we started out on our Machine Learning Journey, most of us have seen authors describe the conundrum of the **trade off between prediction performance and a model (hypothesis)'s interpretability**[^islr]. The general consensus, though not always correct, is that **more complex** models will have **high performance measures but low interpretability** and **less complex** models, the opposite.\n",
    "\n",
    "One might ask, when should one consider the usage of a **complex** or a **simpler** one? Here is a rule of thumb for you to follow.\n",
    "\n",
    "- If the only interest is in **prediction**, for example, one company seeks to develop an algorithm that predicts the **crypto price**, then it is likely that the performance measure should be maximized over interpretability.\n",
    "- On the other hand, if our interest is in **inference**[^inference], for example in a **healthcare** setting where our medical use case is to predict whether a patient has **melanoma (skin cancer)**, then **interpretability** is more important that **high performance**. This is because the **models** are really not the only one making the decision, healthcare practitioners will need to understand the cause of the decision made by the model. In short, doctors wanted to know and agree on which particular area of the **skin cell** propels the model to make the decision that it is cancerous or not.\n",
    "- One more point, can help to do error analysis on where the model did wrong. For example, a beginner trained a model to classify red and white blood cells, started from MNIST, he naturally greyed out the blood cells and received poor results. Doing some error analysis to interpret what went wrong easily tells us that by greying out the cells, the model get confused because they looked very similar, one important feature (i.e. color) is missing.\n",
    "\n",
    "For more understanding in this section, the book[^interpretable_ml_book] Interpretable Machine Learning by Christoph Molnar is a good read.\n",
    "\n",
    "[^islr]: An Introduction to Statistical Learning, James, G., Witten, D., Hastie, T., & Tibshirani, R. pp.24-25\n",
    "[^inference]: Note that the difference between inference and prediction is that inference can be thought of as one step beyond prediction, where we are not only concerned with the outputs of our model but also want to be able to extract a meaningful relationship between our input features and our predictions. Inference may be understood as prediction in the ML community, but not in the statistics community.\n",
    "\n",
    "[^interpretable_ml_book]: https://christophm.github.io/interpretable-ml-book/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0513c402-5cfd-4d18-9192-9409e7c60278",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Computer Vision and Convolutional Neural Networks\n",
    "\n",
    "The computer vision has seen a bloom in the recent decade, empowering many use cases, including but not limited to the **healthcare, automobile and facial recoginition industry**. Most computer vision problems uses a type of neural networks **Convolutional Neural Networks (CNN)**, though recent breakthroughs show more promising results in **vision transformers**[^vision_transformers]. We will however, focus more on **CNN** in this section. For the longest time, CNNs are regarded as **black box model** simply because it is very **complex and hence difficult to interpret**. However, in recent times, as the need to **interpret models** grows, there are quite some methods to give a glimpse of what your CNN is looking at.\n",
    "\n",
    "[^vision_transformers]: https://en.wikipedia.org/wiki/Vision_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3262351e-644d-4ef0-818e-962d2e176acc",
   "metadata": {},
   "source": [
    "### CNN is a black box?\n",
    "\n",
    "Neural Networks are well defined mathematically, so why we cannot use the same way we do to say Logistic Regression to interpret the model? Well ... the feature importance is not so easy to decode as neural networks in computer vision are interpreted pixel level and it is not logical to derive feature importance at a pixel level. We need something at a spatial level.\n",
    "\n",
    "For example, we plot a grayscale image of a number 3 taken from MNIST by pixel level (i.e. $28 \\times 28$), then it is not immediately obvious how we should recover the \"importance\" of any single pixel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d416b8-9518-4305-8931-421b0e1c3697",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/gradcam_and_variants/pixel_level_mnist.PNG\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>MNIST pixel level plot; By Hongnan G.</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935c5c64-adfd-4e27-9a2b-fd8e69eb577e",
   "metadata": {},
   "source": [
    "### High level understanding of CNN\n",
    "\n",
    "The below is referenced from the [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/).\n",
    "\n",
    "We start off with the question, why cannot we use a normal model say, SVM or Logistic Regression to predict a cat image? Can't we just flatten the pixels and feed it as input to the models?\n",
    "\n",
    "- Flattened pixels lose **spatial level information**, it only encodes information sequentially and high level features are not captured. Intuition is that a cat is a cat because of its eyes and ears (example) but when flattened these features may not be clustered together. We may need insane **Feature Engineering** to make classical ML models work (i.e. if you can re-construct the features of a cat's eyes using the flattened pixels then you may be successful).\n",
    "- The power of CNN is that it learns high-level spatial features such as **colors, edges and patterns** that is unique to the image. This is enabled by the number of **hidden layers** that did many transformations to the inputs. So in a sense, the hidden layers of CNN are **implicitly performing feature engineering**.\n",
    "\n",
    "> Let us detail a high level outline of a \"life cycle of a CNN\".\n",
    "\n",
    "1. The input (image) is usually of size $(C, H, W)$ is fed into the CNN. Note we **do not flatten the image**.\n",
    "2. In these **CNN layers**, the network first learns **simple features** such as the cat's edges and shapes, then as it progress to later layers, it learns **highly abstract features** such as more **complex textures and patterns**.\n",
    "3. After propagating to the last layer of the **Convolutional Layer**, we will then use a type of pooling and flatten the learned features and connect it to fully connected layers and predict the classes.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/gradcam_and_variants/cat_cnn_black_box.jpg\" width=\"300\" height=\"300\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Cat and CNN; By Hongnan G.</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d533c5-2a6e-445f-986a-12a183daa2ac",
   "metadata": {},
   "source": [
    "## CNN Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b03cc0-5dcb-480c-b2be-4dec9455184e",
   "metadata": {},
   "source": [
    "### Feature Visualization Through Convolutional Layers\n",
    "\n",
    "As mentioned in the previous section on the high level overview of CNN, an immediate solution is to ask if we can visualize what each layer's output is showing. \n",
    "\n",
    "This method is useful for understanding:\n",
    "\n",
    "- How successive CNN layers transform their inputs. \n",
    "- What each combination of filters does, good for having an intuition of whether the filter detects edges, shapes or more.\n",
    "- Visualizing these filters can give us an understanding of the visual pattern that the CNN is capturing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad7206b-58c8-4e91-892a-977aa97d6c82",
   "metadata": {},
   "source": [
    "#### Visualization \n",
    "\n",
    "Here is a visual of the conv layers' outputs.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/gradcam_and_variants/vgg16_conv_layers_visualization.PNG\" width=\"800\" height=\"800\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>VGG16 Conv layers on a Cat; By Hongnan G.</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2effe-70ea-47ef-be95-23b9a81277f3",
   "metadata": {},
   "source": [
    "#### Takeaways\n",
    "\n",
    "Our takeaways are:\n",
    "\n",
    "- The first few layers act as a collection of various **edge and shape** detectors and the activations retain almost all of the information from the original input image.\n",
    "- As you go deeper, the layers begin to encode high and abstract features, the features become less \"informative and obvious\" and more \"generic\" (more on this intuition later).\n",
    "-  The sparsity of the activations increases with the depth of the layer: in the first layer, almost all filters are activated by the input image, but in the following layers, more and more filters are blank. This means the pattern encoded by the filter isn't found in the input image.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a141ae3-0f2a-4353-9c52-f0e86f4e50aa",
   "metadata": {},
   "source": [
    "#### Intuition\n",
    "\n",
    "As Francis Chollet mentioned, a deep CNN acts as an *information distillation pipeline*, but what is it? Why is it that as you go deeper, the image of a cat looks **less precise and less like a real cat**? The analogy I use (similar to hise) gives you intuition:\n",
    "\n",
    "> Imagine you were tasked to recognize a cat, you will do so instantly. Now you are also tasked to draw a cat, surely you can do so if you can recognize it? You started to draw a cat and compare it with a real cat. You realize that we cannot really remember the specific details of a cat but we can draw the abstract (generic) cat. The neural networks is like our brain, **where we manage to recognize an image by filtering out the irrelevant details and transform it to high-level abstract features**. This is what happens to a CNN and this is ideal! We do not want the model to remember too specific details of an image in fear of it not being able to generalize.\n",
    "\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/gradcam_and_variants/cat_hn.jpg\" width=\"500\" height=\"600\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>A cat image from me</b>\n",
    "</p>\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/gradcam_and_variants/cats_unsplash.jpg\" width=\"500\" height=\"600\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>A cat image from Unsplash</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cbc7a1-eb96-434c-8046-ace879bd9775",
   "metadata": {},
   "source": [
    "#### Readings\n",
    "\n",
    "Readings over this section:\n",
    "\n",
    "- Fran√ßois Chollet: Deep Learning With Python pp.262-267\n",
    "- Christoph Molnar: Interpretable Machine Learning section. 10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71b1e8-5601-4636-ab1f-73ee37def8b8",
   "metadata": {},
   "source": [
    "### Gradcam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f88a0f-7bc8-401c-ab5e-ee9467e7e5a1",
   "metadata": {},
   "source": [
    "- Interpret decision by determining which feature in our inputs had the highest contribution. If model predicted a cat, is it the eyes, ears or body shape that defined the class?\n",
    "- Gradients can be very useful in this. Intuition is that gradients measure the effect on the outputs caused by some inputs. \n",
    "    - i.e. $y = f(x) = 2x$, then every change of 1 unit of change in $x$ causes $2$ units of change in our output $y$. The gradient is 2 here and measures the **rate of change of $y$ with respect to $x$**.\n",
    "    - The same analogy applies here, can we find the pixels $x$ in the image that contributes to the target $y$? In practice however, we often denote a loss function $\\mathcal{L}(\\hat{f(\\mathbf{x})}, \\mathbf{y})$ to minimize it and we can compute the gradient of this loss function with respect to the inputs. That is to say, if our loss $\\mathcal{L}$ is low, then our model is doing something right and we can examine the gradients of the loss with respect to $x$ to find out more.\n",
    "- If we are interested in the cat class, we focus only on the $y_{cat}$.\n",
    "    - Feature maps of a cat: $A_1, A_2, ..., A_k$, each contributes in making the final decision in what $\\hat{y}$ is.\n",
    "    - Computing gradient of $y_{cat}$ with respect to $A_k$ will give us the rate of change of the feature maps with respect to the target.\n",
    "    - For each feature map, we compute the gradients. For example if we have 8 32 by 32 feature maps, we compute gradients for all $8 x 32 x 32$ pixels and average them channel wise. \n",
    "    - Now we have a single 32 by 32 feature map with all the gradient info, we then apply `ReLU` to it, where we set negative values to 0 because we are only interested in the class cat. i.e. negative values in gradient does not mean non-importance, it only pulls the prediction to the other classes.\n",
    "    - Now we have a 32 by 32 feature map with only positive values at certain areas, we need to map it back to the original image. For example if the original image of the cat is 320 x 320, then we need to scale it back to overlay back to the original image. \n",
    "    - The overlayed image will show a heatmap on where the gradients are non-negative, highlighting areas of focus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8658593b-0823-40e1-9a14-6752afe8f290",
   "metadata": {},
   "source": [
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/gradcam_and_variants/gradcam_1.jpg\" width=\"500\" height=\"600\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Gradcam 1</b>\n",
    "</p>\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/reighns/reighns_ml_projects/docs/deep_learning/computer_vision/gradcam_and_variants/gradcam_2.jpg\" width=\"500\" height=\"600\" style=\"margin-left:auto; margin-right:auto\"/>\n",
    "<p style=\"text-align: center\">\n",
    "    <b>Gradcam 2</b>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39765891-460b-4d16-8341-6bfa5ed808ed",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://christophm.github.io/interpretable-ml-book/\n",
    "- https://distill.pub/2017/feature-visualization/\n",
    "- Deep Learning with Python by Francois Chollet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
