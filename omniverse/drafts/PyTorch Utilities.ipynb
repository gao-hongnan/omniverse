{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c7e7e9-33fc-49d1-b65a-a98cc993707e",
   "metadata": {},
   "source": [
    "## Imports, Config and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e062f3e6-d7f2-4651-b7d2-181200b00873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torchvision\n",
    "from typing import Dict, Union, Callable, OrderedDict, Tuple\n",
    "import os, random\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59d99e81-3309-4367-8774-738dead6065d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Seed Number 1992\n"
     ]
    }
   ],
   "source": [
    "def seed_all(seed: int = 1992) -> None:\n",
    "    \"\"\"Seed all random number generators.\"\"\"\n",
    "    print(f\"Using Seed Number {seed}\")\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)  # set PYTHONHASHSEED env var at fixed value\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)  # pytorch (both CPU and CUDA)\n",
    "    np.random.seed(seed)  # for numpy pseudo-random generator\n",
    "    # set fixed value for python built-in pseudo-random generator\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "\n",
    "def seed_worker(_worker_id) -> None:\n",
    "    \"\"\"Seed a worker with the given ID.\"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2 ** 32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "seed_all(seed=1992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d8059e-bc09-45be-948b-71d873b0f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45707da4-17ed-43c7-a975-d38c45482ff7",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e11c5-2db5-417e-a7c9-41ee4d039a1a",
   "metadata": {},
   "source": [
    "### Terminologies\n",
    "\n",
    "- Kernel\n",
    "- Filter\n",
    "- Receptive Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbf14f42-e8d8-40f5-a7c1-034cf637ecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6c4f503-ce8a-4cbd-af17-e9be31796a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_conv2d(x: torch.Tensor, kernel: torch.Tensor) -> torch.Tensor:\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "    input_height, input_width = x.shape\n",
    "\n",
    "    feature_map_height, feature_map_width = (\n",
    "        input_height - kernel_height + 1,\n",
    "        input_width - kernel_width + 1,\n",
    "    )\n",
    "\n",
    "    feature_map = torch.zeros(size=(feature_map_height, feature_map_width))\n",
    "\n",
    "    for height_index in range(feature_map_height):\n",
    "        for width_index in range(feature_map_width):\n",
    "            # 1st iter: height_index = 0, width_index = 0\n",
    "            # 2nd iter: height_index = 0, width_index = 1\n",
    "            receptive_field = x[\n",
    "                height_index : height_index + kernel_height,\n",
    "                width_index : width_index + kernel_width,\n",
    "            ]\n",
    "            feature_map[height_index, width_index] = (\n",
    "                receptive_field * kernel\n",
    "            ).sum()\n",
    "    \n",
    "    return feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4838fa73-07df-4a94-befb-351ff2ea79bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_conv2d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "684069f9-4dbe-42f9-b6d2-7c12249025fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConv2D(nn.Module):\n",
    "    def __init__(self, kernel_size: Tuple[int, int]):\n",
    "        super().__init__()\n",
    "        self.kernel = nn.Parameter(\n",
    "            torch.rand(size=(kernel_size[0], kernel_size[1]))\n",
    "        )\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return my_conv2d(x, self.kernel) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8e169b0-351f-4c1f-8065-d410c7a21b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.3671, 10.2563],\n",
       "        [16.0346, 18.9238]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = MyConv2D((2, 2))\n",
    "conv2d(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5a882-5c71-4d00-a8a9-627df9fa1a90",
   "metadata": {},
   "source": [
    "### CNN Conv2d Layer Output Dimensions Calculation\n",
    "\n",
    "Given:\n",
    "\n",
    "- n: Input image's height/width\n",
    "- f: Filter/Kernel Size\n",
    "- p: Padding Size\n",
    "- s: Stride\n",
    "\n",
    "Given an image of size $n \\times n$ and a kernel of $f \\times f$, our output shape is \n",
    "\n",
    "$$\n",
    "o = n - f + 1\n",
    "$$\n",
    "\n",
    "and if we pad the image with 1 extra layer outside, that means our input image is of size $(n + 2*1) \\times (n + 2*1)$, if you see an image you will be clear why adding one layer around means input width and height add 2 times the padding $p$.\n",
    "\n",
    "hence our new output shape is:\n",
    "\n",
    "$$\n",
    "o = (n+2) - f + 1\n",
    "$$\n",
    "\n",
    "and a generic formula for padding equals $p$ will yield\n",
    "\n",
    "$$\n",
    "o = (n + 2p) - f + 1\n",
    "$$\n",
    "\n",
    "Note that:\n",
    "\n",
    "- If $p=0$, then this is **valid padding** where the output shape is $n - f + 1$;\n",
    "- If $p=\\frac{f-1}{2}$, then this is **same padding** where the output shape is equals to the original input shape $n$.\n",
    "\n",
    "Same padding can be deduced by setting\n",
    "\n",
    "$$\n",
    "(n+2p) - f + 1 = n \\implies p = \\frac{f-1}{2}\n",
    "$$\n",
    "\n",
    "where by construction $f$ must be odd in order to get a whole number for the padding $p$, and that's why most kernels/filters are of odd shape.\n",
    "\n",
    "With stride into action our final shape is:\n",
    "\n",
    "$$\n",
    "o = \\dfrac{n - f + 2p}{s} + 1\n",
    "$$\n",
    "\n",
    "where sometimes $o$ is applied by $\\text{floor}(o)$ if the $o$ is non-integer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ef0fdd-749b-40ef-bb8f-6ee3d59c1eb9",
   "metadata": {},
   "source": [
    "### CNN Pooling Layer Output Dimensions Calculation\n",
    "\n",
    "For general pooling operations:\n",
    "\n",
    "Given:\n",
    "\n",
    "- n: Input image's height/width\n",
    "- f: Filter/Kernel Size\n",
    "- s: Stride\n",
    "\n",
    "Given an image of size $n \\times n$ and a kernel of $f \\times f$, our output shape after pooling is:\n",
    "\n",
    "$$\n",
    "o = \\text{floor}\\left(\\dfrac{n - f}{s} + 1\\right)\n",
    "$$\n",
    "\n",
    "with floor applied to $o$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b8f543-3271-4b16-b13b-4c18313a8740",
   "metadata": {},
   "source": [
    "## LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4d51910-9677-4373-b2ad-79de0da9e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "lenet = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2, stride=1),\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, padding=0, stride=1),\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 5 * 5, 120),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12dd96-c69a-41d2-b0dc-06bcec5aaf85",
   "metadata": {},
   "source": [
    "- X: (1, 1, 28, 28)\n",
    "- 1st conv2d layer: a conv2d layer that applies a filter containing 6 kernels, where each kernel is of size $5 \\times 5$ with a padding of 2 and stride of 1\n",
    "    - n = 28\n",
    "    - f = 5\n",
    "    - p = 2 ; **Note in particular the padding of 2 is derived from the formula $p = (f-1)/2 = 4/2=2$ to get same padding!**\n",
    "    - s = 1\n",
    "    - shape: $o = \\frac{28-5+4}{1} + 1 = 28$\n",
    "    - The final output shape is (1, 6, 28, 28) where\n",
    "        - 1 is the batch size\n",
    "        - 6 is the number of kernels applied, for each kernel our output shape is 28 by 28\n",
    "        - 28, 28 is the output shape by the kernels\n",
    "        \n",
    "- 1st avgpool2d layer: \n",
    "    - n = 28\n",
    "    - f = 2\n",
    "    - s = 2\n",
    "    - o = 14\n",
    "    - The final output shape is (1, 6, 14, 14) where the kernels are halved in size.\n",
    " \n",
    "- 2nd conv2d layer:\n",
    "    - n = 14\n",
    "    - f = 5\n",
    "    - p = 0\n",
    "    - s = 1\n",
    "    - o = 10\n",
    "    - The final output shape is (1, 16, 10, 10)\n",
    "    \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1dbd0db5-b34d-428b-996c-a7c23204027f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape: \t torch.Size([1, 6, 28, 28])\n",
      "Sigmoid output shape: \t torch.Size([1, 6, 28, 28])\n",
      "AvgPool2d output shape: \t torch.Size([1, 6, 14, 14])\n",
      "Conv2d output shape: \t torch.Size([1, 16, 10, 10])\n",
      "Sigmoid output shape: \t torch.Size([1, 16, 10, 10])\n",
      "AvgPool2d output shape: \t torch.Size([1, 16, 5, 5])\n",
      "Flatten output shape: \t torch.Size([1, 400])\n",
      "Linear output shape: \t torch.Size([1, 120])\n",
      "Sigmoid output shape: \t torch.Size([1, 120])\n",
      "Linear output shape: \t torch.Size([1, 84])\n",
      "Sigmoid output shape: \t torch.Size([1, 84])\n",
      "Linear output shape: \t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n",
    "\n",
    "for layer in lenet:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, \"output shape: \\t\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e2d49a9-eead-4939-8637-74fb448f64c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) layer weight shape: \t torch.Size([6, 1, 5, 5])\n",
      "Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) layer weight shape: \t torch.Size([16, 6, 5, 5])\n",
      "Linear(in_features=400, out_features=120, bias=True) layer weight shape: \t torch.Size([120, 400])\n",
      "Linear(in_features=120, out_features=84, bias=True) layer weight shape: \t torch.Size([84, 120])\n",
      "Linear(in_features=84, out_features=10, bias=True) layer weight shape: \t torch.Size([10, 84])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n",
    "for layer in lenet:\n",
    "    X = layer(X)\n",
    "    if hasattr(layer, \"weight\"):\n",
    "        print(layer, \"layer weight shape: \\t\", layer.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afb829e-6403-4596-a98a-d88793e30f90",
   "metadata": {},
   "source": [
    "## Toy Models\n",
    "\n",
    "I created two versions of the same model. The `Sequential` method has a more compact form, but often is more difficult to extract layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1cb064cc-1f9d-4385-82a5-ecc443ed77a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cl1 = torch.nn.Linear(25, 60)\n",
    "        self.cl2 = torch.nn.Linear(60, 16)\n",
    "        self.fc1 = torch.nn.Linear(16, 120)\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x ([type]): [description]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        x = torch.nn.ReLU()(self.cl1(x))\n",
    "        x = torch.nn.ReLU()(self.cl2(x))\n",
    "        x = torch.nn.ReLU()(self.fc1(x))\n",
    "        x = torch.nn.ReLU()(self.fc2(x))\n",
    "        x = torch.nn.LogSoftmax(dim=1)(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ToySequentialModel(torch.nn.Module):\n",
    "    # Create a sequential model pytorch same as ToyModel.\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = torch.nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"cl1\", torch.nn.Linear(25, 60)),\n",
    "                    (\"cl_relu1\", torch.nn.ReLU()),\n",
    "                    (\"cl2\", torch.nn.Linear(60, 16)),\n",
    "                    (\"cl_relu2\", torch.nn.ReLU()),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.head = torch.nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"fc1\", torch.nn.Linear(16, 120)),\n",
    "                    (\"fc_relu_1\", torch.nn.ReLU()),\n",
    "                    (\"fc2\", torch.nn.Linear(120, 84)),\n",
    "                    (\"fc_relu_2\", torch.nn.ReLU()),\n",
    "                    (\"fc3\", torch.nn.Linear(84, 10)),\n",
    "                    (\"fc_log_softmax\", torch.nn.LogSoftmax(dim=1)),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x ([type]): [description]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f8d3c-ecb8-4c8b-a7da-0f86746594aa",
   "metadata": {},
   "source": [
    "## Named Modules\n",
    "\n",
    "Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6a5903f3-d204-4282-bb94-96b785bd8f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "backbone\n",
      "backbone.cl1\n",
      "backbone.cl_relu1\n",
      "backbone.cl2\n",
      "backbone.cl_relu2\n",
      "head\n",
      "head.fc1\n",
      "head.fc_relu_1\n",
      "head.fc2\n",
      "head.fc_relu_2\n",
      "head.fc3\n",
      "head.fc_log_softmax\n"
     ]
    }
   ],
   "source": [
    "for name, layer in ToySequentialModel().named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d216aa-d95d-4723-b95f-23c88c1937dc",
   "metadata": {},
   "source": [
    "## Get Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1409cbb4-11c6-4836-a7de-ab5e81a56e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_layers(\n",
    "    model: Callable, layer_type: str = \"Conv2d\"\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"Create a function that give me the convolutional layers of PyTorch model.\n",
    "\n",
    "    This function is created to be used in conjunction with Visualization of Feature Maps.\n",
    "\n",
    "    Args:\n",
    "        model (Union[torchvision.models, timm.models]): A PyTorch model.\n",
    "        layer_type (str): The type of layer to be extracted.\n",
    "\n",
    "    Returns:\n",
    "        conv_layers (Dict[str, str]): {\"layer1.0.conv1\": layer1.0.conv1, ...}\n",
    "\n",
    "    Example:\n",
    "        >>> resnet18_pretrained_true = timm.create_model(model_name = \"resnet34\", pretrained=True, num_classes=10).to(DEVICE)\n",
    "        >>> conv_layers = get_conv_layers(resnet18_pretrained_true, layer_type=\"Conv2d\")\n",
    "    \"\"\"\n",
    "\n",
    "    if layer_type == \"Conv2d\":\n",
    "        _layer_type = torch.nn.Conv2d\n",
    "    elif layer_type == \"Conv1d\":\n",
    "        _layer_type = torch.nn.Conv1d\n",
    "\n",
    "    conv_layers = {}\n",
    "    for name, layer in model.named_modules():\n",
    "        if isinstance(layer, _layer_type):\n",
    "            conv_layers[name] = name\n",
    "    return conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1625ef1-bf40-4be4-ae09-0ad0ba9cef77",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_pretrained_true = timm.create_model(model_name = \"resnet34\", pretrained=True, num_classes=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c67d29cc-f40f-4991-98b9-21d62e4af708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conv1': 'conv1', 'layer1.0.conv1': 'layer1.0.conv1', 'layer1.0.conv2': 'layer1.0.conv2', 'layer1.1.conv1': 'layer1.1.conv1', 'layer1.1.conv2': 'layer1.1.conv2', 'layer1.2.conv1': 'layer1.2.conv1', 'layer1.2.conv2': 'layer1.2.conv2', 'layer2.0.conv1': 'layer2.0.conv1', 'layer2.0.conv2': 'layer2.0.conv2', 'layer2.0.downsample.0': 'layer2.0.downsample.0', 'layer2.1.conv1': 'layer2.1.conv1', 'layer2.1.conv2': 'layer2.1.conv2', 'layer2.2.conv1': 'layer2.2.conv1', 'layer2.2.conv2': 'layer2.2.conv2', 'layer2.3.conv1': 'layer2.3.conv1', 'layer2.3.conv2': 'layer2.3.conv2', 'layer3.0.conv1': 'layer3.0.conv1', 'layer3.0.conv2': 'layer3.0.conv2', 'layer3.0.downsample.0': 'layer3.0.downsample.0', 'layer3.1.conv1': 'layer3.1.conv1', 'layer3.1.conv2': 'layer3.1.conv2', 'layer3.2.conv1': 'layer3.2.conv1', 'layer3.2.conv2': 'layer3.2.conv2', 'layer3.3.conv1': 'layer3.3.conv1', 'layer3.3.conv2': 'layer3.3.conv2', 'layer3.4.conv1': 'layer3.4.conv1', 'layer3.4.conv2': 'layer3.4.conv2', 'layer3.5.conv1': 'layer3.5.conv1', 'layer3.5.conv2': 'layer3.5.conv2', 'layer4.0.conv1': 'layer4.0.conv1', 'layer4.0.conv2': 'layer4.0.conv2', 'layer4.0.downsample.0': 'layer4.0.downsample.0', 'layer4.1.conv1': 'layer4.1.conv1', 'layer4.1.conv2': 'layer4.1.conv2', 'layer4.2.conv1': 'layer4.2.conv1', 'layer4.2.conv2': 'layer4.2.conv2'}\n"
     ]
    }
   ],
   "source": [
    ">>> resnet18_pretrained_true = timm.create_model(model_name = \"resnet34\", pretrained=True, num_classes=10).to(DEVICE)\n",
    ">>> conv_layers = get_conv_layers(resnet18_pretrained_true, layer_type=\"Conv2d\")\n",
    ">>> print(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4d207b8b-3baa-4f56-b33a-bf514822b4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'head.fc2': tensor([[ 0.0697,  0.0544, -0.0157, -0.1059, -0.0464, -0.0090,  0.0532, -0.1273,\n",
      "         -0.0286, -0.0151,  0.0963,  0.2205,  0.0745, -0.0110, -0.1127, -0.0367,\n",
      "         -0.0681,  0.0463, -0.0833,  0.1288,  0.1058,  0.0976, -0.0251,  0.0980,\n",
      "         -0.0110,  0.1170, -0.0650,  0.2091, -0.1773,  0.0363, -0.1452,  0.0036,\n",
      "          0.0112, -0.0304, -0.0620, -0.0658, -0.0543,  0.0072,  0.0436,  0.0703,\n",
      "          0.0254, -0.0614,  0.0164, -0.1003, -0.0396,  0.0349,  0.0089, -0.1243,\n",
      "         -0.1037, -0.0491,  0.0627, -0.1347,  0.0010, -0.1290, -0.0280, -0.0344,\n",
      "          0.1487, -0.1764, -0.0233,  0.0082,  0.1270,  0.0368,  0.0103, -0.0929,\n",
      "          0.0038,  0.1346, -0.0688, -0.0437, -0.1205, -0.1596, -0.0240, -0.1001,\n",
      "         -0.0300, -0.1119,  0.0344, -0.1587,  0.0329, -0.0424,  0.0999,  0.0732,\n",
      "          0.1116,  0.0220, -0.0570,  0.0232]]), 'head.fc3': tensor([[ 0.0256, -0.0924,  0.0456,  0.0972,  0.0107,  0.0527,  0.0208,  0.0373,\n",
      "          0.0451,  0.0712]])}\n"
     ]
    }
   ],
   "source": [
    "activation = {}\n",
    "\n",
    "def get_intermediate_features(name: str) -> Callable:\n",
    "    \"\"\"Get the intermediate features of a model. Forward Hook.\n",
    "\n",
    "    This is using forward hook with reference https://discuss.pytorch.org/t/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator/17254/5\n",
    "\n",
    "    Args:\n",
    "        name (str): name of the layer.\n",
    "\n",
    "    Returns:\n",
    "        Callable: [description]\n",
    "    \"\"\"\n",
    "\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "\n",
    "    return hook\n",
    "\n",
    "\n",
    "# The below is testing the forward hook functionalities, especially getting intermediate features.\n",
    "# Note that both models are same organically but created differently.\n",
    "# Due to seeding issues, you can check whether they are the same output or not by running them separately.\n",
    "# We also used assertion to check that the output from model(x) is same as torch.nn.LogSoftmax(dim=1)(fc3_output)\n",
    "\n",
    "use_sequential_model = True\n",
    "x = torch.randn(1, 25)\n",
    "\n",
    "if not use_sequential_model:\n",
    "\n",
    "    model = ToyModel()\n",
    "\n",
    "    model.fc2.register_forward_hook(get_intermediate_features(\"fc2\"))\n",
    "    model.fc3.register_forward_hook(get_intermediate_features(\"fc3\"))\n",
    "    output = model(x)\n",
    "    print(activation)\n",
    "    fc2_output = activation[\"fc2\"]\n",
    "    fc3_output = activation[\"fc3\"]\n",
    "    # assert output and logsoftmax fc3_output are the same\n",
    "    assert torch.allclose(output, torch.nn.LogSoftmax(dim=1)(fc3_output))\n",
    "else:\n",
    "    sequential_model = ToySequentialModel()\n",
    "\n",
    "    # Do this if you want all, if not you can see below.\n",
    "    # for name, layer in sequential_model.named_modules():\n",
    "    #     layer.register_forward_hook(get_intermediate_features(name))\n",
    "    sequential_model.head.fc2.register_forward_hook(\n",
    "        get_intermediate_features(\"head.fc2\")\n",
    "    )\n",
    "    sequential_model.head.fc3.register_forward_hook(\n",
    "        get_intermediate_features(\"head.fc3\")\n",
    "    )\n",
    "    sequential_model_output = sequential_model(x)\n",
    "    print(activation)\n",
    "    fc2_output = activation[\"head.fc2\"]\n",
    "    fc3_output = activation[\"head.fc3\"]\n",
    "    assert torch.allclose(\n",
    "        sequential_model_output, torch.nn.LogSoftmax(dim=1)(fc3_output)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1b0b1a-6a15-4680-9a22-273735b543f3",
   "metadata": {},
   "source": [
    "## How to freeze layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8968a23a-6550-413f-b630-4cde691ea76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet18_pretrained_true = timm.create_model(model_name = \"resnet34\", pretrained=True, num_classes=10).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e6b2484-07d2-4354-a74a-361970bed956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.]) tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "norm = torch.nn.InstanceNorm2d(num_features=3, track_running_stats=True)\n",
    "print(norm.running_mean, norm.running_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17512f21-59fe-492d-8e7e-6b5207ad6210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3414e-03, -4.7338e-05,  1.1239e-03]) tensor([1.0010, 0.9984, 0.9989])\n",
      "tensor([-2.5486e-03, -8.9943e-05,  2.1355e-03]) tensor([1.0018, 0.9969, 0.9979])\n",
      "tensor([-0.0036, -0.0001,  0.0030]) tensor([1.0026, 0.9956, 0.9970])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 24, 24)\n",
    "\n",
    "out = norm(x)\n",
    "print(norm.running_mean, norm.running_var)\n",
    "\n",
    "out = norm(x)\n",
    "print(norm.running_mean, norm.running_var)\n",
    "\n",
    "\n",
    "out = norm(x)\n",
    "print(norm.running_mean, norm.running_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1243ae5b-aca8-47da-8317-07ecbeaece19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0160, -0.0018,  0.0068]) tensor([1.0002, 1.0082, 0.9904])\n"
     ]
    }
   ],
   "source": [
    "norm.eval()\n",
    "out = norm(x)\n",
    "print(norm.running_mean, norm.running_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "506e482a-e9bf-4b9f-88ff-ce5e3a3b5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_batchnorm_layers(model: Callable) -> None:\n",
    "    \"\"\"Freeze the batchnorm layers of a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (CustomNeuralNet): model to be frozen.\n",
    "\n",
    "    Example:\n",
    "        >>> model = timm.create_model(\"efficientnet_b0\", pretrained=True)\n",
    "        >>> model.apply(freeze_batchnorm_layers) # to freeze during training\n",
    "    \"\"\"\n",
    "    # https://discuss.pytorch.org/t/how-to-freeze-bn-layers-while-training-the-rest-of-network-mean-and-var-wont-freeze/89736/19\n",
    "    # https://discuss.pytorch.org/t/should-i-use-model-eval-when-i-freeze-batchnorm-layers-to-finetune/39495/3\n",
    "    classname = model.__class__.__name__\n",
    "\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, torch.nn.InstanceNorm2d):\n",
    "            module.eval()\n",
    "        if isinstance(module, torch.nn.BatchNorm2d):\n",
    "            \n",
    "            if hasattr(module, \"weight\"):\n",
    "                module.weight.requires_grad_(False)\n",
    "            if hasattr(module, \"bias\"):\n",
    "                module.bias.requires_grad_(False)\n",
    "            module.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1f6b80c7-8fc8-4a7a-ad7f-2dede93299f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.apply(freeze_batchnorm_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fea61953-2161-468e-b90b-4e2d47d6f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ac287313-7786-4c8a-952d-e77934b7ec2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0036, -0.0001,  0.0030]), tensor([1.0026, 0.9956, 0.9970]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.running_mean, norm.running_var"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
