# Reading List

-   [Building Production-Ready RAG Applications: Jerry Liu](https://www.youtube.com/watch?v=TRjq7t2Ms5I)
    -   31st January 2024
-   [Attention is all you need (Transformer) - Model explanation (including math), Inference and Training](https://www.youtube.com/watch?v=bCz4OMemCcA&t=51s)
    -   30th January 2024

## Broad

-   https://aman.ai/primers/ai/
    -   Goldmine of resources

## Training Stability

-   https://karpathy.github.io/2019/04/25/recipe/
-   https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial3/Activation_Functions.html
-   https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial4/Optimization_and_Initialization.html

### Performance

-   https://pytorch-lightning.readthedocs.io/en/0.10.0/performance.html

### Distributed

-   https://github.com/hkproj/pytorch-transformer-distributed
    -   Maybe start on this since has instructions and I have done before on my
        own.

## CUDA Semantics and PyTorch Memory Management

-   [Understanding GPU Memory 1: Visualizing All Allocations over Time](https://pytorch.org/blog/understanding-gpu-memory-1/)
    -   4th February 2024
-   [Understanding GPU Memory 2: Finding and Removing Reference Cycles](https://pytorch.org/blog/understanding-gpu-memory-2/)
    -   4th February 2024
-   [CUDA SEMANTICS](https://pytorch.org/docs/stable/notes/cuda.html)
-   [CUDA-MODE Lectures - Jeremy Howard](https://github.com/cuda-mode/lectures)

## Attention Is All You Need (Transformer, Encoder, Decoder)

### Papers

-   [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
    -   4th February 2024

## LLM

-   https://github.com/openai/openai-cookbook/tree/main
-   https://github.com/mlabonne/llm-course

## CICD

-   https://microsoft.github.io/code-with-engineering-playbook/continuous-integration/#code-style-checks
    -   7th February 2024
