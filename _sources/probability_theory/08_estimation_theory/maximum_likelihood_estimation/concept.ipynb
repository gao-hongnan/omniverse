{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb53ede",
   "metadata": {},
   "source": [
    "# Concept\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "\n",
    "## Likelihood\n",
    "\n",
    "### Some Intuition\n",
    "\n",
    "**_This section is adapted from {cite}`chan_2021`._**\n",
    "\n",
    "Consider a set of $N$ data points\n",
    "$\\mathcal{S}=\\left\\{x^{(1)}, x^{(2)}, \\ldots, x^{(n)}\\right\\}$. We want to\n",
    "describe these data points using a probability distribution. What would be the\n",
    "most general way of defining such a distribution?\n",
    "\n",
    "Since we have $N$ data points, and we do not know anything about them, the most\n",
    "general way to define a distribution is as a high-dimensional probability\n",
    "density function (PDF) $f_{\\mathbf{X}}(\\mathbf{x})$. This is a PDF of a random\n",
    "vector $\\mathbf{X}=\\left[x^{(1)}, \\ldots, x^{(n)}\\right]^{T}$. A particular\n",
    "realization of this random vector is\n",
    "$\\mathbf{x}=\\left[x^{(1)}, \\ldots, x^{(n)}\\right]^{T}$.\n",
    "\n",
    "$f_{\\mathbf{X}}(\\mathbf{x})$ is the most general description for the $N$ data\n",
    "points because $f_{\\mathbf{X}}(\\mathbf{x})$ is the **joint** PDF of all\n",
    "variables. It provides the complete statistical description of the vector\n",
    "$\\mathbf{X}$. For example, we can compute the mean vector\n",
    "$\\mathbb{E}[\\mathbf{X}]$, the covariance matrix\n",
    "$\\operatorname{Cov}(\\mathbf{X})$, the marginal distributions, the conditional\n",
    "distribution, the conditional expectations, etc. In short, if we know\n",
    "$f_{\\mathbf{X}}(\\mathbf{x})$, we know everything about $\\mathbf{X}$.\n",
    "\n",
    "The joint PDF $f_{\\mathbf{X}}(\\mathbf{x})$ is always **parameterized** by a\n",
    "certain parameter $\\boldsymbol{\\theta}$. For example, if we assume that\n",
    "$\\mathbf{X}$ is drawn from a joint Gaussian distribution, then\n",
    "$f_{\\mathbf{X}}(\\mathbf{x})$ is parameterized by the mean vector\n",
    "$\\boldsymbol{\\mu}$ and the covariance matrix $\\boldsymbol{\\Sigma}$. So we say\n",
    "that the parameter $\\boldsymbol{\\theta}$ is\n",
    "$\\boldsymbol{\\theta}=(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$. To state the\n",
    "dependency on the parameter explicitly, we write\n",
    "\n",
    "$$\n",
    "f_{\\mathbf{X}}(\\mathbf{x} ; \\boldsymbol{\\theta})=\\mathrm{PDF} \\text { of the random vector } \\mathbf{X} \\text { with a parameter } \\boldsymbol{\\theta} .\n",
    "$$\n",
    "\n",
    "When you express the joint PDF as a function of $\\mathbf{x}$ and\n",
    "$\\boldsymbol{\\theta}$, you have two variables to play with. The first variable\n",
    "is the **observation** $\\mathbf{x}$, which is given by the measured data. We\n",
    "usually think about the probability density function\n",
    "$f_{\\mathbf{X}}(\\mathbf{x})$ in terms of $\\mathbf{x}$, because the PDF is\n",
    "evaluated at $\\mathbf{X}=\\mathbf{x}$. In estimation, however, $\\mathbf{x}$ is\n",
    "something that you cannot control. When your boss hands a dataset to you,\n",
    "$\\mathbf{x}$ is already fixed. You can consider the probability of getting this\n",
    "particular $\\mathbf{x}$, but you cannot change $\\mathbf{x}$.\n",
    "\n",
    "The second variable stated in $f_{\\mathbf{X}}(\\mathbf{x} ; \\boldsymbol{\\theta})$\n",
    "is the **parameter** $\\boldsymbol{\\theta}$. This parameter is what we want to\n",
    "find out, and it is the subject of interest in an estimation problem. Our goal\n",
    "is to find the optimal $\\boldsymbol{\\theta}$ that can offer the \"best\n",
    "explanation\" to data $\\mathbf{x}$, in the sense that it can maximize\n",
    "$f_{\\mathbf{X}}(\\mathbf{x} ; \\boldsymbol{\\theta})$.\n",
    "\n",
    "The likelihood function is the PDF that shifts the emphasis to\n",
    "$\\boldsymbol{\\theta}$, let's define it formally.\n",
    "\n",
    "### Definition\n",
    "\n",
    "```{prf:definition} Likelihood Function\n",
    ":label: def:likelihood\n",
    "\n",
    "Let $\\mathbf{X}=\\left[x^{(1)}, \\ldots, x^{(n)}\\right]^{T}$ be a random vector drawn from a joint PDF $f_{\\mathbf{X}}(\\mathbf{x} ; \\boldsymbol{\\theta})$, and let $\\mathbf{x}=\\left[x^{(1)}, \\ldots, x^{(n)}\\right]^{T}$ be the realizations. The likelihood function is a\n",
    "function of the parameter $\\boldsymbol{\\theta}$ given the realizations $\\mathbf{x}$ :\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{\\theta} \\mid \\mathbf{x}) \\stackrel{\\text { def }}{=} f_{\\mathbf{X}}(\\mathbf{x} ; \\boldsymbol{\\theta})\n",
    "$$ (eq:likelihood)\n",
    "```\n",
    "\n",
    "```{prf:remark} Likelihood is not Conditional PDF\n",
    ":label: rem:likelihood\n",
    "\n",
    "A word of caution: $\\mathcal{L}(\\boldsymbol{\\theta} \\mid \\mathbf{x})$ is not a conditional PDF because $\\boldsymbol{\\theta}$ is not a random variable. The correct way to interpret $\\mathcal{L}(\\boldsymbol{\\theta} \\mid \\mathbf{x})$ is to view it as a function of $\\boldsymbol{\\theta}$.\n",
    "```\n",
    "\n",
    "### Independence and Identically Distributed (IID)\n",
    "\n",
    "While $f_{\\mathbf{X}}(\\mathbf{x})$ provides us with a complete picture of the\n",
    "random vector $\\mathbf{X}$, using $f_{\\mathbf{X}}(\\mathbf{x})$ is tedious. We\n",
    "need to describe how each $x^{(n)}$ is generated and describe how $x^{(n)}$ is\n",
    "related to $X_{m}$ for all pairs of $n$ and $m$. If the vector $\\mathbf{X}$\n",
    "contains $N$ entries, then there are $N^{2} / 2$ pairs of correlations we need\n",
    "to compute. When $N$ is large, finding $f_{\\mathbf{X}}(\\mathbf{x})$ would be\n",
    "very difficult if not impossible.\n",
    "\n",
    "What does this mean? Two things.\n",
    "\n",
    "1. There is no assumption of **independence** between the data points. This\n",
    "   means that describing the joint PDF $f_{\\mathbf{X}}(\\mathbf{x})$ is very\n",
    "   difficult.\n",
    "2. Each data point _can_ be drawn from a different distribution\n",
    "   $f_{X^{(n)}}(x^{(n)})$ for each $n$.\n",
    "\n",
    "Hope is not lost.\n",
    "\n",
    "Enter the **independence and identically distributed (IID)** assumption. This\n",
    "assumption states that the data points $\\mathbf{x}^{(n)}$ are independent and\n",
    "identically distributed.\n",
    "\n",
    "In other words, each data point $\\mathbf{x}^{(n)}$ is drawn from **identical**\n",
    "distribution $f_{\\mathbf{X}}(\\mathbf{x} ; \\boldsymbol{\\theta})$ parameterized by\n",
    "$\\boldsymbol{\\theta}$ and each pair of data points $\\mathbf{x}^{(n)}$ and\n",
    "$\\mathbf{x}^{(m)}$ are **independent** of each other.\n",
    "\n",
    "Now, we can write the problem in a much simpler way, where the joint PDF\n",
    "$f_{\\mathbf{X}}(\\mathbf{x})$ is replaced by the product of the PDFs of each data\n",
    "point $f_{x^{(n)}}(x^{(n)})$.\n",
    "\n",
    "$$\n",
    "f_{\\mathbf{X}}(\\mathbf{x})=f_{x^{(1)}, \\ldots, x^{(n)}}\\left(x^{(1)}, \\ldots, x^{(n)}\\right)=\\prod_{n=1}^{N} f_{x^{(n)}}\\left(x^{(n)}\\right) .\n",
    "$$\n",
    "\n",
    "or in our context, we can add the **parameter** $\\boldsymbol{\\theta}$ to the\n",
    "PDFs.\n",
    "\n",
    "$$\n",
    "f_{\\mathbf{X}}(\\mathbf{x} ; \\boldsymbol{\\theta})=f_{x^{(1)}, \\ldots, x^{(n)}}\\left(x^{(1)}, \\ldots, x^{(n)}\\right)=\\prod_{n=1}^{N} f_{x^{(n)}}\\left(x^{(n)} ; \\boldsymbol{\\theta}\\right) .\n",
    "$$\n",
    "\n",
    "Let's formally redefine the likelihood function with the IID assumption. Note\n",
    "this is an ubiquitous assumption in machine learning and therefore we will stick\n",
    "to this unless otherwise stated.\n",
    "\n",
    "```{prf:definition} Likelihood Function with IID Assumption\n",
    ":label: def:likelihood-iid\n",
    "\n",
    "Given $\\textrm{i.i.d.}$ random variables $x^{(1)}, \\ldots, x^{(n)}$ that all have the same PDF $f_{x^{(n)}}\\left(x^{(n)}\\right)$, the **likelihood function** is defined as:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{\\theta} \\mid \\mathbf{x}) \\stackrel{\\text { def }}{=} \\prod_{n=1}^{N} f_{x^{(n)}}\\left(x^{(n)} ; \\boldsymbol{\\theta}\\right)\n",
    "$$\n",
    "```\n",
    "\n",
    "Notice that in the previous sections, there was an implicit assumption that the\n",
    "random vector $\\mathbf{X}$ is a vector of **univariate\\*** random variables.\n",
    "This is not always the case. In fact, most of the time, the random vector\n",
    "$\\mathbf{X}$ is a \"vector\" (collection) of **multivariate** random variables in\n",
    "the machine learning realm.\n",
    "\n",
    "Let's redefine the likelihood function for the higher dimensional case, and also\n",
    "take the opportunity to introduce the definition in the context of machine\n",
    "learning.\n",
    "\n",
    "### Likelihood in the Context of Machine Learning\n",
    "\n",
    "```{prf:definition} Likelihood Function with IID Assumption (Higher Dimension)\n",
    ":label: def:likelihood-iid-higher-dim\n",
    "\n",
    "Given a dataset $\\mathcal{S} = \\left\\{\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)}\\right\\}$ where each $\\mathbf{x}^{(n)}$ is a vector of $D$-dimensional drawn $\\textrm{i.i.d.}$ from the same underlying distribution\n",
    "$\\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{X} ; \\boldsymbol{\\theta}\\right)$, the **likelihood function** is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(\\boldsymbol{\\theta} \\mid \\mathcal{S}) &\\stackrel{\\text { def }}{=} \\mathbb{P}_{\\mathcal{D}}\\left(\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)} ; \\boldsymbol{\\theta}\\right) \\\\\n",
    "&= \\prod_{n=1}^{N} \\mathbb{P}_{\\mathcal{D}}\\left(\\mathbf{x}^{(n)} ; \\boldsymbol{\\theta}\\right)\n",
    "\\end{aligned}\n",
    "$$ (eq:likelihood-machine-learning-1)\n",
    "\n",
    "which means what is the probability of observing a sequence of $N$ data points $\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)}$?\n",
    "\n",
    "Think of it as flipping $N$ coins, and what is the sequence of observing the permutation of, say, $HHTTTHH$?\n",
    "```\n",
    "\n",
    "### Likelihood in the Context of Supervised Learning\n",
    "\n",
    "In supervised learning, there is often a label $y$ associated with each data\n",
    "point $\\mathbf{x}$.\n",
    "\n",
    "```{prf:remark} Where's the $y$?\n",
    ":label: rem:where-y\n",
    "\n",
    "Some people may ask, isn't the setting in our classification problem a supervised one with labels?\n",
    "Where are the $y$ in the likelihood function? Good point, the $y$ is not included in our current section\n",
    "for simplicity. However, the inclusion of $y$ can be merely thought as denoting \"an additional\"\n",
    "random variable in the likelihood function above.\n",
    "\n",
    "For example, let's say $y$ is the target column of a classification problem on breast cancer, denoting\n",
    "whether the patient has cancer or not. Then, the likelihood function can be written as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{\\theta} \\mid \\mathcal{S}) \\stackrel{\\text { def }}{=} \\prod_{n=1}^{N} \\mathbb{P}_{\\mathcal{D}}\\left(\\mathbf{x}^{(n)}, y^{(n)} ; \\boldsymbol{\\theta}\\right)\n",
    "$$\n",
    "\n",
    "where $\\mathcal{S}$ is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\left\\{\\left(\\mathbf{x}^{(1)}, y^{(1)}\\right), \\ldots, \\left(\\mathbf{x}^{(n)}, y^{(n)}\\right)\\right\\}\n",
    "$$\n",
    "```\n",
    "\n",
    "```{prf:definition} Likelihood Function with IID Assumption (Supervised Learning)\n",
    ":label: def:likelihood-iid-supervised-learning\n",
    "\n",
    "More concretely, for a typical supervised learning problems, the learner $\\mathcal{A}$ receives a labeled sample dataset $\\mathcal{S}$\n",
    "containing $N$ i.i.d. samples $\\left(\\mathbf{x}^{(n)}, y^{(n)}\\right)$ drawn from $\\mathbb{P}_{\\mathcal{D}}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\left\\{\\left(\\mathbf{x}^{(1)}, y^{(1)}\\right), \\left(\\mathbf{x}^{(2)}, y^{(2)}\\right), \\ldots, \\left(\\mathbf{x}^{(N)}, y^{(N)}\\right)\\right\\} \\subset \\mathbb{R}^{D} \\quad \\overset{\\small{\\text{i.i.d.}}}{\\sim} \\quad \\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{X}, \\mathcal{Y} ; \\boldsymbol{\\beta}\\right)\n",
    "$$ (eq-dataset-machine-learning)\n",
    "\n",
    "where $\\mathbb{P}_{\\mathcal{D}}$ is assumed to be the underlying (joint) distribution that generates the dataset $\\mathcal{S}$.\n",
    "\n",
    "So in this setting, we generally assume that the tuple $\\left(\\mathbf{x}^{(n)}, y^{(n)}\\right)$ is drawn from the joint distribution $\\mathbb{P}_{\\mathcal{D}}$\n",
    "and not just $\\mathbf{x}^{(n)}$ alone.\n",
    "\n",
    "Note carefully that this does not say anything about the independence of $\\mathbf{x}^{(n)}$ and $y^{(n)}$.\n",
    "It only states that each pair $\\left(\\mathbf{x}^{(n)}, y^{(n)}\\right)$ is independent, leading to this:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(\\boldsymbol{\\theta} \\mid \\mathcal{S}) &\\stackrel{\\text { def }}{=} \\mathbb{P}_{\\mathcal{D}}\\left(\\left(\\mathbf{x}^{(1)}, y^{(1)}\\right), \\ldots, \\left(\\mathbf{x}^{(n)}, y^{(n)}\\right); \\boldsymbol{\\theta}\\right) \\\\\n",
    "&= \\prod_{n=1}^{N} \\mathbb{P}_{\\mathcal{D}}\\left(\\mathbf{x}^{(n)}, y^{(n)} ; \\boldsymbol{\\theta}\\right)\n",
    "\\end{aligned}\n",
    "$$ (eq:likelihood-machine-learning-2)\n",
    "\n",
    "which answers the question of what is the probability of observing a sequence of $N$\n",
    "data points $\\left(\\mathbf{x}^{(1)}, y^{(1)}\\right), \\ldots, \\left(\\mathbf{x}^{(n)}, y^{(n)}\\right)$?\n",
    "```\n",
    "\n",
    "### Conditional Likelihood in the Context of Machine Learning\n",
    "\n",
    "Now in discriminative algorithms such as\n",
    "[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression), we are\n",
    "interested in the conditional likelihood function.\n",
    "\n",
    "```{prf:definition} Conditional Likelihood Function (Machine Learning)\n",
    ":label: def:conditional-likelihood-machine-learning\n",
    "\n",
    "The conditional likelihood function is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(\\boldsymbol{\\theta} \\mid \\mathcal{S}) &\\stackrel{\\text { def }}{=} \\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{Y}\\mid \\mathcal{X} ; \\boldsymbol{\\theta}\\right) \\\\\n",
    "&= \\mathbb{P}_{\\mathcal{D}}\\left(y^{(1)}, y^{(2)}, \\ldots, y^{(N)} \\mid \\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\ldots, \\mathbf{x}^{(N)} ; \\boldsymbol{\\theta}\\right) \\\\\n",
    "&= \\prod_{n=1}^{N} \\mathbb{P}_{\\mathcal{D}}\\left(y^{(n)} \\mid \\mathbf{x}^{(n)} ; \\boldsymbol{\\theta}\\right)\n",
    "\\end{aligned}\n",
    "$$ (eq:conditional-likelihood-machine-learning-1)\n",
    "\n",
    "where we abuse notation and define $\\mathcal{X} = \\left\\{\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\ldots, \\mathbf{x}^{(N)}\\right\\}$\n",
    "and $\\mathcal{Y} = \\left\\{y^{(1)}, y^{(2)}, \\ldots, y^{(N)}\\right\\}$.\n",
    "\n",
    "We are instead interested in the probability of observing a sequence of $N$\n",
    "conditional data points $\\left(y^{(1)} \\mid \\mathbf{x}^{(1)}, y^{(2)} \\mid \\mathbf{x}^{(2)}, \\ldots, y^{(N)} \\mid \\mathbf{x}^{(N)}\\right)$.\n",
    "```\n",
    "\n",
    "Now why does {eq}`eq:conditional-likelihood-machine-learning-1` still hold for\n",
    "the conditional likelihood function to be able to factorize into a product of\n",
    "conditional probabilities?\n",
    "\n",
    "Did we also assume that the conditional data points\n",
    "$\\left(y^{(1)} \\mid \\mathbf{x}^{(1)}, y^{(2)} \\mid \\mathbf{x}^{(2)}, \\ldots, y^{(N)} \\mid \\mathbf{x}^{(N)}\\right)$\n",
    "are $\\textrm{i.i.d.}$ as well?\n",
    "\n",
    "We can prove that this equation holds via\n",
    "[**marginilization**](https://en.wikipedia.org/wiki/Marginal_distribution) and\n",
    "the [**Fubini's Theorem**](https://en.wikipedia.org/wiki/Fubini%27s_theorem).\n",
    "\n",
    "```{prf:proof}\n",
    "The proof is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{Y} \\mid \\mathcal{X} ; \\boldsymbol{\\theta}\\right) &= \\dfrac{\\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{Y}, \\mathcal{X} ; \\boldsymbol{\\theta}\\right)}{\\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{X} ; \\boldsymbol{\\theta}\\right)} \\\\\n",
    "&= \\dfrac{\\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{Y}, \\mathcal{X} ; \\boldsymbol{\\theta}\\right)}{\\int \\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{Y}, \\mathcal{X} ; \\boldsymbol{\\theta}\\right) d\\mathcal{Y}}  &&\\text{ Marginalization} \\\\\n",
    "&= \\dfrac{\\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{Y}, \\mathcal{X} ; \\boldsymbol{\\theta}\\right)}{\\int \\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{Y} \\mid \\mathcal{X} ; \\boldsymbol{\\theta}\\right) \\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{X} ; \\boldsymbol{\\theta}\\right) d\\mathcal{Y}}  &&\\text{ Marginalization} \\\\\n",
    "&= \\dfrac{\\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{Y}, \\mathcal{X} ; \\boldsymbol{\\theta}\\right)}{\\int \\int \\cdots \\int \\prod_{n=1}^{N} \\mathbb{P}_{\\mathcal{D}}\\left(y^{(n)} \\mid \\mathbf{x}^{(n)} ; \\boldsymbol{\\theta}\\right) \\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{X} ; \\boldsymbol{\\theta}\\right) d\\mathcal{Y}}  &&\\text{ Fubini's Theorem} \\\\\n",
    "&= \\dfrac{\\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{Y}, \\mathcal{X} ; \\boldsymbol{\\theta}\\right)}{\\int \\int \\cdots \\int \\prod_{n=1}^{N} \\mathbb{P}_{\\mathcal{D}}\\left(y^{(n)} \\mid \\mathbf{x}^{(n)} ; \\boldsymbol{\\theta}\\right) \\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{X} ; \\boldsymbol{\\theta}\\right) d\\mathbf{x}^{(n)}}  &&\\text{ Fubini's Theorem} \\\\\n",
    "&= \\prod_{n=1}^{N} \\mathbb{P}_{\\mathcal{D}}\\left(y^{(n)} \\mid \\mathbf{x}^{(n)} ; \\boldsymbol{\\theta}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "See [proof here](https://stats.stackexchange.com/questions/331215/defining-conditional-likelihood).\n",
    "```\n",
    "\n",
    "Therefore, the conditional likelihood function is a product of conditional\n",
    "probabilities, a consequence of the\n",
    "[**i.i.d. assumption**](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)\n",
    "for the data points in $\\mathcal{S}$.\n",
    "\n",
    "### The Log-Likelihood Function\n",
    "\n",
    "We will later see in an example that why the log-likelihood function is useful.\n",
    "For now, let's just say that due to numerical reasons (underflow), we will use\n",
    "the log-likelihood function instead of the likelihood function. The intuition is\n",
    "that the likelihood defined in {eq}`eq:likelihood-machine-learning-1` is a\n",
    "product of individual PDFs. If we have 1 billion samples (i.e.\n",
    "$N = 1,000,000,000$), then the likelihood function will be a product of 1\n",
    "billion PDFs. This is a very small number and will cause\n",
    "[**arithmetic underflow**](https://en.wikipedia.org/wiki/Arithmetic_underflow).\n",
    "The log-likelihood function is a solution to this problem.\n",
    "\n",
    "```{prf:definition} Log-Likelihood Function\n",
    ":label: def:log-likelihood\n",
    "\n",
    "Given a dataset $\\mathcal{S} = \\left\\{\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)}\\right\\}$ where each $\\mathbf{x}^{(n)}$ is a vector of $D$-dimensional drawn $\\textrm{i.i.d.}$ from the same underlying distribution\n",
    "$\\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{X} ; \\boldsymbol{\\theta}\\right)$, the **log-likelihood function** is defined as:\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\boldsymbol{\\theta} \\mid \\mathcal{S}) \\stackrel{\\text { def }}{=} \\sum_{n=1}^{N} \\log \\mathbb{P}_{\\mathcal{D}}\\left(\\mathbf{x}^{(n)} ; \\boldsymbol{\\theta}\\right)\n",
    "$$ (e:log-likelihood-machine-learning)\n",
    "```\n",
    "\n",
    "One will soon see that **maximization** of the log-likelihood function is\n",
    "equivalent to **maximization** of the likelihood function. They give the same\n",
    "result.\n",
    "\n",
    "Let's walk through an example:\n",
    "\n",
    "```{prf:example} Log-Likelihood of Bernoulli Distribution\n",
    ":label: ex:log-likelihood-bernoulli\n",
    "\n",
    "The log-likelihood of a sequence of $\\textrm{i.i.d.}$ Bernoulli *univariate* random variables\n",
    "$x^{(1)}, \\ldots, x^{(n)}$ with parameter $\\theta$.\n",
    "\n",
    "If $x^{(1)}, \\ldots, x^{(n)}$ are i.i.d. Bernoulli random variables, we have\n",
    "\n",
    "$$\n",
    "f_{\\mathbf{X}}(\\mathbf{x} ; \\theta)=\\prod_{n=1}^{N}\\left\\{\\theta^{x^{(n)}}(1-\\theta)^{1-x^{(n)}}\\right\\} .\n",
    "$$\n",
    "\n",
    "Taking the log on both sides of the equation yields the log-likelihood function:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log \\mathcal{L}(\\theta \\mid \\mathbf{x}) & =\\log \\left\\{\\prod_{n=1}^{N}\\left\\{\\theta^{x^{(n)}}(1-\\theta)^{1-x^{(n)}}\\right\\}\\right\\} \\\\\n",
    "& =\\sum_{n=1}^{N} \\log \\left\\{\\theta^{x^{(n)}}(1-\\theta)^{1-x^{(n)}}\\right\\} \\\\\n",
    "& =\\sum_{n=1}^{N} x^{(n)} \\log \\theta+\\left(1-x^{(n)}\\right) \\log (1-\\theta) \\\\\n",
    "& =\\left(\\sum_{n=1}^{N} x^{(n)}\\right) \\cdot \\log \\theta+\\left(N-\\sum_{n=1}^{N} x^{(n)}\\right) \\cdot \\log (1-\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "```\n",
    "\n",
    "Now there will be more examples of higher-dimensional log-likelihood functions\n",
    "in the next section. Furthermore, the section Maximum Likelihood Estimation for\n",
    "Priors in [Naive Bayes](../../../influential/naive_bayes/02_concept.md),details\n",
    "one example of log-likelihood function for a higher-dimensional multivariate\n",
    "Bernoulli (Catagorical) distribution.\n",
    "\n",
    "### Visualizing the Likelihood Function\n",
    "\n",
    "This section mainly details how the likelihood function, despite being a\n",
    "function of $\\boldsymbol{\\theta}$, also depends on the underlying dataset\n",
    "$\\mathcal{S}$. The presence of both should be kept in mind when we talk about\n",
    "the likelihood function.\n",
    "\n",
    "For a more detailed analysis, see page 471-472 of Professor Stanley Chan's book\n",
    "\"Introduction to Probability for Data Science\" (see references section).\n",
    "\n",
    "## Maximum Likelihood Estimation\n",
    "\n",
    "After rigorously defining the likelihood function, we can now talk about the\n",
    "term **maximum** in maximum likelihood estimation.\n",
    "\n",
    "The action of maximization is in itself under\n",
    "[optimization theory](https://en.wikipedia.org/wiki/Mathematical_optimization),\n",
    "a branch in mathematics. Consequently, the maximum likelihood estimation problem\n",
    "is an optimization problem that seeks to find the parameter\n",
    "$\\boldsymbol{\\theta}$ that maximizes the likelihood function.\n",
    "\n",
    "```{prf:definition} Maximum Likelihood Estimation\n",
    ":label: def:maximum-likelihood-estimation\n",
    "\n",
    "Given a dataset $\\mathcal{S}$ consisting of $N$ samples defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\left\\{\\mathbf{x}^{(1)}, \\ldots, \\mathbf{x}^{(n)}\\right\\},\n",
    "$$\n",
    "\n",
    "where $\\mathcal{S}$ is $\\textrm{i.i.d.}$ generated from the distribution $\\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{X} ; \\boldsymbol{\\theta}\\right)$, parametrized by $\\boldsymbol{\\theta}$, where the parameter $\\boldsymbol{\\theta}$ can be a vector of parameters defined as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta} = \\left\\{\\theta_{1}, \\ldots, \\theta_{k}\\right\\}.\n",
    "$$\n",
    "\n",
    "\n",
    "We define the likelihood function to be:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{\\theta}) = \\mathcal{L}(\\boldsymbol{\\theta} \\mid \\mathcal{S}) \\stackrel{\\text { def }}{=} \\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{X} ; \\boldsymbol{\\theta}\\right),\n",
    "$$\n",
    "\n",
    "\n",
    "then the maximum-likelihood estimate of the parameter $\\boldsymbol{\\theta}$ is a parameter that maximizes the likelihood function:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\boldsymbol{\\theta}} &\\stackrel{\\text { def }}{=} \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\operatorname{argmax}} \\mathcal{L}\\left(\\boldsymbol{\\theta} \\mid \\mathcal{S}\\right) \\\\\n",
    "&\\stackrel{\\text{ def }}{=} \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\operatorname{argmax}}\\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{X} ; \\boldsymbol{\\theta}\\right)\n",
    "\\end{aligned}\n",
    "$$ (eq:maximum-likelihood-estimation)\n",
    "```\n",
    "\n",
    "```{prf:remark} Maximum Likelihood Estimation for $\\mathcal{S}$ with Label $y$\n",
    ":label: rmk:maximum-likelihood-estimation\n",
    "\n",
    "To be more verbose, let's also define the maximum likelihood estimate of the parameter $\\boldsymbol{\\theta}$ for a dataset $\\mathcal{S}$ with label $y$.\n",
    "\n",
    "First, we redefine $\\mathcal{S}$ to be:\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\left\\{\\left(\\mathbf{x}^{(1)}, y^{(1)}\\right), \\ldots, \\left(\\mathbf{x}^{(n)}, y^{(n)}\\right)\\right\\}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{S}$ is generated from the distribution $\\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{X}, \\mathcal{Y} ; \\boldsymbol{\\theta}\\right)$. The likelihood function is then defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{\\theta}) = \\mathcal{L}(\\boldsymbol{\\theta} \\mid \\mathcal{S}) \\stackrel{\\text { def }}{=} \\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{X}, \\mathcal{Y}; \\boldsymbol{\\theta}\\right),\n",
    "$$\n",
    "\n",
    "then the maximum-likelihood estimate of the parameter $\\boldsymbol{\\theta}$ is a parameter that maximizes the likelihood function:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\boldsymbol{\\theta}} &\\stackrel{\\text { def }}{=} \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\operatorname{argmax}} \\mathcal{L}\\left(\\boldsymbol{\\theta} \\mid \\mathcal{S}, y\\right) \\\\\n",
    "&\\stackrel{\\text{ def }}{=} \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\operatorname{argmax}} \\mathbb{P}_{\\mathcal{D}}\\left(\\mathcal{X}, \\mathcal{Y}; \\boldsymbol{\\theta}\\right)\n",
    "\\end{aligned}\n",
    "$$ (eq:maximum-likelihood-estimation-for-label-y)\n",
    "```\n",
    "\n",
    "## Coin Toss Example\n",
    "\n",
    "```{figure} ../assets/coin_generator.jpg\n",
    "---\n",
    "name: coin_generator\n",
    "height: 500px\n",
    "---\n",
    "A coin generator that generates a sequence of coin flips.\n",
    "```\n",
    "\n",
    "Let's see how this works in a concrete example. Suppose we have a coin generator\n",
    "as shown in {numref}`coin_generator`. We know for a fact that:\n",
    "\n",
    "1. The coin generator generates coins **independently**.\n",
    "2. The coin generator generates coins with an **identical** probability $\\theta$\n",
    "   (denoted $p$ in the diagram) of being heads.\n",
    "\n",
    "Consequently, the probability that a coin generated by the coin generator is\n",
    "heads is $\\theta$, and the probability that a coin generated by the coin\n",
    "generator is tails is $1-\\theta$.\n",
    "\n",
    "Let's say we press the button on the coin generator $N$ times, and we observe\n",
    "$N$ coin flips. Let's denote the observed coin flips as\n",
    "$X^{(1)}, \\ldots, X^{(N)}$, where the realizations of each $X^{(n)}$ are\n",
    "$x^{(n)} = 1$ if the $n$th flip is heads and $x^{(n)} = 0$ if the $n$th flip is\n",
    "tails. This sequence of observations can be further collated into our familiar\n",
    "dataset $\\mathcal{S}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\left\\{x^{(1)}, \\ldots, x^{(N)}\\right\\}.\n",
    "$$\n",
    "\n",
    "Then the probability of observing this dataset $\\mathcal{S}$ is equivalent to\n",
    "asking what is the probability of observing this sequence of random variables\n",
    "$X^{(1)}, \\ldots, X^{(N)}$ and is given by:\n",
    "\n",
    "```{math}\n",
    ":label: eq:coin-toss-likelihood-1\n",
    "\n",
    "\\mathbb{P}(X ; \\theta) = \\prod_{n=1}^N \\theta^{x^{(n)}}(1-\\theta)^{1-x^{(n)}}.\n",
    "```\n",
    "\n",
    "Our goal is to find the value of $\\theta$. How? Maximum likelihood estimation!\n",
    "We want to find the value of $\\theta$ that maximizes the **joint probability**\n",
    "of observing the sequence of coin flips. This is equivalent to maximizing the\n",
    "**likelihood** of observing the sequence of coin flips. The likelihood function\n",
    "is given by the exact same equation as the joint probability, except that we do\n",
    "a notational change:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta \\mid \\mathcal{S}) = \\prod_{n=1}^N \\theta^{x^{(n)}}(1-\\theta)^{1-x^{(n)}}.\n",
    "$$\n",
    "\n",
    "Notice that this equation is none other than the product of $N$ Bernoulli random\n",
    "variables, each with parameter $\\theta$. This is not surprising since coin toss\n",
    "is usually modelled as a Bernoulli random variable.\n",
    "\n",
    "If we flip $13$ coins and get the sequence \"HHHTHTTHHHHHT\", then the probability\n",
    "of observing this sequence is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}(X ; \\theta) &= \\theta^{x^{(1)}}(1-\\theta)^{1-x^{(1)}} \\times \\theta^{x^{(2)}}(1-\\theta)^{1-x^{(2)}} \\times \\theta^{x^{(3)}}(1-\\theta)^{1-x^{(3)}} \\times \\theta^{x^{(4)}}(1-\\theta)^{1-x^{(4)}} \\times \\theta^{x^{(5)}}(1-\\theta)^{1-x^{(5)}} \\times \\theta^{x^{(6)}}(1-\\theta)^{1-x^{(6)}} \\times \\theta^{x^{(7)}}(1-\\theta)^{1-x^{(7)}} \\times \\theta^{x^{(8)}}(1-\\theta)^{1-x^{(8)}} \\times \\theta^{x^{(9)}}(1-\\theta)^{1-x^{(9)}} \\times \\theta^{x^{(10)}}(1-\\theta)^{1-x^{(10)}} \\times \\theta^{x^{(11)}}(1-\\theta)^{1-x^{(11)}} \\times \\theta^{x^{(12)}}(1-\\theta)^{1-x^{(12)}} \\times \\theta^{x^{(13)}}(1-\\theta)^{1-x^{(13)}} \\\\\n",
    "&= \\theta^{9}(1-\\theta)^{4}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "One nice thing about this example will be that we know the answer going in.\n",
    "Indeed, if we said verbally, \"I flipped 13 coins, and 9 came up heads, what is\n",
    "our best guess for the probability that the coin comes us heads?, \" everyone\n",
    "would correctly guess $9/13$. What this maximum likelihood method will give us\n",
    "is a way to get that number from first principals in a way that will generalize\n",
    "to vastly more complex situations {cite}`zhang2023dive`.\n",
    "\n",
    "For our example, the plot of $P(X \\mid \\theta)$ is as follows:\n",
    "\n",
    "We know that a coin toss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8067b9",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"522.83125pt\" height=\"377.39625pt\" viewBox=\"0 0 522.83125 377.39625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-09-10T13:43:30.151084</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.0, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 377.39625 \n",
       "L 522.83125 377.39625 \n",
       "L 522.83125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 69.23125 339.84 \n",
       "L 515.63125 339.84 \n",
       "L 515.63125 7.2 \n",
       "L 69.23125 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"mc515197981\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc515197981\" x=\"89.522159\" y=\"339.84\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(81.570597 354.438438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc515197981\" x=\"170.76704\" y=\"339.84\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(162.815478 354.438438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc515197981\" x=\"252.011922\" y=\"339.84\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(244.060359 354.438438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc515197981\" x=\"333.256803\" y=\"339.84\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(325.30524 354.438438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc515197981\" x=\"414.501684\" y=\"339.84\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(406.550122 354.438438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mc515197981\" x=\"495.746565\" y=\"339.84\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(487.795003 354.438438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_7\">\n",
       "     <!-- $\\theta$ -->\n",
       "     <g transform=\"translate(289.33125 368.116562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-Oblique-3b8\" d=\"M 2913 2219 \n",
       "L 925 2219 \n",
       "Q 791 1284 928 888 \n",
       "Q 1100 400 1566 400 \n",
       "Q 2034 400 2391 891 \n",
       "Q 2703 1322 2913 2219 \n",
       "z\n",
       "M 3009 2750 \n",
       "Q 3094 3638 2984 3950 \n",
       "Q 2813 4444 2353 4444 \n",
       "Q 1875 4444 1525 3956 \n",
       "Q 1250 3563 1034 2750 \n",
       "L 3009 2750 \n",
       "z\n",
       "M 2444 4913 \n",
       "Q 3194 4913 3494 4250 \n",
       "Q 3794 3591 3566 2422 \n",
       "Q 3341 1256 2781 594 \n",
       "Q 2225 -72 1475 -72 \n",
       "Q 722 -72 425 594 \n",
       "Q 128 1256 353 2422 \n",
       "Q 581 3591 1134 4250 \n",
       "Q 1691 4913 2444 4913 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-Oblique-3b8\" transform=\"translate(0 0.234375)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path id=\"m224b8f269b\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m224b8f269b\" x=\"69.23125\" y=\"324.72\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.00000 -->\n",
       "      <g transform=\"translate(20.878125 328.519219) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"286.279297\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"349.902344\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m224b8f269b\" x=\"69.23125\" y=\"278.546364\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.00005 -->\n",
       "      <g transform=\"translate(20.878125 282.345583) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"286.279297\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"349.902344\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m224b8f269b\" x=\"69.23125\" y=\"232.372728\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.00010 -->\n",
       "      <g transform=\"translate(20.878125 236.171947) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"286.279297\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"349.902344\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m224b8f269b\" x=\"69.23125\" y=\"186.199092\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.00015 -->\n",
       "      <g transform=\"translate(20.878125 189.998311) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"286.279297\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"349.902344\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m224b8f269b\" x=\"69.23125\" y=\"140.025456\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 0.00020 -->\n",
       "      <g transform=\"translate(20.878125 143.824675) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"286.279297\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"349.902344\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m224b8f269b\" x=\"69.23125\" y=\"93.85182\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 0.00025 -->\n",
       "      <g transform=\"translate(20.878125 97.651039) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"286.279297\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"349.902344\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_7\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m224b8f269b\" x=\"69.23125\" y=\"47.678184\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_14\">\n",
       "      <!-- 0.00030 -->\n",
       "      <g transform=\"translate(20.878125 51.477403) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"286.279297\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"349.902344\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_15\">\n",
       "     <!-- $\\mathbb{P}(X ; \\theta)$ -->\n",
       "     <g transform=\"translate(14.798437 190.02) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"STIXGeneral-Italic-2119\" d=\"M 1229 4179 \n",
       "L 3360 4179 \n",
       "Q 3827 4179 4108 3894 \n",
       "Q 4390 3610 4390 3142 \n",
       "Q 4390 2534 4032 2118 \n",
       "Q 3571 1581 2605 1581 \n",
       "L 1517 1581 \n",
       "L 1094 0 \n",
       "L 109 0 \n",
       "L 1229 4179 \n",
       "z\n",
       "M 3872 3808 \n",
       "L 3354 1914 \n",
       "Q 3718 2022 3936 2380 \n",
       "Q 4154 2739 4154 3136 \n",
       "Q 4154 3558 3872 3808 \n",
       "z\n",
       "M 3110 1875 \n",
       "L 3654 3878 \n",
       "Q 3507 3942 3283 3942 \n",
       "L 2150 3942 \n",
       "L 1581 1818 \n",
       "L 2598 1818 \n",
       "Q 2938 1818 3110 1875 \n",
       "z\n",
       "M 1907 3942 \n",
       "L 1408 3942 \n",
       "L 416 237 \n",
       "L 915 237 \n",
       "L 1907 3942 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-28\" d=\"M 1984 4856 \n",
       "Q 1566 4138 1362 3434 \n",
       "Q 1159 2731 1159 2009 \n",
       "Q 1159 1288 1364 580 \n",
       "Q 1569 -128 1984 -844 \n",
       "L 1484 -844 \n",
       "Q 1016 -109 783 600 \n",
       "Q 550 1309 550 2009 \n",
       "Q 550 2706 781 3412 \n",
       "Q 1013 4119 1484 4856 \n",
       "L 1984 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-Oblique-58\" d=\"M 878 4666 \n",
       "L 1516 4666 \n",
       "L 2316 2981 \n",
       "L 3763 4666 \n",
       "L 4500 4666 \n",
       "L 2578 2438 \n",
       "L 3738 0 \n",
       "L 3103 0 \n",
       "L 2163 1966 \n",
       "L 459 0 \n",
       "L -275 0 \n",
       "L 1906 2509 \n",
       "L 878 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-3b\" d=\"M 750 3309 \n",
       "L 1409 3309 \n",
       "L 1409 2516 \n",
       "L 750 2516 \n",
       "L 750 3309 \n",
       "z\n",
       "M 750 794 \n",
       "L 1409 794 \n",
       "L 1409 256 \n",
       "L 897 -744 \n",
       "L 494 -744 \n",
       "L 750 256 \n",
       "L 750 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-29\" d=\"M 513 4856 \n",
       "L 1013 4856 \n",
       "Q 1481 4119 1714 3412 \n",
       "Q 1947 2706 1947 2009 \n",
       "Q 1947 1309 1714 600 \n",
       "Q 1481 -109 1013 -844 \n",
       "L 513 -844 \n",
       "Q 928 -128 1133 580 \n",
       "Q 1338 1288 1338 2009 \n",
       "Q 1338 2731 1133 3434 \n",
       "Q 928 4138 513 4856 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#STIXGeneral-Italic-2119\" transform=\"translate(0 0.234375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-28\" transform=\"translate(68.699982 0.234375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-Oblique-58\" transform=\"translate(107.713654 0.234375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-3b\" transform=\"translate(176.219513 0.234375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-Oblique-3b8\" transform=\"translate(229.393341 0.234375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-29\" transform=\"translate(290.574982 0.234375)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_14\">\n",
       "    <path d=\"M 89.522159 324.72 \n",
       "L 165.079899 324.611972 \n",
       "L 177.266631 324.362915 \n",
       "L 185.797343 323.981658 \n",
       "L 192.703158 323.461414 \n",
       "L 198.796524 322.775249 \n",
       "L 204.077442 321.952014 \n",
       "L 208.952135 320.954966 \n",
       "L 213.420603 319.798904 \n",
       "L 217.482847 318.511576 \n",
       "L 221.138867 317.132199 \n",
       "L 224.794886 315.517083 \n",
       "L 228.450906 313.639417 \n",
       "L 231.700701 311.727585 \n",
       "L 234.950497 309.566553 \n",
       "L 238.200292 307.136275 \n",
       "L 241.450087 304.416738 \n",
       "L 244.699882 301.3882 \n",
       "L 247.949678 298.031456 \n",
       "L 251.199473 294.328112 \n",
       "L 254.449268 290.260882 \n",
       "L 257.699063 285.813908 \n",
       "L 260.948859 280.97308 \n",
       "L 264.198654 275.726373 \n",
       "L 267.448449 270.064191 \n",
       "L 270.698244 263.97971 \n",
       "L 274.354264 256.625404 \n",
       "L 278.010284 248.731993 \n",
       "L 281.666303 240.305549 \n",
       "L 285.728547 230.333383 \n",
       "L 289.790791 219.746862 \n",
       "L 294.25926 207.439555 \n",
       "L 299.133953 193.302612 \n",
       "L 304.41487 177.282232 \n",
       "L 310.91446 156.82484 \n",
       "L 330.007008 96.181126 \n",
       "L 334.475476 82.928011 \n",
       "L 338.53772 71.550023 \n",
       "L 342.19374 61.992985 \n",
       "L 345.443535 54.139863 \n",
       "L 348.287106 47.831154 \n",
       "L 351.130677 42.10392 \n",
       "L 353.568023 37.69716 \n",
       "L 356.00537 33.787395 \n",
       "L 358.036492 30.930683 \n",
       "L 360.067614 28.456601 \n",
       "L 362.098736 26.380784 \n",
       "L 363.723633 25.016738 \n",
       "L 365.348531 23.923922 \n",
       "L 366.973429 23.108808 \n",
       "L 368.598326 22.577336 \n",
       "L 370.223224 22.334881 \n",
       "L 371.848121 22.386223 \n",
       "L 373.066795 22.620055 \n",
       "L 374.691692 23.195132 \n",
       "L 376.31659 24.073812 \n",
       "L 377.941488 25.258359 \n",
       "L 379.566385 26.750296 \n",
       "L 381.191283 28.550384 \n",
       "L 382.81618 30.658592 \n",
       "L 384.847302 33.725784 \n",
       "L 386.878424 37.269561 \n",
       "L 388.909546 41.284362 \n",
       "L 390.940669 45.7625 \n",
       "L 393.378015 51.733849 \n",
       "L 395.815361 58.336935 \n",
       "L 398.252708 65.546038 \n",
       "L 401.096279 74.681992 \n",
       "L 403.93985 84.545733 \n",
       "L 407.189645 96.627596 \n",
       "L 410.845664 111.121811 \n",
       "L 414.907908 128.144667 \n",
       "L 420.188826 151.301474 \n",
       "L 436.031578 221.625691 \n",
       "L 440.093822 238.341929 \n",
       "L 443.749841 252.457611 \n",
       "L 446.999637 264.117142 \n",
       "L 449.843207 273.545875 \n",
       "L 452.686778 282.190997 \n",
       "L 455.124125 288.942775 \n",
       "L 457.561471 295.065592 \n",
       "L 459.998818 300.548238 \n",
       "L 462.02994 304.626275 \n",
       "L 464.061062 308.261859 \n",
       "L 466.092184 311.463371 \n",
       "L 468.123306 314.24382 \n",
       "L 470.154428 316.620791 \n",
       "L 472.18555 318.616286 \n",
       "L 474.216672 320.256481 \n",
       "L 476.247794 321.571371 \n",
       "L 478.278916 322.594284 \n",
       "L 480.310038 323.361267 \n",
       "L 482.34116 323.910311 \n",
       "L 484.778506 324.336386 \n",
       "L 487.622077 324.596809 \n",
       "L 491.684321 324.711564 \n",
       "L 495.340341 324.719999 \n",
       "L 495.340341 324.719999 \n",
       "\" clip-path=\"url(#p983ce27769)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 69.23125 339.84 \n",
       "L 69.23125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 515.63125 339.84 \n",
       "L 515.63125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 69.23125 339.84 \n",
       "L 515.63125 339.84 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 69.23125 7.2 \n",
       "L 515.63125 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p983ce27769\">\n",
       "   <rect x=\"69.23125\" y=\"7.2\" width=\"446.4\" height=\"332.64\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "parent_dir = str(Path().resolve().parents[3])\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Optional\n",
    "import rich\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "\n",
    "def find_root_dir(current_path: Path | None = None, marker: str = '.git') -> Path | None:\n",
    "    \"\"\"\n",
    "    Find the root directory by searching for a directory or file that serves as a\n",
    "    marker.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    current_path : Path | None\n",
    "        The starting path to search from. If None, the current working directory\n",
    "        `Path.cwd()` is used.\n",
    "    marker : str\n",
    "        The name of the file or directory that signifies the root.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path | None\n",
    "        The path to the root directory. Returns None if the marker is not found.\n",
    "    \"\"\"\n",
    "    if not current_path:\n",
    "        current_path = Path.cwd()\n",
    "    current_path = current_path.resolve()\n",
    "    for parent in [current_path, *current_path.parents]:\n",
    "        if (parent / marker).exists():\n",
    "            return parent\n",
    "    return None\n",
    "\n",
    "current_file_path = Path(\"__file__\")\n",
    "root_dir          = find_root_dir(current_file_path, marker='omnivault')\n",
    "\n",
    "if root_dir is not None:\n",
    "    sys.path.append(str(root_dir))\n",
    "    from omnivault.utils.visualization.style import use_svg_display\n",
    "    from omnivault.utils.reproducibility.seed import seed_all\n",
    "else:\n",
    "    raise ImportError(\"Root directory not found.\")\n",
    "\n",
    "use_svg_display()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "theta = np.arange(0, 1, 0.001)\n",
    "likelihood = theta**9 * (1-theta)**4\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(theta, likelihood)\n",
    "ax.set_xlabel(r'$\\theta$')\n",
    "ax.set_ylabel(r'$\\mathbb{P}(X ; \\theta)$')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e9462c",
   "metadata": {},
   "source": [
    "This has its maximum value somewhere near our expected $9/13 \\approx 0.7\\ldots$.\n",
    "To see if it is exactly there, we can turn to calculus. Notice that at the\n",
    "maximum, the gradient of the function is flat. Thus, we could find the maximum\n",
    "likelihood estimate by finding the values of $\\theta$ where the derivative is\n",
    "zero, and finding the one that gives the highest probability. We compute:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "0 & = \\frac{d}{d\\theta} \\mathbb{P}(X ; \\theta) \\\\\n",
    "& = \\frac{d}{d\\theta} \\theta^9(1-\\theta)^4 \\\\\n",
    "& = 9\\theta^8(1-\\theta)^4 - 4\\theta^9(1-\\theta)^3 \\\\\n",
    "& = \\theta^8(1-\\theta)^3(9-13\\theta).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This has three solutions: $0$, $1$ and $9/13$. The first two are clearly minima,\n",
    "not maxima as they assign probability $0$ to our sequence. The final value does\n",
    "_not_ assign zero probability to our sequence, and thus must be the maximum\n",
    "likelihood estimate $\\hat \\theta = 9/13$ {cite}`zhang2023dive`.\n",
    "\n",
    "We can justify this intuition by deriving the maximum likelihood estimate for\n",
    "$\\theta$ if we assume that the coin generator follows a Bernoulli distribution.\n",
    "The more generic case of the maximum likelihood estimate for $\\theta$ is given\n",
    "by the following.\n",
    "\n",
    "```{prf:definition} Maximum Likelihood Estimation for Bernoulli Distribution\n",
    ":label: def:maximum-likelihood-estimation-for-bernoulli-distribution\n",
    "\n",
    "The Maximum Likelihood estimate for a set of $\\textrm{i.i.d.}$ Bernoulli random variables $\\left\\{x^{(1)}, \\ldots, x^{(n)}\\right\\}$ with $x^{(n)} \\sim \\operatorname{Bernoulli}(\\theta)$ for $n=1, \\ldots, N$ is derived as follows.\n",
    "\n",
    "We know that the log-likelihood function of a set of i.i.d. Bernoulli random variables is given by\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\theta \\mid \\mathbf{x})=\\left(\\sum_{n=1}^{N} x^{(n)}\\right) \\cdot \\log \\theta+\\left(N-\\sum_{n=1}^{N} x^{(n)}\\right) \\cdot \\log (1-\\theta)\n",
    "$$\n",
    "\n",
    "Thus, to find the ML estimate, we need to solve the optimization problem\n",
    "\n",
    "$$\n",
    "\\widehat{\\theta}=\\underset{\\theta \\in \\Theta}{\\operatorname{argmax}}\\left\\{\\left(\\sum_{n=1}^{N} x^{(n)}\\right) \\cdot \\log \\theta+\\left(N-\\sum_{n=1}^{N} x^{(n)}\\right) \\cdot \\log (1-\\theta)\\right\\} .\n",
    "$$\n",
    "\n",
    "Taking the derivative with respect to $\\theta$ and setting it to zero, we obtain\n",
    "\n",
    "$$\n",
    "\\frac{d}{d \\theta}\\left\\{\\left(\\sum_{n=1}^{N} x^{(n)}\\right) \\cdot \\log \\theta+\\left(N-\\sum_{n=1}^{N} x^{(n)}\\right) \\cdot \\log (1-\\theta)\\right\\}=0 .\n",
    "$$\n",
    "\n",
    "This gives us\n",
    "\n",
    "$$\n",
    "\\frac{\\left(\\sum_{n=1}^{N} x^{(n)}\\right)}{\\theta}-\\frac{N-\\sum_{n=1}^{N} x^{(n)}}{1-\\theta}=0\n",
    "$$\n",
    "\n",
    "Rearranging the terms yields\n",
    "\n",
    "$$\n",
    "\\widehat{\\theta}=\\frac{1}{N} \\sum_{n=1}^{N} x^{(n)}\n",
    "$$\n",
    "```\n",
    "\n",
    "Indeed, since we have $9$ heads and $4$ tails, we have:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\theta} &= \\frac{1}{N} \\sum_{n=1}^{N} x^{(n)} \\\\\n",
    "&= \\frac{1}{13} \\sum_{n=1}^{13} x^{(n)} \\\\\n",
    "&= \\frac{1}{13} \\cdot 9 \\\\\n",
    "&= \\frac{9}{13}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "since there are $9$ heads and $4$ tails, resulting in a sum of $9$ when you sum\n",
    "up the $x^{(n)}$'s. Thus, the maximum likelihood estimate for $\\theta$ is\n",
    "$\\frac{9}{13}$.\n",
    "\n",
    "## Visualizing Likelihood and Maximum Likelihood Estimation as $N$ Increases\n",
    "\n",
    "Read section 8.1.1 aznd 8.1.2 of Introduction to Probability for Data Science\n",
    "written by Stanley H. Chan {cite}`chan_2021` for more details.\n",
    "\n",
    "## Numerical Optimization and the Negative Log-Likelihood\n",
    "\n",
    "**_The following section is adapted from section 22.7 from Dive Into Deep\n",
    "Learning, {cite}`zhang2023dive`._**\n",
    "\n",
    "The example on coin toss is nice, but what if we have billions of parameters and\n",
    "data examples?\n",
    "\n",
    "### Numerical Underflow\n",
    "\n",
    "First, notice that if we make the assumption that all the data examples are\n",
    "independent, we can no longer practically consider the likelihood itself as it\n",
    "is a product of many probabilities. Indeed, each probability is in $[0,1]$, say\n",
    "typically of value about $1/2$, and the product of $(1/2)^{1000000000}$ is far\n",
    "below machine precision. We cannot work with that directly.\n",
    "\n",
    "Let's check the smallest representable positive number greater than zero for the\n",
    "`float32` data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b49f8e72",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1920929e-07\n"
     ]
    }
   ],
   "source": [
    "print(np.finfo(np.float32).eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b140e9",
   "metadata": {},
   "source": [
    "Let's cook up a simple example to illustrate this. We will generate a random\n",
    "sequence of $1000000000$ coin tosses, and compute the likelihood of the sequence\n",
    "given that the coin is fair. We will then compute the log-likelihood of the\n",
    "sequence given that the coin is fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c41ca491",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/runner/work/omniverse/omniverse/omnivault/utils/reproducibility/seed.py:120: UserWarning: Deterministic mode is activated. This will negatively impact performance and may cause increase in CUDA memory footprint.\n",
      "  configure_deterministic_mode()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "seed_all(42)\n",
    "\n",
    "N = 1000000000\n",
    "X = np.random.randint(0, 2, size=N)\n",
    "theta = 0.5\n",
    "likelihood = (theta ** np.sum(X)) * ((1 - theta) ** (N - np.sum(X)))\n",
    "print(f\"Likelihood: {likelihood}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e7f244",
   "metadata": {},
   "source": [
    "So the likelihood is $0$ because the $1000000000$ when multiplied is less than\n",
    "the smallest representable positive number greater than zero for the `float32`\n",
    "data type.\n",
    "\n",
    "However, recall that the logarithm turns products to sums, in which case\n",
    "\n",
    "$$\n",
    "\\log\\left(\\left(1/2\\right)^{1000000000}\\right) = 1000000000\\cdot\\log(1/2) \\approx -301029995.6\\ldots\n",
    "$$\n",
    "\n",
    "This number fits perfectly within even a single precision $32$-bit float. Thus,\n",
    "we should consider the _log-likelihood_, which is\n",
    "\n",
    "$$\n",
    "\\log(\\mathbb{P}(X ; \\boldsymbol{\\theta})).\n",
    "$$\n",
    "\n",
    "Since the function $x \\mapsto \\log(x)$ is increasing, maximizing the likelihood\n",
    "is the same thing as maximizing the log-likelihood.\n",
    "\n",
    "We often work with loss functions, where we wish to minimize the loss. We may\n",
    "turn maximum likelihood into the minimization of a loss by taking\n",
    "$-\\log(\\mathbb{P}(X ; \\boldsymbol{\\theta}))$, which is the _negative\n",
    "log-likelihood_.\n",
    "\n",
    "To illustrate this, consider the coin flipping problem from before, and pretend\n",
    "that we do not know the closed form solution. We may compute that\n",
    "\n",
    "$$\n",
    "-\\log(\\mathbb{P}(X ; \\boldsymbol{\\theta})) = -\\log\\left(\\theta^{x^{(n)}}(1-\\theta)^{1-x^{(n)}}\\right) = -\\left(n_H\\log(\\theta) + n_T\\log(1-\\theta)\\right)\n",
    "$$\n",
    "\n",
    "where $n_H$ is the number of heads and $n_T$ is the number of tails. This form\n",
    "is just like in\n",
    "{prf:ref}`def:maximum-likelihood-estimation-for-bernoulli-distribution`.\n",
    "\n",
    "This can be written into code, and freely optimized even for billions of coin\n",
    "flips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a358f79f",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated theta: 0.9713101437659449\n",
      "Empirical theta: 0.9713101437890875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set up our data\n",
    "n_H = 8675309\n",
    "n_T = 256245\n",
    "\n",
    "# Initialize our parameters\n",
    "theta = np.array(0.5)\n",
    "\n",
    "# Perform gradient descent\n",
    "lr = 1e-9\n",
    "for iter in range(100):\n",
    "    # Compute the gradient of the loss function with respect to theta\n",
    "    grad_loss = -(n_H / theta - n_T / (1 - theta))\n",
    "\n",
    "    # Update theta using the gradient\n",
    "    theta -= lr * grad_loss\n",
    "\n",
    "# Check output\n",
    "print(f\"Estimated theta: {theta}\")\n",
    "print(f\"Empirical theta: {n_H / (n_H + n_T)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eabdcc4",
   "metadata": {},
   "source": [
    "### Mathematical Convenience\n",
    "\n",
    "Numerical convenience is not the only reason why people like to use negative\n",
    "log-likelihoods. There are several other reasons why it is preferable.\n",
    "\n",
    "The second reason we consider the log-likelihood is the simplified application\n",
    "of calculus rules. As discussed above, due to independence assumptions, most\n",
    "probabilities we encounter in machine learning are products of individual\n",
    "probabilities.\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(X ; \\boldsymbol{\\theta}) = p(x_1\\mid\\boldsymbol{\\theta})\\cdot p(x_2\\mid\\boldsymbol{\\theta})\\cdots p(x_n\\mid\\boldsymbol{\\theta}).\n",
    "$$\n",
    "\n",
    "This means that if we directly apply the product rule to compute a derivative we\n",
    "get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\mathbb{P}(X ; \\boldsymbol{\\theta}) & = \\left(\\frac{\\partial}{\\partial \\boldsymbol{\\theta}}\\mathbb{P}(x_1\\mid\\boldsymbol{\\theta})\\right)\\cdot \\mathbb{P}(x_2\\mid\\boldsymbol{\\theta})\\cdots \\mathbb{P}(x_n\\mid\\boldsymbol{\\theta}) \\\\\n",
    "& \\quad + \\mathbb{P}(x_1\\mid\\boldsymbol{\\theta})\\cdot \\left(\\frac{\\partial}{\\partial \\boldsymbol{\\theta}}\\mathbb{P}(x_2\\mid\\boldsymbol{\\theta})\\right)\\cdots \\mathbb{P}(x_n\\mid\\boldsymbol{\\theta}) \\\\\n",
    "& \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\vdots \\\\\n",
    "& \\quad + \\mathbb{P}(x_1\\mid\\boldsymbol{\\theta})\\cdot \\mathbb{P}(x_2\\mid\\boldsymbol{\\theta}) \\cdots \\left(\\frac{\\partial}{\\partial \\boldsymbol{\\theta}}\\mathbb{P}(x_n\\mid\\boldsymbol{\\theta})\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This requires $n(n-1)$ multiplications, along with $(n-1)$ additions, so it is\n",
    "proportional to quadratic time in the inputs! Sufficient cleverness in grouping\n",
    "terms will reduce this to linear time, but it requires some thought. For the\n",
    "negative log-likelihood we have instead\n",
    "\n",
    "$$\n",
    "-\\log\\left(\\mathbb{P}(X ; \\boldsymbol{\\theta})\\right) = -\\log(\\mathbb{P}(x_1\\mid\\boldsymbol{\\theta})) - \\log(\\mathbb{P}(x_2\\mid\\boldsymbol{\\theta})) \\cdots - \\log(\\mathbb{P}(x_n\\mid\\boldsymbol{\\theta})),\n",
    "$$\n",
    "\n",
    "which then gives\n",
    "\n",
    "$$\n",
    "- \\frac{\\partial}{\\partial \\boldsymbol{\\theta}} \\log\\left(\\mathbb{P}(X ; \\boldsymbol{\\theta})\\right) = \\frac{1}{\\mathbb{P}(x_1\\mid\\boldsymbol{\\theta})}\\left(\\frac{\\partial}{\\partial \\boldsymbol{\\theta}}\\mathbb{P}(x_1\\mid\\boldsymbol{\\theta})\\right) + \\cdots + \\frac{1}{\\mathbb{P}(x_n\\mid\\boldsymbol{\\theta})}\\left(\\frac{\\partial}{\\partial \\boldsymbol{\\theta}}\\mathbb{P}(x_n\\mid\\boldsymbol{\\theta})\\right).\n",
    "$$\n",
    "\n",
    "This requires only $n$ divides and $n-1$ sums, and thus is linear time in the\n",
    "inputs.\n",
    "\n",
    "### Information Theory\n",
    "\n",
    "The third and final reason to consider the negative log-likelihood is the\n",
    "relationship to information theory. We will discuss this separately in the\n",
    "section on information theory. This is a rigorous mathematical theory which\n",
    "gives a way to measure the degree of information or randomness in a random\n",
    "variable. The key object of study in that field is the entropy which is\n",
    "\n",
    "$$\n",
    "H(p) = -\\sum_{i} p_i \\log_2(p_i),\n",
    "$$\n",
    "\n",
    "which measures the randomness of a source. Notice that this is nothing more than\n",
    "the average $-\\log$ probability, and thus if we take our negative log-likelihood\n",
    "and divide by the number of data examples, we get a relative of entropy known as\n",
    "cross-entropy. This theoretical interpretation alone would be sufficiently\n",
    "compelling to motivate reporting the average negative log-likelihood over the\n",
    "dataset as a way of measuring model performance.\n",
    "\n",
    "## Maximum Likelihood for Continuous Variables\n",
    "\n",
    "**_The following section is adapted from section 22.7 from Dive Into Deep\n",
    "Learning, {cite}`zhang2023dive`._**\n",
    "\n",
    "Everything that we have done so far assumes we are working with discrete random\n",
    "variables, but what if we want to work with continuous ones?\n",
    "\n",
    "The short summary is that nothing at all changes, except we replace all the\n",
    "instances of the probability with the probability density. Recalling that we\n",
    "write densities with lower case $p$, this means that for example we now say\n",
    "\n",
    "$$\n",
    "-\\log\\left(p(X ; \\boldsymbol{\\theta})\\right) = -\\log(p(x^{(1)} ; \\boldsymbol{\\theta})) - \\log(p(x^{(2)} ; \\boldsymbol{\\theta})) \\cdots - \\log(p(x_n ; \\boldsymbol{\\theta})) = -\\sum_i \\log(p(x^{(n)} ; \\theta)).\n",
    "$$\n",
    "\n",
    "The question becomes, \"Why is this OK?\" After all, the reason we introduced\n",
    "densities was because probabilities of getting specific outcomes themselves was\n",
    "zero, and thus is not the probability of generating our data for any set of\n",
    "parameters zero?\n",
    "\n",
    "Indeed, this is the case, and understanding why we can shift to densities is an\n",
    "exercise in tracing what happens to the epsilons.\n",
    "\n",
    "Let's first re-define our goal. Suppose that for continuous random variables we\n",
    "no longer want to compute the probability of getting exactly the right value,\n",
    "but instead matching to within some range $\\epsilon$. For simplicity, we assume\n",
    "our data is repeated observations $x^{(1)}, \\ldots, x^{(N)}$ of identically\n",
    "distributed random variables $X^{(1)}, \\ldots, X^{(N)}$. As we have seen\n",
    "previously, this can be written as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&P(X^{(1)} \\in [x^{(1)}, x^{(1)}+\\epsilon], X^{(2)} \\in [x^{(2)}, x^{(2)}+\\epsilon], \\ldots, X^{(N)} \\in [x^{(N)}, x^{(N)}+\\epsilon] ;\\boldsymbol{\\theta}) \\\\\n",
    "\\approx &\\epsilon^Np(x^{(1)} ; \\boldsymbol{\\theta})\\cdot p(x^{(2)} ; \\boldsymbol{\\theta}) \\cdots p(x_n ;\\boldsymbol{\\theta}).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus, if we take negative logarithms of this we obtain\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&-\\log(P(X^{(1)} \\in [x^{(1)}, x^{(1)}+\\epsilon], X^{(2)} \\in [x^{(2)}, x^{(2)}+\\epsilon], \\ldots, X^{(N)} \\in [x^{(N)}, x^{(N)}+\\epsilon] ; \\boldsymbol{\\theta})) \\\\\n",
    "\\approx & -N\\log(\\epsilon) - \\sum_{i} \\log(p(x^{(n)} ; \\boldsymbol{\\theta})).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If we examine this expression, the only place that the $\\epsilon$ occurs is in\n",
    "the additive constant $-N\\log(\\epsilon)$. This does not depend on the parameters\n",
    "$\\boldsymbol{\\theta}$ at all, so the optimal choice of $\\boldsymbol{\\theta}$\n",
    "does not depend on our choice of $\\epsilon$! If we demand four digits or\n",
    "four-hundred, the best choice of $\\boldsymbol{\\theta}$ remains the same, thus we\n",
    "may freely drop the epsilon to see that what we want to optimize is\n",
    "\n",
    "$$\n",
    "- \\sum_{i} \\log(p(x^{(n)} ; \\boldsymbol{\\theta})).\n",
    "$$\n",
    "\n",
    "Thus, we see that the maximum likelihood point of view can operate with\n",
    "continuous random variables as easily as with discrete ones by replacing the\n",
    "probabilities with probability densities.\n",
    "\n",
    "(estimation-theory-mle-common-distributions)=\n",
    "\n",
    "## Maximum Likelihood Estimation for Common Distributions\n",
    "\n",
    "### Maximum Likelihood for Univariate Gaussian\n",
    "\n",
    "Suppose that we are given a set of i.i.d. univariate Gaussian random variables\n",
    "$X^{(1)}, \\ldots, X^{(N)}$, where both the mean $\\mu$ and the variance\n",
    "$\\sigma^2$ are unknown.\n",
    "\n",
    "Let $\\boldsymbol{\\theta}=\\left[\\mu, \\sigma^2\\right]^T$ be the parameter. Find\n",
    "the maximum likelihood estimate of $\\boldsymbol{\\theta}$.\n",
    "\n",
    "**First**, we define the likelihood and log-likelihood functions. Since the\n",
    "random variables are i.i.d., the likelihood function is given by:\n",
    "\n",
    "```{math}\n",
    ":label: eq:gaussian-likelihood-1\n",
    "\n",
    "\\begin{aligned}\n",
    "\\overbrace{\\mathcal{L}\\left(\\boldsymbol{\\theta} \\mid \\mathcal{S} = \\left\\{X^{(1)}, \\ldots, X^{(N)}\\right\\}\\right)}^{\\mathbb{P}\\left(\\mathcal{S} = \\left\\{X^{(1)}, \\ldots, X^{(N)}\\right\\} ; \\boldsymbol{\\theta}\\right)} &= \\prod_{n=1}^N \\overbrace{f_{\\boldsymbol{\\theta}}\\left(x^{(n)}\\right)}^{\\mathcal{N}\\left(x^{(n)} ; \\mu, \\sigma^2\\right) \\\\} \\\\\n",
    "&= \\prod_{n=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{\\left(x^{(n)} - \\mu\\right)^2}{2\\sigma^2}\\right).\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "The log-likelihood function is given by:\n",
    "\n",
    "```{math}\n",
    ":label: eq:gaussian-likelihood-2\n",
    "\n",
    "\\begin{aligned}\n",
    "\\overbrace{\\log\\mathcal{L}\\left(\\boldsymbol{\\theta} \\mid \\mathcal{S} = \\left\\{X^{(1)}, \\ldots, X^{(N)}\\right\\}\\right)}^{\\log\\mathbb{P}\\left(\\mathcal{S} = \\left\\{X^{(1)}, \\ldots, X^{(N)}\\right\\} ; \\boldsymbol{\\theta}\\right)} &= \\log\\left(\\prod_{n=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{\\left(x^{(n)} - \\mu\\right)^2}{2\\sigma^2}\\right)\\right) \\\\\n",
    "&= \\sum_{n=1}^N \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{\\left(x^{(n)} - \\mu\\right)^2}{2\\sigma^2}\\right)\\right) &&(*)\\\\\n",
    "&= \\sum_{n=1}^N \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\sum_{n=1}^N \\log\\left(\\exp\\left(-\\frac{\\left(x^{(n)} - \\mu\\right)^2}{2\\sigma^2}\\right)\\right) &&(**)\\\\\n",
    "&= \\sum_{n=1}^N \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\sum_{n=1}^N -\\frac{\\left(x^{(n)} - \\mu\\right)^2}{2\\sigma^2} \\cdot \\underbrace{\\log(e)}_{1} &&(***)\\\\\n",
    "&= \\left(\\sum_{n=1}^N -\\frac{1}{2} \\log(2\\pi\\sigma^2)\\right) - \\sum_{n=1}^N \\frac{\\left(x^{(n)} - \\mu\\right)^2}{2\\sigma^2} &&(****)\\\\\n",
    "&= -\\frac{N}{2} \\log(2\\pi\\sigma^2) - \\sum_{n=1}^N \\frac{\\left(x^{(n)} - \\mu\\right)^2}{2\\sigma^2}.\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "where\n",
    "\n",
    "-   $(*)$ is the log-product rule, meaning that the logarithm of the product of\n",
    "    $N$ terms is the sum of the logarithms of the $N$ terms.\n",
    "-   $(**)$ is again the log-product rule.\n",
    "-   $(***)$ is the log-exponential rule, meaning that the logarithm of the\n",
    "    exponential of a term is the term multiplied by the logarithm of $e$.\n",
    "-   $(****)$ is just writing\n",
    "    $\\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) = \\log\\left(\\sqrt{2\\pi\\sigma^2}^{-\\frac{1}{2}}\\right)$\n",
    "    and therefore the log of it is just $-\\frac{1}{2} \\log(2\\pi\\sigma^2)$.\n",
    "\n",
    "Then, we take the derivative of the log-likelihood function with respect to\n",
    "$\\mu$ and $\\sigma^2$ and set them to zero to find the maximum likelihood\n",
    "estimates.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\mu} \\log \\mathcal{L}\\left(\\boldsymbol{\\theta} \\mid \\mathcal{S}=\\left\\{X^{(1)}, \\ldots, X^{(N)}\\right\\}\\right) & =\\frac{\\partial}{\\partial \\mu}\\left(-\\frac{N}{2} \\log \\left(2 \\pi \\sigma^2\\right)-\\sum_{n=1}^N \\frac{\\left(x^{(n)}-\\mu\\right)^2}{2 \\sigma^2}\\right) \\\\\n",
    "& =0-\\frac{\\partial}{\\partial \\mu}\\left(\\sum_{n=1}^N \\frac{\\left(x^{(n)}-\\mu\\right)^2}{2 \\sigma^2}\\right) \\\\\n",
    "& =-\\frac{1}{2 \\sigma^2} \\sum_{n=1}^N \\frac{\\partial}{\\partial \\mu}\\left(\\left(x^{(n)}-\\mu\\right)^2\\right) \\\\\n",
    "& =-\\frac{1}{2 \\sigma^2} \\sum_{n=1}^N 2\\left(x^{(n)}-\\mu\\right)(-1) \\\\\n",
    "& =\\frac{1}{\\sigma^2}\\sum_{n=1}^N \\left(x^{(n)}- \\mu\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So the partial derivative of the log-likelihood function with respect to $\\mu$\n",
    "is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mu} \\log \\mathcal{L}\\left(\\boldsymbol{\\theta} \\mid \\mathcal{S}=\\left\\{X^{(1)}, \\ldots, X^{(N)}\\right\\}\\right)=\\frac{1}{\\sigma^2}\\sum_{n=1}^N \\left(x^{(n)}- \\mu\\right)\n",
    "$$\n",
    "\n",
    "and setting it to zero gives:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{1}{\\sigma^2}\\sum_{n=1}^N \\left(x^{(n)}- \\mu\\right) = 0 &\\iff \\sum_{n=1}^N x^{(n)}- \\mu = 0 \\\\\n",
    "&\\iff \\sum_{n=1}^N x^{(n)} = \\mu N \\\\\n",
    "&\\iff \\mu = \\frac{1}{N}\\sum_{n=1}^N x^{(n)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "resulting in the maximum likelihood estimate for $\\mu$ to be:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{N}\\sum_{n=1}^N x^{(n)}.\n",
    "$$\n",
    "\n",
    "Similarly, we have for $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\sigma^2} \\log \\mathcal{L}\\left(\\boldsymbol{\\theta} \\mid \\mathcal{S}=\\left\\{X^{(1)}, \\ldots, X^{(N)}\\right\\}\\right) & =\\frac{\\partial}{\\partial \\sigma^2}\\left(-\\frac{N}{2} \\log \\left(2 \\pi \\sigma^2\\right)-\\sum_{n=1}^N \\frac{\\left(x^{(n)}-\\mu\\right)^2}{2 \\sigma^2}\\right) \\\\\n",
    "& =-\\frac{N}{2} \\frac{\\partial}{\\partial \\sigma^2}\\left(\\log \\left(2 \\pi \\sigma^2\\right)\\right)-\\sum_{n=1}^N \\frac{\\partial}{\\partial \\sigma^2}\\left(\\frac{\\left(x^{(n)}-\\mu\\right)^2}{2 \\sigma^2}\\right) \\\\\n",
    "& =-\\frac{N}{2} \\frac{1}{\\sigma^2}-\\sum_{n=1}^N \\frac{-1}{2\\left(\\sigma^2\\right)^2}\\left(x^{(n)}-\\mu\\right)^2 \\\\\n",
    "& =-\\frac{N}{2 \\sigma^2}+\\frac{1}{2\\left(\\sigma^2\\right)^2} \\sum_{n=1}^N\\left(x^{(n)}-\\mu\\right)^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So, the partial derivative with respect to $\\sigma^2$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\sigma^2} \\log \\mathcal{L}\\left(\\boldsymbol{\\theta} \\mid \\mathcal{S}=\\left\\{X^{(1)}, \\ldots, X^{(N)}\\right\\}\\right)=-\\frac{N}{2 \\sigma^2}+\\frac{1}{2\\left(\\sigma^2\\right)^2} \\sum_{n=1}^N\\left(x^{(n)}-\\mu\\right)^2\n",
    "$$\n",
    "\n",
    "and setting it to zero gives:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-\\frac{N}{2 \\sigma^2}+\\frac{1}{2\\left(\\sigma^2\\right)^2} \\sum_{n=1}^N\\left(x^{(n)}-\\mu\\right)^2 = 0 &\\iff \\frac{N}{2 \\sigma^2} = \\frac{1}{2\\left(\\sigma^2\\right)^2} \\sum_{n=1}^N\\left(x^{(n)}-\\mu\\right)^2 \\\\\n",
    "&\\iff \\sigma^2 = \\frac{1}{N}\\sum_{n=1}^N\\left(x^{(n)}-\\mu\\right)^2.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "resulting in the maximum likelihood estimate for $\\sigma^2$ to be:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{N}\\sum_{n=1}^N\\left(x^{(n)}-\\hat{\\mu}\\right)^2.\n",
    "$$\n",
    "\n",
    "Note in particular that we placed $\\mu$ by $\\hat{\\mu}$.\n",
    "\n",
    "Overall, the maximum likelihood estimate for the parameters of a Gaussian\n",
    "distribution is:\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\theta}} = \\begin{bmatrix} \\hat{\\mu} \\\\ \\hat{\\sigma}^2 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{N}\\sum_{n=1}^N x^{(n)} \\\\ \\frac{1}{N}\\sum_{n=1}^N\\left(x^{(n)}-\\hat{\\mu}\\right)^2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Maximum Likelihood Estimation for Multivariate Gaussian\n",
    "\n",
    "See\n",
    "[my proof on multiple linear regression](../../../machine_learning/linear_models/linear_regression/concept.md),\n",
    "they have similar vein of logic. See\n",
    "[here](https://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian)\n",
    "also.\n",
    "\n",
    "Suppose that we are given a set of $\\textrm{i.i.d.}$ $D$-dimensional Gaussian\n",
    "random vectors $\\mathbf{X}^{(1)}, \\ldots, \\mathbf{X}^{(N)}$ such that:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^{(n)} = \\begin{bmatrix} X^{(n)}_1 \\\\ \\vdots \\\\ X^{(n)}_D \\end{bmatrix} \\sim \\mathcal{N}\\left(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}\\right)\n",
    "$$\n",
    "\n",
    "where the mean vector $\\boldsymbol{\\mu}$ and the covariance matrix\n",
    "$\\boldsymbol{\\Sigma}$ are given by\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu} = \\begin{bmatrix} \\mu_1 \\\\ \\vdots \\\\ \\mu_D \\end{bmatrix}, \\quad \\boldsymbol{\\Sigma} = \\begin{bmatrix} \\sigma_1^2 & \\cdots & \\sigma_{1D} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\sigma_{D1} & \\cdots & \\sigma_D^2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now the $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$ are unknown, and we want to\n",
    "find the maximum likelihood estimate for them.\n",
    "\n",
    "As usual, we find the likelihood function for the parameters $\\boldsymbol{\\mu}$\n",
    "and $\\boldsymbol{\\Sigma}$, and then find the maximum likelihood estimate for\n",
    "them.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}\\left(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma} \\mid \\mathcal{S}=\\left\\{\\mathbf{X}^{(1)}, \\ldots, \\mathbf{X}^{(N)}\\right\\}\\right) & =\\prod_{n=1}^N f_{\\mathbf{X}^{(n)}}\\left(\\mathbf{x}^{(n)} ; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}\\right) \\\\\n",
    "& =\\prod_{n=1}^N \\frac{1}{\\sqrt{(2 \\pi)^{D}|\\boldsymbol{\\Sigma}|}} \\exp \\left\\{-\\frac{1}{2}\\left(\\mathbf{x}^{(n)}-\\boldsymbol{\\mu}\\right)^{T} \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}^{(n)}-\\boldsymbol{\\mu}\\right)\\right\\} \\\\\n",
    "& =\\left(\\frac{1}{\\sqrt{(2 \\pi)^{D}|\\boldsymbol{\\Sigma}|}}\\right)^N \\exp \\left\\{-\\frac{1}{2}\\sum_{n=1}^N\\left(\\mathbf{x}^{(n)}-\\boldsymbol{\\mu}\\right)^{T} \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}^{(n)}-\\boldsymbol{\\mu}\\right)\\right\\} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and consequently the log-likelihood function is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log \\mathcal{L}\\left(\\boldsymbol{\\mu}, \\mathbf{\\Sigma} \\mid \\mathcal{S}=\\left\\{\\mathbf{x}^{(1)}, \\ldots, \\mathbf{X}^{(N)}\\right\\}\\right) & =\\log \\left(\\prod_{n=1}^N \\frac{1}{\\sqrt{(2 \\pi)^D|\\boldsymbol{\\Sigma}|}} \\exp \\left\\{-\\frac{1}{2}\\left(\\mathbf{x}^{(n)}-\\boldsymbol{\\mu}\\right)^T \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}^{(n)}-\\boldsymbol{\\mu}\\right)\\right\\}\\right) \\\\\n",
    "& =\\sum_{n=1}^N \\log \\left(\\frac{1}{\\sqrt{(2 \\pi)^D|\\mathbf{\\Sigma}|}}\\right)+\\sum_{n=1}^N \\log \\left(\\exp \\left\\{-\\frac{1}{2}\\left(\\mathbf{x}^{(n)}-\\boldsymbol{\\mu}\\right)^T \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}^{(n)}-\\boldsymbol{\\mu}\\right)\\right\\}\\right) \\\\\n",
    "& =\\sum_{n=1}^N\\left(-\\frac{D}{2} \\log (2 \\pi)-\\frac{1}{2} \\log (|\\mathbf{\\Sigma}|)-\\frac{1}{2}\\left(\\mathbf{x}^{(n)}-\\boldsymbol{\\mu}\\right)^T \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}^{(n)}-\\boldsymbol{\\mu}\\right)\\right) \\\\\n",
    "& =-\\frac{N D}{2} \\log (2 \\pi)-\\frac{N}{2} \\log (|\\boldsymbol{\\Sigma}|)-\\frac{1}{2} \\sum_{n=1}^N\\left(\\mathbf{x}^{(n)}-\\boldsymbol{\\mu}\\right)^T \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}^{(n)}-\\boldsymbol{\\mu}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Finding the ML estimate requires taking the derivative with respect to both\n",
    "$\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$ :\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{d}{d \\boldsymbol{\\mu}}\\left\\{-\\frac{N D}{2} \\log (2 \\pi)-\\frac{N}{2} \\log (|\\boldsymbol{\\Sigma}|)-\\frac{1}{2} \\sum_{n=1}^N\\left(\\mathbf{x}^{(n)}-\\boldsymbol{\\mu}\\right)^T \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}^{(n)}-\\boldsymbol{\\mu}\\right)\\right\\}=0 \\\\\n",
    "& \\frac{d}{d \\boldsymbol{\\Sigma}}\\left\\{-\\frac{N D}{2} \\log (2 \\pi)-\\frac{N}{2} \\log (|\\boldsymbol{\\Sigma}|)-\\frac{1}{2} \\sum_{n=1}^N\\left(\\mathbf{x}^{(n)}-\\boldsymbol{\\mu}\\right)^T \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}^{(n)}-\\boldsymbol{\\mu}\\right)\\right\\}=0 .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "After some tedious algebraic steps (see Duda et al., Pattern Classification,\n",
    "Problem 3.14), we have that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\widehat{\\boldsymbol{\\mu}}=\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}^{(n)}, \\\\\n",
    "& \\widehat{\\boldsymbol{\\Sigma}}=\\frac{1}{N} \\sum_{n=1}^{N}\\left(\\mathbf{x}^{(n)}-\\widehat{\\boldsymbol{\\mu}}\\right)\\left(\\mathbf{x}^{(n)}-\\widehat{\\boldsymbol{\\mu}}\\right)^{T} .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## References and Further Readings\n",
    "\n",
    "-   Chan, Stanley H. \"Chapter 8.1. Maximum-Likelihood Estimation.\" In\n",
    "    Introduction to Probability for Data Science. Ann Arbor, Michigan: Michigan\n",
    "    Publishing Services, 2021."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "mystnb": {
   "number_source_lines": true
  },
  "source_map": [
   16,
   526,
   601,
   703,
   707,
   714,
   726,
   767,
   791
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}