{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c21f97",
   "metadata": {},
   "source": [
    "# Concept: K-Means Clustering\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Organized_Chaos-orange)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7707f0e8",
   "metadata": {
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def find_root_dir(current_path: Path | None = None, marker: str = '.git') -> Path | None:\n",
    "    \"\"\"\n",
    "    Find the root directory by searching for a directory or file that serves as a\n",
    "    marker.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    current_path : Path | None\n",
    "        The starting path to search from. If None, the current working directory\n",
    "        `Path.cwd()` is used.\n",
    "    marker : str\n",
    "        The name of the file or directory that signifies the root.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path | None\n",
    "        The path to the root directory. Returns None if the marker is not found.\n",
    "    \"\"\"\n",
    "    if not current_path:\n",
    "        current_path = Path.cwd()\n",
    "    current_path = current_path.resolve()\n",
    "    for parent in [current_path, *current_path.parents]:\n",
    "        if (parent / marker).exists():\n",
    "            return parent\n",
    "    return None\n",
    "\n",
    "root_dir = find_root_dir(marker='omnivault')\n",
    "\n",
    "if root_dir is not None:\n",
    "    sys.path.append(str(root_dir))\n",
    "    from omnivault.utils.visualization.style import use_svg_display\n",
    "else:\n",
    "    raise ImportError(\"Root directory not found.\")\n",
    "\n",
    "use_svg_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3200fc",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "\n",
    "Let's first look at an example by randomly generating data points[^y] that can\n",
    "be partitioned into 3 distinct clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "257c2c13",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"451.442625pt\" height=\"335.86825pt\" viewBox=\"0 0 451.442625 335.86825\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-09-03T12:54:49.393632</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.0, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 335.86825 \n",
       "L 451.442625 335.86825 \n",
       "L 451.442625 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 34.240625 298.312 \n",
       "L 444.242625 298.312 \n",
       "L 444.242625 7.2 \n",
       "L 34.240625 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"m3d4ca98cc4\" d=\"M 0 3.535534 \n",
       "C 0.937635 3.535534 1.836992 3.163008 2.5 2.5 \n",
       "C 3.163008 1.836992 3.535534 0.937635 3.535534 0 \n",
       "C 3.535534 -0.937635 3.163008 -1.836992 2.5 -2.5 \n",
       "C 1.836992 -3.163008 0.937635 -3.535534 0 -3.535534 \n",
       "C -0.937635 -3.535534 -1.836992 -3.163008 -2.5 -2.5 \n",
       "C -3.163008 -1.836992 -3.535534 -0.937635 -3.535534 0 \n",
       "C -3.535534 0.937635 -3.163008 1.836992 -2.5 2.5 \n",
       "C -1.836992 3.163008 -0.937635 3.535534 0 3.535534 \n",
       "z\n",
       "\" style=\"stroke: #000000\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#pdb264f7e00)\">\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"384.287709\" y=\"215.736672\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"250.836062\" y=\"115.138302\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"268.215886\" y=\"63.657528\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"250.588798\" y=\"58.103823\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"385.156142\" y=\"257.535041\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"319.022014\" y=\"38.460141\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"328.759844\" y=\"32.029001\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"369.4944\" y=\"270.471194\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"79.977449\" y=\"146.401169\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"327.326413\" y=\"21.754494\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"418.526779\" y=\"199.325992\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"192.41827\" y=\"159.726953\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"97.729051\" y=\"143.762398\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"263.009087\" y=\"77.976333\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"226.936023\" y=\"56.275988\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"87.845689\" y=\"144.070199\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"121.395676\" y=\"120.150169\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"337.002186\" y=\"237.287339\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"113.971931\" y=\"141.366045\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"347.622426\" y=\"259.658737\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"257.767899\" y=\"76.084871\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"373.73282\" y=\"211.684001\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"289.49441\" y=\"87.997694\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"284.053118\" y=\"32.392383\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"135.024001\" y=\"148.062597\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"365.732562\" y=\"235.653343\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"368.655719\" y=\"209.306399\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"294.4068\" y=\"104.429662\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"52.87708\" y=\"84.462011\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"311.94727\" y=\"242.067495\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"113.873129\" y=\"106.806603\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"99.020641\" y=\"139.923883\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"143.563455\" y=\"145.428057\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"103.325411\" y=\"122.594489\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"239.085528\" y=\"48.695121\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"325.07873\" y=\"194.352024\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"409.008229\" y=\"209.695718\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"328.417626\" y=\"215.226727\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"258.995664\" y=\"76.161666\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"290.888299\" y=\"50.403414\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"131.095932\" y=\"200.980479\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"108.301808\" y=\"174.565221\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"238.516057\" y=\"44.072371\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"337.491122\" y=\"264.641646\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"354.252653\" y=\"216.224039\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"376.470463\" y=\"247.203312\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"140.511963\" y=\"117.457176\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"352.484847\" y=\"102.457338\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"139.702142\" y=\"145.290769\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"310.001632\" y=\"71.069729\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"334.878399\" y=\"186.974921\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"244.94592\" y=\"42.023518\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"286.120881\" y=\"43.897914\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"335.571648\" y=\"249.526154\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"370.529812\" y=\"221.705865\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"123.287995\" y=\"128.59978\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"262.97197\" y=\"95.86349\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"410.791142\" y=\"195.857555\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"140.456574\" y=\"102.833077\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"302.887182\" y=\"64.316737\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"67.291098\" y=\"119.489523\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"181.215176\" y=\"124.795899\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"108.241505\" y=\"131.846458\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"117.701747\" y=\"125.059466\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"285.127783\" y=\"77.102078\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"190.98224\" y=\"135.210603\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"283.501615\" y=\"57.740402\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"321.162117\" y=\"185.510307\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"197.229156\" y=\"51.679014\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"224.487088\" y=\"20.432364\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"316.096047\" y=\"97.168937\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"309.43733\" y=\"211.181542\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"379.381343\" y=\"235.255008\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"241.650359\" y=\"20.604698\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"315.59228\" y=\"215.102401\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"269.386389\" y=\"66.070808\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"317.363748\" y=\"71.757882\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"92.065532\" y=\"169.516752\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"127.467764\" y=\"119.526512\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"278.508758\" y=\"57.105849\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"327.178651\" y=\"230.754285\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"323.023353\" y=\"248.131304\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"245.66058\" y=\"101.62995\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"293.705031\" y=\"59.386014\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"358.488035\" y=\"199.5703\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"425.60617\" y=\"208.769878\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"378.531684\" y=\"223.843344\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"100.773664\" y=\"124.170639\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"84.600466\" y=\"105.017932\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"369.014731\" y=\"270.112313\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"361.005073\" y=\"258.004028\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"280.889087\" y=\"71.932741\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"300.879881\" y=\"220.93672\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"266.43678\" y=\"37.976799\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"326.527231\" y=\"235.125519\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"340.039141\" y=\"99.889337\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"87.475516\" y=\"142.526894\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"81.029394\" y=\"115.374745\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"306.524823\" y=\"242.63995\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"346.690173\" y=\"190.249064\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"324.101592\" y=\"218.531296\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"379.451163\" y=\"229.412053\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"76.189447\" y=\"94.250694\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"345.717611\" y=\"247.503279\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"313.334289\" y=\"230.264455\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"284.40414\" y=\"58.314357\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"105.390454\" y=\"125.278572\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"327.513994\" y=\"72.365719\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"276.093149\" y=\"57.53302\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"241.907048\" y=\"45.72413\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"131.971964\" y=\"117.886943\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"266.947033\" y=\"74.705791\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"334.222352\" y=\"231.099989\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"135.114982\" y=\"132.062727\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"279.754032\" y=\"24.404687\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"55.916192\" y=\"129.645283\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"281.55736\" y=\"60.137635\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"307.244282\" y=\"85.301443\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"134.144405\" y=\"136.989466\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"99.052898\" y=\"135.45896\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"251.382966\" y=\"81.36741\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"387.132557\" y=\"223.886343\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"250.207904\" y=\"66.17246\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"227.460775\" y=\"72.548359\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"332.854177\" y=\"185.041372\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"340.259363\" y=\"212.191011\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"117.466559\" y=\"151.970797\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"324.911593\" y=\"251.428759\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"68.64773\" y=\"132.668798\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"122.819487\" y=\"141.884853\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"127.261118\" y=\"136.556991\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"147.550514\" y=\"126.551356\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"307.266874\" y=\"225.088164\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"114.445692\" y=\"150.577208\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"114.691107\" y=\"109.618188\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"166.360711\" y=\"184.055665\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"253.238885\" y=\"109.007427\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"97.247081\" y=\"143.610196\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"319.5011\" y=\"204.613714\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"98.030033\" y=\"145.779943\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"303.915638\" y=\"64.492773\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"292.368352\" y=\"83.919845\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"328.167416\" y=\"285.079636\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"367.448873\" y=\"236.538656\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"155.345987\" y=\"108.186222\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"72.405744\" y=\"146.076059\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"95.970933\" y=\"97.183565\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"94.890958\" y=\"167.515946\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"361.635438\" y=\"257.851324\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "     <use xlink:href=\"#m3d4ca98cc4\" x=\"316.687707\" y=\"268.732362\" style=\"fill: #ffffff; stroke: #000000\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m6b620a80d3\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6b620a80d3\" x=\"87.78949\" y=\"298.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- −2 -->\n",
       "      <g transform=\"translate(80.418396 312.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6b620a80d3\" x=\"152.174279\" y=\"298.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- −1 -->\n",
       "      <g transform=\"translate(144.803185 312.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6b620a80d3\" x=\"216.559068\" y=\"298.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(213.377818 312.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6b620a80d3\" x=\"280.943857\" y=\"298.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(277.762607 312.910437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6b620a80d3\" x=\"345.328646\" y=\"298.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(342.147396 312.910437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6b620a80d3\" x=\"409.713435\" y=\"298.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(406.532185 312.910437) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_7\">\n",
       "     <!-- $\\mathbf{x}_1$: Feature 1 -->\n",
       "     <g transform=\"translate(206.241625 326.588562) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-Bold-78\" d=\"M 1422 1791 \n",
       "L 159 3500 \n",
       "L 1344 3500 \n",
       "L 2059 2463 \n",
       "L 2784 3500 \n",
       "L 3969 3500 \n",
       "L 2706 1797 \n",
       "L 4031 0 \n",
       "L 2847 0 \n",
       "L 2059 1106 \n",
       "L 1281 0 \n",
       "L 97 0 \n",
       "L 1422 1791 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-3a\" d=\"M 750 794 \n",
       "L 1409 794 \n",
       "L 1409 0 \n",
       "L 750 0 \n",
       "L 750 794 \n",
       "z\n",
       "M 750 3309 \n",
       "L 1409 3309 \n",
       "L 1409 2516 \n",
       "L 750 2516 \n",
       "L 750 3309 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-46\" d=\"M 628 4666 \n",
       "L 3309 4666 \n",
       "L 3309 4134 \n",
       "L 1259 4134 \n",
       "L 1259 2759 \n",
       "L 3109 2759 \n",
       "L 3109 2228 \n",
       "L 1259 2228 \n",
       "L 1259 0 \n",
       "L 628 0 \n",
       "L 628 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-Bold-78\" transform=\"translate(0 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(65.458984 -16.3125) scale(0.7)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-3a\" transform=\"translate(112.729492 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(146.420898 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-46\" transform=\"translate(178.208008 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(235.727539 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(297.250977 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(358.530273 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(397.739258 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(461.118164 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(502.231445 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(563.754883 0.09375)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-31\" transform=\"translate(595.541992 0.09375)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path id=\"m2f56e1856c\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2f56e1856c\" x=\"34.240625\" y=\"274.767902\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(20.878125 278.567121) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2f56e1856c\" x=\"34.240625\" y=\"226.590765\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(20.878125 230.389984) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2f56e1856c\" x=\"34.240625\" y=\"178.413628\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(20.878125 182.212847) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2f56e1856c\" x=\"34.240625\" y=\"130.236491\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(20.878125 134.03571) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2f56e1856c\" x=\"34.240625\" y=\"82.059354\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(20.878125 85.858573) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m2f56e1856c\" x=\"34.240625\" y=\"33.882217\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_13\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(20.878125 37.681436) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- $\\mathbf{x}_2$: Feature 2 -->\n",
       "     <g transform=\"translate(14.798438 185.756) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-Bold-78\" transform=\"translate(0 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(65.458984 -15.625) scale(0.7)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-3a\" transform=\"translate(112.729492 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(146.420898 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-46\" transform=\"translate(178.208008 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(235.727539 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" transform=\"translate(297.250977 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(358.530273 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" transform=\"translate(397.739258 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(461.118164 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(502.231445 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(563.754883 0.78125)\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-32\" transform=\"translate(595.541992 0.78125)\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 34.240625 298.312 \n",
       "L 34.240625 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 444.242625 298.312 \n",
       "L 444.242625 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 34.240625 298.312 \n",
       "L 444.242625 298.312 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 34.240625 7.2 \n",
       "L 444.242625 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pdb264f7e00\">\n",
       "   <rect x=\"34.240625\" y=\"7.2\" width=\"410.002\" height=\"291.112\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(\n",
    "    n_samples=150,\n",
    "    n_features=2,\n",
    "    centers=3,\n",
    "    cluster_std=0.5,\n",
    "    shuffle=True,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=\"white\", marker=\"o\", edgecolor=\"black\", s=50)\n",
    "plt.xlabel(\"$\\mathbf{x}_1$: Feature 1\")\n",
    "plt.ylabel(\"$\\mathbf{x}_2$: Feature 2\")\n",
    "\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7acafb9",
   "metadata": {},
   "source": [
    "The question on hand is, if we are given data of this form, how do we cluster\n",
    "them into $3$ distinct clusters?\n",
    "\n",
    "Visually, we can literally just circle out the $3$ clusters. The luxury of such\n",
    "simplicity is because we are working with $2$ features, i.e.\n",
    "$\\mathbf{x} \\in \\mathbb{R}^{2}$. In addition, the dataset generated is\n",
    "relatively simple to partition, i.e. the clusters are well separated.\n",
    "\n",
    "However, in the real world, we are working with $D$-dimensional features where\n",
    "$\\mathbf{x}$ resides in $\\mathbb{R}^{D}$. $D$ can be very large and we are\n",
    "unable to visually inspect anymore.\n",
    "\n",
    "In any case, even with such a simple dataset, how do we tell the machine to find\n",
    "the $3$ clusters that our visuals have identified?\n",
    "\n",
    "### The Hypothesis Space\n",
    "\n",
    "Formulating such a problem is not trivial is non-trivial. We first have to\n",
    "formulate the problem in a way that the machine can understand, and that is done\n",
    "mathematically.\n",
    "\n",
    "For one, there is no ground truth labels as in the supervised setting, and\n",
    "therefore our learner or hypothesis need not output a label, but rather a\n",
    "cluster assignment.\n",
    "\n",
    "Retrospectively, we can think of the learner as a function $h(\\cdot)$ that takes\n",
    "in many data points\n",
    "$\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots, \\mathbf{x}^{(N)}$ and outputs a\n",
    "cluster assignment $\\mathcal{A}(\\mathbf{x})$ for each data point $\\mathbf{x}$.\n",
    "\n",
    "For now, let's informally define the hypothesis space $\\mathcal{H}$ to be the\n",
    "set of all possible cluster assignments $\\mathcal{A}(\\cdot)$.\n",
    "\n",
    "$$\n",
    "\\mathcal{H} = \\{\\mathcal{A}(\\cdot) \\mid \\mathcal{A}(\\cdot) \\text{ somehow assigns the data points to the correct cluster.}\\}\n",
    "$$\n",
    "\n",
    "We will make this more precise later.\n",
    "\n",
    "### The Loss/Cost/Objective Function\n",
    "\n",
    "The second part of formulating a machine learning problem is to define the loss\n",
    "function $\\mathcal{L}(\\cdot)$ and subsequently the cost function\n",
    "$\\mathcal{J}(\\cdot)$.\n",
    "\n",
    "In supervised learning, we have our typical loss functions such as cross-entropy\n",
    "loss (classification), and in regression, we have mean squared error. We also\n",
    "have metrics like accuracy, precision, recall, etc to measure the performance of\n",
    "the model.\n",
    "\n",
    "This means, given a hypothesis $\\hat{y}:=h(\\mathbf{x})$, how close is it to the\n",
    "true label $y$? In unsupervised, we do not have such ground truth label $y$ to\n",
    "compare with, but the notion of closeness is still there.\n",
    "\n",
    "#### The Notion of Similarity and Closeness\n",
    "\n",
    "To define such a metric for unsupervised learning, we can fall back on our\n",
    "intuition. The purpose of clustering is to group similar data points together.\n",
    "So we seek to find a metric that measures the similarity between data points in\n",
    "a dataset.\n",
    "\n",
    "A very simple idea is to use\n",
    "[**intra-cluster variance**](https://stats.stackexchange.com/questions/120509/inter-cluster-variance).\n",
    "For example, within a cluster, the data points are close to each other if the\n",
    "variance is small.\n",
    "\n",
    "Consequently, to make our intuition precise, we need to define a metric rule and\n",
    "an assignment $\\mathcal{A}(\\cdot)$ to assign data points to clusters. We also\n",
    "need to define the notion of closeness and similarity between data points.\n",
    "\n",
    "Lastly, such algorithms require an initial guess of the cluster centers, so that\n",
    "eventually the algorithm can converge to the optimal cluster centers, since we\n",
    "have no way of knowing the optimal cluster centers beforehand, especially in\n",
    "high dimensional space.\n",
    "\n",
    "More formally, the optimization problem requires us to minimize the sum of\n",
    "squared distances between each data point and its cluster center. This is\n",
    "equivalent to minimizing the variance within each cluster.\n",
    "\n",
    "Let's look at some definitions first that will gradually lead us to the\n",
    "formulation of the objective function.\n",
    "\n",
    "## Problem Formulation\n",
    "\n",
    "```{prf:remark} Remark\n",
    ":label: remark-kmeans-problem-statement\n",
    "\n",
    "Although K-Means does not explicitly model the underlying distribution $\\mathcal{D}$,\n",
    "we can still apply the learning theory framework to K-Means.\n",
    "```\n",
    "\n",
    "**Given** a set $\\mathcal{S}$ containing $N$ data points:\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\left\\{\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots, \\mathbf{x}^{(N)}\\right\\} \\subset \\mathcal{X} = \\mathbb{R}^{D}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{X} = \\mathbb{R}^{D}$ and the vector $\\mathbf{x}^{(n)}$ is the\n",
    "$n$-th sample with $D$ number of features, given by:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{(n)} \\in \\mathbb{R}^{D} = \\begin{bmatrix} x_1^{(n)} & x_2^{(n)} & \\cdots & x_D^{(n)} \\end{bmatrix}^{\\mathrm{T}} \\quad \\text{where } n = 1, \\ldots, N.\n",
    "$$\n",
    "\n",
    "We can further write $\\mathcal{S}$ as a disjoint union [^disjoint_union] of $K$\n",
    "sets, as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{S} &:= \\left\\{\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots, \\mathbf{x}^{(N)}\\right\\} \\subset \\mathbb{R}^{D} = C_1 \\sqcup C_2 \\sqcup \\cdots \\sqcup C_K \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $C_k$ is the set of data points that belong to cluster $k$:\n",
    "\n",
    "```{math}\n",
    ":label: eq:cluster-def\n",
    "\n",
    "C_k = \\left\\{\\mathbf{x}^{(n)} \\in \\mathbb{R}^{D} \\middle\\vert y^{(n)} = k\\right\\} .\n",
    "```\n",
    "\n",
    "The notation $y^{(n)} \\in \\{1, 2, \\dots, K\\}$ may seem strange at first glance,\n",
    "since we are not given the labels $y^{(n)}$ in an unsupervised problem. Indeed,\n",
    "this $y^{(n)}$ (**latent**) is generally not known to us, but we can have a\n",
    "mental model that for each data point, there is an underlying cluster label\n",
    "$y^{(n)}$ that it should belong to.\n",
    "\n",
    "More concretely, we say that $y^{(n)} \\in \\{1, 2, \\dots, K\\}$ in equation\n",
    "{eq}`eq:cluster-def` refers to the cluster (ground truth) label of data point\n",
    "$\\mathbf{x}^{(n)}$.\n",
    "\n",
    "We further define $\\mathcal{C}$ as the collection of these\n",
    "clusters[^collection_of_clusters],\n",
    "\n",
    "$$\n",
    "\\mathcal{C} = \\left\\{C_1, C_2, \\dots, C_K\\right\\}.\n",
    "$$\n",
    "\n",
    "To this end, we have decomposed the $N$ data points into $K$ clusters, where $K$\n",
    "is a [_priori_](https://en.wikipedia.org/wiki/A_priori_and_a_posteriori), a\n",
    "pre-defined number.\n",
    "\n",
    "---\n",
    "\n",
    "The **K-Means** algorithm aims to group the data points into a set\n",
    "$\\hat{\\mathcal{C}}$ containing $K$ clusters:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{C}} = \\left\\{ \\hat{C}_1, \\hat{C}_2 \\dots, \\hat{C}_K \\right\\}\n",
    "$$\n",
    "\n",
    "where $\\hat{C}_k$ is the set of data points $\\mathbf{x}^{(n)} \\in \\mathcal{S}$\n",
    "assigned by $\\mathcal{A}(\\cdot)$ (explained shortly) to the $ k $-th cluster:\n",
    "\n",
    "$$\n",
    "\\hat{C}_k = \\left\\{\\mathbf{x}^{(n)} \\in \\mathbb{R}^{D} \\middle\\vert \\mathcal{A}(n):= \\hat{y}^{(n)} = k\\right\\}.\n",
    "$$\n",
    "\n",
    "Note that $\\mathcal{A}(\\cdot)$ is the assignment map that \"predicts\" and\n",
    "\"classifies\" each data point into their respective clusters.\n",
    "\n",
    "To this end, the **goal** of such an **unsupervised problem** is to find the\n",
    "**_clusters_** $\\hat{\\mathcal{C}}$, the predicted clusters learnt by K-Means\n",
    "that best approximate the ground truth clusters $\\mathcal{C}$.\n",
    "\n",
    "In other words, we want to find the clusters $\\hat{\\mathcal{C}}$ that are the\n",
    "closest to the ground truth clusters $\\mathcal{C}$, we will make precise the\n",
    "notion of _close_ later.\n",
    "\n",
    "It is also customary to denote $\\hat{C}_k$ to be the set that contains the\n",
    "indices of the data points that belong to cluster $k$:\n",
    "\n",
    "$$\n",
    "\\hat{C}_k = \\left\\{n \\in \\{1, 2, \\dots, N\\} \\middle\\vert \\mathcal{A}(n):= \\hat{y}^{(n)} = k\\right\\}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "[K-Means clustering](https://en.wikipedia.org/wiki/K-means_clustering)'s goal is\n",
    "to find the clusters $\\hat{\\mathcal{C}}$ that are the closest to the ground\n",
    "truth clusters $\\mathcal{C}$ (**hard clustering**). In other words, we aim to\n",
    "partition $N$ data points $\\mathcal{S}$ into $K$ clusters $\\hat{\\mathcal{C}}$.\n",
    "The problem in itself seems manageable, since we can simply partition the data\n",
    "points into $K$ clusters and minimize the intra-cluster distance (variances).\n",
    "However, it is computationally challenging to solve the problem\n",
    "([NP-hard](https://en.wikipedia.org/wiki/NP-hardness)).\n",
    "\n",
    "Consequently, there are many heuristics that are used to solve the problem. We\n",
    "will talk about one of the most popular heuristics, the\n",
    "[Lloyd's algorithm](https://en.wikipedia.org/wiki/Lloyd%27s_algorithm) in this\n",
    "section.\n",
    "\n",
    "In this algorithm, there exists $K$ centroids (centers)\n",
    "\n",
    "$$\n",
    "\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\dots, \\boldsymbol{v}_K \\in \\mathbb{R}^{D}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{v}_k$ is defined to be the centroid of cluster $C_k$. Each\n",
    "centroid $\\boldsymbol{v}_k$ is a vector that has the same dimension as the data\n",
    "points $\\mathbf{x}^{(n)}$ and is the **representative vector** of the cluster\n",
    "$C_k$.\n",
    "\n",
    "By representative vector, we mean that $\\boldsymbol{v}_k$ is a vector that can\n",
    "\"describe\" the cluster $C_k$. By construction, the centroids can be defined as\n",
    "any vector $\\boldsymbol{v}_k$ that has the same dimension as the data points\n",
    "$\\mathbf{x}^{(n)}$. However, an intuitive choice is to use the **mean** of the\n",
    "data points $\\boldsymbol{\\mu}_k$ in the cluster $\\hat{C}_k$ as the\n",
    "representative vector $\\boldsymbol{v}_k$.\n",
    "\n",
    "Next, the formulation of the assignment rule $\\mathcal{A}(\\cdot)$ can be made\n",
    "clear by the intuition below:\n",
    "\n",
    "> Since $\\mathcal{A}(\\cdot)$ is an assignment rule, an intuitive way is to find\n",
    "> a representative vector $\\boldsymbol{v}_k$ in each cluster $\\hat{C}_k$, and\n",
    "> assign every data point $\\mathbf{x}^{(n)}$ that is closest to this\n",
    "> representative. This is similar to the\n",
    "> [nearest-neighbour search algorithm](<https://en.wikipedia.org/wiki/Nearest_neighbor_search#:~:text=Nearest%20neighbor%20search%20(NNS)%2C,the%20larger%20the%20function%20values.>).\n",
    "\n",
    "Consequently, given the representative vectors\n",
    "$\\left\\{\\boldsymbol{v}_k\\right\\}_{k=1}^K$, we need an assignment function\n",
    "$\\mathcal{A}(n) = \\hat{y}^{(n)}$ that assigns each data point $\\mathbf{x}^{(n)}$\n",
    "to the cluster $\\hat{C}_k$. An intuitive choice is to compare \"closeness\" of\n",
    "each $\\mathbf{x}^{(n)}$ to the representative vectors\n",
    "$\\left\\{\\boldsymbol{v}_k\\right\\}_{k=1}^K$ and assign it to the cluster\n",
    "$\\hat{C}_k$ that is closest to the representative vector $\\boldsymbol{v}_k$.\n",
    "\n",
    "We will make these intuition more precise later by proving it.\n",
    "\n",
    "---\n",
    "\n",
    "To this end, we have tidied up the flow of the Lloyd's algorithm (more details\n",
    "in subsequent sections), we now finalize the problem statement by defining an\n",
    "appropriate [**loss**](https://en.wikipedia.org/wiki/Loss_function) and\n",
    "[**objective**](https://en.wikipedia.org/wiki/Mathematical_optimization)\n",
    "function. More formally, we want to find the assignment $\\mathcal{A}(\\cdot)$ and\n",
    "the cluster center $\\boldsymbol{v}_k$ such that the\n",
    "[**sum of squared distances**](https://en.wikipedia.org/wiki/Residual_sum_of_squares#:~:text=In%20statistics%2C%20the%20residual%20sum,such%20as%20a%20linear%20regression.)\n",
    "between each data point and its cluster center is minimized. This means\n",
    "partitioning the data points according to the\n",
    "[**Voronoi Diagram**](https://en.wikipedia.org/wiki/Voronoi_diagram).\n",
    "\n",
    "To this end, we can define an _empirical_ cost function that measures the\n",
    "quality of the requirements listed earlier.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}\\left(\\left\\{\\hat{y}^{(n)}\\right\\}_{n=1}^N,\\left\\{\\boldsymbol{v}_{k}\\right\\}_{k=1}^k \\middle \\vert \\mathcal{S}\\right) &= \\sum_{n=1}^{N} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_{\\hat{y}^{(n)}} \\right\\|^2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that the clustering error $\\widehat{\\mathcal{J}}$ depends on **both** the\n",
    "**cluster assignments** $\\hat{y}^{(n)}$, which define the clusters $\\hat{C}_k$,\n",
    "and the **cluster representatives** $\\boldsymbol{v}_k$, for $k=1, \\ldots, K$. As\n",
    "mentioned earlier, finding the optimal cluster means\n",
    "$\\left\\{\\boldsymbol{v}_k\\right\\}_{k=1}^K$ and cluster assignments\n",
    "$\\left\\{\\hat{y}^{(n)}\\right\\}_{n=1}^N$ that minimize the clustering error\n",
    "$\\widehat{\\mathcal{J}}$ is a\n",
    "[NP-hard problem](https://cseweb.ucsd.edu/~avattani/papers/kmeans_hardness.pdf).\n",
    "The difficulty stems from the fact that the clustering error $\\mathcal{J}$ is a\n",
    "[non-convex](https://en.wikipedia.org/wiki/Convex_optimization) function of the\n",
    "cluster means and assignments. In other words, there are many local minima of\n",
    "the clustering error $\\mathcal{J}$, and finding the global minimum is hard.\n",
    "\n",
    "While jointly optimizing the cluster means and assignments is\n",
    "hard[^jointly_optimizing], separately optimizing either the cluster means for\n",
    "given assignments or vice-versa is easy. In what follows, we present simple\n",
    "closed-form solutions for these sub-problems. The $k$-means method simply\n",
    "combines these solutions in an alternating fashion {cite}`jung2022machine`.\n",
    "\n",
    "More concretely, we want to show:\n",
    "\n",
    "-   For fixed cluster assignments $\\mathcal{A}(n) = \\hat{y}^{(n)}$, the\n",
    "    clustering error $\\widehat{\\mathcal{J}}$ is minimized by setting the cluster\n",
    "    representatives $\\boldsymbol{v}_k$ equal to the cluster means, this means\n",
    "    the mean vector is the optimal choice for the cluster center.\n",
    "\n",
    "    $$\n",
    "    \\boldsymbol{\\mu}_1, \\boldsymbol{\\mu}_2, \\dots, \\boldsymbol{\\mu}_K \\in \\mathbb{R}^{D}\n",
    "    $$\n",
    "\n",
    "    where each $\\boldsymbol{\\mu}_k$ is the mean vector of the data points in\n",
    "    cluster $C_k$.\n",
    "\n",
    "-   Furthermore, now when we obtain the cluster means $\\boldsymbol{\\mu}_k$ (now\n",
    "    we fix $\\boldsymbol{\\mu}_k$, we can assign data points $\\mathbf{x}^{(n)}$ to\n",
    "    the cluster $\\hat{C}_k$ that is closest to the cluster mean\n",
    "    $\\boldsymbol{\\mu}_k$. This assignment action is called the **assignment\n",
    "    function** $\\mathcal{A}$, a function that does the assignment of data points\n",
    "    to clusters. We will show later that the clustering error\n",
    "    $\\widehat{\\mathcal{J}}$ is minimized when the assignment function is the\n",
    "    **nearest neighbor assignment** function $\\mathcal{A}^{*}(\\cdot)$,\n",
    "\n",
    "    $$\n",
    "    \\mathcal{A}^{*}(n) = \\underset{k}{\\operatorname{argmin}} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{\\mu}_k \\right\\|^2\n",
    "    $$\n",
    "\n",
    "    where it assigns data points $\\mathbf{x}^{(n)}$ to the cluster $k$ whose\n",
    "    center $\\boldsymbol{\\mu}_k$ is closest.\n",
    "\n",
    "We see that instead of jointly optimizing the cluster means and assignments in\n",
    "one step, we alternate between the two steps. We first fix the cluster\n",
    "assignments and optimize the cluster means, and then we fix the cluster means\n",
    "and optimize the cluster assignments. Readings who are familiar with data\n",
    "structures and algorithms will notice this looks like a\n",
    "[greedy algorithm](https://en.wikipedia.org/wiki/Greedy_algorithm), and those\n",
    "who have learnt the\n",
    "[expectation maximization](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)\n",
    "algorithm will notice that this is a special case of the expectation\n",
    "maximization algorithm.\n",
    "\n",
    "In the following sections, we will phrase K-Means (Lloyd's algorithm) as an\n",
    "optimization problem, in which the goal is to find the optimal cluster centers\n",
    "and cluster assignments that minimize the clustering error. We will also prove\n",
    "why this is the case.\n",
    "\n",
    "## Partition and Voronoi Regions\n",
    "\n",
    "First off, let's see the definition of Voronoi regions (extracted from\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Voronoi_diagram)):\n",
    "\n",
    "```{prf:definition} Voronoi Region\n",
    ":label: def-voronoi-region\n",
    "\n",
    "Let $X$ be a metric space with distance function $d$. Let $K$ be a set of\n",
    "indices and let $\\left(P_k\\right)_{k \\in K}$ be a tuple (ordered collection) of\n",
    "nonempty subsets (the sites) in the space $X$. The Voronoi cell, or Voronoi\n",
    "region, $R_k$, associated with the site $P_k$ is the set of all points in $X$\n",
    "whose distance to $P_k$ is not greater than their distance to the other sites\n",
    "$P_j$, where $j$ is any index different from $k$. In other words, if\n",
    "$d(x, A)=\\inf \\{d(x, a) \\mid a \\in A\\}$ denotes the distance between the point\n",
    "$x$ and the subset $A$, then\n",
    "\n",
    "$$\n",
    "R_k=\\left\\{x \\in X \\mid d\\left(x, P_k\\right) \\leq d\\left(x, P_j\\right) \\text { for all } j \\neq k\\right\\}\n",
    "$$\n",
    "\n",
    "The Voronoi diagram is simply the tuple of cells $\\left(R_k\\right)_{k \\in K}$.\n",
    "In principle, some of the sites can intersect and even coincide (an application\n",
    "is described below for sites representing shops), but usually they are assumed\n",
    "to be disjoint. In addition, infinitely many sites are allowed in the definition\n",
    "(this setting has applications in geometry of numbers and crystallography), but\n",
    "again, in many cases only finitely many sites are considered.\n",
    "\n",
    "In the particular case where the space is a finite-dimensional Euclidean space,\n",
    "each site is a point, there are finitely many points and all of them are\n",
    "different, then the Voronoi cells are convex polytopes and they can be\n",
    "represented in a combinatorial way using their vertices, sides, two-dimensional\n",
    "faces, etc. Sometimes the induced combinatorial structure is referred to as the\n",
    "Voronoi diagram. In general however, the Voronoi cells may not be convex or even\n",
    "connected.\n",
    "\n",
    "In the usual Euclidean space, we can rewrite the formal definition in usual\n",
    "terms. Each Voronoi polygon $R_k$ is associated with a generator point $P_k$.\n",
    "Let $X$ be the set of all points in the Euclidean space. Let $P_1$ be a point\n",
    "that generates its Voronoi region $R_1, P_2$ that generates $R_2$, and $P_3$\n",
    "that generates $R_3$, and so on. Then, all locations in the Voronoi polygon are\n",
    "closer to the generator point of that polygon than any other generator point in\n",
    "the Voronoi diagram in Euclidean plane\".\n",
    "```\n",
    "\n",
    "K-Means can be formulated via the lens of\n",
    "[**Voronoi regions**](https://en.wikipedia.org/wiki/Voronoi_diagram) where we\n",
    "define $C_k \\in \\mathcal{C}$ as the **partition** of the data set $\\mathcal{S}$,\n",
    "where each subset is a cluster. We say that $C_k$ is a representative of the\n",
    "cluster $k$ and induces a **Voronoi partition** of $\\mathbb{R}^D$. More\n",
    "formally, we define the Voronoi partition as follows:\n",
    "\n",
    "```{prf:definition} K-Means Voronoi Partition\n",
    ":label: def:kmeans-voronoi-partition\n",
    "\n",
    "Let $\\mathcal{C} = \\{C_1, C_2, \\ldots, C_K\\}$ be a partition of $\\mathcal{S}$, where $C_k \\in C$ is a subset of $\\mathcal{S}$.\n",
    "Then $\\mathcal{C}$ induces a **Voronoi partition** of $\\mathbb{R}^D$, which decomposes $\\mathbb{R}^D$ into $K$ convex cells,\n",
    "each corresponding to some $C_k \\in \\mathcal{C}$ and containing the region of space whose nearest representative is $C_k$.\n",
    "\n",
    "More concretely, the Voronoi region $C_k$, contains all points $\\mathbf{x} \\in \\mathbb{R}^D$ such that\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left\\|\\mathbf{x} - \\boldsymbol{v}_k \\right\\|^2 \\leq \\left\\|\\mathbf{x} - \\boldsymbol{v}_j \\right\\|^2 \\text{ for all } j \\neq k\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "which means that the distance between $\\mathbf{x}$ and $\\boldsymbol{v}_k$ is less than or equal to the distance between $\\mathbf{x}$ and any other cluster center $\\boldsymbol{v}_j$.\n",
    "\n",
    "Also,\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\bigsqcup_{k=1}^K C_k\n",
    "$$\n",
    "```\n",
    "\n",
    "For a visual representation, see {cite:ps}`pml1Book`'s\n",
    "[figure](https://github.com/probml/pyprobml/blob/master/notebooks/book1/21/kmeans_voronoi.ipynb).\n",
    "\n",
    "## Assignment\n",
    "\n",
    "````{prf:definition} Assignment\n",
    ":label: def:assignment\n",
    "\n",
    "An assignment $\\mathcal{A}(\\cdot)$ is a surjective map,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{A} : \\mathbb{Z}^{+} &\\to \\mathbb{Z}^{+} \\\\\n",
    "\\{1, 2, \\dots, N\\} &\\to \\{1, 2, \\dots, K\\} .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In this case, $\\mathcal{A}(n) = k$ means that data point $\\mathbf{x}^{(n)}$ is assigned to cluster $k$.\n",
    "\n",
    "One should see that the assignment function $\\mathcal{A}(\\cdot)$ gives rise to the prediction $\\hat{y}^{(n)}$\n",
    "for each data point $\\mathbf{x}^{(n)}$.\n",
    "\n",
    "```{math}\n",
    ":label: eq:assignment-prediction\n",
    "\n",
    "\\hat{y}^{(n)} = \\mathcal{A}(n) \\quad \\text{for } n = 1, 2, \\dots, N.\n",
    "```\n",
    "````\n",
    "\n",
    "```{prf:example} Assignment\n",
    ":label: example:assignment\n",
    "\n",
    "For example, if we have 4 data points $\\mathbf{x}^{(1)}$, $\\mathbf{x}^{(2)}$, $\\mathbf{x}^{(3)}$, and $\\mathbf{x}^{(4)}$,\n",
    "and we want to partition them into 3 clusters $\\hat{C}_1$, $\\hat{C}_2$, and $\\hat{C}_3$, we can define an assignment as follows:\n",
    "\n",
    "- Assign $\\mathbf{x}^{(1)}$ to $\\hat{C}_1$, $\\hat{C}_1 = \\left\\{\\mathbf{x}^{(1)}\\right\\}$.\n",
    "- Assign $\\mathbf{x}^{(3)}$ to $\\hat{C}_2$, $\\hat{C}_2 = \\left\\{\\mathbf{x}^{(3)}\\right\\}$.\n",
    "- Assign $\\mathbf{x}^{(2)}$ and $\\mathbf{x}^{(4)}$ to $\\hat{C}_3$, $\\hat{C}_3 = \\left\\{\\mathbf{x}^{(2)}, \\mathbf{x}^{(4)}\\right\\}$.\n",
    "\n",
    "We can make this more precise by defining an assignment function $\\mathcal{A}$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{A} : \\mathbb{Z}^{+} &\\to \\mathbb{Z}^{+} \\\\\n",
    "\\{1, 2, 3, 4\\} &\\to \\{1, 2, 3\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\mathcal{A}(1) = 1$,\n",
    "- $\\mathcal{A}(2) = 3$,\n",
    "- $\\mathcal{A}(3) = 2$, and\n",
    "- $\\mathcal{A}(4) = 3$.\n",
    "```\n",
    "\n",
    "We have seen earlier that the assignment function of the K-Means algorithm\n",
    "follows the nearest-neighbour rule, but we did not explicitly define it here\n",
    "just yet. We will derive that the optimal assignment $\\mathcal{A}^{*}(\\cdot)$ is\n",
    "the one that minimizes the cost function:\n",
    "\n",
    "```{math}\n",
    ":label: eq:assignment-optimal-1\n",
    "\n",
    "\\mathcal{A}^{*}(n) = \\underset{k}{\\operatorname{argmin}} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{\\mu}_k \\right\\|^2\n",
    "```\n",
    "\n",
    "### Assignments are Equivalent to Clusters\n",
    "\n",
    "Note that when we mention the assignment function $\\mathcal{A}(\\cdot)$, we are\n",
    "implicitly referring to the clusters $\\hat{C}_1$, $\\hat{C}_2$, $\\ldots$,\n",
    "$\\hat{C}_K$. They have equivalent meanings.\n",
    "\n",
    "## Centroids (Representatives)\n",
    "\n",
    "```{prf:definition} Centroids\n",
    ":label: def:centroids\n",
    "\n",
    "The centroids $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K$ of a partition $\\hat{\\mathcal{C}}$ are the representatives of each cluster $C_k \\in \\hat{\\mathcal{C}}$:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{v}_k \\text{ represents cluster } C_k \\text{ for } k = 1, 2, \\ldots, K.\n",
    "$$\n",
    "```\n",
    "\n",
    "## Hypothesis Space\n",
    "\n",
    "For completeness sake, let’s define the hypothesis space $\\mathcal{H}$ for\n",
    "K-Means.\n",
    "\n",
    "Intuitively, the hypothesis space $\\mathcal{H}$ is the set of all possible\n",
    "clusterings of the data.\n",
    "\n",
    "Formally, given a set of $N$ data points\n",
    "$\\left\\{\\mathbf{x}^{(n)}\\right\\}_{n=1}^N$, let $C_k$ be the Voronoi cell of the\n",
    "$k$-th cluster center $\\boldsymbol{\\mu}_k$.\n",
    "\n",
    "Then, we can write the class of functions $\\mathcal{H}$ as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{H} &= \\left\\{\\mathcal{A}: \\mathbb{Z} \\rightarrow \\mathbb{Z} \\mid \\mathcal{A}(n) \\in \\{1, 2, \\dots, K\\} \\text{ for all } n \\in \\{1, 2, \\dots, N\\}\\right\\} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This means the hypothesis space $\\mathcal{H}$ is finite with cardinality $K^N$.\n",
    "\n",
    "For more details, see [here](https://stats.stackexchange.com/posts/502352/) and\n",
    "[here](https://courses.cs.washington.edu/courses/cse446/16sp/clustering_1.pdf).\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "We make precise the notion of closeness and similarity between data points by\n",
    "defining a loss function utilizing the\n",
    "[**euclidean distance**](https://en.wikipedia.org/wiki/Euclidean_distance). In\n",
    "practice, we can use other distance metrics such as\n",
    "[**manhattan distance**](https://simple.wikipedia.org/wiki/Manhattan_distance)\n",
    "that suits one's needs.\n",
    "\n",
    "```{prf:definition} K-Means Loss Function\n",
    ":label: def:kmeans-loss\n",
    "\n",
    "For any assignment $\\mathcal{A}(\\cdot)$ that maps the set $\\{1, 2, \\ldots, N\\}$ to $\\{1, 2, \\ldots, K\\}$ and\n",
    "any centroids $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\dots, \\boldsymbol{v}_K \\in \\mathbb{R}^{D}$,\n",
    "we construct the loss function as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{L}}_{\\mathcal{S}}\\left(\\mathcal{A}, \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K \\right) &= \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_{\\mathcal{A}(n)} \\right\\|^2 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "```\n",
    "\n",
    "This just means the loss for one single data point $\\mathbf{x}^{(n)}$ is the\n",
    "squared euclidean distance between the data point and its assigned centroid.\n",
    "\n",
    "As a reminder, the hat symbol $\\widehat{\\cdot}$ is used to denote an estimate of\n",
    "a quantity or function. In this case, the loss function\n",
    "$\\widehat{\\mathcal{L}}_{\\mathcal{S}}$ is an estimate of the loss function\n",
    "$\\mathcal{L}$ since we do not have access to all the data points $\\mathbf{x}$.\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "However, we are not interested in the loss for a single data point, but rather\n",
    "the loss for all data points in the dataset $\\mathcal{S}$.\n",
    "\n",
    "To this end, the cost function in K-Means is the sum of the loss function over\n",
    "all data points $\\left\\{\\mathbf{x}^{(n)}\\right\\}_{n=1}^N$, defined as follows:\n",
    "\n",
    "````{prf:definition} K-Means Cost Function\n",
    ":label: def:kmeans-cost\n",
    "\n",
    "For any assignment $\\mathcal{A}(\\cdot)$ that maps the set $\\{1, 2, \\ldots, N\\}$ to $\\{1, 2, \\ldots, K\\}$ and\n",
    "any centroids $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\dots, \\boldsymbol{v}_K \\in \\mathbb{R}^{D}$,\n",
    "we construct the cost function as follows:\n",
    "\n",
    "```{math}\n",
    ":label: eq:kmeans-cost-1\n",
    "\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\mathcal{A}, \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K) &:= \\widehat{\\mathcal{J}}\\left(\\left\\{\\hat{y}^{(n)}\\right\\}_{n=1}^N,\\left\\{\\boldsymbol{v}_{k}\\right\\}_{k=1}^k \\middle \\vert \\mathcal{S}\\right) \\\\\n",
    "&\\overset{\\text{(a)}}{=} \\sum_{n=1}^{N} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_{\\mathcal{A}(n)} \\right\\|^2 \\\\\n",
    "&\\overset{\\text{(b)}}{=} \\sum_{n=1}^{N} \\sum_{\\mathcal{A}(n) = k} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2 \\\\\n",
    "&\\overset{\\text{(c)}}{=} \\sum_{n=1}^{N} \\sum_{k=1}^{K} r^{(n)}_k \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2 \\\\\n",
    "&\\overset{\\text{(d)}}{=} \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\mathbb{I}\\left\\{\\mathcal{A}(n) = k\\right\\} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2 \\\\\n",
    "&\\overset{\\text{(e)}}{=} \\sum_{k=1}^K \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2 \\\\\n",
    "\\end{aligned}\n",
    "```\n",
    "````\n",
    "\n",
    "where\n",
    "\n",
    "-   $\\mathcal{A}(n) = k$ means that data point $\\mathbf{x}^{(n)}$ is assigned to\n",
    "    cluster $k$.\n",
    "-   $r^{(n)}_k$ is an indicator function that is equal to 1 if\n",
    "    $\\mathcal{A}(n) = k$ and 0 otherwise.\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    r^{(n)}_k &= \\begin{cases} 1 & \\text{if } \\mathcal{A}(n) = k \\\\ 0 & \\text{otherwise} \\end{cases}\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "-   $\\hat{C}_k$ is the set of data points that are assigned to cluster $k$.\n",
    "-   $\\left\\|\\cdot\\right\\|$ is the euclidean norm.\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\left\\|\\mathbf{x} - \\boldsymbol{v}\\right\\|^2 &= \\left(\\mathbf{x} - \\boldsymbol{v}\\right)^{\\top} \\left(\\mathbf{x} - \\boldsymbol{v}\\right) \\\\\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "-   All 5 forms are equivalent[^equivalent-k-means-cost-function].\n",
    "\n",
    "It is worth a reminder that we have not formally defined what the assignment\n",
    "$\\mathcal{A}(\\cdot)$ is, as well as the representative vectors (centroids)\n",
    "$\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\dots, \\boldsymbol{v}_K$. We will show\n",
    "later that $\\boldsymbol{v}_k$ is the mean of the data points in cluster $k$ and\n",
    "that\n",
    "$\\mathcal{A}(n)=\\underset{k}{\\operatorname{argmin}} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{\\mu}_k \\right\\|^2$\n",
    "is the assignment that minimizes the cost function\n",
    "$\\widehat{\\mathcal{J}}_{\\mathcal{S}}$.\n",
    "\n",
    "```{prf:remark} Cost Function is a Function of Assignment and Centroids\n",
    ":label: remark:cost-function-is-a-function-of-assignment-and-centroids\n",
    "\n",
    "The cost function is a function **both** the assignment $\\mathcal{A}(\\cdot)$ and the cluster centers $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\dots, \\boldsymbol{v}_K$,\n",
    "which adds up the squared euclidean distance between each data point and its assigned cluster center. The\n",
    "total cost is what we are minimizing. Note that the problem is equivalent to minimizing each\n",
    "cluster's cost individually.\n",
    "\n",
    "We also call the loss sum of squared error (SSE) , which is just the intra-cluster variance, a measure of how spread out the data points are within a cluster.\n",
    "```\n",
    "\n",
    "## Objective Function\n",
    "\n",
    "Finally, we define the objective function as the cost function\n",
    "$\\widehat{\\mathcal{J}}_{\\mathcal{S}}$ that we are minimizing.\n",
    "\n",
    "````{prf:definition} K-Means Objective Function\n",
    ":label: def:kmeans-objective\n",
    "\n",
    "The **objective** function is to **minimize** the above expression in equation {eq}`eq:kmeans-cost-1`:\n",
    "\n",
    "```{math}\n",
    ":label: eq:k-means-objective-function-1\n",
    "\n",
    "\\begin{alignat}{3}\n",
    "\\underset{\\mathcal{A}, \\boldsymbol{v}_k}{\\operatorname{argmin}} &\\quad& \\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\mathcal{A}, \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K) &= \\sum_{n=1}^{N} \\sum_{\\mathcal{A}(n) = k} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2  \\\\\n",
    "\\text{subject to} &\\quad& \\hat{C}_1 \\sqcup \\hat{C}_2 \\sqcup \\cdots \\sqcup \\hat{C}_K &= \\mathcal{S} \\\\\n",
    "&\\quad& \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K &\\in \\mathbb{R}^D \\\\\n",
    "\\end{alignat}\n",
    "```\n",
    "````\n",
    "\n",
    "This just means, for all possible assignments $\\mathcal{A}(\\cdot)$ and cluster\n",
    "centers $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K$, we want\n",
    "to find the assignment $\\mathcal{A}(\\cdot)$ and cluster centers\n",
    "$\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K$ that minimize the\n",
    "cost function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}$.\n",
    "\n",
    "In other words, of all possible sets (there's a lot, as we should see later)\n",
    "$\\hat{\\mathcal{C}} = \\left\\{\\hat{C}_1, \\hat{C}_2, \\ldots, \\hat{C}_K\\right\\}$, we\n",
    "want to find the set $\\hat{\\mathcal{C}}$ that minimizes the cost function\n",
    "$\\widehat{\\mathcal{J}}_{\\mathcal{S}}$.\n",
    "\n",
    "```{prf:theorem} Minimizing Individual Cluster's Cost is Equivalent to Minimizing the Objective Function\n",
    ":label: thm:minimizing-individual-clusters-cost-is-equivalent-to-minimizing-the-objective-function\n",
    "\n",
    "The objective function is equivalent to minimizing each cluster's cost individually.\n",
    "```\n",
    "\n",
    "Recall we mentioned that optimizing the objective function\n",
    "$\\widehat{\\mathcal{J}}_{\\mathcal{S}}$ means we are finding the optimal\n",
    "assignment $\\mathcal{A}^*(\\cdot)$ and the optimal cluster centers\n",
    "$\\boldsymbol{v}_1^*, \\boldsymbol{v}_2^*, \\ldots, \\boldsymbol{v}_K^*$ at the same\n",
    "time. This is challenging as $\\widehat{\\mathcal{J}}_{\\mathcal{S}}$ is a\n",
    "non-convex function of $\\mathcal{A}(\\cdot)$ and\n",
    "$\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K$. We will now fall\n",
    "back on heuristics to find the local optimum. In what follows, we will list the\n",
    "_necessary_ conditions to minimize the objective function\n",
    "$\\widehat{\\mathcal{J}}_{\\mathcal{S}}$.\n",
    "\n",
    "## The Necessary Conditions to Minimize the Objective Function\n",
    "\n",
    "With all the definitions in place, we can now formally state the necessary\n",
    "conditions to minimize the objective function.\n",
    "\n",
    "Note a necessary condition only guarantees that if a solution is optimal, then\n",
    "the conditions must be satisfied. However, if a solution does satisfy the\n",
    "conditions, it does not necessarily mean that it is optimal. In short, we may\n",
    "land ourselves with a **local** minimum that is not **globally** optimal.\n",
    "\n",
    "### Condition 1: The Optimal Assignment\n",
    "\n",
    "````{prf:criterion} K-Means Optimal Assignment\n",
    ":label: criterion:kmeans-optimal-assignment\n",
    "\n",
    "Fix the cluster centers $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K$, we seek\n",
    "the optimal assignment $\\mathcal{A}^*(\\cdot)$ that minimizes the cost function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\cdot)$.\n",
    "\n",
    "We claim that the optimal assignment $\\mathcal{A}^*(\\cdot)$ follows the *nearest neighbor* rule, which means that,\n",
    "\n",
    "```{math}\n",
    ":label: eq:k-means-criterion-1.1\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathcal{A}^*(n) = \\underset{k \\in \\{1, 2, \\ldots, K\\}}{\\operatorname{argmin}} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2 .\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "Then the assignment $\\mathcal{A}^*$ is the optimal assignment that minimizes the cost function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}$.\n",
    "\n",
    "This is quite intuitive as we are merely assigning each data point $\\mathbf{x}^{(n)}$ to cluster $k$\n",
    "whose center $\\boldsymbol{v}_k$ is closest to $\\mathbf{x}^{(n)}$.\n",
    "\n",
    "We rephrase the claim by saying that for any assignment $\\mathcal{A}$, we have\n",
    "\n",
    "```{math}\n",
    ":label: eq:k-means-criterion-1.2\n",
    "\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\mathcal{A}, \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K) &\\geq \\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\mathcal{A}^*, \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K) \\\\\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "Let's prove this claim.\n",
    "````\n",
    "\n",
    "```{prf:proof}\n",
    "In equation {eq}`eq:k-means-criterion-1.2`, we have that $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K$ are fixed.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\mathcal{A}, \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K) &= \\sum_{n=1}^{N} \\sum_{\\mathcal{A}(n) = k} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2 \\\\\n",
    "&\\geq \\sum_{n=1}^{N} \\sum_{\\mathcal{A}^*(n) = k} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2 \\\\\n",
    "&= \\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\mathcal{A}^*, \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is just a proof by definition of $\\mathcal{A}^*$ since\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_{\\mathcal{A}(n)} \\right\\|^2 \\geq \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_{\\mathcal{A}^*(n)} \\right\\|^2 .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If you look at it intuitively, it just means there does not exist any other arrangement/assignment $\\mathcal{A}$\n",
    "that can reduce the cost function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}$ better than the optimal assignment $\\mathcal{A}^*$ (nearest neighbor rule).\n",
    "```\n",
    "\n",
    "### Condition 2: The Optimal Cluster Centers (Centroids)\n",
    "\n",
    "````{prf:criterion} K-Means Optimal Cluster Centers\n",
    ":label: criterion:kmeans-optimal-cluster-centers\n",
    "\n",
    "Fix the assignment $\\mathcal{A}^*(\\cdot)$, we seek the optimal cluster centers $\\boldsymbol{v}_1^*, \\boldsymbol{v}_2^*, \\ldots, \\boldsymbol{v}_K^*$ that minimize the cost function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}$.\n",
    "\n",
    "We claim that the optimal cluster centers is the mean of the data points assigned to each cluster.\n",
    "\n",
    "```{math}\n",
    ":label: eq:k-means-criterion-2.1\n",
    "\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{v}_k^* = \\frac{1}{\\left|\\hat{C}_k^*\\right|} \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k^*} \\mathbf{x}^{(n)}\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "where $\\left|\\hat{C}_k^*\\right|$ is the number of data points assigned to cluster $k$. We can denote it\n",
    "as $N_k$ for convenience.\n",
    "\n",
    "We can also rephrase this claim by saying that for any cluster centers $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K$, fixing the assignment $\\mathcal{A}^*$, we have\n",
    "\n",
    "```{math}\n",
    ":label: eq:k-means-criterion-2.2\n",
    "\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\mathcal{A}^*, \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_K) &\\geq \\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\mathcal{A}^*, \\boldsymbol{v}_1^*, \\boldsymbol{v}_2^*, \\ldots, \\boldsymbol{v}_K^*) \\\\\n",
    "\\end{aligned}\n",
    "```\n",
    "````\n",
    "\n",
    "````{prf:proof}\n",
    "This proof in short just says that the mean minimizes the sum of squared distances.\n",
    "\n",
    "Since we established ({prf:ref}`thm:minimizing-individual-clusters-cost-is-equivalent-to-minimizing-the-objective-function`)\n",
    "that minimizing each individual cluster $\\hat{C}_k$ is equivalent to minimizing the cost function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}$,\n",
    "we can now fix any cluster $\\hat{C}_k$ (i.e. also fixing the assignment $\\mathcal{A}^*$) and seek the optimal cluster center $\\boldsymbol{v}_k^*$ that minimizes the cost function $\\widehat{\\mathcal{J}}_{\\hat{C}_k}$.\n",
    "\n",
    "Note after fixing the assignment $\\mathcal{A}^*(\\cdot)$, $\\widehat{\\mathcal{J}}_{\\hat{C}_k}$ is now just a\n",
    "function of $\\boldsymbol{v}_k$ and is the cost for that cluster.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}_{\\hat{C}_k}(\\boldsymbol{v}_k) = \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can now take the derivative of $\\widehat{\\mathcal{J}}_{\\hat{C}_k}$ with respect to $\\boldsymbol{v}_k$ and set it to zero to find the optimal cluster center $\\boldsymbol{v}_k^*$.\n",
    "\n",
    "```{math}\n",
    ":label: eq:derivative-of-k-means-cost-function\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\boldsymbol{v}_k} \\widehat{\\mathcal{J}}_{\\hat{C}_k}(\\boldsymbol{v}_k) &= \\frac{\\partial}{\\partial \\boldsymbol{v}_k} \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right\\|^2 \\\\\n",
    "&= 2 \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\left(\\mathbf{x}^{(n)} - \\boldsymbol{v}_k \\right) \\\\\n",
    "&= 2 \\left( \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\mathbf{x}^{(n)} - \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\boldsymbol{v}_k \\right) \\\\\n",
    "&= 2 \\left( \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\mathbf{x}^{(n)} - N_k \\boldsymbol{v}_k \\right) \\\\\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "where $N_k$ is the number of data points assigned to cluster $k$.\n",
    "\n",
    "Now to minimize equation {eq}`eq:derivative-of-k-means-cost-function`, we set it to zero and solve for $\\boldsymbol{v}_k$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\boldsymbol{v}_k} \\widehat{\\mathcal{J}}_{\\hat{C}_k}(\\boldsymbol{v}_k) = 0 &\\iff 2 \\left( \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\mathbf{x}^{(n)} - N_k \\boldsymbol{v}_k \\right) = 0 \\\\\n",
    "&\\iff \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\mathbf{x}^{(n)} - N_k \\boldsymbol{v}_k = 0 \\\\\n",
    "&\\iff \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\mathbf{x}^{(n)} = N_k \\boldsymbol{v}_k \\\\\n",
    "&\\iff \\boldsymbol{v}_k = \\frac{1}{N_k} \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k} \\mathbf{x}^{(n)} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "recovering {prf:ref}`criterion:kmeans-optimal-cluster-centers`.\n",
    "\n",
    "There are other variants of [proof](https://math.stackexchange.com/questions/967138/formal-proof-that-mean-minimize-squared-error-function).\n",
    "````\n",
    "\n",
    "```{prf:remark} Notation\n",
    ":label: prf:remark:kmeans-optimal-cluster-centers-notation\n",
    "\n",
    "We will now denote $\\boldsymbol{v}_k^*$ as $\\boldsymbol{\\mu}_k$ in the following sections.\n",
    "```\n",
    "\n",
    "### Objective Function Re-defined\n",
    "\n",
    "We can now re-define the objective function in equation\n",
    "{eq}`eq:k-means-objective-function-1` in terms of the optimal cluster centers\n",
    "and assignments.\n",
    "\n",
    "```{math}\n",
    ":label: eq:k-means-objective-function-2\n",
    "\n",
    "\\begin{alignat}{4}\n",
    "\\underset{\\mathcal{A}, \\boldsymbol{\\mu}_k}{\\operatorname{argmin}} &\\quad& \\widehat{\\mathcal{J}}_{\\mathcal{S}}(\\mathcal{A}, \\boldsymbol{\\mu}_1, \\boldsymbol{\\mu}_2, \\ldots, \\boldsymbol{\\mu}_K) &= \\sum_{n=1}^{N} \\sum_{\\mathcal{A}(n) = k} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{\\mu}_k \\right\\|^2  \\\\\n",
    "\\text{subject to} &\\quad& \\hat{C}_1 \\cup \\hat{C}_2 \\cup \\cdots \\cup \\hat{C}_K &= \\mathcal{S} \\\\\n",
    "&\\quad& \\boldsymbol{\\mu}_1, \\boldsymbol{\\mu}_2, \\ldots, \\boldsymbol{\\mu}_K &\\in \\mathbb{R}^D \\\\\n",
    "&\\quad& \\hat{y}^{(n)} := \\mathcal{A}(n) &= \\underset{k}{\\operatorname{argmin}} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{\\mu}_k \\right\\|^2 \\\\\n",
    "\\end{alignat}\n",
    "```\n",
    "\n",
    "```{prf:remark} Cost Function is a function of assignments and cluster centers\n",
    ":label: remark:kmeans-cost-function-is-a-function-of-assignments-and-cluster-centers\n",
    "\n",
    "Reminder!\n",
    "\n",
    "The cost function in equation {eq}`eq:k-means-objective-function-2` is a function of **both** the cluster assignments and cluster centers.\n",
    "And therefore we are minimizing the cost function with respect to the cluster assignments and cluster centers. However,\n",
    "jointly optimizing both the cluster assignments and cluster centers is computationally challenging, and therefore\n",
    "we split to two steps, first optimizing the cluster assignments and then optimizing the cluster centers in a greedy manner.\n",
    "```\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "We are now ready to define the full Lloyd's algorithm for K-Means.\n",
    "\n",
    "````{prf:algorithm} Lloyd's Algorithm (K-Means)\n",
    ":label: lloyd-kmeans-algorithm\n",
    "\n",
    "Given a set of data points (samples)\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\left\\{\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots, \\mathbf{x}^{(N)}\\right\\}\n",
    "$$\n",
    "\n",
    "the K-Means algorithm aims to group the data points into $K$ clusters\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{C}} = \\left\\{ \\hat{C}_1, \\hat{C}_2, \\dots, \\hat{C}_K \\right\\}\n",
    "$$\n",
    "\n",
    "such that the sum of squared distances\n",
    "between each data point and its cluster center is minimized.\n",
    "\n",
    "In code, $\\hat{\\mathcal{C}}$ can be treated as a dictionary/hash map,\n",
    "where the **key** is the cluster number and the **value** is the set of data points assigned to that cluster.\n",
    "\n",
    "\n",
    "1. **Initialization Step**: Initialize $K$ cluster centers $\\boldsymbol{\\mu}_1^{[0]}, \\boldsymbol{\\mu}_2^{[0]}, \\dots, \\boldsymbol{\\mu}_K^{[0]}$ randomly (best to be far apart)\n",
    "where the superscript $[0]$ denotes the iteration number $t=0$.\n",
    "    - In the very first iteration, there are no data points in any cluster $\\hat{C}_k^{[0]} = \\emptyset$. Therefore, the cluster centers are just randomly chosen for simplicity.\n",
    "    - By random, we mean that the cluster centers are randomly chosen from the data points $\\mathcal{S} = \\left\\{\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots, \\mathbf{x}^{(N)}\\right\\}$\n",
    "    and not randomly chosen from the feature space $\\mathbb{R}^D$.\n",
    "    - Subsequent iterations will have data points in the clusters $\\hat{C}_k^{[t]} \\neq \\emptyset$ and thus\n",
    "    $\\boldsymbol{\\mu}_k^{[t]}$ will be the mean of the data points in cluster $k$.\n",
    "    - Each $\\boldsymbol{\\mu}_k^{[0]} = \\begin{bmatrix} \\mu_{1k}^{[0]} & \\mu_{2k}^{[0]} & \\cdots & \\mu_{Dk}^{[0]} \\end{bmatrix}^{\\mathrm{T}}$ is a $D$-dimensional vector, where $D$ is the number of features, and represents the\n",
    "    mean vector of all the data points in cluster $k$.\n",
    "    - Note that $\\mu_{dk}^{[0]}$ is the mean value of the $d$-th feature in cluster $k$.\n",
    "    - We denote $\\boldsymbol{\\mu} = \\begin{bmatrix} \\boldsymbol{\\mu}_1 & \\boldsymbol{\\mu}_2 & \\cdots & \\boldsymbol{\\mu}_K \\end{bmatrix}_{K \\times D}^{\\mathrm{T}}$ to be the collection of all $\\boldsymbol{\\mu}_1, \\boldsymbol{\\mu}_2, \\dots, \\boldsymbol{\\mu}_K$.\n",
    "\n",
    "\n",
    "2. **Assignment Step (E)**: For $t=0, 1, 2, \\dots$, assign each data point $\\mathbf{x}^{(n)}$ to the closest cluster center $\\boldsymbol{\\mu}_k^{[t]}$,\n",
    "\n",
    "    ```{math}\n",
    "    :label: eq:kmeans-classify\n",
    "\n",
    "    \\begin{aligned}\n",
    "    \\hat{y}^{(n)[t]} := \\mathcal{A}^{*(n)[t]} &= \\underset{k \\in \\{1, 2, \\ldots, K\\}}{\\operatorname{argmin}} \\left\\| \\mathbf{x}^{(n)} - \\boldsymbol{\\mu}_k^{[t]} \\right\\|^2 \\\\\n",
    "    \\end{aligned}\n",
    "    ```\n",
    "\n",
    "    In other words, $\\hat{y}^{(n)[t]}$ is the output of the optimal assignment rule at the $t$-th iteration\n",
    "    and is the index of the cluster center $\\boldsymbol{\\mu}_k^{[t]}$ that is closest to $\\mathbf{x}^{(n)}$.\n",
    "\n",
    "    For instance, if $K = 3$, and for the first sample point $n=1$,\n",
    "    assume the closest cluster center is $\\boldsymbol{\\mu}_2^{[t]}$, then the assignment $\\mathcal{A}^{*}$ will\n",
    "    assign this point to cluster $k=2$, $\\hat{y}^{(1)} = 2$. Note that $\\hat{y}^{(n)}$ is a scalar and has the same superscript as $\\mathbf{x}^{(n)}$, indicating they belong to the same sample.\n",
    "\n",
    "    For notational convenience, we can also denote $\\hat{C}_k^{[t]}$ as the set of data points that are assigned to cluster $k$:\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\hat{C}_k^{[t]} &= \\left\\{ \\mathbf{x}^{(n)} \\mid \\hat{y}^{(n)} = k \\right\\}\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "    Mathematically, this means partitioning the data points using [Voronoi Diagram](https://en.wikipedia.org/wiki/Voronoi_diagram),\n",
    "    as mentioned in the previous section {prf:ref}`def:kmeans-voronoi-partition`.\n",
    "\n",
    "    **Note, at this step, we obtain the cost function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}^{[t]}$ at iteration $t$**:\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\widehat{\\mathcal{J}}_{\\mathcal{S}}^{[t]}\\left(\\mathcal{A}^{*(1)[t]}, \\mathcal{A}^{*(2)[t]}, \\dots, \\mathcal{A}^{*(N)[t]}, \\boldsymbol{\\mu}_1^{[t]}, \\boldsymbol{\\mu}_2^{[t]}, \\dots, \\boldsymbol{\\mu}_K^{[t]}\\right) &= \\widehat{\\mathcal{J}}_{\\mathcal{S}}^{[t]}\\left(\\hat{C}_1^{[t]}, \\hat{C}_2^{[t]}, \\dots, \\hat{C}_K^{[t]}, \\boldsymbol{\\mu}_1^{[t]}, \\boldsymbol{\\mu}_2^{[t]}, \\dots, \\boldsymbol{\\mu}_K^{[t]}\\right) \\\\\n",
    "      &= \\sum_{k=1}^K \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k^{[t]}} \\left\\| \\mathbf{x}^{(n)} - \\boldsymbol{\\mu}_k^{[t]} \\right\\|^2 \\\\\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "    This makes sense because once we obtain the cluster centers $\\boldsymbol{\\mu}_k^{[t]}$ for the $t$-th iteration,\n",
    "    then it induces the partition $\\hat{C}_k^{[t]}$ and the cost function $\\widehat{\\mathcal{J}}_{\\mathcal{S}}^{[t]}$.\n",
    "    **However, one important thing to realize is that the mean $\\boldsymbol{\\mu}_k^{[t]}$ is not\n",
    "    necessarily the \"best\", and hence we need to update the cluster centers $\\boldsymbol{\\mu}_k^{[t]}$ in the next step.**\n",
    "\n",
    "3. **Update Step (M)**: Update the cluster centers for the next iteration.\n",
    "\n",
    "    ```{math}\n",
    "    :label: eq:kmeans-recenter\n",
    "\n",
    "    \\begin{aligned}\n",
    "    \\boldsymbol{\\mu}_k^{[t+1]} &= \\frac{1}{|\\hat{C}_k^{[t]}|} \\sum_{\\mathbf{x}^{(n)} \\in \\hat{C}_k^{[t]}} \\mathbf{x}^{(n)} \\\\\n",
    "    \\end{aligned}\n",
    "    ```\n",
    "\n",
    "    Notice that the cluster center $\\boldsymbol{\\mu}_k^{[t+1]}$ is the mean of all data points that are assigned to cluster $k$.\n",
    "\n",
    "    It may be confusing to define the cluster center $\\boldsymbol{\\mu}_k^{[t+1]}$ for the next iteration, but this will\n",
    "    be apparent later.\n",
    "\n",
    "4. Repeat steps 2 and 3 until the centroids stop changing.\n",
    "\n",
    "    ```{math}\n",
    "    :label: eq:kmeans-convergence\n",
    "\n",
    "    \\begin{aligned}\n",
    "    \\boldsymbol{\\mu}_k^{[t+1]} = \\boldsymbol{\\mu}_k^{[t]}\n",
    "    \\end{aligned}\n",
    "    ```\n",
    "\n",
    "    In other words,\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\widehat{\\mathcal{J}}_{\\mathcal{S}}^{[t+1]}\\left(\\mathcal{A}^{*[t+1]}, \\boldsymbol{\\mu}^{[t+1]} \\right) = \\widehat{\\mathcal{J}}_{\\mathcal{S}}^{[t]}\\left(\\mathcal{A}^{*[t]}, \\boldsymbol{\\mu}^{[t]} \\right)\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "    This is the convergence condition.\n",
    "````\n",
    "\n",
    "```{prf:remark} K-Means is a Greedy Algorithm\n",
    ":label: remark-kmeans-greedy\n",
    "\n",
    "It is important to recognize that the K-Means (Lloyd's) Algorithm optimizes two objectives in an alternating fashion.\n",
    "It alternatively changes both the assignment step $\\mathcal{A}^{*}(\\cdot)$ and the update step $\\boldsymbol{\\mu}_k^{[t+1]}$\n",
    "to greedily minimize the cost function $\\widehat{\\mathcal{J}}(\\mathcal{A}, \\boldsymbol{\\mu})$.\n",
    "```\n",
    "\n",
    "## Model Fitting\n",
    "\n",
    "The fitting of the K-Means model is straightforward, we directly apply the\n",
    "K-Means Algorithm defined in {prf:ref}`lloyd-kmeans-algorithm` to the training\n",
    "data $\\mathcal{S}$ to obtain the optimal assignment $\\mathcal{A}^{*}$ and the\n",
    "optimal cluster centers $\\boldsymbol{\\mu}^{*}$.\n",
    "\n",
    "## Model Inference\n",
    "\n",
    "The inference of the K-Means model is also straightforward, for the new data\n",
    "point $\\mathbf{x}^{(q)}$, we apply the assignment step $\\mathcal{A}^{*}(\\cdot)$\n",
    "to it and find the closest cluster center $\\boldsymbol{\\mu}_k^{*}$, then we\n",
    "assign it to cluster $k$. In other words, we assign $\\mathbf{x}^{(q)}$ to the\n",
    "cluster that has the closest cluster center.\n",
    "\n",
    "## Convergence\n",
    "\n",
    "In this section, we will prove that the K-Means Algorithm converges to a local\n",
    "minimum of the cost function\n",
    "$\\widehat{\\mathcal{J}}(\\mathcal{A}, \\boldsymbol{\\mu})$.\n",
    "\n",
    "### Lemma 1: Stirling Numbers of the Second Kind\n",
    "\n",
    "```{prf:lemma} Stirling Numbers of the Second Kind\n",
    ":label: stirling-numbers\n",
    "\n",
    "The Stirling Numbers of the Second Kind $S(n, k)$ are defined as the number of ways to partition a set of $n$ elements into $k$ non-empty subsets.\n",
    "\n",
    "There are at most $k^n$ ways to partition a set of $n$ elements into $k$ non-empty subsets.\n",
    "\n",
    "In our case, since there are $N$ data points, and we want to partition them into $K$ clusters, there are at most $K^N$ ways to partition the data points into $K$ clusters.\n",
    "\n",
    "In other words, the assignment step $\\mathcal{A}(\\cdot)$ has at most $K^N$ possible mappings.\n",
    "The same applies to the update step $\\boldsymbol{\\mu}_k$ since $\\boldsymbol{\\mu}_k$ is dependent on the assignment step $\\mathcal{A}(\\cdot)$.\n",
    "```\n",
    "\n",
    "(cost-function-monotically-decreases)=\n",
    "\n",
    "### Lemma 2: Cost Function of K-Means Monotonically Decreases\n",
    "\n",
    "```{prf:lemma} Cost Function of K-Means Monotonically Decreases\n",
    ":label: kmeans-monotonic-decrease\n",
    "\n",
    "The cost function $\\widehat{\\mathcal{J}}$ of K-Means monotonically decreases. This means\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}_{\\mathcal{S}}^{[t+1]}\\left(\\hat{C}_1^{[t+1]}, \\ldots, \\hat{C}_K^{[t+1]}, \\boldsymbol{\\mu}_1^{[t+1]}, \\ldots, \\boldsymbol{\\mu}_K^{[t+1]} \\right) \\leq \\widehat{\\mathcal{J}}_{\\mathcal{S}}^{[t]}\\left(\\hat{C}_1^{[t]}, \\ldots, \\hat{C}_K^{[t]}, \\boldsymbol{\\mu}_1^{[t]}, \\ldots, \\boldsymbol{\\mu}_K^{[t]} \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for each iteration $t$.\n",
    "```\n",
    "\n",
    "````{prf:proof}\n",
    "This is a consequence of {prf:ref}`criterion:kmeans-optimal-assignment` and {prf:ref}`criterion:kmeans-optimal-cluster-centers`.\n",
    "\n",
    "In particular, the objective function $\\widehat{\\mathcal{J}}$ is made up of two steps, the assignment step and the update step. We minimize the assignment step by finding the optimal assignment $\\mathcal{A}^{*}(\\cdot)$, and we minimize the update step by finding the optimal cluster centers $\\boldsymbol{\\mu}_k^{*}$ based on the optimal assignment $\\mathcal{A}^{*}(\\cdot)$ at each iteration.\n",
    "\n",
    "Consequently, if we can show that the following:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}_{\\mathcal{S}}\\left(\\hat{C}_1^{[t]}, \\ldots, \\hat{C}_K^{[t]}, \\boldsymbol{\\mu}_1^{[t+1]}, \\ldots, \\boldsymbol{\\mu}_K^{[t+1]} \\right) \\leq \\widehat{\\mathcal{J}}_{\\mathcal{S}}\\left(\\hat{C}_1^{[t]}, \\ldots, \\hat{C}_K^{[t]}, \\boldsymbol{\\mu}_1^{[t]}, \\ldots, \\boldsymbol{\\mu}_K^{[t]} \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and then,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}_{\\mathcal{S}}\\left(\\hat{C}_1^{[t+1]}, \\ldots, \\hat{C}_K^{[t+1]}, \\boldsymbol{\\mu}_1^{[t+1]}, \\ldots, \\boldsymbol{\\mu}_K^{[t+1]} \\right) \\leq \\widehat{\\mathcal{J}}_{\\mathcal{S}}\\left(\\hat{C}_1^{[t]}, \\ldots, \\hat{C}_K^{[t]}, \\boldsymbol{\\mu}_1^{[t+1]}, \\ldots, \\boldsymbol{\\mu}_K^{[t+1]} \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "then we can easily show that the cost function $\\widehat{\\mathcal{J}}$ monotonically decreases.\n",
    "\n",
    "---\n",
    "\n",
    "**First**, once all the samples $\\left\\{\\mathbf{x}^{(n)}\\right\\}_{n=1}^N$ are assigned to the clusters as\n",
    "per the **assignment step** in {eq}`eq:kmeans-classify`, we will recover the cost at\n",
    "the $t$-th iteration, defined as:\n",
    "\n",
    "```{math}\n",
    ":label: eq:kmeans-convergence-1\n",
    "\n",
    "\\widehat{\\mathcal{J}}_{\\mathcal{S}}^{[t]}\\left(\\hat{C}_1^{[t]}, \\ldots, \\hat{C}_K^{[t]}, \\boldsymbol{\\mu}_1^{[t]}, \\ldots, \\boldsymbol{\\mu}_K^{[t]} \\right) = \\sum_{k=1}^K \\sum_{n \\in \\hat{C}_k^{[t]}} \\left\\|\\mathbf{x}^{(n)} - \\boldsymbol{\\mu}_k^{[t]}\\right\\|^2\n",
    "````\n",
    "\n",
    "Note in particular that the base case is we initialized the cluster centers\n",
    "$\\boldsymbol{\\mu}_k^{[0]}$ for the first iteration $t=0$ and this induces the\n",
    "clusters $\\hat{C}_1^{[0]}, \\ldots, \\hat{C}_K^{[0]}$ for which we assigned each\n",
    "data point $\\mathbf{x}^{(n)}$ to the closest cluster center\n",
    "$\\boldsymbol{\\mu}_k^{[0]}$. If we just look at the base case, the mean is\n",
    "randomly initialized, and so there may be room of improvement, which is why we\n",
    "need the **update step**.\n",
    "\n",
    "---\n",
    "\n",
    "**Next**, we recalculate the cluster centers $\\boldsymbol{\\mu}_k^{[t+1]}$ based\n",
    "on the clusters for the $t$-th iteration. In other words, we find the cluster\n",
    "centers for the $t+1$-th iteration based on the cluster assignments for the\n",
    "$t$-th iteration. We claim that this new cluster centers\n",
    "$\\boldsymbol{\\mu}_k^{[t+1]}$ will minimize the cost function\n",
    "$\\widehat{\\mathcal{J}}$.\n",
    "\n",
    "We **fix** the assignment $\\hat{C}_1^{[t]}, \\ldots, \\hat{C}_K^{[t]}$, and then\n",
    "show that the cluster centers\n",
    "$\\boldsymbol{\\mu}_k^{[t+1]} = \\dfrac{1}{\\left|\\hat{C}_k^{[t]}\\right|} \\sum_{n \\in \\hat{C}_k^{[t]}} \\mathbf{x}^{(n)}$\n",
    "minimizes the cost function $\\widehat{\\mathcal{J}}$, which means:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{\\mu}_1^{[t+1]}, \\ldots, \\boldsymbol{\\mu}_K^{[t+1]} = \\underset{\\boldsymbol{\\mu}_1, \\ldots, \\boldsymbol{\\mu}_K}{\\operatorname{argmin}} \\widehat{\\mathcal{J}}_{\\mathcal{S}}\\left(\\hat{C}_1^{[t]}, \\ldots, \\hat{C}_K^{[t]}, \\boldsymbol{\\mu}_1, \\ldots, \\boldsymbol{\\mu}_K \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and subsequently,\n",
    "\n",
    "```{math}\n",
    ":label: eq:kmeans-convergence-2\n",
    "\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}_{\\mathcal{S}}\\left(\\hat{C}_1^{[t]}, \\ldots, \\hat{C}_K^{[t]}, \\boldsymbol{\\mu}_1^{[t+1]}, \\ldots, \\boldsymbol{\\mu}_K^{[t+1]} \\right) \\leq \\widehat{\\mathcal{J}}_{\\mathcal{S}}\\left(\\hat{C}_1^{[t]}, \\ldots, \\hat{C}_K^{[t]}, \\boldsymbol{\\mu}_1^{[t]}, \\ldots, \\boldsymbol{\\mu}_K^{[t]} \\right)\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "because this step cannot increase the cost\n",
    "$\\widehat{\\mathcal{J}}\\left(\\hat{C}_1^{[t]}, \\ldots, \\hat{C}_K^{[t]}, \\boldsymbol{\\mu}_1^{[t]}, \\ldots, \\boldsymbol{\\mu}_K^{[t]} \\right)$\n",
    "as the new cluster centers minimizes the cost function $\\widehat{\\mathcal{J}}$\n",
    "when we replace the cluster centers\n",
    "$\\boldsymbol{\\mu}_1^{[t]}, \\ldots, \\boldsymbol{\\mu}_K^{[t]}$ by the new cluster\n",
    "centers $\\boldsymbol{\\mu}_1^{[t+1]}, \\ldots, \\boldsymbol{\\mu}_K^{[t+1]}$.\n",
    "\n",
    "---\n",
    "\n",
    "**Next**, as we have finished one cycle in the $t$-th iteration, now we turn our\n",
    "attention to the $t+1$-th iteration. As usual, we look at the first step, which\n",
    "is the **assignment step**. We **fix** the cluster centers\n",
    "$\\boldsymbol{\\mu}_k^{[t+1]}$ found in the previous step, and then show that the\n",
    "assignment $\\hat{C}_1^{[t+1]}, \\ldots, \\hat{C}_K^{[t+1]}$ will minimize the cost\n",
    "function $\\widehat{\\mathcal{J}}$, which means:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{C}_1^{[t+1]}, \\ldots, \\hat{C}_K^{[t+1]} = \\underset{\\hat{C}_1, \\ldots, \\hat{C}_K}{\\operatorname{argmin}} \\widehat{\\mathcal{J}}_{\\mathcal{S}}\\left(\\hat{C}_1, \\ldots, \\hat{C}_K, \\boldsymbol{\\mu}_1^{[t+1]}, \\ldots, \\boldsymbol{\\mu}_K^{[t+1]} \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and subsequently,\n",
    "\n",
    "```{math}\n",
    ":label: eq:kmeans-convergence-3\n",
    "\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}_{\\mathcal{S}}\\left(\\hat{C}_1^{[t+1]}, \\ldots, \\hat{C}_K^{[t+1]}, \\boldsymbol{\\mu}_1^{[t+1]}, \\ldots, \\boldsymbol{\\mu}_K^{[t+1]} \\right) \\leq \\widehat{\\mathcal{J}}_{\\mathcal{S}}\\left(\\hat{C}_1^{[t]}, \\ldots, \\hat{C}_K^{[t]}, \\boldsymbol{\\mu}_1^{[t+1]}, \\ldots, \\boldsymbol{\\mu}_K^{[t+1]} \\right)\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "because this step cannot increase the cost\n",
    "$\\widehat{\\mathcal{J}}\\left(\\hat{C}_1^{[t]}, \\ldots, \\hat{C}_K^{[t]}, \\boldsymbol{\\mu}_1^{[t+1]}, \\ldots, \\boldsymbol{\\mu}_K^{[t+1]} \\right)$\n",
    "as the new assignments minimizes the cost function $\\widehat{\\mathcal{J}}$ when\n",
    "we replace the cluster assignments $\\hat{C}_1^{[t]}, \\ldots, \\hat{C}_K^{[t]}$ by\n",
    "the new assignments $\\hat{C}_1^{[t+1]}, \\ldots, \\hat{C}_K^{[t+1]}$.\n",
    "\n",
    "---\n",
    "\n",
    "**Finally**, we can show that the cost function $\\widehat{\\mathcal{J}}$ is\n",
    "**decreasing** in each iteration.\n",
    "\n",
    "Combining {eq}`eq:kmeans-convergence-2` and {eq}`eq:kmeans-convergence-3`, we\n",
    "have the following inequality:\n",
    "\n",
    "```{math}\n",
    ":label: eq:kmeans-convergence-4\n",
    "\n",
    "\\begin{aligned}\n",
    "\\widehat{\\mathcal{J}}_{\\mathcal{S}}^{[t+1]} &= \\widehat{\\mathcal{J}}_{\\mathcal{S}}\\left(\\hat{C}_1^{[t+1]}, \\ldots, \\hat{C}_K^{[t+1]}, \\boldsymbol{\\mu}_1^{[t+1]}, \\ldots, \\boldsymbol{\\mu}_K^{[t+1]} \\right) \\\\\n",
    "&\\leq \\widehat{\\mathcal{J}}_{\\mathcal{S}}\\left(\\hat{C}_1^{[t]}, \\ldots, \\hat{C}_K^{[t]}, \\boldsymbol{\\mu}_1^{[t + 1]}, \\ldots, \\boldsymbol{\\mu}_K^{[t + 1]} \\right) \\\\\n",
    "&\\leq \\widehat{\\mathcal{J}}_{\\mathcal{S}}\\left(\\hat{C}_1^{[t]}, \\ldots, \\hat{C}_K^{[t]}, \\boldsymbol{\\mu}_1^{[t]}, \\ldots, \\boldsymbol{\\mu}_K^{[t]} \\right) = \\widehat{\\mathcal{J}}_{\\mathcal{S}}^{[t]}\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "### Lemma 3: Monotone Convergence Theorem\n",
    "\n",
    "```{prf:lemma} Monotone Convergence Theorem\n",
    ":label: monotone-convergence\n",
    "\n",
    "The [Monotone Convergence Theorem](https://en.wikipedia.org/wiki/Monotone_convergence_theorem) states\n",
    "that if a sequence of functions $\\{f_n\\}$ is non-decreasing and bounded, then the sequence $\\{f_n\\}$ converges to a limit.\n",
    "\n",
    "In our case, the sequence of functions $\\{f_n\\}$ is the sequence of cost functions $\\left\\{\\widehat{\\mathcal{J}}^{[t]}\\right\\}$, and the limit is the cost function $\\widehat{\\mathcal{J}}^{*}$.\n",
    "\n",
    "So it is guaranteed that the sequence of cost functions $\\left\\{\\widehat{\\mathcal{J}}^{[t]}\\right\\}$ converges to the cost function $\\widehat{\\mathcal{J}}^{*}$ locally.\n",
    "```\n",
    "\n",
    "### K-Means Converges in Finite Steps\n",
    "\n",
    "We are now left to show that the sequence of cost functions\n",
    "$\\left\\{\\widehat{\\mathcal{J}}^{[t]}\\right\\}$ is finite, so that\n",
    "$\\left\\{\\widehat{\\mathcal{J}}^{[t]}\\right\\}$ converges in finite steps.\n",
    "\n",
    "Since {prf:ref}`stirling-numbers` states that there exists $K^N$ possible\n",
    "assignments $\\mathcal{A}(\\cdot)$, and simiarly exists $K^N$ possible cluster\n",
    "centers $\\boldsymbol{\\mu}_k$, then there exists $K^N$ possible cost functions\n",
    "$\\widehat{\\mathcal{J}}$. Then,\n",
    "\n",
    "-   At each iteration $t$, the cost function $\\widehat{\\mathcal{J}}^{[t]}$\n",
    "    decreases monotonically.\n",
    "-   This means at $t+1$, if the cost function $\\widehat{\\mathcal{J}}^{[t + 1]}$\n",
    "    decreases, then this means the assignments $\\mathcal{A}^{*[t + 1]}$ are\n",
    "    different from the assignments $\\mathcal{A}^{*[t]}$. Consequently, the\n",
    "    partition never repeats if the cost function $\\widehat{\\mathcal{J}}$\n",
    "    decreases.\n",
    "-   This means it will loop over each possible assignment $\\mathcal{A}$, and\n",
    "    eventually converge to the unique solution $\\mathcal{A}^{*}(\\cdot)$.\n",
    "\n",
    "For that specific initialization, the algorithm has an unique solution, and it\n",
    "is guaranteed to converge to that solution.\n",
    "\n",
    "### Local Minima\n",
    "\n",
    "It is known that K-Means converges in finite steps but does not guarantee\n",
    "convergence to the global minimum. This means that for different\n",
    "initializations, K-Means can converge to different local minima.\n",
    "\n",
    "We can initialize the algorithm with different initializations, and run the\n",
    "algorithm multiple times. Then, we can choose the best solution among the\n",
    "different local minima.\n",
    "\n",
    "## How to find $K$?\n",
    "\n",
    "Since $K$ is a _priori_, we need to choose $K$ before we run the algorithm.\n",
    "Choosing the wrong $K$ will result in a poor clustering. So, how do we choose\n",
    "the right $K$?\n",
    "\n",
    "### Choose $K$ that Minimizes the Cost Function\n",
    "\n",
    "In normal supervised problem, we usually run the algorithm on the train dataset\n",
    "and choose the model that minimizes the cost function on the train dataset, or\n",
    "one that maximizes the performance.\n",
    "\n",
    "Can we do the same for K-Means? The answer is no, this is because our cost\n",
    "funtion monotonically decreases with increasing $K$.\n",
    "\n",
    "This is because we \"cover\" more input space $\\mathcal{X}$ with increasing $K$,\n",
    "thus decreasing the cost function {cite:ps}`pml1Book`.\n",
    "\n",
    "### Elbow Method\n",
    "\n",
    "While this may not be the best method, it is a simple and widely recognized one\n",
    "to choose $K$.\n",
    "\n",
    "The simple heuristic is described as follows:\n",
    "\n",
    "```{prf:algorithm} Elbow Method\n",
    ":label: elbow-method\n",
    "\n",
    "1. Run K-Means with $K$ from 1 to $K_{\\max}$.\n",
    "2. For each $k=0,1,\\ldots, K_{\\max}$, compute the cost function $\\widehat{\\mathcal{J}}_k$\n",
    "3. Plot the cost function $\\widehat{\\mathcal{J}}_k$ against $k$.\n",
    "4. Find the \"elbow\" of the curve, which is the point where the cost function $\\widehat{\\mathcal{J}}_k$ starts to decrease more slowly.\n",
    "```\n",
    "\n",
    "### Other Methods\n",
    "\n",
    "See {cite}`pml1Book` for more methods.\n",
    "\n",
    "## Time and Space Complexity\n",
    "\n",
    "### Brute Force Search and Global Minimum\n",
    "\n",
    "The hypothesis space $\\mathcal{H}$ is finite, implying that if we do a brute\n",
    "force search over all possible clusterings, we can find the global minimum.\n",
    "\n",
    "Quoting from\n",
    "[CMU 10-315](http://www.cs.cmu.edu/~ninamf/courses/315sp19/homeworks/hw6.pdf),\n",
    "we consider the brute-force search to be the following:\n",
    "\n",
    "```{prf:algorithm} Brute Force Search for K-Means\n",
    ":label: brute-force-search-kmeans\n",
    "\n",
    "For each possible cluster\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{C}} = \\left\\{\\hat{C}_1, \\hat{C}_2, \\dots, \\hat{C}_K\\right\\}\n",
    "$$\n",
    "\n",
    "induced by the assignment $\\mathcal{A}$ in $\\mathcal{H}$, compute the\n",
    "optimal centroids\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\mu}} = \\left\\{\\hat{\\boldsymbol{\\mu}}_1, \\hat{\\boldsymbol{\\mu}}_2, \\dots, \\hat{\\boldsymbol{\\mu}}_K\\right\\}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{\\mu}}_k = \\frac{1}{|\\hat{C}_k|} \\sum_{\\mathbf{x} \\in \\hat{C}_k} \\mathbf{x}\n",
    "$$\n",
    "\n",
    "is the mean of the points in the $k$-th cluster.\n",
    "\n",
    "Then, compute the cost function $\\widehat{\\mathcal{J}}$ centroids $\\hat{\\boldsymbol{\\mu}}$.\n",
    "\n",
    "Repeat this for all possible clusterings $\\mathcal{A}(\\cdot)$ in $\\mathcal{H}$ and finally\n",
    "return the clustering $\\hat{C}$ that gives the minimum cost function $\\widehat{\\mathcal{J}}$.\n",
    "```\n",
    "\n",
    "Then the time complexity of the brute force search is exponential with respect\n",
    "to the number of inputs since there are $K^N$ possible clusterings and we are\n",
    "looping over each possible clustering to find the global minimum. Indeed, this\n",
    "has time complexity\n",
    "\n",
    "$$\n",
    "\\mathcal{O}\\left(K^N\\right)\n",
    "$$\n",
    "\n",
    "### Lloyd’s Algorithm\n",
    "\n",
    "Let $T$ denote the number of iterations of Lloyd’s algorithm.\n",
    "\n",
    "Then, the average time complexity of Lloyd’s algorithm is\n",
    "\n",
    "$$\n",
    "\\mathcal{O}(T N K D)\n",
    "$$\n",
    "\n",
    "where $N$ is the number of data points, $K$ is the number of clusters, and $D$\n",
    "is the number of features.\n",
    "\n",
    "This can be easily seen in the python implementation written\n",
    "[here](https://github.com/gao-hongnan/gaohn-probability-stats/blob/machine-learning/src/clustering/kmeans/kmeans.py).\n",
    "We are essentially looping like this:\n",
    "\n",
    "```python\n",
    "for t in range(max_iter):\n",
    "    # E Step: Assign each data point to the closest cluster center\n",
    "    for n in range(n_samples):\n",
    "      # compute argmin distance O(KD) since we are looping over all\n",
    "      # K cluster centers and each cluster center has D features\n",
    "\n",
    "      # do assignment which requires you to loop over all\n",
    "      # K cluster centers: O(N)\n",
    "\n",
    "    # so total O(NKD) here already\n",
    "\n",
    "  # M step: Update the cluster centers\n",
    "  for k in range(n_clusters):\n",
    "    # compute the mean of the points in the k-th cluster: O(KD)\n",
    "    # since we are looping over all K cluster centers and each\n",
    "    # cluster center has D features\n",
    "```\n",
    "\n",
    "where the total time complexity approximately $\\mathcal{O}(T N K D)$.\n",
    "\n",
    "The worst case complexity is given by\n",
    "$\\mathcal{O}\\left(N^{(K+2/D)}\\right)$[^worst-case-time-complexity].\n",
    "\n",
    "```{list-table} Time Complexity of K-Means\n",
    ":header-rows: 1\n",
    ":name: time-complexity-kmeans\n",
    "\n",
    "* - Train\n",
    "  - Inference\n",
    "* - $\\mathcal{O}(NKD)$\n",
    "  - $\\mathcal{O}(KD)$\n",
    "```\n",
    "\n",
    "For space complexity, we need to store the cluster centers $\\boldsymbol{\\mu}_k$\n",
    "and the cluster assignments $\\mathcal{A}(n)$, where the former is a $K \\times D$\n",
    "matrix and the latter is a $N$-dimensional vector. We typically do not include\n",
    "the input data $\\left\\{\\mathbf{x}^{(n)}\\right\\}_{n=1}^N$ in the space complexity\n",
    "since it is given. If included that is $\\mathcal{O}(ND)$, totalling\n",
    "$\\mathcal{O}(N + KD + ND)$.\n",
    "\n",
    "Inference wise, even for a single data point, we need to compute the distance to\n",
    "all cluster centers, so you need to invoke the cluster centers\n",
    "$\\boldsymbol{\\mu}_k$, so roughly is $\\mathcal{O}(KD)$.\n",
    "\n",
    "```{list-table} Space Complexity of K-Means\n",
    ":header-rows: 1\n",
    ":name: space-complexity-kmeans\n",
    "\n",
    "* - Train\n",
    "  - Inference\n",
    "* - $\\mathcal{O}(KD + N)$\n",
    "  - $\\mathcal{O}(KD)$\n",
    "```\n",
    "\n",
    "## When to Use K-Means?\n",
    "\n",
    "See\n",
    "[google's article](https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages).\n",
    "\n",
    "-   Relatively simple to implement.\n",
    "\n",
    "-   Scales to large data sets.\n",
    "\n",
    "-   Guarantees local convergence.\n",
    "\n",
    "-   Can warm-start the positions of centroids.\n",
    "\n",
    "-   Easily adapts to new examples.\n",
    "\n",
    "-   Generalizes to clusters of different shapes and sizes, such as elliptical\n",
    "    clusters.\n",
    "\n",
    "## When can K-Means Fail?\n",
    "\n",
    "-   The number of clusters ($K$) is specified a **priori**, which means we need\n",
    "    to specify the number of clusters before running the algorithm. Choosing $K$\n",
    "    may not be straightforward, especially in the case of high-dimensional data.\n",
    "-   The Lloyd’s algorithm is sensitive to the initial cluster centers. This\n",
    "    means that the algorithm may converge to a local minimum instead of the\n",
    "    global minimum. To remedy this, we can run the algorithm multiple times with\n",
    "    different initial cluster centers.\n",
    "-   K-Means assumes spherical clusters. This is not obvious.\n",
    "\n",
    "    -   K-Means assumes spherical shape because the algorithm uses the euclidean\n",
    "        distance metric to measure the similarity between observations and\n",
    "        centroids. euclidean distance is a measure of straight-line distance\n",
    "        between two points in a euclidean space, and it assumes that the data is\n",
    "        isotropic, meaning that the **variance** along all dimensions is equal,\n",
    "        or (the same in all directions) from the centroid of the cluster. Now\n",
    "        imagine a cluster with an elliptical shape. And imagine the principal\n",
    "        axis is quite long, then two points at the extreme ends of the cluster\n",
    "        will have a large euclidean distance. This means that the cluster may be\n",
    "        split into two clusters by K-Means, which is not what we want. On the\n",
    "        contrary, if the cluster is spherical, then the euclidean distance\n",
    "        between two points at the extreme ends of the cluster will be\n",
    "        equidistant to the centroid.\n",
    "    -   Further quoting\n",
    "        [the answer here](https://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means),\n",
    "        K-means is a special case of Gaussian Mixture Models (GMM). GMM assumes\n",
    "        that the data comes from a mixture of $K$ Gaussian distributions. In\n",
    "        other words, there is a certain probability that the data comes from one\n",
    "        of $K$ of the Gaussian distributions.\n",
    "\n",
    "        If we make the the probability to be in each of the $K$ Gaussians equal\n",
    "        and make the covariance matrices to be $\\sigma^2 \\mathbf{I}$, where\n",
    "        $\\sigma^2 $ is the same fixed constant for each of the $K$ Gaussians,\n",
    "        and take the limit when $\\sigma^2 \\rightarrow 0$ then we get K-means.\n",
    "\n",
    "        So, what does this tell us about the drawbacks of K-means?\n",
    "\n",
    "        1. K-means leads to clusters that look multivariate Gaussian.\n",
    "        2. Since the variance across the variables is the same, K-means leads to\n",
    "           clusters that look spherical.\n",
    "        3. Not only do clusters look spherical, but since the covariance matrix\n",
    "           is the same across the $K$ groups, K-means leads to clusters that\n",
    "           look like the same sphere.\n",
    "        4. K-means tends towards equal sized groups.\n",
    "\n",
    "        Overall, if we interpret K-Means from the perspective of probabilistic\n",
    "        modeling, then we can see that K-Means is a special case of GMM. And\n",
    "        recall that\n",
    "        [in the geometry of multivariate gaussian](https://gao-hongnan.github.io/gaohn-galaxy/probability_theory/05_joint_distributions/0507_multivariate_gaussian/geometry_of_multivariate_gaussian.html),\n",
    "        the shape of the multivariate gaussian is determined by the covariance\n",
    "        matrix. Since we have deduced that the covariance matrix is\n",
    "        $\\sigma^2 \\mathbf{I}$, a diagonal matrix with equal variance across all\n",
    "        features, then the shape is a sphere since the axis has equal length.\n",
    "\n",
    "-   Scaling with number of dimensions. As the number of dimensions increases, a\n",
    "    distance-based similarity measure converges to a constant value between any\n",
    "    given examples. Reduce dimensionality either by using PCA on the feature\n",
    "    data, or by using “spectral clustering” to modify the clustering algorithm.\n",
    "-   Best to feature scale if we use euclidean distance as the distance metric.\n",
    "    This is because features with larger scale will dominate the distance\n",
    "    metric.\n",
    "-   For categorical features, we no longer use mean as the cluster center.\n",
    "    Instead, we use the mode.\n",
    "\n",
    "## K-Means++\n",
    "\n",
    "We have seen earlier that convergence can be an issue with K-Means, and it is\n",
    "recommended to use different seed initializations to get better results.\n",
    "\n",
    "We state a better initialization method, K-Means++. The intuition behind this\n",
    "approach is that spreading out the $K$ initial cluster centers is a good thing:\n",
    "the first cluster center is chosen uniformly at random from the data points that\n",
    "are being clustered, after which each subsequent cluster center is chosen from\n",
    "the remaining data points with probability proportional to its squared distance\n",
    "from the point's closest existing cluster center.\n",
    "\n",
    "As we have seen, K-Means is optimizing a non-convex objective function, which\n",
    "means that it can get stuck in local optima. This means different\n",
    "initializations of the cluster centers can lead to different final cluster\n",
    "assignments. So initialization is usually key to convergence. We usually\n",
    "initialize randomly by choosing $K$ data points from the dataset $\\mathcal{S}$\n",
    "at random and set these as the initial cluster centers. However, this can lead\n",
    "to poor results, especially if the data is not well-separated. Furthermore, we\n",
    "can do **multiple restarts** by running K-Means multiple times with different\n",
    "random initializations and choosing the best solution. This can be slow, and\n",
    "hence we can use a smarter initialization method.\n",
    "\n",
    "K-Means++ is an improved initialization method for the K-Means algorithm, which\n",
    "aims to choose the initial cluster centers in a smarter way. The goal is to\n",
    "spread out the initial cluster centers more uniformly across the data space. The\n",
    "K-Means++ initialization method is as follows:\n",
    "\n",
    "1. **Initialization Step**:\n",
    "\n",
    "    - Choose the first cluster center $\\boldsymbol{\\mu}_1^{[0]}$ uniformly at\n",
    "      random from the data points\n",
    "      $\\mathcal{S} = \\left\\{\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots, \\mathbf{x}^{(N)}\\right\\}$.\n",
    "    - For $k = 2, 3, \\dots, K$:\n",
    "\n",
    "        - Compute the squared distances $D(\\mathbf{x}^{(n)})$ for each data\n",
    "          point $\\mathbf{x}^{(n)} \\in \\mathcal{S}$ to the nearest cluster center\n",
    "          that has already been chosen:\n",
    "\n",
    "        $$\n",
    "        \\begin{aligned}\n",
    "        D(\\mathbf{x}^{(n)}) = \\min_{1 \\leq j \\leq k-1} \\left\\| \\mathbf{x}^{(n)} - \\boldsymbol{\\mu}_j^{[0]} \\right\\|^2\n",
    "        \\end{aligned}\n",
    "        $$\n",
    "\n",
    "        - Choose the next cluster center $\\boldsymbol{\\mu}_k^{[0]}$ with\n",
    "          probability proportional to the squared distance\n",
    "          $D(\\mathbf{x}^{(n)})$:\n",
    "\n",
    "        $$\n",
    "        \\begin{aligned}\n",
    "        \\mathbb{P}[\\boldsymbol{\\mu}_k^{[0]} = \\mathbf{x}^{(n)}] = \\frac{D(\\mathbf{x}^{(n)})}{\\sum_{i=1}^N D(\\mathbf{x}^{(i)})}\n",
    "        \\end{aligned}\n",
    "        $$\n",
    "\n",
    "Once the initial cluster centers are chosen using the K-Means++ initialization,\n",
    "the rest of the K-Means algorithm proceeds as usual, following the steps in\n",
    "{prf:ref}`lloyd-kmeans-algorithm`.\n",
    "\n",
    "The primary advantage of the K-Means++ initialization is that it tends to\n",
    "provide better starting points for the K-Means algorithm, often leading to a\n",
    "faster convergence and better final clustering results.\n",
    "\n",
    "What is more surprising is that this method can be shown to guarantee that the\n",
    "recontruction error is never more than $\\mathcal{O}(\\log K)$ worse than optimal\n",
    "{cite:ps}`pml1Book`.\n",
    "\n",
    "## K-Medoids\n",
    "\n",
    "See section 21.3.5 of Probabilistic Machine Learning: An Introduction by Kevin\n",
    "P. Murphy.\n",
    "\n",
    "## References and Further Readings\n",
    "\n",
    "I also highly recommend Nathaniel Dake's\n",
    "[blog on K-Means](https://www.nathanieldake.com/Machine_Learning/04-Unsupervised_Learning_Cluster_Analysis-02-Cluster-Analysis-K-Means-Clustering.html)\n",
    "here, he does a fantatic job in explaining the intuition behind K-Means and\n",
    "provide visualizations to help you understand the algorithm, especially how\n",
    "K-Means can fail.\n",
    "\n",
    "-   Murphy, Kevin P. \"Chapter 21.3. K-Means Clustering.\" In Probabilistic\n",
    "    Machine Learning: An Introduction. MIT Press, 2022.\n",
    "-   Hal Daumé III. \"Chapter 3.4. K-Means Clustering.\" In A Course in Machine\n",
    "    Learning, January 2017.\n",
    "-   Hal Daumé III. \"Chapter 15.1. K-Means Clustering.\" In A Course in Machine\n",
    "    Learning, January 2017.\n",
    "-   Bishop, Christopher M. \"Chapter 9.1. K-Means Clustering.\" In Pattern\n",
    "    Recognition and Machine Learning. New York: Springer-Verlag, 2016.\n",
    "-   James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n",
    "    \"Chapter 12.4.1. K-Means Clustering.\" In An Introduction to Statistical\n",
    "    Learning: With Applications in R. Boston: Springer, 2022.\n",
    "-   Hastie, Trevor, Tibshirani, Robert and Friedman, Jerome. \"Chapter 14.3.\n",
    "    Cluster Analysis.\" In The Elements of Statistical Learning. New York, NY,\n",
    "    USA: Springer New York Inc., 2001.\n",
    "-   Raschka, Sebastian. \"Chapter 10.1. Grouping objects by similarity using\n",
    "    k-means.\" In Machine Learning with PyTorch and Scikit-Learn.\n",
    "-   Jung, Alexander. \"Chapter 8.1. Hard Clustering with K-Means.\" In Machine\n",
    "    Learning: The Basics. Singapore: Springer Nature Singapore, 2023.\n",
    "-   Vincent, Tan. \"Lecture 17a.\" In Data Modelling and Computation (MA4270).\n",
    "\n",
    "[^disjoint_union]:\n",
    "    Disjoint union indicates that each data point $\\mathbf{x}^{(n)}$ can be\n",
    "    assigned to one and only one cluster $C_k$.\n",
    "\n",
    "[^collection_of_clusters]:\n",
    "    Note $C$ is not the same as $\\mathcal{S}$ even though they both represent\n",
    "    all samples. The cardinality of $C$ is $K$, while the cardinality of\n",
    "    $\\mathcal{S}$ is $N$.\n",
    "\n",
    "[^jointly_optimizing]:\n",
    "    This means that we are jointly optimizing the assignments $\\mathcal{A}$ and\n",
    "    the cluster centers $\\boldsymbol{\\mu}_k$.\n",
    "\n",
    "[^y]:\n",
    "    There's actually no ground truth target labels in unsupervised learning,\n",
    "    this is for education purposes.\n",
    "\n",
    "[^equivalent-k-means-cost-function]:\n",
    "    The reason of writing so many equivalent forms is because many textbooks use\n",
    "    different notations, so I tried to list a few common ones.\n",
    "\n",
    "[^worst-case-time-complexity]:\n",
    "    Refer to “How slow is the k-means method?” D. Arthur and S. Vassilvitskii -\n",
    "    SoCG2006. for more details."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "mystnb": {
   "number_source_lines": true
  },
  "source_map": [
   16,
   29,
   76,
   83,
   104
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}