{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "This section implements the linear regression algorithm from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"\"\"Linear Regression Class.\n",
    "\n",
    "High-level module implementing linear regression with support for multiple\n",
    "solvers and regularization.\n",
    "\n",
    "Features:\n",
    "    - Closed Form Solution\n",
    "    - Batch Gradient Descent\n",
    "    - Stochastic Gradient Descent\n",
    "\n",
    "Supports L1 and L2 regularization.\n",
    "\n",
    "NOTE:\n",
    "    1. Regularization parameter `C` is only applicable when `regularization` is set.\n",
    "    2. Ensure that input data does not contain NaN or infinite values.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import functools\n",
    "import logging\n",
    "from typing import Any, Callable, List, Optional, TypeVar\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from typing_extensions import Concatenate, ParamSpec\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "P = ParamSpec(\"P\")\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "def not_fitted(func: Callable[Concatenate[LinearRegression, P], T]) -> Callable[Concatenate[LinearRegression, P], T]:\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(self: LinearRegression, *args: P.args, **kwargs: P.kwargs) -> T:\n",
    "        if not self._fitted:\n",
    "            raise NotFittedError\n",
    "        else:\n",
    "            return func(self, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class MyLinearRegression:\n",
    "    \"\"\"\n",
    "    Linear Regression model supporting multiple solvers and regularization.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    coef_ : NDArray[np.floating[Any]]\n",
    "        Coefficient vector.\n",
    "    intercept_ : float\n",
    "        Intercept term.\n",
    "    has_intercept : bool\n",
    "        Whether to include an intercept in the model.\n",
    "    solver : str\n",
    "        Solver type: {\"Closed Form Solution\", \"Batch Gradient Descent\",\n",
    "        \"Stochastic Gradient Descent\"}.\n",
    "    learning_rate : float\n",
    "        Learning rate for gradient descent solvers.\n",
    "    loss_function : LossFunction\n",
    "        Loss function to be minimized.\n",
    "    regularization : Optional[str]\n",
    "        Type of regularization: {\"l1\", \"l2\"} or None.\n",
    "    C : Optional[float]\n",
    "        Regularization strength. Must be a positive float.\n",
    "    num_epochs : int\n",
    "        Number of epochs for gradient descent solvers.\n",
    "    _fitted : bool\n",
    "        Flag indicating whether the model has been fitted.\n",
    "    loss_history : List[float]\n",
    "        History of loss values during training.\n",
    "    optimal_betas : NDArray[np.floating[Any]]\n",
    "        Optimal beta coefficients after fitting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        has_intercept: bool = True,\n",
    "        solver: str = \"Closed Form Solution\",\n",
    "        learning_rate: float = 0.1,\n",
    "        loss_function: Optional[Any] = None,\n",
    "        regularization: Optional[str] = None,\n",
    "        C: Optional[float] = None,\n",
    "        num_epochs: Optional[int] = 1000,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Linear Regression model with specified parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        has_intercept : bool, default=True\n",
    "            Whether to include an intercept in the model.\n",
    "        solver : str, default=\"Closed Form Solution\"\n",
    "            Solver type: {\"Closed Form Solution\", \"Batch Gradient Descent\", \"Stochastic Gradient Descent\"}.\n",
    "        learning_rate : float, default=0.01\n",
    "            Learning rate for gradient descent solvers.\n",
    "        loss_function : LossFunction, default=LossFunction.l2_loss()\n",
    "            Loss function to be minimized.\n",
    "        regularization : Optional[str], default=None\n",
    "            Type of regularization: {\"l1\", \"l2\"} or None.\n",
    "        C : Optional[float], default=None\n",
    "            Regularization strength. Must be a positive float.\n",
    "        num_epochs : int, default=1000\n",
    "            Number of epochs for gradient descent solvers.\n",
    "        \"\"\"\n",
    "        self.solver = solver\n",
    "        self.has_intercept = has_intercept\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_function = loss_function or self.l2_loss\n",
    "        self.regularization = regularization\n",
    "        self.C = C\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        self.coef_: NDArray[np.floating[Any]]\n",
    "        self.intercept_: float | None = None\n",
    "        self._fitted: bool = False\n",
    "        self.optimal_betas: NDArray[np.floating[Any]]\n",
    "\n",
    "        self.loss_history: List[float] = []\n",
    "\n",
    "        # Validate regularization parameters\n",
    "        if self.regularization is not None and self.C is None:\n",
    "            raise ValueError(\"Regularization strength 'C' must be provided when using regularization.\")\n",
    "\n",
    "        if self.regularization not in {None, \"l1\", \"l2\"}:\n",
    "            raise ValueError(\"Regularization must be one of {None, 'l1', 'l2'}.\")\n",
    "\n",
    "        # Validate solver\n",
    "        valid_solvers = {\"Closed Form Solution\", \"Batch Gradient Descent\", \"Stochastic Gradient Descent\"}\n",
    "        if self.solver not in valid_solvers:\n",
    "            raise ValueError(f\"Solver must be one of {valid_solvers}.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def l2_loss(y_true: NDArray[np.floating[Any]], y_pred: NDArray[np.floating[Any]]) -> float:\n",
    "        return np.square(y_true - y_pred).mean()  # type: ignore[no-any-return]\n",
    "\n",
    "    def _add_regularization(self, loss: float, w: NDArray[np.floating[Any]]) -> float:\n",
    "        \"\"\"\n",
    "        Apply regularization to the loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        loss : float\n",
    "            Current loss value.\n",
    "        w : NDArray[np.floating[Any]]\n",
    "            Weight vector.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Updated loss with regularization.\n",
    "        \"\"\"\n",
    "        if self.regularization == \"l1\":\n",
    "            loss += self.C * np.abs(w[1:]).sum()\n",
    "        elif self.regularization == \"l2\":\n",
    "            assert self.C is not None\n",
    "            loss += (0.5 * self.C) * np.square(w[1:]).sum()\n",
    "        return loss\n",
    "\n",
    "    def _initialize_weights(self, n_features: int) -> NDArray[np.floating[Any]]:\n",
    "        \"\"\"\n",
    "        Initialize weights for gradient descent solvers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_features : int\n",
    "            Number of features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray[np.floating[Any]]\n",
    "            Initialized weight vector.\n",
    "        \"\"\"\n",
    "        return np.zeros((n_features, 1))\n",
    "\n",
    "    def _check_input_shapes(\n",
    "        self, X: NDArray[np.floating[Any]], y_true: NDArray[np.floating[Any]]\n",
    "    ) -> tuple[NDArray[np.floating[Any]], NDArray[np.floating[Any]]]:\n",
    "        \"\"\"\n",
    "        Validate and reshape input data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : NDArray[np.floating[Any]]\n",
    "            Input feature matrix.\n",
    "        y_true : NDArray[np.floating[Any]]\n",
    "            True target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[NDArray[np.floating[Any]], NDArray[np.floating[Any]]]\n",
    "            Reshaped feature matrix and target vector.\n",
    "        \"\"\"\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "            logging.info(\"Reshaped X to 2D array with shape %s.\", X.shape)\n",
    "\n",
    "        if y_true.ndim == 1:\n",
    "            y_true = y_true.reshape(-1, 1)\n",
    "            logging.info(\"Reshaped y_true to 2D array with shape %s.\", y_true.shape)\n",
    "\n",
    "        if X.shape[0] != y_true.shape[0]:\n",
    "            raise ValueError(\"Number of samples in X and y_true must be equal.\")\n",
    "\n",
    "        return X, y_true\n",
    "\n",
    "    def fit(self, X: NDArray[np.floating[Any]], y_true: NDArray[np.floating[Any]]) -> LinearRegression:\n",
    "        \"\"\"\n",
    "        Fit the Linear Regression model to the data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : NDArray[np.floating[Any]]\n",
    "            Input feature matrix of shape (n_samples, n_features).\n",
    "        y_true : NDArray[np.floating[Any]]\n",
    "            True target values of shape (n_samples,).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        LinearRegression\n",
    "            The fitted model.\n",
    "        \"\"\"\n",
    "        X, y_true = self._check_input_shapes(X, y_true)\n",
    "\n",
    "        # Add intercept term if necessary\n",
    "        if self.has_intercept:\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "            logging.info(\"Added intercept term to X.\")\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        logging.info(\"Fitting model with %d samples and %d features.\", n_samples, n_features)\n",
    "\n",
    "        if self.solver == \"Closed Form Solution\":\n",
    "            XtX = X.T @ X\n",
    "            det = np.linalg.det(XtX)\n",
    "            if det == 0:\n",
    "                logging.warning(\"Singular matrix encountered. Using pseudo-inverse instead.\")\n",
    "                XtX_inv = np.linalg.pinv(XtX)\n",
    "            else:\n",
    "                XtX_inv = np.linalg.inv(XtX)\n",
    "            Xty = X.T @ y_true\n",
    "            self.optimal_betas = XtX_inv @ Xty\n",
    "            logging.info(\"Computed optimal betas using Closed Form Solution.\")\n",
    "\n",
    "        elif self.solver in {\"Batch Gradient Descent\", \"Stochastic Gradient Descent\"}:\n",
    "            assert self.num_epochs is not None\n",
    "            self.optimal_betas = self._initialize_weights(n_features)\n",
    "            logging.info(\"Initialized weights for Gradient Descent.\")\n",
    "\n",
    "            for epoch in range(1, self.num_epochs + 1):\n",
    "                if self.solver == \"Batch Gradient Descent\":\n",
    "                    y_pred = X @ self.optimal_betas\n",
    "                elif self.solver == \"Stochastic Gradient Descent\":\n",
    "                    indices = np.random.permutation(n_samples)  # noqa: NPY002\n",
    "                    for i in indices:\n",
    "                        xi = X[i].reshape(1, -1)\n",
    "                        yi = y_true[i]\n",
    "                        y_pred = xi @ self.optimal_betas\n",
    "                        error = y_pred - yi\n",
    "                        gradient = xi.T @ error\n",
    "                        if self.regularization == \"l2\":\n",
    "                            assert self.C is not None\n",
    "                            gradient[1:] += self.C * self.optimal_betas[1:]\n",
    "                        elif self.regularization == \"l1\":\n",
    "                            gradient[1:] += self.C * np.sign(self.optimal_betas[1:])\n",
    "                        self.optimal_betas -= self.learning_rate * gradient\n",
    "                    continue  # Skip the rest of the loop for SGD\n",
    "\n",
    "                error = y_pred - y_true\n",
    "                loss = self.loss_function(y_true=y_true, y_pred=y_pred)\n",
    "                loss = self._add_regularization(loss, self.optimal_betas)\n",
    "                self.loss_history.append(loss)\n",
    "\n",
    "                gradient = (2 / n_samples) * (X.T @ error)\n",
    "                if self.regularization == \"l2\":\n",
    "                    assert self.C is not None\n",
    "                    gradient[1:] += self.C * self.optimal_betas[1:]\n",
    "                elif self.regularization == \"l1\":\n",
    "                    gradient[1:] += self.C * np.sign(self.optimal_betas[1:])\n",
    "\n",
    "                self.optimal_betas -= self.learning_rate * gradient\n",
    "\n",
    "                if epoch % 100 == 0 or epoch == 1:\n",
    "                    logging.info(\"Epoch %d | Loss: %.4f\", epoch, loss)\n",
    "\n",
    "        # Set coefficients and intercept\n",
    "        if self.has_intercept:\n",
    "            self.intercept_ = float(self.optimal_betas[0])\n",
    "            self.coef_ = self.optimal_betas[1:].flatten()\n",
    "            logging.info(\"Set intercept and coefficients.\")\n",
    "        else:\n",
    "            self.coef_ = self.optimal_betas.flatten()\n",
    "            self.intercept_ = 0.0\n",
    "            logging.info(\"Set coefficients without intercept.\")\n",
    "\n",
    "        self._fitted = True\n",
    "        logging.info(\"Model fitting complete.\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    @not_fitted\n",
    "    def predict(self, X: NDArray[np.floating[Any]]) -> NDArray[np.floating[Any]]:\n",
    "        \"\"\"\n",
    "        Predict target values using the fitted model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : NDArray[np.floating[Any]]\n",
    "            Input feature matrix of shape (n_samples, n_features).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray[np.floating[Any]]\n",
    "            Predicted target values of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        if self.has_intercept:\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "        assert self.optimal_betas is not None\n",
    "        y_pred = X @ self.optimal_betas\n",
    "        return y_pred.flatten()\n",
    "\n",
    "    def residuals(self, X: NDArray[np.floating[Any]], y_true: NDArray[np.floating[Any]]) -> NDArray[np.floating[Any]]:\n",
    "        \"\"\"\n",
    "        Calculate residuals between true and predicted values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : NDArray[np.floating[Any]]\n",
    "            Input feature matrix.\n",
    "        y_true : NDArray[np.floating[Any]]\n",
    "            True target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NDArray[np.floating[Any]]\n",
    "            Residuals of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        residuals = y_true.flatten() - y_pred\n",
    "        return residuals\n",
    "\n",
    "    @not_fitted\n",
    "    def score(self, X: NDArray[np.floating[Any]], y_true: NDArray[np.floating[Any]]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the coefficient of determination R^2 of the prediction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : NDArray[np.floating[Any]]\n",
    "            Input feature matrix.\n",
    "        y_true : NDArray[np.floating[Any]]\n",
    "            True target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            R^2 score.\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_total = np.sum((y_true.flatten() - np.mean(y_true)) ** 2)\n",
    "        ss_res = np.sum((y_true.flatten() - y_pred) ** 2)\n",
    "        r2_score = 1 - (ss_res / ss_total)\n",
    "        return float(r2_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 14:27:17,957 - INFO - Reshaped y_true to 2D array with shape (7000, 1).\n",
      "2024-09-29 14:27:17,977 - INFO - Added intercept term to X.\n",
      "2024-09-29 14:27:18,043 - INFO - Fitting model with 7000 samples and 11 features.\n",
      "2024-09-29 14:27:18,070 - INFO - Computed optimal betas using Closed Form Solution.\n",
      "/var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_68141/987029641.py:292: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.intercept_ = float(self.optimal_betas[0])\n",
      "2024-09-29 14:27:18,071 - INFO - Set intercept and coefficients.\n",
      "2024-09-29 14:27:18,071 - INFO - Model fitting complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-3.2902622092179315e-15</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m-3.2902622092179315e-15\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-8.93729534823251e-15</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m-8.93729534823251e-15\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">65.74652523</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.51408752</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32.05245375</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50.32013901</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52.76364276</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.69437951</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47.88858774</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">69.03001556</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30.41026743</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.47178546</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m65.74652523\u001b[0m, \u001b[1;36m19.51408752\u001b[0m, \u001b[1;36m32.05245375\u001b[0m, \u001b[1;36m50.32013901\u001b[0m, \u001b[1;36m52.76364276\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;36m4.69437951\u001b[0m, \u001b[1;36m47.88858774\u001b[0m, \u001b[1;36m69.03001556\u001b[0m, \u001b[1;36m30.41026743\u001b[0m,  \u001b[1;36m3.47178546\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">array</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">65.74652523</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.51408752</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32.05245375</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50.32013901</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">52.76364276</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.69437951</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47.88858774</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">69.03001556</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30.41026743</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.47178546</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35marray\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m65.74652523\u001b[0m, \u001b[1;36m19.51408752\u001b[0m, \u001b[1;36m32.05245375\u001b[0m, \u001b[1;36m50.32013901\u001b[0m, \u001b[1;36m52.76364276\u001b[0m,\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1;36m4.69437951\u001b[0m, \u001b[1;36m47.88858774\u001b[0m, \u001b[1;36m69.03001556\u001b[0m, \u001b[1;36m30.41026743\u001b[0m,  \u001b[1;36m3.47178546\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Value HN 183.222001509455\n",
      "First Value SKLEARN 183.22200150945596\n",
      "HN MSE 1.317631501630165e-25\n",
      "SKLEARN MSE 3.1767035436560316e-25\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rich.pretty import pprint\n",
    "\n",
    "\n",
    "def regression_test(solver: str = \"Closed Form Solution\") -> None:\n",
    "    np.random.seed(1930)\n",
    "    X, y_true = make_regression(\n",
    "        n_samples=10000, n_features=10, random_state=1930, coef=False\n",
    "    )  # returns 2-d array of 1000 by 10\n",
    "    x_train, x_val, y_train, y_val = train_test_split(X, y_true, test_size=0.3, random_state=1930)\n",
    "\n",
    "    lr_SKLEARN = LinearRegression(fit_intercept=True).fit(x_train, y_train)\n",
    "    lr_CUSTOM = MyLinearRegression(solver=solver, has_intercept=True, num_epochs=10000).fit(x_train, y_train)\n",
    "\n",
    "    # Debugged, the intercept is the one with major difference why? Answer: https://stackoverflow.com/questions/66881829/implementation-of-linear-regression-closed-form-solution/66886954?noredirect=1#comment118259946_66886954\n",
    "\n",
    "    pprint(lr_CUSTOM.intercept_)\n",
    "    pprint(lr_SKLEARN.intercept_)\n",
    "    pprint(lr_CUSTOM.coef_)\n",
    "    pprint(lr_SKLEARN.coef_)\n",
    "\n",
    "    pred_CUSTOM = lr_CUSTOM.predict(x_val)\n",
    "    pred_SKLEARN = lr_SKLEARN.predict(x_val)\n",
    "    print(\"First Value HN\", pred_CUSTOM[0])\n",
    "    print(\"First Value SKLEARN\", pred_SKLEARN[0])\n",
    "    print(\"HN MSE\", mean_squared_error(y_val, pred_CUSTOM))\n",
    "    print(\"SKLEARN MSE\", mean_squared_error(y_val, pred_SKLEARN))\n",
    "\n",
    "regression_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 14:28:20,867 - INFO - Reshaped y_true to 2D array with shape (422, 1).\n",
      "2024-09-29 14:28:20,873 - INFO - Added intercept term to X.\n",
      "2024-09-29 14:28:20,874 - INFO - Fitting model with 422 samples and 2 features.\n",
      "2024-09-29 14:28:20,876 - INFO - Initialized weights for Gradient Descent.\n",
      "2024-09-29 14:28:20,876 - INFO - Epoch 1 | Loss: 29468.6469\n",
      "2024-09-29 14:28:20,879 - INFO - Epoch 100 | Loss: 5777.2529\n",
      "2024-09-29 14:28:20,882 - INFO - Epoch 200 | Loss: 5619.4054\n",
      "2024-09-29 14:28:20,883 - INFO - Epoch 300 | Loss: 5475.2313\n",
      "2024-09-29 14:28:20,887 - INFO - Epoch 400 | Loss: 5343.5459\n",
      "2024-09-29 14:28:20,894 - INFO - Epoch 500 | Loss: 5223.2675\n",
      "2024-09-29 14:28:20,896 - INFO - Epoch 600 | Loss: 5113.4080\n",
      "2024-09-29 14:28:20,900 - INFO - Epoch 700 | Loss: 5013.0648\n",
      "2024-09-29 14:28:20,903 - INFO - Epoch 800 | Loss: 4921.4137\n",
      "2024-09-29 14:28:20,904 - INFO - Epoch 900 | Loss: 4837.7017\n",
      "2024-09-29 14:28:20,906 - INFO - Epoch 1000 | Loss: 4761.2411\n",
      "2024-09-29 14:28:20,907 - INFO - Epoch 1100 | Loss: 4691.4037\n",
      "2024-09-29 14:28:20,909 - INFO - Epoch 1200 | Loss: 4627.6159\n",
      "2024-09-29 14:28:20,911 - INFO - Epoch 1300 | Loss: 4569.3536\n",
      "2024-09-29 14:28:20,912 - INFO - Epoch 1400 | Loss: 4516.1381\n",
      "2024-09-29 14:28:20,913 - INFO - Epoch 1500 | Loss: 4467.5323\n",
      "2024-09-29 14:28:20,915 - INFO - Epoch 1600 | Loss: 4423.1369\n",
      "2024-09-29 14:28:20,917 - INFO - Epoch 1700 | Loss: 4382.5872\n",
      "2024-09-29 14:28:20,918 - INFO - Epoch 1800 | Loss: 4345.5500\n",
      "2024-09-29 14:28:20,919 - INFO - Epoch 1900 | Loss: 4311.7210\n",
      "2024-09-29 14:28:20,920 - INFO - Epoch 2000 | Loss: 4280.8225\n",
      "2024-09-29 14:28:20,922 - INFO - Epoch 2100 | Loss: 4252.6005\n",
      "2024-09-29 14:28:20,923 - INFO - Epoch 2200 | Loss: 4226.8231\n",
      "2024-09-29 14:28:20,924 - INFO - Epoch 2300 | Loss: 4203.2787\n",
      "2024-09-29 14:28:20,925 - INFO - Epoch 2400 | Loss: 4181.7737\n",
      "2024-09-29 14:28:20,927 - INFO - Epoch 2500 | Loss: 4162.1316\n",
      "2024-09-29 14:28:20,928 - INFO - Epoch 2600 | Loss: 4144.1910\n",
      "2024-09-29 14:28:20,930 - INFO - Epoch 2700 | Loss: 4127.8044\n",
      "2024-09-29 14:28:20,932 - INFO - Epoch 2800 | Loss: 4112.8373\n",
      "2024-09-29 14:28:20,947 - INFO - Epoch 2900 | Loss: 4099.1666\n",
      "2024-09-29 14:28:20,961 - INFO - Epoch 3000 | Loss: 4086.6802\n",
      "2024-09-29 14:28:20,969 - INFO - Epoch 3100 | Loss: 4075.2754\n",
      "2024-09-29 14:28:21,034 - INFO - Epoch 3200 | Loss: 4064.8585\n",
      "2024-09-29 14:28:21,036 - INFO - Epoch 3300 | Loss: 4055.3439\n",
      "2024-09-29 14:28:21,037 - INFO - Epoch 3400 | Loss: 4046.6535\n",
      "2024-09-29 14:28:21,038 - INFO - Epoch 3500 | Loss: 4038.7159\n",
      "2024-09-29 14:28:21,042 - INFO - Epoch 3600 | Loss: 4031.4659\n",
      "2024-09-29 14:28:21,046 - INFO - Epoch 3700 | Loss: 4024.8439\n",
      "2024-09-29 14:28:21,056 - INFO - Epoch 3800 | Loss: 4018.7956\n",
      "2024-09-29 14:28:21,058 - INFO - Epoch 3900 | Loss: 4013.2711\n",
      "2024-09-29 14:28:21,059 - INFO - Epoch 4000 | Loss: 4008.2252\n",
      "2024-09-29 14:28:21,061 - INFO - Epoch 4100 | Loss: 4003.6164\n",
      "2024-09-29 14:28:21,064 - INFO - Epoch 4200 | Loss: 3999.4068\n",
      "2024-09-29 14:28:21,065 - INFO - Epoch 4300 | Loss: 3995.5619\n",
      "2024-09-29 14:28:21,067 - INFO - Epoch 4400 | Loss: 3992.0500\n",
      "2024-09-29 14:28:21,068 - INFO - Epoch 4500 | Loss: 3988.8423\n",
      "2024-09-29 14:28:21,069 - INFO - Epoch 4600 | Loss: 3985.9125\n",
      "2024-09-29 14:28:21,071 - INFO - Epoch 4700 | Loss: 3983.2365\n",
      "2024-09-29 14:28:21,072 - INFO - Epoch 4800 | Loss: 3980.7923\n",
      "2024-09-29 14:28:21,073 - INFO - Epoch 4900 | Loss: 3978.5598\n",
      "2024-09-29 14:28:21,074 - INFO - Epoch 5000 | Loss: 3976.5207\n",
      "2024-09-29 14:28:21,076 - INFO - Epoch 5100 | Loss: 3974.6582\n",
      "2024-09-29 14:28:21,078 - INFO - Epoch 5200 | Loss: 3972.9571\n",
      "2024-09-29 14:28:21,080 - INFO - Epoch 5300 | Loss: 3971.4033\n",
      "2024-09-29 14:28:21,082 - INFO - Epoch 5400 | Loss: 3969.9841\n",
      "2024-09-29 14:28:21,083 - INFO - Epoch 5500 | Loss: 3968.6879\n",
      "2024-09-29 14:28:21,088 - INFO - Epoch 5600 | Loss: 3967.5039\n",
      "2024-09-29 14:28:21,091 - INFO - Epoch 5700 | Loss: 3966.4225\n",
      "2024-09-29 14:28:21,093 - INFO - Epoch 5800 | Loss: 3965.4348\n",
      "2024-09-29 14:28:21,094 - INFO - Epoch 5900 | Loss: 3964.5326\n",
      "2024-09-29 14:28:21,096 - INFO - Epoch 6000 | Loss: 3963.7086\n",
      "2024-09-29 14:28:21,097 - INFO - Epoch 6100 | Loss: 3962.9559\n",
      "2024-09-29 14:28:21,099 - INFO - Epoch 6200 | Loss: 3962.2685\n",
      "2024-09-29 14:28:21,100 - INFO - Epoch 6300 | Loss: 3961.6406\n",
      "2024-09-29 14:28:21,101 - INFO - Epoch 6400 | Loss: 3961.0671\n",
      "2024-09-29 14:28:21,103 - INFO - Epoch 6500 | Loss: 3960.5432\n",
      "2024-09-29 14:28:21,104 - INFO - Epoch 6600 | Loss: 3960.0648\n",
      "/var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_68141/987029641.py:292: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  self.intercept_ = float(self.optimal_betas[0])\n",
      "2024-09-29 14:28:21,105 - INFO - Set intercept and coefficients.\n",
      "2024-09-29 14:28:21,105 - INFO - Model fitting complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKLEARN Coefficients: [938.23786125]\n",
      "HONGNAN Coefficients: [892.45056539]\n",
      "HN MSE 2604.751428467252\n",
      "SKLEARN MSE 2548.072398725969\n",
      "Coefficient of determination: 0.47\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def regression_diabetes() -> None:\n",
    "    diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "    # Use only one feature\n",
    "    diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
    "\n",
    "    diabetes_X_train = diabetes_X[:-20]\n",
    "    diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "    diabetes_y_train = diabetes_y[:-20]\n",
    "    diabetes_y_test = diabetes_y[-20:]\n",
    "\n",
    "    lr_SKLEARN = LinearRegression()\n",
    "    lr_CUSTOM = MyLinearRegression(has_intercept=True, solver=\"Batch Gradient Descent\", num_epochs=6666)\n",
    "\n",
    "    lr_CUSTOM.fit(diabetes_X_train, diabetes_y_train)\n",
    "    lr_SKLEARN.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    diabetes_y_pred = lr_SKLEARN.predict(diabetes_X_test)\n",
    "    pred_CUSTOM = lr_CUSTOM.predict(diabetes_X_test)\n",
    "\n",
    "    # The coefficients\n",
    "    print(\"SKLEARN Coefficients:\", lr_SKLEARN.coef_)\n",
    "    print(\"HONGNAN Coefficients:\", lr_CUSTOM.coef_)\n",
    "    # The mean squared error\n",
    "    print(\"HN MSE\", mean_squared_error(diabetes_y_test, pred_CUSTOM))\n",
    "    print(\"SKLEARN MSE\", mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "    # The coefficient of determination: 1 is perfect prediction\n",
    "    print(\"Coefficient of determination: %.2f\" % r2_score(diabetes_y_test, diabetes_y_pred))\n",
    "\n",
    "    # Plot outputs\n",
    "    plt.scatter(diabetes_X_test, diabetes_y_test, color=\"black\")\n",
    "    plt.plot(diabetes_X_test, diabetes_y_pred, color=\"blue\", linewidth=3)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "regression_diabetes()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaohn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcd450edf2a38ed593ba5af012e093d4d4ca63d14078fa56cd9563b2046eb57e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
