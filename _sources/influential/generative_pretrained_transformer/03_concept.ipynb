{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e89ed0d",
   "metadata": {},
   "source": [
    "# The Concept of Generative Pre-trained Transformers (GPT)\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Organized_Chaos-orange)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67be19d1",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Iterable, Optional, Tuple, TypeVar\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from rich.pretty import pprint\n",
    "from torch import nn\n",
    "\n",
    "def configure_deterministic_mode() -> None:\n",
    "    \"\"\"\n",
    "    See https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html\n",
    "    and https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\n",
    "    \"\"\"\n",
    "    # fmt: off\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    torch.backends.cudnn.benchmark        = False\n",
    "    torch.backends.cudnn.deterministic    = True\n",
    "    torch.backends.cudnn.enabled          = False\n",
    "\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    # fmt: on\n",
    "    warnings.warn(\n",
    "        \"Deterministic mode is activated. This will negatively impact performance and may cause increase in CUDA memory footprint.\",\n",
    "        category=UserWarning,\n",
    "        stacklevel=2,\n",
    "    )\n",
    "\n",
    "\n",
    "def seed_all(\n",
    "    seed: int = 1992,\n",
    "    seed_torch: bool = True,\n",
    "    set_torch_deterministic: bool = True,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Seed all random number generators.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int\n",
    "        Seed number to be used, by default 1992.\n",
    "    seed_torch : bool\n",
    "        Whether to seed PyTorch or not, by default True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    seed: int\n",
    "        The seed number.\n",
    "    \"\"\"\n",
    "    # fmt: off\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)       # set PYTHONHASHSEED env var at fixed value\n",
    "    np.random.default_rng(seed)                    # numpy pseudo-random generator\n",
    "    random.seed(seed)                              # python's built-in pseudo-random generator\n",
    "\n",
    "    if seed_torch:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)           # pytorch (both CPU and CUDA)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.enabled = False\n",
    "\n",
    "        if set_torch_deterministic:\n",
    "            configure_deterministic_mode()\n",
    "    # fmt: on\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d6d272",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "The problem that GPT-2 aims to solve is to demonstrate that language models,\n",
    "given **_large_** enough capacity in terms of parameters, and **_large_** enough\n",
    "**_unlabeled and high-quality_** text data, can solve specialized natural\n",
    "language processing tasks such as question answering, translation, and\n",
    "summarization, in a\n",
    "[**_zero-shot_**](https://en.wikipedia.org/wiki/Zero-shot_learning) manner -\n",
    "without the need for task-specific architectures or supervised fine-tuning.\n",
    "\n",
    "The emphasis on the _large and high-quality_ text data cannot be understated as\n",
    "the authors are hinging on the fact that the dataset is so **_diverse_**, and\n",
    "therefore _bound_ to have examples of the _specialized_ tasks that the model can\n",
    "learn from.\n",
    "\n",
    "For example, if we are looking at translation tasks, then the data is bound to\n",
    "have somewhat **sequential** and **natural occuring translation text** such as:\n",
    "\n",
    "```python\n",
    "The translation of the french sentence 'As-tu aller au cine ́ma?' to english is 'Did you go to the cinema?'.\n",
    "```\n",
    "\n",
    "The model can learn from such examples and generalize to perform well on the\n",
    "translation task via the\n",
    "[**_autoregressive_**](https://en.wikipedia.org/wiki/Autoregressive_model),\n",
    "[**_self-supervised_**](https://en.wikipedia.org/wiki/Self-supervised_learning)\n",
    "learning paradigm without the need for supervised fine-tuning.\n",
    "\n",
    "## From GPT-1 to GPT-2\n",
    "\n",
    "In\n",
    "[**Natural Language Understanding**](https://en.wikipedia.org/wiki/Natural-language_understanding)\n",
    "(NLU), there are a wide range of tasks, such as textual entailment, question\n",
    "answering, semantic similarity assessment, and document classification. These\n",
    "tasks are inherently labeled, but given the scarcity of such data, it makes\n",
    "[discriminative](https://en.wikipedia.org/wiki/Discriminative_model) models such\n",
    "as Bidirectional Long Short-Term Memory (Bi-LSTM) underperform\n",
    "{cite}`radford2018improving`, leading to poor performance on these tasks.\n",
    "\n",
    "In the GPT-1 paper\n",
    "[_Improving Language Understanding by Generative Pre-Training_](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf),\n",
    "the authors demonstrated that _generative pre-training_ of a language model on a\n",
    "diverse corpus of _unlabeled_ text, followed by _discriminative fine-tuning_ on\n",
    "each specific task, can overcome the constraints of the small amount of\n",
    "annotated data for these specific tasks. The process is collectively termed as\n",
    "[semi-supervised learning](https://en.wikipedia.org/wiki/Semi-supervised_learning)\n",
    "and the goal is to learn an **_universal representation_** of the natural\n",
    "language space that can be used across a wide range of tasks.\n",
    "\n",
    "The pretraining objective is to predict the next token in a sequence, in an\n",
    "**_autoregressive_** manner, given the previous tokens. The pretrained model,\n",
    "often known as the **_foundational model_** (or _backbone_), serves as a base\n",
    "from which specialized capabilities can be added through _fine-tuning_ on\n",
    "specific tasks. In the fine-tuning phase, task-specific adaptations are\n",
    "necessary: the input format must be adjusted to align with the particular\n",
    "requirements of the task at hand, and the model's final layer—or \"head\"—needs to\n",
    "be replaced to accommodate the task's specific class structure. The author\n",
    "showed that this approach yielded state-of-the-art results on a wide range of\n",
    "NLU tasks.\n",
    "\n",
    "Notwithstanding the success of this approach, the same set of authors came up\n",
    "with a new paper in the following year, titled\n",
    "[_Language Models are Unsupervised Multitask Learners_](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf),\n",
    "where they introduced a new model, _GPT-2_, that was larger in model capacity,\n",
    "and trained on a much larger unlabeled corpus, **WebText**. However, the key\n",
    "innovation was to void the supervised fine-tuning step, and instead, they\n",
    "demonstrated that GPT-2 could be used directly on a wide range of NLU tasks\n",
    "directly, with what they termed as the _zero-shot transfer_. The motivation is\n",
    "that the authors think that foundational language models should be competent\n",
    "generalists, rather than narrowly experts {cite}`radford2019language`. They call\n",
    "for the need to shift the language model paradigm to one that is generic enough\n",
    "to handle NLU tasks without the need to curate specific training data for each\n",
    "specific task.\n",
    "\n",
    "In what follows, we would first review the key concepts and ideas of the GPT-2\n",
    "paper, formalize the autoregressive self-supervised learning paradigm, and then\n",
    "take a look at the implementation of the GPT-2 model.\n",
    "\n",
    "## GPT-2 Paper Key Ideas\n",
    "\n",
    "In this section, we would review the key ideas from the GPT-2 paper.\n",
    "\n",
    "### Abstract Overview\n",
    "\n",
    "Below are the key ideas from the abstract of the GPT-2 paper:\n",
    "\n",
    "-   All **previous pretrained language models** necessitated a secondary stage\n",
    "    of **_supervised fine-tuning_** to tailor them to specific downstream tasks.\n",
    "-   The authors showcased that, given sufficient **_model capacity_** and\n",
    "    **_data_**, language models can be adeptly adjusted to a broad spectrum of\n",
    "    tasks **_without the need for task-specific architectural modifications_**.\n",
    "-   When tasked with a question-answering challenge, specifically conditioned on\n",
    "    a document and questions using the\n",
    "    [CoQA dataset](https://huggingface.co/datasets/stanfordnlp/coqa) — comprised\n",
    "    of over 127,700 training examples — the model demonstrates the capability to\n",
    "    **_match or surpass the performance of three baseline models_**.\n",
    "-   An emphasis is placed on the **_model's capacity_** as being integral to the\n",
    "    success of **_zero-shot transfer_**. It's highlighted that the model's\n",
    "    performance escalates in a **_log-linear fashion_** relative to the number\n",
    "    of parameters, signifying that as the model's capacity increases\n",
    "    _logarithmically_, its **performance** improves _linearly_.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this section, we would discuss the key ideas from the introduction of the\n",
    "GPT-2 paper.\n",
    "\n",
    "#### Key 1. Competent Generalists over Narrow Experts (1)\n",
    "\n",
    "-   The authors cited other works that have demonstrated significant success of\n",
    "    machine learning systems through a **_combination_** of **_large-scale\n",
    "    data_**, **_high model capacity_**, along with **_supervised fine-tuning_**.\n",
    "-   However, such systems, termed as \"**_narrow experts_**,\" are fragile, as\n",
    "    they are highly dependent on the specific training regime and task. A slight\n",
    "    **_perturbation_** to the input distribution can cause the model to perform\n",
    "    poorly.\n",
    "-   The authors then expressed the desire for \"**_competent generalists_**\" that\n",
    "    can perform well across a wide range of tasks **_without_** the need for\n",
    "    task-specific architectures or supervised fine-tuning.\n",
    "\n",
    "#### Key 2. IID Assumption Fails in Real World (2, 3)\n",
    "\n",
    "-   The overarching goal in machine learning is to **_generalize to unseen data\n",
    "    points_**. To streamline the modeling of machine learning objectives, it's\n",
    "    commonly assumed that the training and test data are drawn from the same\n",
    "    distribution, a concept known as the\n",
    "    [**_Independent and Identically Distributed (i.i.d.)_**](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)\n",
    "    assumption.\n",
    "    -   As an aside, the i.i.d. assumption is foundational in statistical\n",
    "        modeling because it simplifies the process significantly. For example,\n",
    "        it allows us to\n",
    "        [**_express joint probability distributions_**](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)\n",
    "        as the product of marginal distributions.\n",
    "    -   Furthermore, evaluation techniques such as **_resampling_** and\n",
    "        **_cross-validation_** with a holdout set rely on the assumption that\n",
    "        the training and test data are drawn from the same distribution.\n",
    "-   However, as the authors highlighted, the i.i.d. assumption fails in the real\n",
    "    world. The distribution of the test data is often different from the\n",
    "    training data, and the model's performance degrades significantly when the\n",
    "    test data distribution is different from the training data distribution.\n",
    "-   They attribute this to the prevalence of **single** task training on\n",
    "    **single** domain datasets, which limits the model's ability to generalize\n",
    "    across diverse conditions and tasks.\n",
    "\n",
    "```{admonition} References\n",
    ":class: seealso\n",
    "\n",
    "-   [On the importance of the i.i.d. assumption in statistical learning](https://stats.stackexchange.com/questions/213464/on-the-importance-of-the-i-i-d-assumption-in-statistical-learning)\n",
    "-   [Independent and identically distributed random variables - Wikipedia](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)\n",
    "-   [Independence and Identically Distributed (IID) - GAO Hongnan](https://gao-hongnan.github.io/gaohn-galaxy/probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html#independence-and-identically-distributed-iid)\n",
    "```\n",
    "\n",
    "#### Key 3. Multi-Task Learning is Nacent (4)\n",
    "\n",
    "-   The author then underscored that **_multi-task learning_** represents a\n",
    "    **_promising framework_**. By training a single model on **_multiple tasks\n",
    "    simultaneously_**, the model is enabled to leverage **_generalizable latent\n",
    "    space embeddings and representations_** to excel across various tasks.\n",
    "-   It was further pointed out that recent work in the field utilizes, for\n",
    "    example, **_10 (dataset, objective) pairs_** {cite}`mccann2018natural` to\n",
    "    train a singular model (an approach known as\n",
    "    [**_meta-learning_**](<https://en.wikipedia.org/wiki/Meta-learning_(computer_science)>)).\n",
    "    This implies that:\n",
    "    -   Each dataset and its corresponding objective are unique.\n",
    "    -   For instance, one dataset might focus on **_sentiment data_**, with the\n",
    "        goal of **_predicting sentence sentiment_**, whereas another dataset\n",
    "        might concentrate on **_named entity recognition_**, aiming to\n",
    "        **_identify named entities within a sentence_**.\n",
    "-   The **_challenge_** then circles back to the **_compilation, curation, and\n",
    "    annotation_** of these datasets and objectives to ensure the model's\n",
    "    generalizability. Essentially, this dilemma mirrors the initial issue of\n",
    "    **_single-task training on single-domain datasets_**. The implication is\n",
    "    that training a **_multi-task model_** might require an equivalent volume of\n",
    "    curated data as training several **_single-task models_**. Furthermore,\n",
    "    scalability becomes a concern when the focus is limited to merely **_10\n",
    "    (dataset, objective) pairs_**.\n",
    "\n",
    "#### Key 4. From Word Embeddings to Contextual Embeddings (5,6)\n",
    "\n",
    "-   Initially, **_word embeddings_** such as **Word2Vec** and **GloVe**\n",
    "    revolutionized the representation of words by mapping them into dense,\n",
    "    fixed-dimensional vectors within a continuous $D$ dimensional space, hinging\n",
    "    on the fact that words occuring in similar contexts/documents are similar\n",
    "    semantically. These vectors were then used as input to a model to perform a\n",
    "    specific task.\n",
    "-   The next advancement is capturing more _contextual information_ by using\n",
    "    **_contextual embeddings_**, where the word embeddings are **conditioned**\n",
    "    on the entire context of the sentence.\n",
    "    [**Recurrent Neural Networks**](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
    "    (RNNs) is one example and the context embeddings can be \"transferred\" to\n",
    "    other downstream tasks.\n",
    "\n",
    "    Specifically, **unidirectional RNNs** are adept at assimilating context from\n",
    "    preceding elements, whereas **bidirectional RNNs** excel in integrating\n",
    "    context from both preceding and succeeding elements. Nonetheless, both\n",
    "    strategies grapple with challenges in encoding long-range dependencies.\n",
    "\n",
    "    Moreover, RNNs are notoriously plagued by the\n",
    "    [**_gradient vanishing problem_**](https://ai.stackexchange.com/questions/20075/why-does-the-transformer-do-better-than-rnn-and-lstm-in-long-range-context-depen),\n",
    "    which means that the model is **biased** by the most _recent_ tokens in the\n",
    "    sequence, and the model’s performance **degrades** as the _sequence length_\n",
    "    **increases**.\n",
    "\n",
    "-   **_Self-attention mechanisms_**, foundational to the **Transformer\n",
    "    architecture**, mark a paradigm shift by enabling each token to \"attend\" to\n",
    "    every other token within a sequence concurrently.\n",
    "\n",
    "    -   This allows the model to capture long-range dependencies and is the\n",
    "        basis for the Transformer architecture. Consequently, self-attention is\n",
    "        non-sequential by design and operates over a _set_ of tokens, and not a\n",
    "        _sequence_ of tokens. This calls for the need to introduce positional\n",
    "        encodings to the input embeddings to capture the sequential nature of\n",
    "        the tokens.\n",
    "\n",
    "    -   This advancement transcends the limitations of static word embeddings.\n",
    "        Now, given two sentences, _I went to the river bank_ versus _i went to\n",
    "        the bank to withdraw money_, the word \"bank\" in the first sentence is\n",
    "        semantically different from the word \"bank\" in the second sentence. The\n",
    "        contextual embeddings can capture this difference.\n",
    "\n",
    "-   The authors then went on to mention that the above methods would still\n",
    "    require supervised fine-tuning to adapt to a specific task.\n",
    "\n",
    "    If there are minimal or no supervised data is available, there are other\n",
    "    lines of work using language model to handle it - commonsense reasoning\n",
    "    (Schwartz et al., 2017) and sentiment analysis (Radford et al., 2017).\n",
    "\n",
    "```{admonition} References\n",
    ":class: seealso\n",
    "\n",
    "-   [Why does the transformer do better than RNN and LSTM in long-range context dependencies?](https://ai.stackexchange.com/questions/20075/why-does-the-transformer-do-better-than-rnn-and-lstm-in-long-range-context-depen)\n",
    "-   [How Transformer is Bidirectional - Machine Learning](https://stackoverflow.com/questions/55158554/how-transformer-is-bidirectional-machine-learning)\n",
    "```\n",
    "\n",
    "#### Key 5. Zero Shot Learning and Zero Shot Transfer (7)\n",
    "\n",
    "-   Building upon the foundational concepts introduced previously, the authors\n",
    "    explore the utilization of **_general methods of transfer_** to illustrate\n",
    "    how language models can adeptly execute downstream tasks in a **_zero-shot\n",
    "    manner_**, without necessitating any modifications to parameters or\n",
    "    architecture.\n",
    "\n",
    "-   **_Zero-shot learning (ZSL)_** is characterized by a model's capability to\n",
    "    accurately execute tasks or recognize categories that it was not explicitly\n",
    "    trained to handle. The crux of ZSL lies in its ability to **_generalize from\n",
    "    known to unknown_** classes or tasks by harnessing side information or\n",
    "    semantic relationships.\n",
    "\n",
    "    -   For example, a model trained to recognize on a set of animals (including\n",
    "        horses) but not on zebra, should be able to recognize a zebra as\n",
    "        something close to horse, given the semantic relationship between the\n",
    "        two animals.\n",
    "\n",
    "-   **_Zero-shot transfer_**, often discussed within the context of **transfer\n",
    "    learning**, involves applying a model trained on one set of tasks or domains\n",
    "    to a completely new task or domain without any additional training. Here,\n",
    "    the focus is on the transferability of learned features or knowledge across\n",
    "    different but related tasks or domains. Zero-shot transfer extends the\n",
    "    concept of transfer learning by not requiring any examples from the target\n",
    "    domain during training, relying instead on the model's ability to generalize\n",
    "    across different contexts based on its pre-existing knowledge.\n",
    "\n",
    "```{admonition} References\n",
    ":class: seealso\n",
    "\n",
    "-   [Zero-shot learning - Wikipedia](https://en.wikipedia.org/wiki/Zero-shot_learning)\n",
    "-   [What is the difference between one-shot learning, transfer learning, and fine-tuning? - AI Stack Exchange](https://ai.stackexchange.com/questions/21719/what-is-the-difference-between-one-shot-learning-transfer-learning-and-fine-tun)\n",
    "-   [Zero-Shot Learning in Modern NLP - Joe Davison](https://joeddav.github.io/blog/2020/05/29/ZSL.html)\n",
    "-   [Zero-Shot Learning Through Cross-Modal Transfer - arXiv](https://arxiv.org/abs/1301.3666)\n",
    "-   [Zero shot learning available labels in testing set - AI Stack Exchange](https://ai.stackexchange.com/questions/23527/zero-shot-learning-available-labels-in-testing-set)\n",
    "-   [Zero-Shot Learning: Can You Classify an Object Without Seeing It Before?](https://www.theaidream.com/post/zero-shot-learning-can-you-classify-an-object-without-seeing-it-before)\n",
    "-   [A Survey of Zero-Shot Learning: Settings, Methods, and Applications](https://dl.acm.org/doi/10.1145/3293318)\n",
    "```\n",
    "\n",
    "### Section 2. Approach\n",
    "\n",
    "In this section, we would discuss the key ideas from the approach section of the\n",
    "GPT-2 paper.\n",
    "\n",
    "#### Key 1. Modeling Language Models over Joint Probability Distributions (1)\n",
    "\n",
    "Language models strive to approximate the complex and inherently unknown\n",
    "distribution of the natural language space, denoted as $\\mathcal{D}$. In\n",
    "contrast to supervised learning, which explicitly separates inputs\n",
    "($\\mathcal{X}$) from labels ($\\mathcal{Y}$), unsupervised learning —\n",
    "particularly when employing self-supervision as seen in language modeling —\n",
    "blurs this distinction. Here, $\\mathcal{Y}$ is conceptually a shifted\n",
    "counterpart of $\\mathcal{X}$, facilitating a unified approach where\n",
    "$\\mathcal{D}$ can be modeled exclusively over the space of $\\mathcal{X}$. This\n",
    "scenario allows us to frame $\\mathcal{D}$ as a probability distribution across\n",
    "sequences of tokens within $\\mathcal{X}$, parameterized by\n",
    "$\\boldsymbol{\\Theta}$.\n",
    "\n",
    "In this context, the essence of language modeling is to characterize the\n",
    "**_joint probability distribution_** of sequences\n",
    "$\\mathbf{x} = (x_1, x_2, \\ldots, x_T)$ within $\\mathcal{X}$. The goal is to\n",
    "maximize the likelihood of observing these sequences in a corpus $\\mathcal{S}$,\n",
    "denoted as $\\hat{\\mathcal{L}}(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}})$, where\n",
    "$\\hat{\\boldsymbol{\\Theta}}$ represents the estimated parameter space that\n",
    "approximates the true parameter space $\\boldsymbol{\\Theta}$.\n",
    "\n",
    "#### Key 2. Decompose Joint Distributions as Conditional Distributions via Chain Rule (2)\n",
    "\n",
    "The joint probability of a sequence in natural language, **inherently ordered**\n",
    "{cite}`radford2019language`, can be factorized into the product of conditional\n",
    "probabilities of each token in the sequence using the\n",
    "[**chain rule of probability**](<https://en.wikipedia.org/wiki/Chain_rule_(probability)>).\n",
    "This approach not only enables **_tractable sampling_** from and\n",
    "**_estimation_** of the distribution\n",
    "$\\mathbb{P}(\\mathbf{x} ; \\boldsymbol{\\Theta})$ but also facilitates modeling\n",
    "conditionals in forms such as\n",
    "$\\mathbb{P}(x_{t-k} \\ldots x_t \\mid x_1 \\ldots x_{t-k-1} ; \\boldsymbol{\\Theta})$\n",
    "{cite}`radford2019language`. Given a corpus $\\mathcal{S}$ with $N$ sequences,\n",
    "the likelihood function\n",
    "$\\hat{\\mathcal{L}}(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}})$ represents the\n",
    "likelihood of observing these sequences. The ultimate objective is to maximize\n",
    "this likelihood, effectively _approximating_ the joint probability distribution\n",
    "through conditional probability distributions.\n",
    "\n",
    "#### Key 3. Conditional on Task (3)\n",
    "\n",
    "In the GPT-2 paper, _Language Models are Unsupervised Multitask Learners_, the\n",
    "authors introduced the concept of _conditional on task_ where the GPT model\n",
    "$\\mathcal{G}$ theoretically should not only learn the conditional probability\n",
    "distribution $\\mathbb{P}(x_t \\mid x_{<t} ; \\boldsymbol{\\Theta})$ but also learn\n",
    "the conditional probability distribution\n",
    "$\\mathbb{P}(x_t \\mid x_{<t} ; \\boldsymbol{\\Theta}, \\mathcal{T})$ where\n",
    "$\\mathcal{T}$ is the task that the model should implicitly learn\n",
    "{cite}`radford2019language`. This is a powerful concept because if such a\n",
    "hypothesis is correct, then the GPT model $\\mathcal{G}$ can indeed be a\n",
    "multi-task learner, and can be used directly on a wide range of NLU tasks\n",
    "without the need for supervised fine-tuning for downstream domain-specific\n",
    "tasks.\n",
    "\n",
    "In practice, the authors mentioned that task conditioning is often implemented\n",
    "at an architectural level, via task specific encoder and decoder in the paper\n",
    "[_One Model To Learn Them All_](https://arxiv.org/abs/1706.05137)\n",
    "{cite}`kaiser2017model`, for instance, or at an algorithmic level, such as the\n",
    "inner and outer loop optimization framework, as seen in the paper\n",
    "[_Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks_](https://arxiv.org/abs/1703.03400)\n",
    "{cite}`finn2017modelagnostic`.\n",
    "\n",
    "However, the authors further mentioned that without task-specific architectural\n",
    "changes, one can leverage the sequential nature of the natural language space\n",
    "where we can construct a tasks, inputs and outputs all as a sequence of symbols\n",
    "{cite}`radford2019language`. For example, a translation task can be formulated\n",
    "as a sequence of symbols via\n",
    "`(translate to french, english sequence, french sequence)`, where the model can\n",
    "now learn to also condition on the task `(translate to french)` in addition to\n",
    "the sequence of tokens. The paper _The Natural Language Decathlon: Multitask\n",
    "Learning as Question Answering_ exemplifies this concept with their model\n",
    "**Multitask Question Answering Network (MQAN)**, where a single model is trained\n",
    "to perform many diverse natural language processing tasks simultaneously.\n",
    "\n",
    "#### Key 4. Optimizing Unsupervised is the same as Optimizing Supervised (4)\n",
    "\n",
    "The GPT-2 paper _Language Models are Unsupervised Multitask Learners_\n",
    "demonstrated that they want to do away with the supervised fine-tuning phase via\n",
    "an interesting hypothesis, that **optimizing the unsupervised objective is the\n",
    "same as optimizing the supervised objective** because the _global minimum_ of\n",
    "the unsupervised objective is the same as the _global minimum_ of the supervised\n",
    "objective {cite}`radford2019language`.\n",
    "\n",
    "#### Key 5. Large Language Models has Capacity to Infer and Generalize (5)\n",
    "\n",
    "In what follows, the author added that the internet contains a vast amount of\n",
    "information that is passively available without the need for interactive\n",
    "communication. The example that I provided on the french-to-english translation\n",
    "would bound to exist naturally in the internet. They speculate that if the\n",
    "language model is **large** enough in terms of **capacity**, then it should be\n",
    "able to learn to perform the tasks demonstrated in natural language sequences in\n",
    "order to better predict them, regardless of their method of procurement\n",
    "{cite}`radford2019language`.\n",
    "\n",
    "In the figure below, we can see examples of naturally occurring demonstrations\n",
    "of English to French and French to English translation found throughout the\n",
    "WebText training set.\n",
    "\n",
    "```{figure} ./assets/gpt-2-table-1.png\n",
    "---\n",
    "name: decoder-concept-gpt-2-table-1\n",
    "---\n",
    "\n",
    "Examples of naturally occurring demonstrations of English to French and French\n",
    "to English translation found throughout the WebText training set.\n",
    "\n",
    "**Image Credit:**\n",
    "[Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "```\n",
    "\n",
    "### 2.1. Training Dataset\n",
    "\n",
    "#### Key 1. Rejection of CommonCrawl (1,2)\n",
    "\n",
    "-   Prior research often focused on training language models on **_single-domain\n",
    "    datasets_**, which relates to the concept of models becoming **_narrow\n",
    "    experts_**.\n",
    "-   To cultivate **_competent generalists_**, the authors contend that models\n",
    "    need exposure to a **_diverse array_** of tasks and domains.\n",
    "-   **_CommonCrawl_**, housing an expansive collection of web scrapes\n",
    "    (essentially capturing the entirety of the internet), is recognized for its\n",
    "    diversity.\n",
    "-   Nevertheless, CommonCrawl was ultimately **_rejected_** by the authors due\n",
    "    to **_significant data quality issues_**.\n",
    "\n",
    "#### Key 2. Construction of WebText Dataset\n",
    "\n",
    "-   The authors sought to compile a web scrape prioritizing **_document quality\n",
    "    over quantity_**.\n",
    "-   To attain a certain level of document quality without the exorbitant costs\n",
    "    of manual curation, the authors employed a strategy of **_indirect human\n",
    "    curation_**. This involved scraping all **_outbound links from Reddit_**\n",
    "    that garnered a minimum of **_3 karma_**. Karma, in this scenario, acts as a\n",
    "    heuristic for content deemed interesting, educational, or entertaining by\n",
    "    the Reddit community.\n",
    "    -   **_Outbound links_** refer to instances where a Reddit post links out to\n",
    "        external websites; the authors included the content from these external\n",
    "        sites in their dataset, contingent on the originating post receiving at\n",
    "        least 3 karma.\n",
    "-   The resulting dataset, dubbed **_WebText_**, comprises text from\n",
    "    approximately **_45 million links_**.\n",
    "-   Subsequent preprocessing efforts, including **_de-duplication,\n",
    "    heuristic-based cleaning_**, and the **_exclusion of Wikipedia links_**,\n",
    "    resulted in a dataset spanning about **_40GB of text (8 million\n",
    "    documents)_**.\n",
    "-   The snapshot of the dataset is **_December 2017_**.\n",
    "-   Wikipedia's exclusion was deliberate, stemming from the authors' intention\n",
    "    to minimize overlap with training sources prevalent in other studies. This\n",
    "    decision aimed to facilitate more \"authentic\" **_evaluation/testing_**\n",
    "    scenarios for their model by reducing data leakage.\n",
    "\n",
    "### 2.2. Input Representation\n",
    "\n",
    "#### Key 1. Byte Pair Encoding (BPE) (1,2,3)\n",
    "\n",
    "-   Traditional tokenization methods often involve steps such as\n",
    "    **_lower-casing_**, **_punctuation stripping_**, and **_splitting on\n",
    "    whitespace_**. Additionally, these methods might encode out-of-vocabulary\n",
    "    words using a special token to enable the model to handle unseen words\n",
    "    during evaluation or testing phases. For instance, language models (LMs) may\n",
    "    struggle with interpreting emojis due to such constraints.\n",
    "-   These conventional approaches can inadvertently restrict the natural\n",
    "    language input space $\\mathcal{X}$, consequently limiting the model space\n",
    "    $\\mathcal{H}$. This limitation stems from the fact that the scope of\n",
    "    $\\mathcal{H}$ is inherently dependent on the comprehensiveness of\n",
    "    $\\mathcal{X}$ as we can see\n",
    "    $\\mathcal{H} = \\mathcal{H}(\\mathcal{X} ; \\boldsymbol{\\Theta})$, which means\n",
    "    that the model space $\\mathcal{H}$ is a function of the input space\n",
    "    $\\mathcal{X}$ and the parameter space $\\boldsymbol{\\Theta}$.\n",
    "-   To resolve this, the idea of **_byte-level encoding_** can be used - since\n",
    "    you theoretically can encode any character in the world in **_UTF-8\n",
    "    encoding_**.\n",
    "-   However, the limitation is current byte-level language models tend to\n",
    "    perform poorly on word level tasks.\n",
    "-   The authors then introduced the BPE algorithm (is \"byte-level\" because it\n",
    "    operates on UTF-8 encoded strings) where they striked a balance between\n",
    "    character-level and word-level tokenization.\n",
    "-   So in summary, BPE is the **tokenizer** used to encode the input text into a\n",
    "    sequence of tokens - which form the input representation to the model.\n",
    "\n",
    "```{admonition} References\n",
    ":class: seealso\n",
    "\n",
    "-   [https://github.com/karpathy/minbpe](https://github.com/karpathy/minbpe)\n",
    "-   [https://github.com/openai/tiktoken](https://github.com/openai/tiktoken)\n",
    "-   [Byte Pair Encoding on Hugging Face's NLP Course](https://huggingface.co/learn/nlp-course/en/chapter6/5)\n",
    "```\n",
    "\n",
    "### 2.3. Model\n",
    "\n",
    "The GPT-2 architecture is a **_transformer_**-based model, and as the name\n",
    "suggests, it is a continuation of the GPT-1 model with some minor modifications.\n",
    "\n",
    "#### Key 1. GPT-2 is a Continuation of GPT-1 with Self-Attention Mechanisms (1)\n",
    "\n",
    "-   GPT-2 utilizes a **Transformer** architecture {cite}`vaswani2017attention`\n",
    "    as its backbone, which is distinguished by **_self-attention mechanisms_**.\n",
    "    This architecture empowers the model to capture complex dependencies and\n",
    "    relationships within the data.\n",
    "\n",
    "#### Key 2. Modifications from GPT-1 and Model Stability (1)\n",
    "\n",
    "-   Modifications from GPT-1 include:\n",
    "\n",
    "    -   **Layer normalization** is repositioned to the **_input_** of each\n",
    "        sub-block, mirroring a **_pre-activation residual network_**. This\n",
    "        modification is believed to offer training stability and model\n",
    "        performance. By normalizing the inputs to each sub-block, it is\n",
    "        conjectured to alleviate issues tied to **_internal covariate shift_**,\n",
    "        thus aiding in smoother and potentially faster training.\n",
    "    -   GPT-2 introduces an **_additional layer normalization step_** that is\n",
    "        executed **_after the final self-attention block_** within the model.\n",
    "        This additional normalization step can help ensure that the outputs of\n",
    "        the transformer layers are normalized before being passed to subsequent\n",
    "        layers or used in further processing, further contributing to model\n",
    "        stability.\n",
    "    -   The GPT-2 paper introduces a modification to the standard weight\n",
    "        initialization for the model's residual layers. Specifically, the\n",
    "        weights are scaled by a factor of\n",
    "        $\\frac{1}{\\sqrt{N_{\\text{decoder_blocks}}}}$, where\n",
    "        $N_{\\text{decoder_blocks}}$ represents the number of blocks (or layers)\n",
    "        in the Transformer's decoder.\n",
    "\n",
    "        The rationale, as quoted from the paper: _\"A modified initialization\n",
    "        which accounts for the accumulation on the residual path with model\n",
    "        depth is used\"_ {cite}`radford2019language`, is to ensure that the\n",
    "        variance of the input to the block is the same as the variance of the\n",
    "        block's output. This is to ensure that the signal is neither amplified\n",
    "        nor diminished as it passes through the block. As the model depth\n",
    "        increases, the activations get added/acculumated, and hence the scaling\n",
    "        factor is $\\frac{1}{\\sqrt{N_{\\text{decoder_blocks}}}}$, to scale it\n",
    "        down.\n",
    "\n",
    "    -   Clearly, we can see the empahsis on model stability. In training large\n",
    "        language models, **numerical stability** is paramount; the cost of\n",
    "        training is significantly high, with every loss and gradient spike that\n",
    "        fails to recover necessitating a return to a previous checkpoint,\n",
    "        resulting in substantial GPU hours and potentially tens of thousands of\n",
    "        dollars wasted.\n",
    "    -   The model's **vocabulary** is expanded to 50,257 tokens.\n",
    "    -   The context window size is increased from 512 to 1024 tokens, enhancing\n",
    "        the model's ability to maintain coherence over longer text spans.\n",
    "    -   A larger batch size of 512, GPT-2 benefits from more stable and\n",
    "        effective gradient estimates during training, contributing to improved\n",
    "        learning outcomes.\n",
    "\n",
    "#### GPT-2 Variants\n",
    "\n",
    "To this end, we encapsulate some key parameters in\n",
    "{numref}`decoder-concept-gpt-2-family` below, which provides specifications for\n",
    "several GPT-2 variants, distinguished by their scale.\n",
    "\n",
    "```{list-table} GPT-2 Family\n",
    ":header-rows: 1\n",
    ":name: decoder-concept-gpt-2-family\n",
    "\n",
    "* - Parameters\n",
    "  - Layers\n",
    "  - d_model\n",
    "  - H\n",
    "  - d_ff\n",
    "  - Activation\n",
    "  - Vocabulary Size\n",
    "  - Context Window\n",
    "* - 117M\n",
    "  - 12\n",
    "  - 768\n",
    "  - 12\n",
    "  - 3072\n",
    "  - GELU\n",
    "  - 50,257\n",
    "  - 1024\n",
    "* - 345M\n",
    "  - 24\n",
    "  - 1024\n",
    "  - 16\n",
    "  - 4096\n",
    "  - GELU\n",
    "  - 50,257\n",
    "  - 1024\n",
    "* - 762M\n",
    "  - 36\n",
    "  - 1280\n",
    "  - 20\n",
    "  - 5120\n",
    "  - GELU\n",
    "  - 50,257\n",
    "  - 1024\n",
    "* - 1542M\n",
    "  - 48\n",
    "  - 1600\n",
    "  - 25\n",
    "  - 6400\n",
    "  - GELU\n",
    "  - 50,257\n",
    "  - 1024\n",
    "```\n",
    "\n",
    "```{admonition} Implementation\n",
    ":class: seealso\n",
    "\n",
    "See\n",
    "[The Implementation of Generative Pre-trained Transformers (GPT)](https://www.gaohongnan.com/transformer/decoder/implementation.html)\n",
    "for a more comprehensive walkthrough of the GPT-2 model architecture, annotated\n",
    "with code.\n",
    "```\n",
    "\n",
    "## Autoregressive Self-Supervised Learning Paradigm\n",
    "\n",
    "Let $\\mathcal{D}$ be the true but unknown distribution of the natural language\n",
    "space. In the context of unsupervised learning with self-supervision, such as\n",
    "language modeling, we consider both the inputs and the implicit labels derived\n",
    "from the same data sequence. Thus, while traditionally we might decompose the\n",
    "distribution $\\mathcal{D}$ of a supervised learning task into input space\n",
    "$\\mathcal{X}$ and label space $\\mathcal{Y}$, in this scenario, $\\mathcal{X}$ and\n",
    "$\\mathcal{Y}$ are intrinsically linked, because $\\mathcal{Y}$ is a shifted\n",
    "version of $\\mathcal{X}$, and so we can consider $\\mathcal{D}$ as a distribution\n",
    "over $\\mathcal{X}$ only.\n",
    "\n",
    "Since $\\mathcal{D}$ is a distribution, we also define it as a probability\n",
    "distribution over $\\mathcal{X}$, and we can write it as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{D} &= \\mathbb{P}(\\mathcal{X} ; \\boldsymbol{\\Theta}) \\\\\n",
    "            &= \\mathbb{P}_{\\{\\mathcal{X} ; \\boldsymbol{\\Theta}\\}}(\\mathbf{x})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\Theta}$ is the parameter space that defines the distribution\n",
    "$\\mathbb{P}(\\mathcal{X} ; \\boldsymbol{\\Theta})$ and $\\mathbf{x}$ is a sample\n",
    "from $\\mathcal{X}$ generated by the distribution $\\mathcal{D}$. It is common to\n",
    "treat $\\mathbf{x}$ as a sequence of tokens (i.e. a sentence is a sequence of\n",
    "tokens), and we can write $\\mathbf{x} = \\left(x_1, x_2, \\ldots, x_T\\right)$,\n",
    "where $T$ is the length of the sequence.\n",
    "\n",
    "Given such a sequence $\\mathbf{x}$, the joint probability of the sequence can be\n",
    "factorized into the product of the conditional probabilities of each token in\n",
    "the sequence via the\n",
    "[chain rule of probability](<https://en.wikipedia.org/wiki/Chain_rule_(probability)>):\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\mathbf{x} ; \\boldsymbol{\\Theta}) = \\prod_{t=1}^T \\mathbb{P}(x_t \\mid x_1, x_2, \\ldots, x_{t-1} ; \\boldsymbol{\\Theta})\n",
    "$$\n",
    "\n",
    "We can do this because natural language are _inherently ordered_. Such\n",
    "decomposition allows for _tractable sampling_ from and _estimation_ of the\n",
    "distribution $\\mathbb{P}(\\mathbf{x} ; \\boldsymbol{\\Theta})$ as well as any\n",
    "conditionals in the form of\n",
    "$\\mathbb{P}(x_{t-k}, x_{t-k+1}, \\ldots, x_{t} \\mid x_{1}, x_{2}, \\ldots, x_{t-k-1} ; \\boldsymbol{\\Theta})$\n",
    "{cite}`radford2019language`.\n",
    "\n",
    "To this end, consider a corpus $\\mathcal{S}$ with $N$ sequences\n",
    "$\\left\\{\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\ldots, \\mathbf{x}_{N}\\right\\}$,\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\left\\{\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\ldots, \\mathbf{x}_{N}\\right\\} \\underset{\\text{i.i.d.}}{\\sim} \\mathcal{D}\n",
    "$$\n",
    "\n",
    "where each sequence $\\mathbf{x}_{n}$ is a sequence of tokens that are sampled\n",
    "$\\text{i.i.d.}$ from the distribution $\\mathcal{D}$.\n",
    "\n",
    "Then, we can frame the\n",
    "[likelihood function](https://gao-hongnan.github.io/gaohn-galaxy/probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html)\n",
    "$\\hat{\\mathcal{L}}(\\cdot)$ as the likelihood of observing the sequences in the\n",
    "corpus $\\mathcal{S}$,\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}\\left(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}}\\right) = \\prod_{n=1}^N \\mathbb{P}(\\mathbf{x}_{n} ; \\hat{\\boldsymbol{\\Theta}})\n",
    "$$\n",
    "\n",
    "where $\\hat{\\boldsymbol{\\Theta}}$ is the estimated parameter space that\n",
    "approximates the true parameter space $\\boldsymbol{\\Theta}$.\n",
    "\n",
    "Subsequently, the objective function is now well-defined, to be the maximization\n",
    "of the likelihood of the sequences in the corpus $\\mathcal{S}$,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\boldsymbol{\\theta}}^{*} &= \\underset{\\hat{\\boldsymbol{\\theta}} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\hat{\\mathcal{L}}\\left(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}}\\right) \\\\\n",
    "                              &= \\underset{\\hat{\\boldsymbol{\\theta}} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\prod_{n=1}^N \\mathbb{P}(\\mathbf{x}_{n} ; \\hat{\\boldsymbol{\\Theta}}) \\\\\n",
    "                              &= \\underset{\\hat{\\boldsymbol{\\theta}} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\prod_{n=1}^N \\prod_{t=1}^{T_n} \\mathbb{P}(x_{n, t} \\mid x_{n, 1}, x_{n, 2}, \\ldots, x_{n, t-1} ; \\hat{\\boldsymbol{\\Theta}}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $T_n$ is the length of the sequence $\\mathbf{x}_{n}$.\n",
    "\n",
    "Owing to the fact that multiplying many probabilities together can lead to\n",
    "[numerical instability](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html#numerical-optimization-and-the-negative-log-likelihood)\n",
    "because the product of many probabilities can be very small, it is common and\n",
    "necessary to use the log-likelihood as the objective function, because it can be\n",
    "proven that maximizing the log-likelihood is equivalent to maximizing the\n",
    "likelihood itself.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\boldsymbol{\\theta}}^{*} &= \\underset{\\hat{\\boldsymbol{\\theta}} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\log\\left(\\hat{\\mathcal{L}}\\left(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}}\\right)\\right) \\\\\n",
    "&= \\underset{\\hat{\\boldsymbol{\\theta}} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\sum_{n=1}^N \\sum_{t=1}^{T_n} \\log \\mathbb{P}(x_{n, t} \\mid x_{n, 1}, x_{n, 2}, \\ldots, x_{n, t-1} ; \\hat{\\boldsymbol{\\Theta}}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Furthermore, since we are treating the the loss function as a form of\n",
    "minimization, we can simply negate the log-likelihood to obtain the negative\n",
    "log-likelihood as the objective function to be minimized,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\boldsymbol{\\theta}}^{*} &= \\underset{\\hat{\\boldsymbol{\\theta}} \\in \\boldsymbol{\\Theta}}{\\text{argmin}} \\left(-\\log\\left(\\hat{\\mathcal{L}}\\left(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}}\\right)\\right)\\right) \\\\\n",
    "&= \\underset{\\hat{\\boldsymbol{\\theta}} \\in \\boldsymbol{\\Theta}}{\\text{argmin}} \\left(-\\sum_{n=1}^N \\sum_{t=1}^{T_n} \\log \\mathbb{P}(x_{n, t} \\mid x_{n, 1}, x_{n, 2}, \\ldots, x_{n, t-1} ; \\hat{\\boldsymbol{\\Theta}})\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "It is worth noting that the objective function is a function of the parameter\n",
    "space $\\hat{\\boldsymbol{\\Theta}}$, and not the data $\\mathcal{S}$, so all\n",
    "analysis such as convergence and consistency will be with respect to the\n",
    "parameter space $\\hat{\\boldsymbol{\\Theta}}$.\n",
    "\n",
    "To this end, we denote the GPT model $\\mathcal{G}$ to be an _autoregressive_ and\n",
    "_self-supervised learning_ model that is trained to maximize the likelihood of\n",
    "observing all data points $\\mathbf{x} \\in \\mathcal{S}$ via the objective\n",
    "function $\\hat{\\mathcal{L}}\\left(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}}\\right)$\n",
    "by learning the conditional probability distribution\n",
    "$\\mathbb{P}(x_t \\mid x_{<t} ; \\hat{\\boldsymbol{\\Theta}})$ over the vocabulary\n",
    "$\\mathcal{V}$ of tokens, conditioned on the contextual preciding tokens\n",
    "$x_{<t} = \\left(x_1, x_2, \\ldots, x_{t-1}\\right)$. We are clear that although\n",
    "the goal is to model the joint probability distribution of the token sequences,\n",
    "we can do so by estimating the joint probability distribution via the\n",
    "conditional probability distributions.\n",
    "\n",
    "```{prf:remark} Simplification of the Objective Function\n",
    ":label: decoder-simplified-objective-function\n",
    "\n",
    "In what follows, we will mostly focus on the inner summand of the objective\n",
    "function, namely, we will look at the loss function for a single sequence\n",
    "$\\mathbf{x}$. And in particular the conditional probability distribution\n",
    "$\\mathbb{P}(x_t \\mid x_{<t} ; \\hat{\\boldsymbol{\\Theta}})$. It should be clear\n",
    "that the objective function is over all $N$ sequences in the corpus $\\mathcal{S}$,\n",
    "where each sequence $\\mathbf{x}_n$ can be decomposed into the product of the\n",
    "conditional probabilities of each token in the sequence.\n",
    "```\n",
    "\n",
    "### Autoregressive Self-Supervised Learning\n",
    "\n",
    "The learning paradigm of an autoregressive self-supervised learning framework\n",
    "can be formalized as a learning algorithm $\\mathcal{A}$ that is trained to\n",
    "predict the next token $x_t$ in a sequence given the previous tokens\n",
    "$x_{<t} = \\left(x_1, x_2, \\ldots, x_{t-1}\\right)$ in the sequence $\\mathbf{x}$\n",
    "(_autoregressive_), where $t \\in \\{1, 2, \\ldots, T\\}$ is the position of the\n",
    "token in the sequence, and _self-supervised_ because the \"label\" $x_t$ is\n",
    "derived from the input sequence $\\mathbf{x}$ itself. The model $\\mathcal{G}$\n",
    "then uses $\\mathcal{A}$ to learn a **_conditional probability distribution_**\n",
    "$\\mathbb{P}(x_t \\mid x_{<t} ; \\boldsymbol{\\Theta})$ over the vocabulary\n",
    "$\\mathcal{V}$ of tokens, conditioned on the contextual preciding tokens\n",
    "$x_{<t} = \\left(x_1, x_2, \\ldots, x_{t-1}\\right)$, where $\\boldsymbol{\\Theta}$\n",
    "is the parameter space that defines the distribution\n",
    "$\\mathbb{P}(x_t \\mid x_{<t} ; \\boldsymbol{\\Theta})$.\n",
    "\n",
    "The distinction between $\\mathcal{V}$ and $\\mathcal{X}$ is that $\\mathcal{V}$ is\n",
    "the vocabulary of tokens, which is a discrete space, and $\\mathcal{X}$ is the\n",
    "natural language space, which is a combinatorial discrete space. We can think of\n",
    "$\\mathcal{X}$ as the natural language space of _**all possible sequences**_\n",
    "$\\mathbf{x}$ that can be formed from the vocabulary $\\mathcal{V}$ (an\n",
    "enumeration over $\\mathcal{V}$). Consequently, there is no confusion that a\n",
    "_sequence_ $\\mathbf{x}$ is a member of $\\mathcal{X}$, and a _token_ $x_t$ is a\n",
    "member of $\\mathcal{V}$.\n",
    "\n",
    "Through this learning algorithm, we can recover all chained conditional\n",
    "probabilities of the form $\\mathbb{P}(x_t \\mid x_{<t} ; \\boldsymbol{\\Theta})$,\n",
    "which implicitly defines the joint probability distribution\n",
    "$\\mathbb{P}(\\mathbf{x}\n",
    "; \\boldsymbol{\\Theta})$ over the natural language space\n",
    "$\\mathcal{X}$[^1].\n",
    "\n",
    "### Estimation of the Conditional Probability Distribution\n",
    "\n",
    "In practice, we can only _**estimate**_ the conditional probability distribution\n",
    "$\\mathbb{P}(x_t \\mid x_{<t} ; \\boldsymbol{\\Theta})$ from the corpus\n",
    "$\\mathcal{S}$, and we can write the process of estimating as:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbb{P}}(x_t \\mid x_{<t} ; \\hat{\\boldsymbol{\\Theta}}) \\approx \\mathbb{P}(x_t \\mid x_{<t} ; \\boldsymbol{\\Theta})\n",
    "$$\n",
    "\n",
    "where $\\hat{\\boldsymbol{\\Theta}}$ is the estimated parameter space that\n",
    "approximates the true parameter space $\\boldsymbol{\\Theta}$.\n",
    "\n",
    "To facilitate the notational burden, we denote the estimated conditional\n",
    "probability distribution\n",
    "$\\hat{\\mathbb{P}}(x_t \\mid x_{<t} ; \\hat{\\boldsymbol{\\Theta}})$ as a function\n",
    "$f_{\\hat{\\boldsymbol{\\Theta}}}(\\cdot)$, and equate them as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_{\\hat{\\boldsymbol{\\Theta}}}(x_t \\mid x_{<t}) &:= \\mathbb{P}(x_t \\mid x_{<t} ; \\boldsymbol{\\Theta}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $f_{\\hat{\\boldsymbol{\\Theta}}}(x_t \\mid x_{<t})$ can be realised as our\n",
    "GPT model $\\mathcal{G}$.\n",
    "\n",
    "To this end, we should be clear that this learning process is to approximate the\n",
    "true distribution $\\mathcal{D}$ of the natural language space $\\mathcal{X}$, but\n",
    "instead of modeling over the entire space $\\mathcal{X}$, consisting of all\n",
    "sequences $\\mathbf{x}$, we model over the vocabulary $\\mathcal{V}$ of tokens,\n",
    "which is to generate the next token in a sequence given the previous tokens in\n",
    "the sequence.\n",
    "\n",
    "### Initial Condition of Conditional Probability Distribution\n",
    "\n",
    "While the earlier conditional distribution seems correct by definition of the\n",
    "[chain rule of probability](<https://en.wikipedia.org/wiki/Chain_rule_(probability)>),\n",
    "it is worth noting that we are being a bit loose when $t=1$. Firstly, when\n",
    "$t=1$, we are actually conditioning on nothing, and so it is the case that we\n",
    "are estimating $\\mathbb{P}(x_1 ; \\boldsymbol{\\Theta})$. But this is not part of\n",
    "the learning process because we would need something to condition on. For the\n",
    "sake of completeness, we can treat the initial token $x_1$ as the initial\n",
    "condition, and we can write the chain rule as:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\mathbf{x} ; \\boldsymbol{\\Theta}) = \\mathbb{P}(x_1 ; \\boldsymbol{\\Theta}) \\prod_{t=2}^T \\mathbb{P}(x_t \\mid x_1, x_2, \\ldots, x_{t-1} ; \\boldsymbol{\\Theta})\n",
    "$$\n",
    "\n",
    "where $\\mathbb{P}(x_1 ; \\boldsymbol{\\Theta})$ can be thought of the \"initial\n",
    "prompt\" or \"initial condition\" of the sequence $\\mathbf{x}$.\n",
    "\n",
    "```{admonition} References\n",
    ":class: seealso\n",
    "\n",
    "-   [Working with Sequences - Dive Into Deep Learning](https://d2l.ai/chapter_recurrent-neural-networks/sequence)\n",
    "-   [How do LLMs learn to be \"Generative\", as we often describe them?](https://github.com/huggingface/transformers/issues/28860)\n",
    "```\n",
    "\n",
    "### Markov Assumption\n",
    "\n",
    "Now suppose that we wish to employ the strategy mentioned above, where we\n",
    "condition only on the $\\tau$ previous time steps, i.e.,\n",
    "$x_{t-1}, \\ldots, x_{t-\\tau}$, rather than the entire sequence history\n",
    "$x_{t-1}, \\ldots, x_1$. Whenever we can throw away the history beyond the\n",
    "previous $\\tau$ steps without any loss in predictive power, we say that the\n",
    "sequence satisfies a Markov condition, i.e., that the future is conditionally\n",
    "independent of the past, given the recent history. When $\\tau=1$, we say that\n",
    "the data is characterized by a first-order Markov model, and when $\\tau=k$, we\n",
    "say that the data is characterized by a $k^{\\text {th }}$-order Markov model\n",
    "{cite}`zhang2023dive`.\n",
    "\n",
    "More formally, a discrete-time Markov chain is a sequence of\n",
    "[random variables](https://en.wikipedia.org/wiki/Random_variable)\n",
    "$X_1, X_2, X_3, \\ldots$ with the\n",
    "[Markov property](https://en.wikipedia.org/wiki/Markov_property), namely that\n",
    "the probability of moving to the next state depends only on the present state\n",
    "and not on the previous states:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\left(X_{t+1} \\mid X_{1}, X_{2}, \\ldots, X_{t}\\right) = \\mathbb{P}\\left(X_{t+1} \\mid X_{t-k+1}, X_{t-k+2}, \\ldots, X_{t}\\right)\n",
    "$$\n",
    "\n",
    "for all $t \\in \\mathbb{N}$ and all states\n",
    "$X_{t+1}, X_{t}, X_{1}, X_{2}, \\ldots$.\n",
    "\n",
    "The [Markov assumption](https://en.wikipedia.org/wiki/Markov_property) is more\n",
    "of an implicit assumption in the autoregressive self-supervised learning\n",
    "framework where we can draw parallels to. We often find it useful to work with\n",
    "models that proceed as though a Markov condition were satisfied, even when we\n",
    "know that this is only approximately true. With real text documents we continue\n",
    "to gain information as we include more and more leftwards context. But these\n",
    "gains diminish rapidly. Thus, sometimes we compromise, obviating computational\n",
    "and statistical difficulties by training models whose validity depends on a\n",
    "$k^{\\text {th }}$-order Markov condition. Even today's massive RNN- and\n",
    "Transformer based language models seldom incorporate more than thousands of\n",
    "words of context {cite}`zhang2023dive`. In short, the Markov assumption is a\n",
    "convenient assumption to simplify the modeling of the joint probability\n",
    "distribution of the token sequences.\n",
    "\n",
    "```{admonition} References\n",
    ":class: seealso\n",
    "\n",
    "-   [GPT-4 absolutely isn’t a Markov chain](https://news.ycombinator.com/item?id=35551452)\n",
    "-   [GPT is a Finite State Markov Chain - Andrej Karpathy](https://twitter.com/karpathy/status/1645115622517542913)\n",
    "-   [Working with Sequences - Dive Into Deep Learning](https://d2l.ai/chapter_recurrent-neural-networks/sequence.html)\n",
    "-   [Why GPT model is a higher order hidden markov model](https://cs.stackexchange.com/questions/160891/why-gpt-model-is-a-higher-order-hidden-markov-model)\n",
    "```\n",
    "\n",
    "### The Estimator Function is Smooth with Respect to the Parameters\n",
    "\n",
    "This assumption is a common one in the context of deep learning, because for\n",
    "when we say that the estimator function $f_{\\hat{\\boldsymbol{\\Theta}}}(\\cdot)$\n",
    "is _smooth_ with respect to the parameter space $\\hat{\\boldsymbol{\\Theta}}$, it\n",
    "means that the estimator function $f_{\\hat{\\boldsymbol{\\Theta}}}(\\cdot)$ is\n",
    "_smooth_ with respect to the parameter space $\\hat{\\boldsymbol{\\Theta}}$ if the\n",
    "function is continuous and differentiable with respect to the parameter space\n",
    "$\\hat{\\boldsymbol{\\Theta}}$ up to a certain order (usually the first for SGD\n",
    "variants and second order for Newton).\n",
    "\n",
    "What this implies is that the derivative of the function with respect to the\n",
    "parameter space $\\hat{\\boldsymbol{\\Theta}}$, denoted as\n",
    "$\\nabla_{\\hat{\\boldsymbol{\\Theta}}} f_{\\hat{\\boldsymbol{\\Theta}}}(\\cdot)$ is\n",
    "continuous. Loosely, you can think of that a small perturbation in the parameter\n",
    "space $\\hat{\\boldsymbol{\\Theta}}$ will result in a small change in the output of\n",
    "the function $f_{\\hat{\\boldsymbol{\\Theta}}}(\\cdot)$ - enabling gradient-based\n",
    "optimization algorithms to work effectively as if not, then taking a step in the\n",
    "direction of the gradient would not guarantee a decrease in the loss function,\n",
    "slowing down convergence.\n",
    "\n",
    "However, this is also not a strict assumption as in practice, piece-wise linear\n",
    "activation functions are not smooth because the derivative is not continuous at\n",
    "$0$, and consequently, $f_{\\hat{\\boldsymbol{\\Theta}}}(\\cdot)$ is\n",
    "[not smooth with respect to the parameter space](https://stats.stackexchange.com/questions/473643/why-are-neural-networks-smooth-functions)\n",
    "$\\hat{\\boldsymbol{\\Theta}}$.\n",
    "\n",
    "### Context Length and Token Context Window\n",
    "\n",
    "Given a coherent sequence of tokens $\\mathbf{x}$, say, _the tabby cat walks by\n",
    "the river bank_, we may not always pass the full sequence to the model. Based on\n",
    "a _context length_ $\\tau$, we can pass a _token context window_ of length $\\tau$\n",
    "to the model. For instance, if $\\tau=4$, then the token context window would be\n",
    "$\\left(x_{t-3}, x_{t-2}, x_{t-1}, x_{t}\\right)$, and the model would be trained\n",
    "to predict the next token $x_{t+1}$ given the token context window. In other\n",
    "words, the sentence above would be broken down into the following token context\n",
    "windows:\n",
    "\n",
    "-   _the tabby cat walks_\n",
    "-   _by the river bank_\n",
    "\n",
    "And the longer the context length, the model would be able to capture\n",
    "longer-range dependenciees in the sequence, but also may increase the\n",
    "computational complexity of the model {cite}`math11112451`.\n",
    "\n",
    "More formally, we can define the token context window as a function\n",
    "$C_{\\tau}(\\mathbf{x}, t)$ that maps a sequence $\\mathbf{x}$ and a position $t$\n",
    "to a token context window of length $\\tau$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "C_{\\tau} : \\mathcal{X} \\times \\mathbb{N} &\\rightarrow \\mathcal{X}^{\\tau} \\\\\n",
    "(\\mathbf{x}, t) &\\mapsto \\left(x_{t-\\tau+1}, x_{t-\\tau+2}, \\ldots, x_{t}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Conditional Entropy and Perplexity as Loss Function\n",
    "\n",
    "Having defined the basis of the autoregressive self-supervised learning\n",
    "framework, we can now define the loss function $\\mathcal{L}$ that is used to\n",
    "train the model $\\mathcal{G}$ to maximize the likelihood of the sequences in the\n",
    "corpus $\\mathcal{S}$. In order to transit towards the final objective/loss\n",
    "function, we would need to define the notion of\n",
    "[_**conditional entropy**_](https://en.wikipedia.org/wiki/Conditional_entropy).\n",
    "\n",
    "#### Conditional Entropy\n",
    "\n",
    "Define $X_t$ as a random variable representing the token at position $t$ in the\n",
    "sequence $\\mathbf{x}$ and $X_{<t} = \\left(X_1, X_2, \\ldots, X_{t-1}\\right)$ as\n",
    "random variables representing the tokens at positions $1, 2, \\ldots, t-1$ in the\n",
    "sequence $\\mathbf{x}$. Then the conditional\n",
    "[entropy](https://en.wikipedia.org/wiki/Shannon_Entropy) of the token $X_t$\n",
    "given a specific realization of $X_{<t}$ is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H\\left(X_t \\mid X_{<t} = x_{<t} \\right) &= -\\sum_{x_t \\in \\mathcal{V}} \\mathbb{P}\\left(x_t \\mid x_{<t} ; \\boldsymbol{\\Theta}\\right) \\log \\mathbb{P}\\left(x_t \\mid x_{<t} ; \\boldsymbol{\\Theta}\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This calculates the conditional entropy given a specific realization of the\n",
    "context $X_{<t} = x_{<t}$, where we see that summation sums over all\n",
    "possibilities of the token $x_t$ in the vocabulary $\\mathcal{V}$, considering\n",
    "the probability of the token $x_t$ given _a particular preceding_ sequence of\n",
    "tokens.\n",
    "\n",
    "To account for all possible realizations of the context $X_{<t}$, we simply sum\n",
    "over all possible realizations of the context $X_{<t}$, and we can write the\n",
    "conditional entropy as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H\\left(X_t \\mid X_{<t}\\right) = -\\sum_{x_{t} \\in \\mathcal{V}} \\sum_{x_{<t} \\in \\mathcal{V}^{<t}} \\mathbb{P}\\left(x_t, x_{<t} ; \\boldsymbol{\\Theta}\\right) \\log \\mathbb{P}\\left(x_t \\mid x_{<t} ; \\boldsymbol{\\Theta}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathbb{P}\\left(x_t, x_{<t} ; \\boldsymbol{\\Theta}\\right)$ is the joint\n",
    "probability distribution of observing the sequence $(x_{<t}, x_t)$,\n",
    "$\\mathbb{P}\\left(x_t \\mid x_{<t} ; \\boldsymbol{\\Theta}\\right)$ is the\n",
    "conditional probability distribution of observing the token $x_t$ given the\n",
    "context $x_{<t}$, and $\\mathcal{V}^{<t}$ is the set of all possible realizations\n",
    "of the context $X_{<t}$.\n",
    "\n",
    "It is worth noting that the conditional entropy $H\\left(X_t \\mid X_{<t}\\right)$\n",
    "is also the conditional expectation of the negative log-likelihood of the token\n",
    "$X_t$ given the context $X_{<t}$, and we can write it as:\n",
    "\n",
    "$$\n",
    "H\\left(X_t \\mid X_{<t}\\right) = -\\mathbb{E}_{\\mathcal{D}}\\left[\\log \\mathbb{P}\\left(X_t \\mid X_{<t} ; \\boldsymbol{\\Theta}\\right)\\right]\n",
    "$$\n",
    "\n",
    "```{admonition} References\n",
    ":class: seealso\n",
    "\n",
    "-   [Conditional Entropy - Wikipedia](https://en.wikipedia.org/wiki/Conditional_entropy)\n",
    "-   [Conditional Expectation - Wikipedia](https://en.wikipedia.org/wiki/Conditional_expectation)\n",
    "```\n",
    "\n",
    "#### Perplexity\n",
    "\n",
    "Language model has a standing history of using\n",
    "[**Perplexity**](https://en.wikipedia.org/wiki/Perplexity) as a measure of the\n",
    "quality of a language model. It is a measure of how well a probability\n",
    "distribution or probability model predicts a sample. Without going into the\n",
    "details, we define the perplexity of a probability distribution\n",
    "$\\mathbb{P}(x_t \\mid x_{<t} ; \\boldsymbol{\\Theta})$ as the exponential of the\n",
    "conditional entropy of the distribution, and we can write it as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Perplexity}\\left(\\mathbb{P}(x_t \\mid x_{<t} ; \\boldsymbol{\\Theta})\\right) &= \\exp\\left(H\\left(X_t \\mid X_{<t}\\right)\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "```{admonition} References\n",
    ":class: seealso\n",
    "\n",
    "-   [Perplexity - Wikipedia](https://en.wikipedia.org/wiki/Perplexity)\n",
    "-   [Perplexity of fixed-length models](https://huggingface.co/docs/transformers/en/perplexity)\n",
    "```\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "Given the definitions of the conditional entropy and perplexity, we can\n",
    "formalize the loss function $\\mathcal{L}$ as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}\\left(\\mathcal{D} ; \\boldsymbol{\\Theta}\\right) &= -\\sum_{\\mathbf{x} \\in \\mathcal{D}} \\sum_{t=1}^T \\log \\mathbb{P}\\left(x_t \\mid C_{\\tau}(\\mathbf{x}, t) ; \\boldsymbol{\\Theta}\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and the objective function is to minimize the loss function $\\mathcal{L}$,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{\\theta}^{*} &= \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\text{argmin}} \\mathcal{L}\\left(\\mathcal{D} ; \\boldsymbol{\\Theta}\\right) \\\\\n",
    "                        &= \\underset{\\boldsymbol{\\theta} \\in \\boldsymbol{\\Theta}}{\\text{argmin}} -\\sum_{\\mathbf{x} \\in \\mathcal{D}} \\sum_{t=1}^T \\log \\mathbb{P}\\left(x_t \\mid C_{\\tau}(\\mathbf{x}, t) ; \\boldsymbol{\\Theta}\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "However, we do not know the true distribution $\\mathcal{D}$, and so we can only\n",
    "estimate the loss function $\\mathcal{L}$ from the corpus $\\mathcal{S}$, and we\n",
    "can write the process of estimating via the negative log-likelihood as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\mathcal{L}}\\left(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}}\\right) &= -\\sum_{\\mathbf{x} \\in \\mathcal{S}} \\sum_{t=1}^T \\log \\mathbb{P}\\left(x_t \\mid C_{\\tau}(\\mathbf{x}, t) ; \\hat{\\boldsymbol{\\Theta}}\\right) \\\\\n",
    "    &= -\\sum_{n=1}^N \\sum_{t=1}^{T_n} \\log \\mathbb{P}\\left(x_{n, t} \\mid C_{\\tau}(\\mathbf{x}_{n}, t) ; \\hat{\\boldsymbol{\\Theta}}\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and consequently, the objective function is to minimize the estimated loss\n",
    "function\n",
    "$\\hat{\\mathcal{L}}\\left(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}}\\right)$,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\boldsymbol{\\theta}}^{*} &= \\underset{\\hat{\\boldsymbol{\\theta}} \\in \\boldsymbol{\\Theta}}{\\text{argmin}} \\hat{\\mathcal{L}}\\left(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}}\\right) \\\\\n",
    "                              &= \\underset{\\hat{\\boldsymbol{\\theta}} \\in \\boldsymbol{\\Theta}}{\\text{argmin}} -\\sum_{n=1}^N \\sum_{t=1}^{T_n} \\log \\mathbb{P}\\left(x_{n, t} \\mid C_{\\tau}(\\mathbf{x}_{n}, t) ; \\hat{\\boldsymbol{\\Theta}}\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "#### Convergence\n",
    "\n",
    "It can be shown that the given the Markov assumption and a token context window\n",
    "size of $\\tau$, the loss function $\\mathcal{L}$ is a\n",
    "[consistent estimator](https://en.wikipedia.org/wiki/Consistent_estimator) of\n",
    "the true distribution $\\mathcal{D}$, and the the objective\n",
    "$\\hat{\\mathcal{L}}\\left(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}}\\right)$\n",
    "converges to the true conditional probability distribution\n",
    "$\\mathbb{P}(x_t \\mid x_{<t} ; \\boldsymbol{\\Theta})$ over $\\mathcal{D}$ as the\n",
    "size of the corpus $\\mathcal{S}$ goes to infinity, if the model has sufficient\n",
    "capacity and the optimization algorithm is appropriate {cite}`math11112451`.\n",
    "\n",
    "Furthermore, the proposition that the conditional entropy\n",
    "$H\\left(X_t \\mid X_{<t}\\right)$ of the true data-generating process is upper\n",
    "bounded by the logarithm of the size of the vocabulary $\\mathcal{V}$, i.e.,\n",
    "$H\\left(X_t \\mid X_{<t}\\right) \\leq \\log |\\mathcal{V}|$ {cite}`math11112451`.\n",
    "\n",
    "The proposition that the conditional entropy has an upper limit, carries\n",
    "significant implications for optimizing autoregressive self-supervised learning\n",
    "models. Specifically, because the conditional entropy cannot exceed the\n",
    "logarithm of the vocabulary size $\\mathcal{V}$, we infer a similar upper limit\n",
    "on perplexity. This cap on perplexity offers a valuable benchmark for evaluating\n",
    "and comparing different models, establishing a theoretical maximum for model\n",
    "performance based on the size of the vocabulary {cite}`math11112451`.\n",
    "\n",
    "### GPT is a Autoregressive Self-Supervised Learning Model\n",
    "\n",
    "Finally, we can piece together the autoregressive self-supervised learning\n",
    "framework to define the GPT model $\\mathcal{G}$ as a model that is trained to\n",
    "maximize the likelihood of the sequences in the corpus $\\mathcal{S}$ via the\n",
    "objective function\n",
    "$\\hat{\\mathcal{L}}\\left(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}}\\right)$ where\n",
    "$\\hat{\\boldsymbol{\\Theta}}$ is the estimated parameter space that approximates\n",
    "the true parameter space $\\boldsymbol{\\Theta}$, and $\\mathcal{S}$ is the corpus\n",
    "of sequences that are sampled $\\text{i.i.d.}$ from the distribution\n",
    "$\\mathcal{D}$.\n",
    "\n",
    "In pseudo-code, the GPT model $\\mathcal{G}$ consists of decoder blocks, each\n",
    "block consisting of a multi-head self-attention mechanism and a position-wise\n",
    "feed-forward neural network, with a head layer to produce a probability\n",
    "distribution over the vocabulary $\\mathcal{V}$ of tokens.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_0 &= \\mathcal{S} \\cdot \\mathbf{W}_{e}+ \\mathbf{W}_{p} \\\\\n",
    "h_{\\ell} &= \\text{DecoderBlock}(h_{\\ell-1}) \\quad \\text{for} \\quad \\ell = 1, 2, \\ldots, L \\\\\n",
    "\\mathbb{P}(x_t \\mid C_{\\tau}(\\mathbf{x}, t) ; \\boldsymbol{\\Theta}) &= \\text{softmax}(h_{L} \\cdot \\mathbf{W}_{e}^{\\top})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "-   $\\mathbf{W}_{e}$ is the embedding matrix that maps the token to a vector\n",
    "    representation in a continuous vector space,\n",
    "-   $\\mathbf{W}_{p}$ is the positional encoding matrix that encodes the position\n",
    "    of the token in the sequence,\n",
    "-   $\\text{DecoderBlock}$ is a function that applies a multi-head self-attention\n",
    "    mechanism and a position-wise feed-forward neural network to the input\n",
    "    sequence,\n",
    "-   $\\mathbf{W}_{e}^{\\top}$ is the transpose of the embedding matrix that maps\n",
    "    the vector representation of the token back to the vocabulary space.\n",
    "\n",
    "Note that it is only a pseudo-code because notations like $\\mathbf{W}_{e}$ are\n",
    "used to denote both the token embedding matrix in $h_0$ and the transformed\n",
    "contextual embedding matrix in the head/linear/last layer. The actual\n",
    "implementation of the GPT model is more complex, and we will take a look at it\n",
    "in later sections.\n",
    "\n",
    "### Conditional on Task\n",
    "\n",
    "In the GPT-2 paper, _Language Models are Unsupervised Multitask Learners_, the\n",
    "authors introduced the concept of _conditional on task_ where the GPT model\n",
    "$\\mathcal{G}$ theoretically should not only learn the conditional probability\n",
    "distribution $\\mathbb{P}(x_t \\mid x_{<t} ; \\boldsymbol{\\Theta})$ but also learn\n",
    "the conditional probability distribution\n",
    "$\\mathbb{P}(x_t \\mid x_{<t} ; \\boldsymbol{\\Theta}, \\mathcal{T})$ where\n",
    "$\\mathcal{T}$ is the task that the model should implicitly learn\n",
    "{cite}`radford2019language`. This is a powerful concept because if such a\n",
    "hypothesis is correct, then the GPT model $\\mathcal{G}$ can indeed be a\n",
    "multi-task learner, and can be used directly on a wide range of NLU tasks\n",
    "without the need for supervised fine-tuning for downstream domain-specific\n",
    "tasks.\n",
    "\n",
    "In practice, the authors mentioned that task conditioning is often implemented\n",
    "at an architectural level, via task specific encoder and decoder in the paper\n",
    "[_One Model To Learn Them All_](https://arxiv.org/abs/1706.05137)\n",
    "{cite}`kaiser2017model`, for instance, or at an algorithmic level, such as the\n",
    "inner and outer loop optimization framework, as seen in the paper\n",
    "[_Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks_](https://arxiv.org/abs/1703.03400)\n",
    "{cite}`finn2017modelagnostic`.\n",
    "\n",
    "However, the authors further mentioned that without task-specific architectural\n",
    "changes, one can leverage the sequential nature of the natural language space\n",
    "where we can construct a tasks, inputs and outputs all as a sequence of symbols\n",
    "{cite}`radford2019language`. For example, a translation task can be formulated\n",
    "as a sequence of symbols via\n",
    "`(translate to french, english sequence, french sequence)`, where the model can\n",
    "now learn to also condition on the task `(translate to french)` in addition to\n",
    "the sequence of tokens. The paper _The Natural Language Decathlon: Multitask\n",
    "Learning as Question Answering_ exemplifies this concept with their model\n",
    "**Multitask Question Answering Network (MQAN)**, where a single model is trained\n",
    "to perform many diverse natural language processing tasks simultaneously.\n",
    "\n",
    "### Supervised Fine-Tuning\n",
    "\n",
    "Though GPT-2 has demonstrated that it can be used directly on a wide range of\n",
    "NLU without the need for supervised fine-tuning, it is worth taking a detour\n",
    "back to how GPT-1 was fine-tuned immediately after the pretraining phase.\n",
    "\n",
    "In the paper _Improving Language Understanding by Generative Pre-Training_,\n",
    "after the pretrained (foundational) model was trained with the objective\n",
    "function\n",
    "$\\hat{\\mathcal{L}}\\left(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}}\\right)$, we\n",
    "would then fine-tune the model on a specific task by replacing the final layer\n",
    "of the model with a task-specific layer, and then train the model on the\n",
    "specific task with the task-specific layer {cite}`radford2018improving`.\n",
    "\n",
    "#### Objective Function for Fine-Tuning\n",
    "\n",
    "More concretely, now our dataset $\\mathcal{S}$ is a dataset of labeled examples\n",
    "$\\mathcal{S} = \\left\\{\\left(\\mathbf{x}_n, y_n\\right)\\right\\}_{n=1}^N$, where it\n",
    "may be sampled together from a new underlying distribution $\\mathcal{D}$,\n",
    "usually a cartesian product $\\mathcal{X} \\times \\mathcal{Y}$ where $\\mathcal{Y}$\n",
    "is the label space. Each input sequence $\\mathbf{x}_n$ is a sequence of tokens,\n",
    "and each output label $y_n$ is a label from the set of labels $\\mathcal{Y}$.\n",
    "\n",
    "A task specific layer is often used to replace the original head layer, for\n",
    "instance, if we are training the model on a text classification task with\n",
    "$\\mathcal{C}$ number of classes, then the task specific layer would be a linear\n",
    "layer with $\\mathcal{C}$ number of output units. Of course, the output of this\n",
    "layer, being the logits, will usually pass into appropriate loss functions such\n",
    "as the cross-entropy loss with a softmax layer on top of the logits to induce a\n",
    "_not so well-calibrated_ probability distribution over the classes\n",
    "$\\mathcal{C}$.\n",
    "\n",
    "If we denote the loss function (or the negative log-likelihood) of the\n",
    "pre-training phase as\n",
    "$\\hat{\\mathcal{L}}_{1}\\left(\\mathcal{S}_{1} ; \\hat{\\boldsymbol{\\Theta}}_{1}\\right)$,\n",
    "then the objective in this second phase is simply to maximize the likelihood of\n",
    "the labeled examples $\\mathcal{S}$ via the objective function\n",
    "$\\hat{\\mathcal{L}}_{2}\\left(\\mathcal{S}_{2} ; \\hat{\\boldsymbol{\\Theta}}_{2}\\right)$\n",
    "where $\\hat{\\boldsymbol{\\Theta}}_{1}$ is the estimated parameter space for the\n",
    "pre-training phase, and $\\hat{\\boldsymbol{\\Theta}}_{2}$ is the estimated\n",
    "parameter space for the fine-tuning phase. Note that the\n",
    "$\\hat{\\boldsymbol{\\Theta}}_{2}$ is initialized with partial weights from the\n",
    "pre-trained model $\\mathcal{G}$, so it naturally should overlap with the\n",
    "$\\hat{\\boldsymbol{\\Theta}}_{1}$ up to the number of _frozen_ layers.\n",
    "\n",
    "We denote the maximization as a minimization of the negative log-likelihood:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\boldsymbol{\\theta}}_{2}^{*} &= \\underset{\\hat{\\boldsymbol{\\theta}}_{2} \\in \\boldsymbol{\\Theta}_{2}}{\\text{argmin}} \\hat{\\mathcal{L}}_{2}\\left(\\mathcal{S}_{2} ; \\hat{\\boldsymbol{\\Theta}}_{2}\\right) \\\\\n",
    "                                    &= \\underset{\\hat{\\boldsymbol{\\theta}}_{2} \\in \\boldsymbol{\\Theta}_{2}}{\\text{argmin}} -\\sum_{n=1}^N \\log \\mathbb{P}\\left(y_n \\mid \\mathbf{x}_n ; \\hat{\\boldsymbol{\\Theta}}_{2}\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "It is also customary to find the expected loss over the dataset $\\mathcal{S}$,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\boldsymbol{\\theta}}_{2}^{*} &= \\underset{\\hat{\\boldsymbol{\\theta}}_{2} \\in \\boldsymbol{\\Theta}_{2}}{\\text{argmin}} \\mathbb{E}_{\\mathcal{S}}\\left[\\hat{\\mathcal{L}}_{2}\\left(\\mathcal{S}_{2} ; \\hat{\\boldsymbol{\\Theta}}_{2}\\right)\\right] \\\\\n",
    "                                    &= \\underset{\\hat{\\boldsymbol{\\theta}}_{2} \\in \\boldsymbol{\\Theta}_{2}}{\\text{argmin}} -\\mathbb{E}_{\\mathcal{S}}\\left[\\sum_{n=1}^N \\log \\mathbb{P}\\left(y_n \\mid \\mathbf{x}_n ; \\hat{\\boldsymbol{\\Theta}}_{2}\\right)\\right] \\\\\n",
    "                                    &= -\\frac{1}{N} \\sum_{n=1}^N \\log \\mathbb{P}\\left(y_n \\mid \\mathbf{x}_n ; \\hat{\\boldsymbol{\\Theta}}_{2}\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $N$ is the number of samples in the dataset $\\mathcal{S}$.\n",
    "\n",
    "#### Auxiliary Loss Function\n",
    "\n",
    "In the context of fine-tuning GPT-1 or similar models for specific tasks, the\n",
    "term \"auxiliary (supplementary) loss\" refers to additional objectives or loss\n",
    "functions that are incorporated into the fine-tuning process alongside the\n",
    "primary loss function. This approach is based on the idea that including\n",
    "auxiliary tasks or losses can help improve the model's performance on the main\n",
    "task by leveraging the knowledge gained during pre-training. The author also\n",
    "mentioned that this method (a) improving generalization of the supervised model,\n",
    "and (b) accelerating convergence {cite}`radford2018improving`.\n",
    "\n",
    "During pre-training, models like GPT-1 learn to predict the next token in a\n",
    "sequence, which is a form of auxiliary task. When fine-tuning these models on\n",
    "downstream tasks, the authors of the GPT-1 paper found it beneficial to include\n",
    "the pre-training loss (the auxiliary loss) in the fine-tuning loss function.\n",
    "This is done by calculating the primary loss for the specific task (e.g.,\n",
    "classification, named-entity recognition) and then combining it with the\n",
    "auxiliary loss, often with a weighting factor to balance their contributions.\n",
    "The weighting factor, denoted as $\\alpha$ in the fine-tuning loss function,\n",
    "allows for adjusting the relative importance of the primary and auxiliary losses\n",
    "during the fine-tuning process.\n",
    "\n",
    "To this end, the final loss function for fine-tuning the GPT-1 model on a\n",
    "specific task is a combination of the primary loss and the auxiliary loss, and\n",
    "we can write it as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\mathcal{L}}_{3}\\left(\\mathcal{S}_{2} ; \\hat{\\boldsymbol{\\Theta}}_{3}\\right) &= \\alpha \\hat{\\mathcal{L}}_{2}\\left(\\mathcal{S}_{2} ; \\hat{\\boldsymbol{\\Theta}}_{2}\\right) + (1 - \\alpha) \\hat{\\mathcal{L}}_{1}\\left(\\mathcal{S}_{1} ; \\hat{\\boldsymbol{\\Theta}}_{1}\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and we can minimize the new auxiliary loss function in the same way.\n",
    "\n",
    "### Optimizing Unsupervised is the same as Optimizing Supervised\n",
    "\n",
    "The GPT-2 paper _Language Models are Unsupervised Multitask Learners_\n",
    "demonstrated that they want to do away with the supervised fine-tuning phase via\n",
    "an interesting hypothesis, that **optimizing the unsupervised objective is the\n",
    "same as optimizing the supervised objective** because the _global minimum_ of\n",
    "the unsupervised objective is the same as the _global minimum_ of the supervised\n",
    "objective {cite}`radford2019language`.\n",
    "\n",
    "Indeed, the unsupervised objective in language modeling is to maximize the\n",
    "likelihood of observing the entire sequence of tokens over the dataset\n",
    "$\\mathcal{S}$. This is an unsupervised task because it does not rely on labeled\n",
    "input-output pairs but rather on the sequence itself. For simplicity, we state\n",
    "the unsupervised objective as simply the argmax of the log-likelihood of the\n",
    "sequence of tokens over the dataset $\\mathcal{S}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\boldsymbol{\\theta}}^{*}_{\\text{unsupervised}} &= \\underset{\\hat{\\boldsymbol{\\theta}}_{\\text{unsupervised}} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\log\\left(\\hat{\\mathcal{L}}\\left(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}}\\right)\\right) \\\\\n",
    "&= \\underset{\\hat{\\boldsymbol{\\theta}}_{\\text{unsupervised}} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\sum_{n=1}^N \\sum_{t=1}^{T_n} \\log \\mathbb{P}(x_{n, t} \\mid x_{n, 1}, x_{n, 2}, \\ldots, x_{n, t-1} ; \\hat{\\boldsymbol{\\Theta}}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In a supervised setting, such as sequence-to-sequence tasks (e.g., translation,\n",
    "summarization), the objective is often to predict a target sequence\n",
    "$\\mathbf{y} = (y_1, y_2, \\ldots, y_{T^{\\prime}})$ given an input sequence\n",
    "$\\mathbf{x} = (x_1, x_2, \\ldots, x_T)$, and we can write the objective as the\n",
    "argmax of the log-likelihood of the target sequence over the dataset\n",
    "$\\mathcal{S}$. And if we define the sequence $\\mathbf{x}$ in the unsupervised\n",
    "objective as a union of the input sequence $\\mathbf{x}$ and the target sequence\n",
    "$\\mathbf{y}$, then the supervised objective is the same as the unsupervised\n",
    "objective:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\boldsymbol{\\theta}}^{*}_{\\text{supervised}} &= \\underset{\\hat{\\boldsymbol{\\theta}}_{\\text{supervised}} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\log\\left(\\hat{\\mathcal{L}}\\left(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}}\\right)\\right) \\\\\n",
    "&= \\underset{\\hat{\\boldsymbol{\\theta}}_{\\text{supervised}} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\sum_{n=1}^N \\sum_{t=1}^{T + T^{\\prime}} \\log \\mathbb{P}(x_{n, t} \\mid x_{n, 1}, x_{n, 2}, \\ldots, x_{n, t-1} ; \\hat{\\boldsymbol{\\Theta}}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "$\\mathbf{x} = (x_1, x_2, \\ldots, x_T) \\cup (y_1, y_2, \\ldots, y_{T^{\\prime}})$.\n",
    "\n",
    "The key insight here is that if we can construct the input sequence $\\mathbf{x}$\n",
    "such that the task-specific labels, are somehow encoded into the input sequence\n",
    "as well, then the supervised task is indeed a subset of the unsupervised task.\n",
    "For example, in the case of a translation task, the input sequence $\\mathbf{x}$\n",
    "can be something like\n",
    "`The translation of the french sentence 'As-tu aller au cine ́ma?' to english is`,\n",
    "and the target sequence $\\mathbf{y}$ can be the english translation\n",
    "`Did you go to the movies?`.\n",
    "\n",
    "However, the authors mention that such learning is much slower than the case\n",
    "where the model is directly trained on the supervised task\n",
    "{cite}`radford2019language`.\n",
    "\n",
    "In what follows, the author added that the internet contains a vast amount of\n",
    "information that is passively available without the need for interactive\n",
    "communication. The example that I provided on the french-to-english translation\n",
    "would bound to exist naturally in the internet. They speculate that if the\n",
    "language model is _large_ enough in terms of capacity, then it should be able to\n",
    "learn to perform the tasks demonstrated in natural language sequences in order\n",
    "to better predict them, regardless of their method of procurement\n",
    "{cite}`radford2019language`.\n",
    "\n",
    "## References and Further Readings\n",
    "\n",
    "```{admonition} References\n",
    ":class: seealso\n",
    "\n",
    "-   [On the importance of the i.i.d. assumption in statistical learning](https://stats.stackexchange.com/questions/213464/on-the-importance-of-the-i-i-d-assumption-in-statistical-learning)\n",
    "-   [Independent and identically distributed random variables - Wikipedia](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)\n",
    "-   [Independence and Identically Distributed (IID) - GAO Hongnan](https://gao-hongnan.github.io/gaohn-galaxy/probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html#independence-and-identically-distributed-iid)\n",
    "-   [Zero-shot learning - Wikipedia](https://en.wikipedia.org/wiki/Zero-shot_learning)\n",
    "-   [What is the difference between one-shot learning, transfer learning, and fine-tuning? - AI Stack Exchange](https://ai.stackexchange.com/questions/21719/what-is-the-difference-between-one-shot-learning-transfer-learning-and-fine-tun)\n",
    "-   [Zero-Shot Learning in Modern NLP - Joe Davison](https://joeddav.github.io/blog/2020/05/29/ZSL.html)\n",
    "-   [Zero-Shot Learning Through Cross-Modal Transfer - arXiv](https://arxiv.org/abs/1301.3666)\n",
    "-   [Zero shot learning available labels in testing set - AI Stack Exchange](https://ai.stackexchange.com/questions/23527/zero-shot-learning-available-labels-in-testing-set)\n",
    "-   [Zero-Shot Learning: Can You Classify an Object Without Seeing It Before?](https://www.theaidream.com/post/zero-shot-learning-can-you-classify-an-object-without-seeing-it-before)\n",
    "-   [A Survey of Zero-Shot Learning: Settings, Methods, and Applications](https://dl.acm.org/doi/10.1145/3293318)\n",
    "-   [Changes in GPT-2/GPT-3 Model During Few-Shot Learning - Stack Overflow](https://stackoverflow.com/questions/66451430/changes-in-gpt2-gpt3-model-during-few-shot-learning)\n",
    "-   [https://github.com/karpathy/minbpe](https://github.com/karpathy/minbpe)\n",
    "-   [Byte Pair Encoding on Hugging Face's NLP Course](https://huggingface.co/learn/nlp-course/en/chapter6/5)\n",
    "-   [Perplexity - Wikipedia](https://en.wikipedia.org/wiki/Perplexity)\n",
    "-   [Perplexity of fixed-length models](https://huggingface.co/docs/transformers/en/perplexity)\n",
    "-   [9.3.2. Perplexity - Dive Into Deep Learning](https://d2l.ai/chapter_recurrent-neural-networks/language-model.html#perplexity)\n",
    "-   [3.3 Evaluating Language Models: Perplexity - Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n",
    "-   [Transformers from Scratch - Brandon Rohrer](https://e2eml.school/transformers.html)\n",
    "-   [Primers • Transformers - Aman Chadha](https://aman.ai/primers/ai/transformers/#positional-encoding)\n",
    "-   [The Illustrated GPT-2 (Visualizing Transformer Language Models) - Jay Alammar](https://jalammar.github.io/illustrated-gpt2/)\n",
    "-   [The Transformer Family Version 2.0 - Lilian Weng](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2)\n",
    "-   [The Random Transformer - hackerllama](https://osanseviero.github.io/hackerllama/blog/posts/random_transformer)\n",
    "-   [Transformers From Scratch - Mat Miller](https://blog.matdmiller.com/posts/2023-06-10_transformers/notebook.html)\n",
    "-   [Andrej Karpathy GPT Implementation Google Colab](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=JB82yzt44REI)\n",
    "-   [Transformers: a Primer - Justin Seonyong Lee](http://www.columbia.edu/~jsl2239/transformers.html)\n",
    "-   [OpenAI GPT Models - Lei Mao](https://leimao.github.io/article/OpenAI-GPT-Models/)\n",
    "-   [Transformer Explained in One Single Page - Lei Mao](https://leimao.github.io/blog/Transformer-Explained/)\n",
    "-   [Autoregressive models](https://deepgenerativemodels.github.io/notes/autoregressive/)\n",
    "-   [The Annotated Transformer - Harvard NLP](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "-   [Tutorial 6: Transformers and Multi-Head Attention - UvA](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)\n",
    "-   [Some Intuition on Attention and the Transformer - Eugene Yan](https://eugeneyan.com/writing/attention/)\n",
    "-   [Let's build GPT: from scratch, in code, spelled out - Andrej Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
    "-   [Why does the transformer do better than RNN and LSTM in long-range context dependencies?](https://ai.stackexchange.com/questions/20075/why-does-the-transformer-do-better-than-rnn-and-lstm-in-long-range-context-depen)\n",
    "-   [How Transformer is Bidirectional - Machine Learning](https://stackoverflow.com/questions/55158554/how-transformer-is-bidirectional-machine-learning)\n",
    "-   [Neural Machine Translation by Jointly Learning to Align and Translate - arXiv](https://arxiv.org/abs/1409.0473)\n",
    "-   [Building Language Models from Scratch - Sebastian Raschka](https://github.com/rasbt/LLMs-from-scratch)\n",
    "-   [Numerical Stability and Initialization - Dive into Deep Learning](https://d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#breaking-the-symmetry)\n",
    "-   [Background: What is a Generative Model? - Google Developers](https://developers.google.com/machine-learning/gan/generative) -\n",
    "    Google Developers\n",
    "-   [Why can we approximate the joint probability distribution using the output vector of the GPT model?](https://ai.stackexchange.com/questions/12579/why-can-we-approximate-the-joint-probability-distribution-using-the-output-vecto)\n",
    "-   [Why Joint Probability in Generative Models? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/65806/why-joint-probability-in-generative-models)\n",
    "-   [CSC412 Winter 2020: Probabilsitic Machine Learning - University of Toronto](https://probmlcourse.github.io/csc412/lectures/week_2/)\n",
    "-   [CS324 - Large Language Models - Stanford University](https://stanford-cs324.github.io/winter2022/lectures/introduction/)\n",
    "-   [Joint Probability Mass Function (PMF)](https://www.probabilitycourse.com/chapter5/5_1_1_joint_pmf.php)\n",
    "-   [Difference Between Joint Probability Distribution and Conditional Probability Distribution - Math Stack Exchange](https://math.stackexchange.com/questions/1566215/difference-between-joint-probability-distribution-and-conditional-probability-di)\n",
    "-   [Residual Networks (ResNets) - Dive into Deep Learning](https://d2l.ai/chapter_convolutional-modern/resnet.html)\n",
    "-   [GPT-1, GPT-2, GPT-3, InstructGPT, ChatGPT, and GPT-4 Summary](https://songhuiming.github.io/pages/2023/05/28/gpt-1-gpt-2-gpt-3-instructgpt-chatgpt-and-gpt-4-summary/)\n",
    "-   [How to Optimize Data Transfers in CUDA C/C++ - NVIDIA Developer Blogs](https://devblogs.nvidia.com/how-optimize-data-transfers-cuda-cc/)\n",
    "-   [How to Overlap Data Transfers in CUDA C/C++ - NVIDIA Developer Blogs](https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/)\n",
    "```\n",
    "\n",
    "## Citations\n",
    "\n",
    "-   [1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n",
    "    Ł. Kaiser, and I. Polosukhin.\n",
    "    [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762). In Advances\n",
    "    in Neural Information Processing Systems, pp. 5998–6008, 2017.\n",
    "-   [2] I. Loshchilov and F. Hutter,\n",
    "    [\"Decoupled weight decay regularization\"](https://arxiv.org/abs/1711.05101),\n",
    "    arXiv preprint arXiv:1711.05101, [Submitted on 14 Nov 2017 (v1), last\n",
    "    revised 4 Jan 2019 (this version, v3)].\n",
    "-   [3] D. P. Kingma and J. Ba,\n",
    "    [\"Adam: A Method for Stochastic Optimization\"](https://arxiv.org/abs/1412.6980),\n",
    "    arXiv preprint arXiv:1412.6980, [Submitted on 22 Dec 2014 (v1), last revised\n",
    "    30 Jan 2017 (this version, v9)].\n",
    "-   [4] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han,\n",
    "    [\"On the Variance of the Adaptive Learning Rate and Beyond\"](https://arxiv.org/abs/1908.03265),\n",
    "    arXiv preprint arXiv:1908.03265, [Submitted on 8 Aug 2019 (v1), last revised\n",
    "    26 Oct 2021 (this version, v4)].\n",
    "-   [5] A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola,\n",
    "    [\"Chapter 9. Recurrent Neural Networks\"](https://d2l.ai/chapter_recurrent-neural-networks/index.html)\n",
    "    in Dive into Deep Learning, Cambridge University Press, 2023.\n",
    "-   [6] A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola,\n",
    "    [\"Chapter 11. Attention Mechanisms and Transformers\"](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)\n",
    "    in Dive into Deep Learning, Cambridge University Press, 2023.\n",
    "-   [7] D. Jurafsky and J. H. Martin,\n",
    "    [\"Chapter 3. N-gram Language Models\"](https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n",
    "    in Speech and Language Processing, 3rd ed., Pearson, 2023. pp. 32-59.\n",
    "-   [8] D. Jurafsky and J. H. Martin,\n",
    "    [\"Chapter 10. Transformers and Large Language Models\"](https://web.stanford.edu/~jurafsky/slp3/10.pdf)\n",
    "    in Speech and Language Processing, 3rd ed., Pearson, 2023. pp. 213-241.\n",
    "-   [9] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever,\n",
    "    [\"Improving Language Understanding by Generative Pre-Training\"](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).\n",
    "-   [10] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n",
    "    [\"Language Models are Unsupervised Multitask Learners\"](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).\n",
    "\n",
    "[^1]:\n",
    "    This part is not concrete as the formalization is not rigorous in the\n",
    "    statistical learning framework, but the general idea is there.\n",
    "\n",
    "[^2]:\n",
    "    [Working with Sequences - Dive Into Deep Learning](https://d2l.ai/chapter_recurrent-neural-networks/sequence.html)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "mystnb": {
   "number_source_lines": true
  },
  "source_map": [
   16,
   29,
   104
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}