# Empirical Risk Minimization

[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)
[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)
[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)
![Tag](https://img.shields.io/badge/Tag-Organized_Chaos-orange)

```{contents}
:local:
```

```{tableofcontents}

```

This chapter talks about the
[Empirical Risk Minimization](https://en.wikipedia.org/wiki/Empirical_risk_minimization)
(ERM) principle. This statistical learning theory defines a family of algorithms
that give theoretical bounds on their performance. The core idea is that we
cannot know exactly how well an algorithm will work in practice (the true
"risk") because we don't know the true distribution of data that the algorithm
will work on, but we can instead measure its performance on a known set of
training data (the "empirical" risk).

## Further Readings

-   Jung, Alexander. "Chapter 4. Empirical Risk Minimization." In Machine
    Learning: The Basics. Springer Nature Singapore, 2023.
-   Deisenroth, Marc Peter, Cheng Soon Ong, and Aldo A. Faisal. "Chapter 8.2.
    Empirical Risk Minimization." In Mathematics for Machine Learning.
    Cambridge: Cambridge University Press, 2021.
-   [Wikipedia: Empirical Risk Minimization](https://en.wikipedia.org/wiki/Empirical_risk_minimization)
-   [Wikipedia: Loss Function](https://en.wikipedia.org/wiki/Loss_function)
