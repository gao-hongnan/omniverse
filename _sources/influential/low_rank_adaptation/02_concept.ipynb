{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6ffb88",
   "metadata": {},
   "source": [
    "# Concept\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Organized_Chaos-orange)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "905a212e",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Iterable, Optional, Tuple, TypeVar\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from rich.pretty import pprint\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ab7ca3",
   "metadata": {},
   "source": [
    "## Reproducibility\n",
    "\n",
    "We first set the seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb777b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn\n",
    "\n",
    "__all__ = [\n",
    "    \"seed_all\",\n",
    "    \"seed_worker\",\n",
    "    \"configure_deterministic_mode\",\n",
    "    \"raise_error_if_seed_is_negative_or_outside_32_bit_unsigned_integer\",\n",
    "]\n",
    "\n",
    "max_seed_value = np.iinfo(np.uint32).max\n",
    "min_seed_value = np.iinfo(np.uint32).min\n",
    "\n",
    "\n",
    "def raise_error_if_seed_is_negative_or_outside_32_bit_unsigned_integer(value: int) -> None:\n",
    "    if not (min_seed_value <= value <= max_seed_value):\n",
    "        raise ValueError(f\"Seed must be within the range [{min_seed_value}, {max_seed_value}]\")\n",
    "\n",
    "\n",
    "def configure_deterministic_mode() -> None:\n",
    "    r\"\"\"\n",
    "    Activates deterministic mode in PyTorch and CUDA to ensure reproducible\n",
    "    results at the cost of performance and potentially higher CUDA memory usage.\n",
    "    It sets deterministic algorithms, disables cudnn benchmarking and enables,\n",
    "    and sets the CUBLAS workspace configuration.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    - `PyTorch Reproducibility <https://pytorch.org/docs/stable/notes/randomness.html>`_\n",
    "    - `PyTorch deterministic algorithms <https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html>`_\n",
    "    - `CUBLAS reproducibility <https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility>`_\n",
    "    \"\"\"\n",
    "\n",
    "    # fmt: off\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    torch.backends.cudnn.benchmark        = False\n",
    "    torch.backends.cudnn.deterministic    = True\n",
    "    torch.backends.cudnn.enabled          = False\n",
    "\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    # fmt: on\n",
    "    warnings.warn(\n",
    "        \"Deterministic mode is activated. This will negatively impact performance and may cause increase in CUDA memory footprint.\",\n",
    "        category=UserWarning,\n",
    "        stacklevel=2,\n",
    "    )\n",
    "\n",
    "\n",
    "def seed_all(\n",
    "    seed: int = 1992,\n",
    "    seed_torch: bool = True,\n",
    "    set_torch_deterministic: bool = True,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Seeds all relevant random number generators to ensure reproducible\n",
    "    outcomes. Optionally seeds PyTorch and activates deterministic\n",
    "    behavior in PyTorch based on the flags provided.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int, default=1992\n",
    "        The seed number for reproducibility.\n",
    "    seed_torch : bool, default=True\n",
    "        If True, seeds PyTorch's RNGs.\n",
    "    set_torch_deterministic : bool, default=True\n",
    "        If True, activates deterministic mode in PyTorch.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    seed : int\n",
    "        The seed number used for reproducibility.\n",
    "    \"\"\"\n",
    "    raise_error_if_seed_is_negative_or_outside_32_bit_unsigned_integer(seed)\n",
    "\n",
    "    # fmt: off\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)       # set PYTHONHASHSEED env var at fixed value\n",
    "    np.random.seed(seed)                           # numpy pseudo-random generator\n",
    "    random.seed(seed)                              # python's built-in pseudo-random generator\n",
    "\n",
    "    if seed_torch:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)           # pytorch (both CPU and CUDA)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.enabled = False\n",
    "\n",
    "        if set_torch_deterministic:\n",
    "            configure_deterministic_mode()\n",
    "    # fmt: on\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8757dd",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Consider that you have access to an open source large language model that is of\n",
    "175 billion parameters, and you want to fine-tune it for your domain specific\n",
    "task for your company. Let's say you succeed in fine-tuning the model and it\n",
    "works well for your task and you spent a lot of time and resources on it.\n",
    "However your data used for fine-tuning is non-stationary and the model's\n",
    "performance degrades over time. So you have to retrain the model again and again\n",
    "to keep up with the data distribution changes. To further exacerbate the\n",
    "problem, your other departments across the company wants to also fine-tune the\n",
    "large language model for their own domain specific tasks.\n",
    "\n",
    "This will become a problem because performing a _full fine-tuning_ for each\n",
    "domain specific task will be computationally expensive and time consuming simply\n",
    "because full fine-tuning requires _adjusting all the parameters_ of the model -\n",
    "which means if your model has 175 billion _trainable_ parameters, you will have\n",
    "to adjust all of them for each domain specific task. Such prohibitive\n",
    "computational cost and time consumption is not feasible for most companies and\n",
    "this is where _Low Rank Adaptation_ comes in - in which it _freezes_ the\n",
    "**backbone** of the model weights and _inject trainable rank decomposition\n",
    "matrices into selected layers_ {cite}`hu2021loralowrankadaptationlarge` of the\n",
    "said model (often a transformer model).\n",
    "\n",
    "To take things into perspective, GPT-3 175B fine-tuned with Adam (note adaptive\n",
    "optimizer like this takes up a lot of memory because it stores the first and\n",
    "second moments of the gradients) and LoRA can reduce the trainable parameters by\n",
    "10,000 times and the GPU memory by 3 folds - all while maintaining the on-par\n",
    "performance with suitable hyperparameters.\n",
    "\n",
    "## Rank And Low-Rank Decomposition Via Matrix Factorization\n",
    "\n",
    "Firstly, we assume that the reader has a basic understanding of the\n",
    "[transformer-based models](https://arxiv.org/abs/1706.03762) and how they work,\n",
    "as well as the definition of\n",
    "[_rank_](<https://en.wikipedia.org/wiki/Rank_(linear_algebra)>) and\n",
    "[_low-rank decomposition_ ](https://en.wikipedia.org/wiki/Low-rank_approximation)in\n",
    "linear algebra.\n",
    "\n",
    "In simple terms, given a matrix $\\mathbf{W}$ with dimensions $d \\times k$, the\n",
    "rank of a matrix is the number of linearly independent rows or columns it\n",
    "contains, denoted as $r$, where $r \\leq \\min (d, k)$. Intuitively, if a matrix\n",
    "is full rank, it represents a wider array of linear transformations, indicating\n",
    "a diverse set of information. Conversely, a low-rank matrix, having fewer\n",
    "linearly independent rows or columns, suggests that it contains redundant\n",
    "information due to dependencies among its elements. For instance, an image of a\n",
    "person can be represented as a low-rank matrix because the pixels in the image\n",
    "often show strong spatial correlations. Techniques like principal component\n",
    "analysis (PCA) exploit this property to compress images, reducing dimensionality\n",
    "while retaining essential features.\n",
    "\n",
    "A low-rank approximation of a matrix $\\mathbf{W}$ is another matrix\n",
    "$\\overset{\\sim}{\\mathbf{W}}$ with the same dimensions as $\\mathbf{W}$, which\n",
    "approximates $\\mathbf{W}$ while having a lower rank, aimed at minimizing the\n",
    "approximation error $\\| \\mathbf{W} - \\overset{\\sim}{\\mathbf{W}} \\|$ under a\n",
    "specific norm. This is actually a minimization problem and is well-defined and\n",
    "used commonly in various applications. A common way to find such an\n",
    "approximation is to use the\n",
    "[singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
    "(SVD) of $\\mathbf{W}$, which decomposes $\\mathbf{W}$ into three matrices\n",
    "$\\mathbf{U}$, $\\mathbf{\\Sigma}$, and $\\mathbf{V}^{T}$ such that\n",
    "$\\mathbf{W} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^{T}$, where $\\mathbf{U}$ and\n",
    "$\\mathbf{V}$ are orthogonal matrices (assume\n",
    "$\\mathbf{W} \\in \\mathbb{R}^{d \\times k}$) and $\\mathbf{\\Sigma}$ is a diagonal\n",
    "matrix with non-negative real numbers on the diagonal. However, for our specific\n",
    "purpose, we will mention another form of low-rank decomposition via matrix\n",
    "factorization. More concretely, we will use two matrices $\\mathbf{A}$ and\n",
    "$\\mathbf{B}$ to approximate a given matrix $\\mathbf{W}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{A}, \\mathbf{B}=\\underset{\\mathbf{A}, \\mathbf{B}}{\\operatorname{argmin}} \\frac{1}{2}\\|\\mathbf{A} \\mathbf{B}-\\mathbf{W}\\|_F^2\n",
    "$$\n",
    "\n",
    "where $\\|\\mathbf{B} \\mathbf{A}-\\mathbf{W}\\|_F^2$ is the objective function to\n",
    "minimize. The Frobenius norm $\\| \\cdot \\|_F$ is used to measure the error\n",
    "between the original matrix $\\mathbf{W}$ and the approximation\n",
    "$\\mathbf{A} \\mathbf{B}$. Lastly, it is important to note that if $\\mathbf{W}$\n",
    "has rank $r$, then $\\mathbf{A}$ and $\\mathbf{B}$ will have dimensions\n",
    "$d \\times r$ and $r \\times k$, respectively, where $r \\lt \\min (d, k)$ (or in\n",
    "our LoRA case, $r \\ll \\min (d, k)$ to emphasise that the $r$ is much smaller\n",
    "than $\\min (d, k)$).\n",
    "\n",
    "## The Autoregressive Self-Supervised Learning Paradigm\n",
    "\n",
    "The authors in LoRA mentioned that while our proposal is agnostic to training\n",
    "objective, they focus on language modeling as our motivating use case. So we\n",
    "detail a brief description of the language modeling problem.\n",
    "\n",
    "Let $\\mathcal{D}$ be the true but unknown distribution of the natural language\n",
    "space. In the context of unsupervised learning with self-supervision, such as\n",
    "language modeling, we consider both the inputs and the implicit labels derived\n",
    "from the same data sequence. Thus, while traditionally we might decompose the\n",
    "distribution $\\mathcal{D}$ of a supervised learning task into input space\n",
    "$\\mathcal{X}$ and label space $\\mathcal{Y}$, in this scenario, $\\mathcal{X}$ and\n",
    "$\\mathcal{Y}$ are intrinsically linked, because $\\mathcal{Y}$ is a shifted\n",
    "version of $\\mathcal{X}$, and so we can consider $\\mathcal{D}$ as a distribution\n",
    "over $\\mathcal{X}$ only.\n",
    "\n",
    "Since $\\mathcal{D}$ is a distribution, we also define it as a probability\n",
    "distribution over $\\mathcal{X}$, and we can write it as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{D} &= \\mathbb{P}(\\mathcal{X} ; \\boldsymbol{\\Theta}) \\\\\n",
    "            &= \\mathbb{P}_{\\{\\mathcal{X} ; \\boldsymbol{\\Theta}\\}}(\\mathbf{x})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\Theta}$ is the parameter space that defines the distribution\n",
    "$\\mathbb{P}(\\mathcal{X} ; \\boldsymbol{\\Theta})$ and $\\mathbf{x}$ is a sample\n",
    "from $\\mathcal{X}$ generated by the distribution $\\mathcal{D}$. It is common to\n",
    "treat $\\mathbf{x}$ as a sequence of tokens (i.e. a sentence is a sequence of\n",
    "tokens), and we can write $\\mathbf{x} = \\left(x_1, x_2, \\ldots, x_T\\right)$,\n",
    "where $T$ is the length of the sequence.\n",
    "\n",
    "Given such a sequence $\\mathbf{x}$, the joint probability of the sequence can be\n",
    "factorized into the product of the conditional probabilities of each token in\n",
    "the sequence via the\n",
    "[chain rule of probability](<https://en.wikipedia.org/wiki/Chain_rule_(probability)>):\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\mathbf{x} ; \\boldsymbol{\\Theta}) = \\prod_{t=1}^T \\mathbb{P}(x_t \\mid x_1, x_2, \\ldots, x_{t-1} ; \\boldsymbol{\\Theta})\n",
    "$$\n",
    "\n",
    "We can do this because natural language are _inherently ordered_. Such\n",
    "decomposition allows for _tractable sampling_ from and _estimation_ of the\n",
    "distribution $\\mathbb{P}(\\mathbf{x} ; \\boldsymbol{\\Theta})$ as well as any\n",
    "conditionals in the form of\n",
    "$\\mathbb{P}(x_{t-k}, x_{t-k+1}, \\ldots, x_{t} \\mid x_{1}, x_{2}, \\ldots, x_{t-k-1} ; \\boldsymbol{\\Theta})$\n",
    "{cite}`radford2019language`.\n",
    "\n",
    "To this end, consider a corpus $\\mathcal{S}$ with $N$ sequences\n",
    "$\\left\\{\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\ldots, \\mathbf{x}_{N}\\right\\}$,\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\left\\{\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\ldots, \\mathbf{x}_{N}\\right\\} \\underset{\\text{i.i.d.}}{\\sim} \\mathcal{D}\n",
    "$$\n",
    "\n",
    "where each sequence $\\mathbf{x}_{n}$ is a sequence of tokens that are sampled\n",
    "$\\text{i.i.d.}$ from the distribution $\\mathcal{D}$.\n",
    "\n",
    "Then, we can frame the\n",
    "[likelihood function](https://gao-hongnan.github.io/gaohn-galaxy/probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html)\n",
    "$\\hat{\\mathcal{L}}(\\cdot)$ as the likelihood of observing the sequences in the\n",
    "corpus $\\mathcal{S}$,\n",
    "\n",
    "$$\n",
    "\\hat{\\mathcal{L}}\\left(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}}\\right) = \\prod_{n=1}^N \\mathbb{P}(\\mathbf{x}_{n} ; \\hat{\\boldsymbol{\\Theta}})\n",
    "$$\n",
    "\n",
    "where $\\hat{\\boldsymbol{\\Theta}}$ is the estimated parameter space that\n",
    "approximates the true parameter space $\\boldsymbol{\\Theta}$.\n",
    "\n",
    "Subsequently, the objective function is now well-defined, to be the maximization\n",
    "of the likelihood of the sequences in the corpus $\\mathcal{S}$,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\boldsymbol{\\theta}}^{*} &= \\underset{\\hat{\\boldsymbol{\\theta}} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\hat{\\mathcal{L}}\\left(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}}\\right) \\\\\n",
    "                              &= \\underset{\\hat{\\boldsymbol{\\theta}} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\prod_{n=1}^N \\mathbb{P}(\\mathbf{x}_{n} ; \\hat{\\boldsymbol{\\Theta}}) \\\\\n",
    "                              &= \\underset{\\hat{\\boldsymbol{\\theta}} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\prod_{n=1}^N \\prod_{t=1}^{T_n} \\mathbb{P}(x_{n, t} \\mid x_{n, 1}, x_{n, 2}, \\ldots, x_{n, t-1} ; \\hat{\\boldsymbol{\\Theta}}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $T_n$ is the length of the sequence $\\mathbf{x}_{n}$.\n",
    "\n",
    "Owing to the fact that multiplying many probabilities together can lead to\n",
    "[numerical instability](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html#numerical-optimization-and-the-negative-log-likelihood)\n",
    "because the product of many probabilities can be very small, it is common and\n",
    "necessary to use the log-likelihood as the objective function, because it can be\n",
    "proven that maximizing the log-likelihood is equivalent to maximizing the\n",
    "likelihood itself.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\boldsymbol{\\theta}}^{*} &= \\underset{\\hat{\\boldsymbol{\\theta}} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\log\\left(\\hat{\\mathcal{L}}\\left(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}}\\right)\\right) \\\\\n",
    "&= \\underset{\\hat{\\boldsymbol{\\theta}} \\in \\boldsymbol{\\Theta}}{\\text{argmax}} \\sum_{n=1}^N \\sum_{t=1}^{T_n} \\log \\mathbb{P}(x_{n, t} \\mid x_{n, 1}, x_{n, 2}, \\ldots, x_{n, t-1} ; \\hat{\\boldsymbol{\\Theta}}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Furthermore, since we are treating the the loss function as a form of\n",
    "minimization, we can simply negate the log-likelihood to obtain the negative\n",
    "log-likelihood as the objective function to be minimized,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\boldsymbol{\\theta}}^{*} &= \\underset{\\hat{\\boldsymbol{\\theta}} \\in \\boldsymbol{\\Theta}}{\\text{argmin}} \\left(-\\log\\left(\\hat{\\mathcal{L}}\\left(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}}\\right)\\right)\\right) \\\\\n",
    "&= \\underset{\\hat{\\boldsymbol{\\theta}} \\in \\boldsymbol{\\Theta}}{\\text{argmin}} \\left(-\\sum_{n=1}^N \\sum_{t=1}^{T_n} \\log \\mathbb{P}(x_{n, t} \\mid x_{n, 1}, x_{n, 2}, \\ldots, x_{n, t-1} ; \\hat{\\boldsymbol{\\Theta}})\\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "It is worth noting that the objective function is a function of the parameter\n",
    "space $\\hat{\\boldsymbol{\\Theta}}$, and not the data $\\mathcal{S}$, so all\n",
    "analysis such as convergence and consistency will be with respect to the\n",
    "parameter space $\\hat{\\boldsymbol{\\Theta}}$.\n",
    "\n",
    "To this end, we denote the model $\\mathcal{M}$ to be an _autoregressive_ and\n",
    "_self-supervised learning_ model that is trained to maximize the likelihood of\n",
    "observing all data points $\\mathbf{x} \\in \\mathcal{S}$ via the objective\n",
    "function $\\hat{\\mathcal{L}}\\left(\\mathcal{S} ; \\hat{\\boldsymbol{\\Theta}}\\right)$\n",
    "by learning the conditional probability distribution\n",
    "$\\mathbb{P}(x_t \\mid x_{<t} ; \\hat{\\boldsymbol{\\Theta}})$ over the vocabulary\n",
    "$\\mathcal{V}$ of tokens, conditioned on the contextual preciding tokens\n",
    "$x_{<t} = \\left(x_1, x_2, \\ldots, x_{t-1}\\right)$. We are clear that although\n",
    "the goal is to model the joint probability distribution of the token sequences,\n",
    "we can do so by estimating the joint probability distribution via the\n",
    "conditional probability distributions.\n",
    "\n",
    "## Task Specific Fine-Tuning\n",
    "\n",
    "We can now look at what the authors define next which is the maximization of\n",
    "conditional probabilities given a task-specific prompt.\n",
    "\n",
    "Suppose we are given a pre-trained autoregressive language model\n",
    "$\\mathcal{M}_{\\Theta}(y \\mid x)$ parametrized by $\\Theta$. For instance,\n",
    "$\\mathcal{M}_{\\Theta}(y \\mid x)$ can be a generic multi-task learner such as GPT\n",
    "based on the Transformer architecture. Consider adapting this pre-trained model\n",
    "to downstream conditional text generation tasks, such as summarization, machine\n",
    "reading comprehension (MRC), and natural language to SQL (NL2SQL). Each\n",
    "downstream task is represented by a training dataset of context-target pairs:\n",
    "$\\mathcal{Z}=\\left\\{\\left(x_i, y_i\\right)\\right\\}_{i=1, \\ldots, N}$, where both\n",
    "$x_i$ and $y_i$ are sequences of tokens. For example, in NL2SQL, $x_i$ is a\n",
    "natural language query and $y_i$ its corresponding SQL command; for\n",
    "summarization, $x_i$ is the content of an article and $y_i$ its summary.\n",
    "\n",
    "During full fine-tuning, the model is initialized to pre-trained weights\n",
    "$\\Theta_{\\mathcal{P}}$ (where $\\Theta_{\\mathcal{P}}$ just denotes the final\n",
    "pretrained weights) and updated to $\\Theta_{\\mathcal{P}}+\\Delta \\Theta$ by\n",
    "repeatedly following the gradient to maximize the conditional language modeling\n",
    "objective:\n",
    "\n",
    "$$\n",
    "\\max_{\\Theta} \\sum_{(x, y) \\in \\mathcal{Z}} \\sum_{t=1}^{|y|} \\log \\left(\\mathcal{M}_{\\Theta}\\left(y_t \\mid x, y_{<t}\\right)\\right)\n",
    "$$\n",
    "\n",
    "One of the main drawbacks for full fine-tuning is that for _each_ downstream\n",
    "task, we learn a different set of parameters $\\Delta \\Theta$ whose dimension\n",
    "$|\\Delta \\Theta|$ equals $\\left|\\Theta_{\\mathcal{P}}\\right|$. Thus, if the\n",
    "pre-trained model is large (such as GPT-3 with\n",
    "$\\left|\\Theta_{\\mathcal{P}}\\right| \\approx 175$ Billion), storing and deploying\n",
    "many independent instances of fine-tuned models can be challenging, if at all\n",
    "feasible. In this paper, we adopt a more parameter-efficient approach, where the\n",
    "task-specific parameter increment $\\Delta \\Theta=\\Delta \\Theta(\\Phi)$ is further\n",
    "encoded by a much smaller-sized set of parameters $\\Phi$ with\n",
    "$|\\Phi| \\ll \\left|\\Theta_{\\mathcal{P}}\\right|$. The task of finding\n",
    "$\\Delta \\Theta$ thus becomes optimizing over $\\Phi$:\n",
    "\n",
    "$$\n",
    "\\max _{\\Theta} \\sum_{(x, y) \\in \\mathcal{Z}} \\sum_{t=1}^{|y|} \\log \\left(p_{\\Theta_{\\mathcal{P}}+\\Delta \\Theta(\\Phi)}\\left(y_t \\mid x, y_{<t}\\right)\\right)\n",
    "$$\n",
    "\n",
    "In the subsequent sections, we propose to use a low-rank representation to\n",
    "encode $\\Delta \\Theta$ that is both compute- and memory-efficient. When the\n",
    "pre-trained model is GPT-3 175B, the number of trainable parameters $|\\Phi|$ can\n",
    "be as small as $0.01 \\%$ of $\\left|\\Theta_{\\mathcal{P}}\\right|$. Note that you\n",
    "can visualize the $\\Delta \\Theta(\\Phi)$ as the low-rank decomposition of the\n",
    "update weights $\\Delta \\mathbf{W}$ in the fine-tuning process.\n",
    "\n",
    "## The Update Weights Of Fine-Tuning Has A Low Intrinsic Rank\n",
    "\n",
    "We describe the author's first big idea in this section, where they hypothesize\n",
    "(with empirical evidence) that the update weights of a large language model\n",
    "during fine-tuning reside in a low-dimensional subspace.\n",
    "\n",
    "The image below illustrates and gives a very simplified visual representation of\n",
    "a single weight update step from a full fine-tuning process (left) versus a\n",
    "LoRA-based fine-tuning process (right). The matrices $\\mathbf{A}$ and\n",
    "$\\mathbf{B}$ (which we explain shortly) are approximations of the update weights\n",
    "$\\Delta \\mathbf{W}$ in the LoRA-based fine-tuning process.\n",
    "\n",
    "```{figure} ./assets/lora_weights_visual_seb.png\n",
    "---\n",
    "name: low-rank-weights-visual-seb\n",
    "---\n",
    "\n",
    "Low-rank decomposition of the update weights $\\Delta \\mathbf{W}$ into two\n",
    "matrices $\\mathbf{A}$ and $\\mathbf{B}$.\n",
    "\n",
    "Image Credit:\n",
    "[Sebastian Raschka](https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-E/01_main-chapter-code/appendix-E.ipynb)\n",
    "```\n",
    "\n",
    "First, we use very rough notations to describe the update weights of fine-tuning\n",
    "as a matrix $\\mathbf{W} \\in \\mathbb{R}^{d \\times k}$ in a gradient-based\n",
    "optimization process. For simplicity we call $\\mathbf{W}$ as the _pre-trained\n",
    "weights_ and $\\Delta \\mathbf{W}$ as the _update weights_ at a given iteration of\n",
    "a gradient-based optimization process. More concretely, we have the below update\n",
    "process:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{W}  &\\leftarrow \\underbrace{\\mathbf{W} - \\alpha \\nabla \\mathbf{W}}_{\\mathbf{W} - \\alpha \\frac{\\partial \\mathcal{J}}{\\partial \\mathbf{W}}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{J}$ is the objective function, $\\alpha$ is the learning rate,\n",
    "and $\\nabla \\mathbf{W}$ is the gradient of the objective function with respect\n",
    "to the pre-trained weights $\\mathbf{W}$ and collectively\n",
    "$-\\alpha \\nabla \\mathbf{W}$ is the update weights $\\Delta \\mathbf{W}$ and that\n",
    "both $\\Delta \\mathbf{W}$ and $\\nabla \\mathbf{W}$ lies in the same subspace.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Delta \\mathbf{W} := -\\alpha \\nabla \\mathbf{W}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To ease the notations, we further denote $\\mathbf{W}^{(t)}$ as the pre-trained\n",
    "weights at iteration $t$ and $\\Delta \\mathbf{W}^{(t)}$ as the update weights at\n",
    "iteration $t$. We can then rewrite the above equation as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{W}^{(t+1)} &= \\mathbf{W}^{(t)} + \\Delta \\mathbf{W}^{(t)} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "to indicate that $\\mathbf{W}^{(t+1)}$ is the updated weights after\n",
    "$\\Delta \\mathbf{W}^{(t)}$ is added to the pre-trained weights\n",
    "$\\mathbf{W}^{(t)}$.\n",
    "\n",
    "Empirical evidence suggests that deep learning models (often large language\n",
    "models) $\\mathcal{M}$ are over-parametrized with respect to their parameter\n",
    "space $\\Theta$ (i.e. the weights of the model). This means the model contains\n",
    "more parameters than are necessary to achieve the minimum error on the training\n",
    "data. This redundancy often implies that many parameters are either not\n",
    "essential or are correlated with others. While the \"full space\" offers maximal\n",
    "degrees of freedom for the model parameters, allowing complex representations\n",
    "and potentially capturing intricate patterns in the data, it can lead to\n",
    "overfitting, and in our context, it can lead to high computational costs and\n",
    "memory usage.\n",
    "\n",
    "As a result, the authors hypothesize that $\\mathcal{M}$ can operate within a\n",
    "much lower-dimensional subspace $\\mathcal{S}$ which means that we can reduce the\n",
    "effective degrees of freedom of the model $\\mathcal{M}$ by projecting the\n",
    "weights of the model into a lower-dimensional subspace while _maintaining the\n",
    "performance of the model_ - and this is what the author mean by \"model resides\n",
    "in a low intrinsic dimension\".\n",
    "\n",
    "More concretely, suppose the weight matrix\n",
    "$\\mathbf{W} \\in \\mathbb{R}^{d \\times k}$ has rank $r$ meaning the maximum number\n",
    "of linearly independent rows or columns it contains is $r$. We say that this\n",
    "weight matrix reside in a subspace $\\mathcal{S}_{W} \\subset \\mathbb{R}^{d}$\n",
    "(column space/range of $\\mathbf{W}$) and the dimension of this subspace is $r$.\n",
    "The authors argue that the update weights $\\Delta \\mathbf{W}$ of the model\n",
    "$\\mathcal{M}$ at a given iteration of the optimization process also reside in a\n",
    "low-dimensional subspace\n",
    "$\\mathcal{S}_{\\Delta \\mathbf{W}} \\subset \\mathbb{R}^{d}$ with\n",
    "$\\dim\\left(\\mathcal{S}_{\\Delta \\mathbf{W}}\\right) \\ll r$. Note that without\n",
    "LoRA, the update weights $\\Delta \\mathbf{W}$ typically have a high rank (not\n",
    "guaranteed to be of same rank of $\\mathbf{W}$), and so the authors intelligently\n",
    "proposed an approximation of the update weights $\\Delta \\mathbf{W}$ using a\n",
    "low-rank decomposition because of the hypothesis that the update weights reside\n",
    "in a low-dimensional subspace and is sufficient to represent the\n",
    "over-parametrized model $\\mathcal{M}$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{W}^{(t+1)} &= \\mathbf{W}^{(t)} + \\Delta \\mathbf{W}^{(t)} \\\\\n",
    "&= \\mathbf{W}^{(t)} + \\mathbf{B}^{(t)} \\mathbf{A}^{(t)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{A}^{(t)}$ and $\\mathbf{B}^{(t)}$ are the low-rank decomposition\n",
    "matrices of the update weights $\\Delta \\mathbf{W}^{(t)}$ at iteration $t$. In\n",
    "other words, the update weights $\\Delta \\mathbf{W}$ are approximated by the\n",
    "product of two low-rank matrices $\\mathbf{A} \\in \\mathbb{R}^{r \\times k}$ and\n",
    "and $\\mathbf{B} \\in \\mathbb{R}^{d \\times r}$, where $r \\ll \\min (d, k)$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Delta \\mathbf{W} \\approx \\mathbf{B} \\mathbf{A}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## Parameters Reduction In LoRA\n",
    "\n",
    "Now, let's do some quick math. Earlier we said our model is of size\n",
    "$\\| \\Theta_{\\mathcal{M}} \\| = 175,000,000,000$ parameters. Then for simplicity\n",
    "case we assume our weight $\\mathbf{W} \\in \\mathbb{R}^{d \\times k}$ of the\n",
    "pretrained model $\\mathcal{M}$ to be of size\n",
    "$d = k = \\sqrt{175,000,000,000} = 418,330$. And if we do not do LoRA, the update\n",
    "weights $\\nabla \\mathbf{W}$ will also be of size\n",
    "$d \\times k = 418,330 \\times 418,330 = 175,000,000,000$ parameters. However, if\n",
    "we decompose the update weights $\\nabla \\mathbf{W}$ into two low-rank matrices\n",
    "$\\mathbf{A}$ and $\\mathbf{B}$, then the number of parameters in the low-rank\n",
    "decomposition is $r \\times (d + k)$. Suppose that we use a LoRA rank of $r = 8$,\n",
    "then $\\mathbf{A} \\in \\mathbb{R}^{8 \\times 418,330}$ and\n",
    "$\\mathbf{B} \\in\n",
    "\\mathbb{R}^{418,330 \\times 8}$, and the number of parameters in\n",
    "the low-rank decomposition is $8 \\times (418,330 + 418,330) = 6,693,280$\n",
    "parameters. We do some quick calculations and see that the reduction in the\n",
    "number of parameters is more than 26100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef085df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 175000000000\n",
      "LoRA parameters: 6693280.212272604\n",
      "Reduction: 99.996175%\n",
      "26145.625829189863\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def compute_lora_parameters(d: int, k: int, r: int) -> int:\n",
    "    parameters_A = r * d\n",
    "    parameters_B = r * k\n",
    "    return parameters_A + parameters_B\n",
    "\n",
    "total_trainable_parameters = 175_000_000_000\n",
    "print(f\"Total trainable parameters: {total_trainable_parameters}\")\n",
    "\n",
    "d = k = math.sqrt(total_trainable_parameters)\n",
    "r = 8\n",
    "lora_parameters = compute_lora_parameters(d, k, r)\n",
    "print(f\"LoRA parameters: {lora_parameters}\")\n",
    "\n",
    "reduction = (total_trainable_parameters - lora_parameters) / total_trainable_parameters\n",
    "print(f\"Reduction: {reduction:.6%}\")\n",
    "print(f\"{total_trainable_parameters / lora_parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e739c99",
   "metadata": {},
   "source": [
    "However, do note that there is no free lunch, we have to acknowledge that the\n",
    "rank $r$ of the low-rank decomposition is a hyperparameter that needs to be\n",
    "tuned. Too small a rank can lead to underfitting, while too large a rank can\n",
    "lead to overfitting. Furthermore, no one knows the underlying \"true\" rank of the\n",
    "model and it may be well the case that the approximation $\\mathbf{B} \\mathbf{A}$\n",
    "is not a good approximation of the update weights $\\Delta \\mathbf{W}$ and cannot\n",
    "capture every nuance. That is fine, for one, during pretraining stage, there is\n",
    "no low rank approximation and we hypothesize that the weight matrix $\\mathbf{W}$\n",
    "is large and sufficient enough to capture all the nuances and knowledge in the\n",
    "huge pretraining dataset. However, during the fine-tuning stage, we hypothesize\n",
    "the domain specific task is not as complex as the pretraining task and that the\n",
    "model has sufficient knowledge to _adapt_ to the domain specific task with a\n",
    "low-rank decomposition/approximation of the update weights $\\Delta \\mathbf{W}$.\n",
    "This brings to our second point, if the target domain specific task\n",
    "$\\mathcal{T}$ is too drastically different from the pretraining task\n",
    "$\\mathcal{P}$, then the low-rank decomposition may not be able to capture the\n",
    "necessary information for the adaptation and the model may not perform well - so\n",
    "here we recommend increasing the rank $r$ where appropriate.\n",
    "\n",
    "## The Low-Rank Adaptation (LoRA) Algorithm\n",
    "\n",
    "Let $\\mathcal{M}$ be our model with some linear layer with weights\n",
    "$\\mathbf{W} \\in \\mathbb{R}^{d \\times k}$, where $d$ is the output dimension and\n",
    "$k$ is the input dimension (get used to this notation with PyTorch). In\n",
    "particular $\\mathbf{W}$ is the original pre-trained weights of the model\n",
    "$\\mathcal{M}$ (correspond to our $\\Theta_{\\mathcal{P}}$ earlier).\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "torch.nn.Linear(in_features=8, out_features=16).weight.shape\n",
    ">>> torch.Size([16, 8])\n",
    "```\n",
    "\n",
    "We define the linear transformation\n",
    "$f_{\\mathbf{W}} : \\mathbb{R}^k \\rightarrow \\mathbb{R}^d$ by\n",
    "$f_{\\mathbf{W}}(\\mathbf{x}) = \\mathbf{x} @ \\mathbf{W}^T$ where\n",
    "$\\mathbf{x} \\in \\mathbb{R}^{1 \\times k}$ (assume batch size of $1$ for\n",
    "simplicity). Note a quirk here is that we usually define the input as\n",
    "$\\mathbb{R}^{\\mathcal{B} \\times k}$ where $\\mathcal{B}$ is the batch size and\n",
    "transpose the weights from torch's `Linear` layer.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "torch.nn.Linear(in_features=8, out_features=16).weight.shape\n",
    "\n",
    "x = torch.randn(1, 8)\n",
    "x @ torch.nn.Linear(in_features=8, out_features=16).weight.T\n",
    "```\n",
    "\n",
    "Next, we define two low rank matrices $\\mathbf{A} \\in \\mathbb{R}^{r \\times k}$\n",
    "and $\\mathbf{B} \\in \\mathbb{R}^{d \\times r}$ where $r$ is the rank of the\n",
    "low-rank decomposition. We define transformations as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f_{\\mathbf{A}} &: \\mathbb{R}^k \\rightarrow \\mathbb{R}^r & \\quad f_{\\mathbf{B}} &: \\mathbb{R}^r \\rightarrow \\mathbb{R}^d \\\\\n",
    "f_{\\mathbf{A}}(\\mathbf{x}) &= \\mathbf{x} @ \\mathbf{A}^T & \\quad f_{\\mathbf{B}}(\\mathbf{y}) &= \\mathbf{y} @ \\mathbf{B}^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For an input $\\mathbf{x} \\in \\mathbb{R}^k$, we technically have the following\n",
    "update:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{y}  &= \\mathbf{x} @ \\left(\\mathbf{W}^T + \\Delta \\mathbf{W}^T\\right) \\\\\n",
    "            &= \\mathbf{x} @ \\left(\\mathbf{W}^T + (\\mathbf{B} \\mathbf{A})^T\\right) \\\\\n",
    "            &= \\mathbf{x} @ \\mathbf{W}^T + \\mathbf{x} @ \\left(\\mathbf{B} \\mathbf{A}\\right)^T \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "But we have our pretrained model weights $\\mathbf{W}$ is _frozen_ so we can\n",
    "compute the frozen output first as:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_{\\text{frozen}} = \\mathbf{x} @ \\mathbf{W}^T\n",
    "$$\n",
    "\n",
    "Why can we do this? Because of the distributive law of matrix multiplication. As\n",
    "we will mention again later, this allows the weight to be updated on the fly\n",
    "during inference, meaning we do not need to store the original pre-trained\n",
    "weights $\\mathbf{W}$ and only need to store the low-rank matrices $\\mathbf{A}$\n",
    "and $\\mathbf{B}$ - which is much more tractable.\n",
    "\n",
    "Then finally we have the following update:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{y}  &= \\mathbf{y}_{\\text{frozen}} + \\mathbf{x} @ \\left(\\mathbf{B} \\mathbf{A}\\right)^T \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "However, three nuances here:\n",
    "\n",
    "1. The pretrained weights $\\mathbf{W}$ of $\\mathcal{M}$ is frozen during\n",
    "   training via `requires_grad=False`. This tells PyTorch not to update the\n",
    "   weights of the pretrained model during backpropagation. This is important\n",
    "   because we want to keep the pretrained weights fixed and only update the\n",
    "   low-rank matrices $\\mathbf{A}$ and $\\mathbf{B}$ - both of which are\n",
    "   trainable.\n",
    "2. They use gaussian initialization for $\\mathbf{A}$ and zero initialization for\n",
    "   $\\mathbf{B}$.\n",
    "\n",
    "    $$\n",
    "     \\begin{aligned}\n",
    "     \\mathbf{A} &\\sim \\mathcal{N}(0, \\sigma^2) \\\\\n",
    "     \\mathbf{B} &= \\mathbf{0}\n",
    "     \\end{aligned}\n",
    "    $$\n",
    "\n",
    "    One of the matrices must be zero at initialization to ensure that the\n",
    "    initial state of the adaptation $\\Delta \\mathbf{W}$ does not alter the\n",
    "    pre-trained weights $\\mathbf{W}$, allowing the training process to start\n",
    "    from the original pre-trained state. In simpler words, your first forward\n",
    "    pass of the model should be from the original pre-trained weights\n",
    "    $\\mathbf{W}$, and not from some random lora weights.\n",
    "\n",
    "    As to why $\\mathbf{A} \\sim \\mathcal{N}(0, \\sigma^2)$, this is a common\n",
    "    initialization strategy for neural networks to break the symmetry and ensure\n",
    "    that the gradients are not too small or too large at the beginning of\n",
    "    training. Just remember, vanishing and exploding gradients are bad, and we\n",
    "    want to avoid them. How to avoid them is to make sure your initial\n",
    "    conditions are good, what it means by good is say each layer weights has\n",
    "    similar distribution (mean and variance) and so pertubations won't be too\n",
    "    large or too small. If you ask \"if $\\mathbf{B} \\mathbf{A}$ is zero, why\n",
    "    don't we just initialize $\\mathbf{A}$ to zero as well?\" - I think one needs\n",
    "    to know that the backpropagation process update both weights $\\mathbf{A}$\n",
    "    and $\\mathbf{B}$ _separately_ and we want stable gradient flow, so\n",
    "    $\\mathbf{A}$ breaks the symmetry! If you initialize all weights with zeros\n",
    "    then every hidden unit will get zero independent of the input. So, when all\n",
    "    the hidden neurons start with the zero weights, then all of them will follow\n",
    "    the same gradient and for this reason \"it affects only the scale of the\n",
    "    weight vector, not the direction\". See\n",
    "    [this very useful thread on the whys](https://datascience.stackexchange.com/questions/26134/initialize-perceptron-weights-with-zero).\n",
    "\n",
    "3. They have a scaling factor, where they scale $\\Delta \\mathbf{W}$ by\n",
    "   $\\frac{\\alpha}{r}$. In LoRA paper, $\\alpha$ is constant in $r$ means that if\n",
    "   once you fix a value of $r$ in your initial experiments, you can keep\n",
    "   $\\alpha$ constant for all future experiments with different values of $r$ -\n",
    "   because you can tune the learning rate scheduler's $\\eta$ instead because yes\n",
    "   both $\\alpha$ is pretty similar in _scaling the gradients\n",
    "   $\\nabla \\mathbf{W}$_ because by the chain rule, any scaling of the weights\n",
    "   will proportionally affect the gradients! Note for less confusion, we use\n",
    "   $\\eta$ for the learning rate in the optimizer and $\\alpha$ for the scaling\n",
    "   factor in LoRA.\n",
    "\n",
    "    $$\n",
    "     \\begin{aligned}\n",
    "     \\mathbf{y} &= \\mathbf{y}_{\\text{frozen}} + \\frac{\\alpha}{r} \\mathbf{x} @ \\left(\\mathbf{B} \\mathbf{A}\\right)^T\n",
    "     \\end{aligned}\n",
    "    $$\n",
    "\n",
    "    Note that $\\alpha$ is generally understood as an amplification factor - and\n",
    "    if $\\alpha$ is large, it amplifies the contribution of the LoRA weights to\n",
    "    the final output of the adapter layer.\n",
    "\n",
    "    Now some quick and rough (read: non-rigorous) math here, suppose we keep\n",
    "    rank $r$ fixed, and we increase $\\alpha$ (LoRA) by a factor of $c$:\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    c \\times \\frac{\\alpha}{r} \\mathbf{x} @ \\left(\\mathbf{B} \\mathbf{A}\\right)^T &= (c \\times \\eta) \\times \\mathbf{x} @ \\left(\\frac{\\mathbf{B}}{\\sqrt{c}} \\frac{\\mathbf{A}}{\\sqrt{c}}\\right)^T \\\\\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "    So in other words, if you keep $r$ fixed, when you increase $\\alpha$ by a\n",
    "    factor of $c$ - this is _equivalent_ to _increasing_ the learning rate\n",
    "    $\\eta$ by a factor of $c$ because in gradient updates we do\n",
    "    $-\\eta \\nabla \\left(\\mathbf{B}\\mathbf{A}\\right)$ and so if you increase\n",
    "    $\\alpha$ by a factor of $c$, you are inevitably increasing the learning rate\n",
    "    $\\eta$ by a factor of $c$. To compensate for this, you can decrease the\n",
    "    initializations of $\\mathbf{A}$ and $\\mathbf{B}$ by a factor of $\\sqrt{c}$\n",
    "    to keep to the same scale as before. Therefore, the authors recommend users\n",
    "    to (1) keep the rank $r$ fixed and (2) tune the learning rate scheduler's\n",
    "    $\\eta$ instead of $\\alpha$ (and maybe the weights as well). One can read a\n",
    "    thread on this\n",
    "    [here](https://civitai.com/articles/2125/what-lora-alpha-actually-does-in-theory).\n",
    "\n",
    "## Merge - No Additional Inference Latency\n",
    "\n",
    "The distributive law of matrix multiplication we saw earlier ensures that the\n",
    "update weights $\\Delta \\mathbf{W}$ can be applied on the fly during inference\n",
    "without too much over memory overhead, and therefore not much additional\n",
    "latency. Recall the equation below:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{y}  &= \\mathbf{x} @ \\left(\\mathbf{W}^T + \\Delta \\mathbf{W}^T\\right) \\\\\n",
    "            &= \\mathbf{x} @ \\left(\\mathbf{W}^T + (\\mathbf{B} \\mathbf{A})^T\\right) \\\\\n",
    "            &= \\mathbf{x} @ \\mathbf{W}^T + \\mathbf{x} @ \\left(\\mathbf{B} \\mathbf{A}\\right)^T \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We easily see that once we obtain the trained low-rank matrices $\\mathbf{A}$ and\n",
    "$\\mathbf{B}$, we can apply the update weights $\\Delta \\mathbf{W}$ on the fly\n",
    "during inference without having to store the original pre-trained weights by\n",
    "just doing an element-wise addition of the frozen output\n",
    "$\\mathbf{y}_{\\text{frozen}}$ and the update\n",
    "$\\mathbf{x} @ \\left(\\mathbf{B} \\mathbf{A}\\right)^T$.\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{x} @ \\mathbf{W}^T \\oplus \\mathbf{x} @ \\left(\\mathbf{B} \\mathbf{A}\\right)^T\n",
    "$$\n",
    "\n",
    "Again, we are reminded that this is a huge advantage because we need not store\n",
    "$N$ instances of the updated weights $\\Delta \\mathbf{W}$ for $N$ different\n",
    "tasks, but only the low-rank matrices $\\mathbf{A}$ and $\\mathbf{B}$. During\n",
    "inference, just apply the low-rank matrices on the fly and you are good to go.\n",
    "\n",
    "```{figure} ./assets/lora_weights_visual.png\n",
    "---\n",
    "name: lora-weights-visual-paper\n",
    "---\n",
    "LoRA update weights $\\Delta \\mathbf{W}$ as a low-rank decomposition of two\n",
    "matrices $\\mathbf{A}$ and $\\mathbf{B}$.\n",
    "\n",
    "Image Credit: [LoRA Paper](https://arxiv.org/pdf/2106.09685)\n",
    "```\n",
    "\n",
    "In practice, it is common to _merge and unload_ the adapter layer after training\n",
    "into the original pretrained base model. This merge operation is done by\n",
    "updating the original pretrained weights $\\mathbf{W}$ with the low-rank\n",
    "decomposition matrices $\\mathbf{A}$ and $\\mathbf{B}$ to obtain the merged\n",
    "weights $\\mathbf{W}_{'}$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{W}^{'} &= \\mathbf{W} \\oplus \\mathbf{B} \\mathbf{A}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This way, during inference, given an input $\\mathbf{x}$, we can compute the\n",
    "output $\\mathbf{y}$ by applying the merged weights $\\mathbf{W}^{'}$ on the input\n",
    "$\\mathbf{x}$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{y} &= \\mathbf{x} @ \\left(\\mathbf{W}^{'}\\right)^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This requires only a single matrix multiplication operation. In total, we can\n",
    "loosely say that if $\\mathbf{x}$ is a $1 \\times k$ vector, and\n",
    "$\\mathbf{W}^{'} \\in \\mathbb{R}^{d \\times k}$, then doing\n",
    "$\\mathbf{x} @ \\left(\\mathbf{W}^{'}\\right)^T$ costs $2 \\times d \\times k$\n",
    "multiplications and $d \\times k$ additions. But if you do not merge, and keep\n",
    "the weights separate, you get\n",
    "$\\mathbf{x} @ \\left(\\mathbf{W}^T\\right) + \\mathbf{x} @ \\left(\\mathbf{B} \\mathbf{A}\\right)^T$\n",
    "which costs $2 \\times d \\times k$ multiplications and $2 \\times d \\times k$\n",
    "additions. So, in terms of computational complexity, even though they are on the\n",
    "same order, the merged weights are slightly more efficient.\n",
    "\n",
    "## References And Further Readings\n",
    "\n",
    "-   [1] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and\n",
    "    W. Chen, \"LoRA: Low-Rank Adaptation of Large Language Models,\" _arXiv\n",
    "    preprint arXiv:2106.09685_, submitted Jun. 17, 2021, revised Oct. 16, 2021.\n",
    "    [Online]. Available: https://arxiv.org/abs/2106.09685\n",
    "-   [2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n",
    "    Ł. Kaiser, and I. Polosukhin.\n",
    "    [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762). In Advances\n",
    "    in Neural Information Processing Systems, pp. 5998–6008, 2017.\n",
    "-   [Low Rank Approximation - Wikipedia](https://en.wikipedia.org/wiki/Low-rank_approximation)\n",
    "-   [Rank (Linear Algebra) - Wikipedia](<https://en.wikipedia.org/wiki/Rank_(linear_algebra)>)\n",
    "-   [Initialize perceptron weights with zero](https://datascience.stackexchange.com/questions/26134/initialize-perceptron-weights-with-zero)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "mystnb": {
   "number_source_lines": true
  },
  "source_map": [
   16,
   29,
   47,
   53,
   151,
   546,
   565
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}