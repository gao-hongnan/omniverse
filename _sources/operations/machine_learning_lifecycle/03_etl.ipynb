{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1263d9f",
   "metadata": {},
   "source": [
    "# Stage 3. Extract, Transform, Load (ETL)\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Brain_Dump-red)\n",
    "![Tag](https://img.shields.io/badge/Level-Beginner-green)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "\n",
    "## The Evolution of Data Engineering (Don't Quote Me On This!)\n",
    "\n",
    "With the basic understanding of **Data Engineering** and its essential role in\n",
    "**Machine Learning**, it's important to recognize the evolution of data handling\n",
    "practices. Traditional **ETL (Extract, Transform, Load)** methodologies have\n",
    "long been the backbone of data pipeline design. They set the stage for\n",
    "collecting, processing, and storing data in a structured manner.\n",
    "\n",
    "However, the modern era of data-driven applications demands a more agile and\n",
    "responsive approach. This is where **DataOps**, encompassing principles of\n",
    "**Continuous Integration/Continuous Deployment (CI/CD)**, comes into play. The\n",
    "process builds on the ETL framework but now with automation, collaboration,\n",
    "monitoring, and quality assurance.\n",
    "\n",
    "In traditional ETL or ELT processes, the main focus is on extracting data from\n",
    "various sources, transforming it into the required format, and then loading it\n",
    "into a target system. These processes are typically batch-oriented and can be\n",
    "run on schedules or triggered manually.\n",
    "\n",
    "In a CI/CD DataOps pipeline, the focus expands to the entire data lifecycle and\n",
    "emphasizes automation, continuous integration, and continuous deployment. This\n",
    "means that the process not only includes the basic ETL or ELT steps but also\n",
    "involves:\n",
    "\n",
    "-   **Continuous Integration**: Automating the process of integrating code\n",
    "    changes from multiple contributors into a shared repository, often followed\n",
    "    by automated building and testing.\n",
    "-   **Continuous Deployment**: Automating the process of deploying the\n",
    "    integrated and tested code to production environments, ensuring that the\n",
    "    data pipeline remains stable and updated.\n",
    "-   **Monitoring and Alerting**: Keeping track of the performance and health of\n",
    "    the data pipeline, triggering alerts if anomalies or issues are detected.\n",
    "-   **Testing and Quality Assurance**: Embedding rigorous testing within the\n",
    "    pipeline to ensure data quality, integrity, and compliance with business\n",
    "    rules.\n",
    "\n",
    "## The ETL/ELT Framework\n",
    "\n",
    "### ETL (Extract, Transform, Load)\n",
    "\n",
    "**ETL** is a process in data handling that involves three main stages:\n",
    "\n",
    "1. **Extract**: Gathering data from various sources.\n",
    "2. **Transform**: Processing this data to fit the desired format, usually\n",
    "   outside the target system. This might include cleaning, aggregating,\n",
    "   filtering, etc.\n",
    "3. **Load**: Finally, loading the transformed data into the destination data\n",
    "   warehouse or database.\n",
    "\n",
    "### ELT (Extract, Load, Transform)\n",
    "\n",
    "**ELT** is a variant of ETL, but with a different order of operations:\n",
    "\n",
    "1. **Extract**: Gathering data from various sources.\n",
    "2. **Load**: Loading the raw data into the destination system.\n",
    "3. **Transform**: Performing transformations within the target system itself,\n",
    "   utilizing the processing capabilities of modern data warehouses.\n",
    "\n",
    "### ELTL (Extract, Load, Transform, Load)\n",
    "\n",
    "This combination could represent a two-step process:\n",
    "\n",
    "1. **Extract**: Gathering data from various sources.\n",
    "2. **Load**: Loading the raw data into a staging area or temporary storage.\n",
    "3. **Transform**: Performing transformations within this temporary storage.\n",
    "4. **Load**: Loading the transformed data into the final destination, such as a\n",
    "   data warehouse or database.\n",
    "\n",
    "This approach might be beneficial when working with massive datasets, allowing\n",
    "for an initial raw data consolidation, followed by transformation and final\n",
    "loading into the target system.\n",
    "\n",
    "### Intuition on When to Use ETL vs ELT\n",
    "\n",
    "In certain scenarios, companies opt for the ELT (Extract, Load, Transform)\n",
    "process, particularly when dealing with complex and unstructured data. During\n",
    "the **extraction** phase, data is collected from various sources and then\n",
    "immediately **loaded** or dumped into a **data lake**, which is a storage\n",
    "repository that holds a vast amount of raw data in its native format.\n",
    "\n",
    "This approach has the advantage of quickly making the data available, preserving\n",
    "its raw state for future use. However, this raw, unstructured data can become\n",
    "unwieldy, particularly when dealing with large volumes.\n",
    "\n",
    "When it's time to analyze or utilize the data, it must be **extracted** again\n",
    "from the data lake. This is followed by the **transformation** phase, where the\n",
    "data is processed and converted into a structured format suitable for analysis.\n",
    "\n",
    "While the ELT paradigm allows for greater flexibility and the ability to\n",
    "accommodate diverse data types, it can lead to inefficiencies when searching\n",
    "through large and unstructured data sets within the data lake. The process of\n",
    "extracting and transforming data from the data lake can be time-consuming and\n",
    "resource-intensive, particularly if the data needs to be combed through\n",
    "extensively.\n",
    "\n",
    "In essence, the ELT approach with a data lake can be both a boon and a\n",
    "challenge. It enables faster data ingestion and provides a flexible repository\n",
    "for raw data, but the subsequent handling and processing of that data might\n",
    "require significant effort, especially when dealing with large quantities of\n",
    "unstructured information.\n",
    "\n",
    "### ETL versus ELT\n",
    "\n",
    "Here's a table that breaks down the comparison, advantages, and disadvantages of\n",
    "both ETL and ELT.\n",
    "\n",
    "| Criteria                | ETL                                                                                      | ELT                                                                                              |\n",
    "| ----------------------- | ---------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------ |\n",
    "| **Basic Process**       | Extract, Transform, Load                                                                 | Extract, Load, Transform                                                                         |\n",
    "| **Data Latency**        | May introduce delays, affecting real-time analysis; near-real-time possible with tooling | Often reduces delays between data collection and availability                                    |\n",
    "| **Scalability**         | Can be scalable with proper architecture and parallel processing                         | Typically leverages modern data warehouses for scalability                                       |\n",
    "| **Flexibility**         | Less adaptable to changing requirements; can be mitigated with design                    | More adaptable to changes in data structure or requirements                                      |\n",
    "| **Pipeline Complexity** | May involve complex transformations, increasing development and maintenance efforts      | Might simplify some aspects of the pipeline, depending on tools and requirements                 |\n",
    "| **Accessibility**       | May require specialized skills, limiting accessibility                                   | Might allow more team participation, especially with common languages like SQL                   |\n",
    "| **Advantages**          | Suitable for complex transformations with structured data; control over transformation   | Flexibility, scalability, and potentially reduced latency; useful for unstructured data handling |\n",
    "| **Disadvantages**       | Potential bottlenecks; complexity; potential rigidity                                    | Might lead to inefficiencies in processing unstructured data; simplicity is context-dependent    |\n",
    "| **Use Case**            | When precise control over transformations and structured data processing is needed       | When handling diverse or unstructured data, or when flexibility and scalability are priorities   |\n",
    "\n",
    "### Sample ELTL Pipeline\n",
    "\n",
    "#### Extract\n",
    "\n",
    "In the extraction phase, data is pulled from various sources which could be\n",
    "structured, semi-structured or unstructured, and could be located in databases,\n",
    "data lakes, data warehouses, or external APIs. The key is to capture the\n",
    "necessary data without losing or modifying any of the original data during the\n",
    "process.\n",
    "\n",
    "#### Data Analysis\n",
    "\n",
    "Post extraction, data analysis provides insights into the nature and quality of\n",
    "the data. This stage involves examining the distribution of the data,\n",
    "identifying potential anomalies or outliers, and assessing the overall data\n",
    "quality. Techniques such as descriptive statistics and data visualization are\n",
    "commonly used.\n",
    "\n",
    "It is important to note that this process is generally applicable and not\n",
    "specific to machine learning. The outcome of this stage guides the\n",
    "decision-making process for subsequent data cleaning and transformation tasks,\n",
    "thereby setting up a solid foundation for downstream tasks such as reporting,\n",
    "analytics, or model training.\n",
    "\n",
    "#### Validate Raw\n",
    "\n",
    "Validation of raw data ensures that the extracted data meets the requirements\n",
    "and constraints for the subsequent stages. It involves checking for data\n",
    "completeness, consistency, and accuracy. This stage might include checking if\n",
    "all expected data has been extracted, if there are any unexpected null or\n",
    "missing values, and if the data aligns with known constraints (like a field that\n",
    "should always be positive).\n",
    "\n",
    "#### Load\n",
    "\n",
    "The load stage involves transferring the extracted data into a target system for\n",
    "storage. The target system can be a data warehouse, a data lake, or a specific\n",
    "database depending on the use case. The focus during this phase is on efficiency\n",
    "and reliability, ensuring that all data is accurately loaded without disrupting\n",
    "existing data or processes.\n",
    "\n",
    "#### Transform\n",
    "\n",
    "The transformation phase involves changing the raw data into a format that is\n",
    "suitable for downstream tasks. This may include cleaning operations (like\n",
    "handling missing values or outliers), integrating data from different sources,\n",
    "aggregating or summarizing data, and converting data types. Additionally,\n",
    "feature engineering for machine learning tasks often takes place in this stage.\n",
    "\n",
    "#### Validate Transformed\n",
    "\n",
    "After transformation, the data needs to be validated again to ensure it meets\n",
    "the specific requirements for downstream tasks. This might involve checking the\n",
    "data against predefined rules or statistical properties (like a specific\n",
    "distribution), checking for unexpected null or missing values after\n",
    "transformation, or comparing a sample of the transformed data against the\n",
    "expected output.\n",
    "\n",
    "#### Load Transformed\n",
    "\n",
    "After the transformed data has been validated, it can be loaded into the target\n",
    "system for storage. This might be a data warehouse, a data lake, or a specific\n",
    "database depending on the use case.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The choice between ETL and ELT depends on various factors like data volume,\n",
    "real-time requirements, team skills, technology stack, and the nature of the\n",
    "transformations. ETL might be more suitable for complex transformations with\n",
    "structured data, while ELT might be preferred for more flexible, scalable\n",
    "handling of diverse or unstructured data.\n",
    "\n",
    "It's crucial to acknowledge that both ETL and ELT can be implemented effectively\n",
    "or poorly, depending on the specific context, tools, and design principles\n",
    "applied. Neither approach is universally superior, and the decision should be\n",
    "based on a comprehensive understanding of the project's unique requirements and\n",
    "constraints.\n",
    "\n",
    "## Batch Processing vs. Stream Processing (TODO as not familiar with stream processing)\n",
    "\n",
    "For real-time or near-real-time ML applications, traditional batch processing of\n",
    "ETL might not be suitable. Instead, stream processing frameworks like Apache\n",
    "Kafka or Apache Flink allow for continuous data processing and may be used as\n",
    "alternatives or complements to ETL."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "mystnb": {
   "number_source_lines": true
  },
  "source_map": [
   16
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}