{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b3362f",
   "metadata": {},
   "source": [
    "# Stage 3. Data Pipeline (Data Engineering and DataOps)\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Brain_Dump-red)\n",
    "![Tag](https://img.shields.io/badge/Level-Beginner-green)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "\n",
    "As we transition from the [**project scoping phase**](./02_project_scoping.md),\n",
    "where we framed the problem and identified **key metrics and components**, we\n",
    "now take a look at the DataOps phase. Even though data engineering and data\n",
    "operations are a whole different beast, we can at least look into the basic\n",
    "lifecycle of it - so at least when you converse with your precious data\n",
    "engineers, you know what they are talking about instead of acting like you know.\n",
    "\n",
    "Intuitively and simply put (data engineers don't bash me), the data operations\n",
    "phase involves **collecting**, **integrating**, **transforming**, and **managing\n",
    "data**. Here, we identify the **data sources**, ensure their **quality**,\n",
    "**preprocess** them for **downstream tasks such as machine learning**, and set\n",
    "up the **operations** needed for **efficient** handling. Additionally, this\n",
    "phase involves setting up sophisticated data pipelines that ensure efficient,\n",
    "reliable, and scalable data flow across different stages, from ingestion to\n",
    "modeling. This involves leveraging technologies like distributed systems and\n",
    "cloud services to manage the vast volumes of data that modern enterprises\n",
    "typically handle.\n",
    "\n",
    "I won't act as if I know the in-depth details of data engineering (yes my data\n",
    "engineers helped me load terabytes of data for pretraining and without them I am\n",
    "jobless), but this post is to draw some reference from those who know. So, let's\n",
    "dive in.\n",
    "\n",
    "## Data Engineering In Machine Learning\n",
    "\n",
    "**Machine learning models** require require data. And from the GPT-2 paper named\n",
    "_Language Models are Unsupervised Multitask Learners_, the authors mentioned\n",
    "that one major key to the success of their model is the **quantity** and\n",
    "**quality** of the data used for training. They have to preprocess the data\n",
    "before feeding it into the model, and imagine the amount of data engineering\n",
    "work behind the scenes in order to automate and scale the process.\n",
    "\n",
    "From data collection, data preprocessing, feature engineering, data\n",
    "transformation, data validation, data versioning, and data pipeline setup, data\n",
    "engineering is not just about cleaning the data, you have to ensure that the\n",
    "data is accessible easily and efficiently.\n",
    "\n",
    "They establish **[data pipelines](https://en.wikipedia.org/wiki/Data_pipeline)**\n",
    "that automate the flow of data from source to destination, allowing for\n",
    "continuous integration and real-time processing. So yes, not only is MLOps all\n",
    "the hype, but DataOps is also a critical part of the machine learning lifecycle.\n",
    "\n",
    "## A Naive DataOps Pipeline\n",
    "\n",
    "In this section we outline a very naive and simple workflow of a data\n",
    "engineering pipeline. This is meant to give you a high-level overview of the\n",
    "process, and by no means encapsulates the complexity of a real-world data\n",
    "engineering workflow. Things like big data paradigms like Hadoop, Spark, and\n",
    "distributed systems are not covered here.\n",
    "\n",
    "DataOps's iterative process consists of several stages:\n",
    "\n",
    "1. **[Data Collection](https://en.wikipedia.org/wiki/Data_collection)**:\n",
    "   Identifying the relevant data sources and collecting the data.\n",
    "\n",
    "2. **Data Ingestion/Integration**: This stage consists of two major parts:\n",
    "\n",
    "    - **[Data Extraction](https://en.wikipedia.org/wiki/Data_extraction)**:\n",
    "      Extracting the collected data from various sources.\n",
    "    - **[Data Loading](https://en.wikipedia.org/wiki/Data_loading)**: Loading\n",
    "      the extracted data into a centralized storage such as a data warehouse,\n",
    "      data lake, or lakehouse.\n",
    "\n",
    "3. **[Data Transformation](https://en.wikipedia.org/wiki/Data_transformation)**:\n",
    "   Transforming the data into a format suitable for downstream tasks. This stage\n",
    "   may include cleaning, aggregating, or restructuring the data.\n",
    "\n",
    "4. **Data Validation**: A crucial step to ensure the accuracy and quality of the\n",
    "   data. Validation techniques can be applied in parallel with the data\n",
    "   transformation stage or immediately after loading the raw data. By performing\n",
    "   this step, one guarantees that the data adheres to the defined standards and\n",
    "   is suitable for further processing and analysis.\n",
    "\n",
    "5. **CI/CD Integration**: Implementing\n",
    "   [Continuous Integration/Continuous Deployment (CI/CD)](https://en.wikipedia.org/wiki/CI/CD)\n",
    "   to automate and streamline the data workflow for the aforementioned stages.\n",
    "\n",
    "These stages can be organized into a\n",
    "**[data pipeline](<https://en.wikipedia.org/wiki/Pipeline_(computing)>)**. A\n",
    "data pipeline is a set of data processing elements connected in series, where\n",
    "the output of one element becomes the input of the next. Elements may be\n",
    "executed in parallel or series, and the pipeline ensures that data transitions\n",
    "smoothly through the stages, maintaining consistency, efficiency, and\n",
    "scalability.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "Here's a high-level overview of the data engineering workflow, in the form of a\n",
    "diagram:\n",
    "\n",
    "```{figure} ./assets/dataops-lifecycle.gif\n",
    "---\n",
    "name: ml-lifecycle-03-dataops-lifecycle\n",
    "height: 400px\n",
    "---\n",
    "\n",
    "DataOps Lifecycle.\n",
    "\n",
    "**Image Credit:**\n",
    "[Deepak](https://www.linkedin.com/in/mr-deepak-bhardwaj)\n",
    "```\n",
    "\n",
    "We will now give a grossly simplified example of a data engineering workflow.\n",
    "This by no means represent the actual (and often much more complex) workflow in\n",
    "the industry, however, it should give you a good idea of the general process.\n",
    "\n",
    "### Staging/Experiment/Development\n",
    "\n",
    "**Legends**:\n",
    "\n",
    "-   Staging: The staging environment is where the code is deployed for testing\n",
    "    purposes. It is a replica of the production environment where the code is\n",
    "    tested before it is deployed to production.\n",
    "-   Production: The production environment is where the code is deployed for\n",
    "    production use. It is the environment where the code is used by the end\n",
    "    users.\n",
    "\n",
    "There are many more environments in a typical software development lifecycle,\n",
    "like QA, UAT, etc. However, for the sake of simplicity, we will focus on the\n",
    "staging and production environments.\n",
    "\n",
    "#### Step 1. Data Extraction\n",
    "\n",
    "-   Source data is identified and extracted from various internal and external\n",
    "    databases and APIs.\n",
    "-   Data is extracted using either full or incremental refreshes, depending on\n",
    "    the source system.\n",
    "-   The data can be extracted via pure code level such as using Python, or using\n",
    "    modern tech stacks such as Airbyte, FiveTran or orchestration tools such as\n",
    "    Airflow.\n",
    "\n",
    "A sample python DAG for this step is as follows:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, Any\n",
    "\n",
    "class Config(BaseModel):\n",
    "    ...\n",
    "\n",
    "class Logger(BaseModel):\n",
    "    ...\n",
    "\n",
    "class Connection(BaseModel):\n",
    "    ...\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Extract:\n",
    "    def __init__(\n",
    "        self, cfg: Config, logger: Logger, connection: Connection, metadata: Metadata\n",
    "    ) -> None:\n",
    "        self.cfg = cfg\n",
    "        self.logger = logger\n",
    "        self.connection = connection\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def extract_from_connection(self) -> None:\n",
    "        \"\"\"Extract data from data warehouse.\"\"\"\n",
    "        self.logger.info(f\"Extracting data from {self.connection.name}...\")\n",
    "\n",
    "    def run(self) -> None:\n",
    "        \"\"\"Run the extract process.\"\"\"\n",
    "        self.extract_from_connection()\n",
    "```\n",
    "\n",
    "where\n",
    "\n",
    "-   `cfg`, `logger` and `metadata` are the configuration, logger and metadata\n",
    "    objects respectively.\n",
    "-   `connection` is the connection object that represents the data source. It\n",
    "    can be API, database, etc.\n",
    "\n",
    "#### Step 2. Data Loading to Staging Lake\n",
    "\n",
    "Let's assume that we want to extract our data from a remote API and load it to a\n",
    "staging layer in Google Cloud Storage (GCS), where the GCS serves as the staging\n",
    "data lake.\n",
    "\n",
    "Let's have a look a templated DAG for this step.\n",
    "\n",
    "First, we define a base class for the load process.\n",
    "\n",
    "```python\n",
    "class Validator(ABC):\n",
    "    @abstractmethod\n",
    "    def validate(self, data: Any) -> bool:\n",
    "        \"\"\"Validates the data. Returns True if valid, False otherwise.\"\"\"\n",
    "\n",
    "class DVC(ABC):\n",
    "    @abstractmethod\n",
    "    def commit(self, message: str) -> None:\n",
    "        \"\"\"Commits the changes to the DVC repository.\"\"\"\n",
    "\n",
    "class Load(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: Config,\n",
    "        logger: Logger,\n",
    "        metadata: Metadata,\n",
    "        dvc: Optional[DVC] = None,\n",
    "        validator: Optional[Validator] = None,\n",
    "    ) -> None:\n",
    "        self.cfg = cfg\n",
    "        self.logger = logger\n",
    "        self.metadata = metadata\n",
    "        self.dvc = dvc\n",
    "        self.validator = validator\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_to_staging(self) -> None:\n",
    "        \"\"\"Load data to staging.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_to_production(self) -> None:\n",
    "        \"\"\"Load data to production.\"\"\"\n",
    "\n",
    "    def run(self, is_staging: bool = True) -> None:\n",
    "        \"\"\"Run the load stage.\"\"\"\n",
    "        self.logger.info(\"Running load stage\")\n",
    "        self.load_to_staging() if is_staging else self.load_to_production()\n",
    "        self.logger.info(\"Load stage complete\")\n",
    "```\n",
    "\n",
    "Then, we define a class that inherits from the base class.\n",
    "\n",
    "```python\n",
    "class LoadToLake(Load):\n",
    "    def load_to_staging(self) -> None:\n",
    "        \"\"\"Load data to staging.\"\"\"\n",
    "        self.logger.info(f\"Loading data to staging {self.cfg.staging_lake}\")\n",
    "\n",
    "    def load_to_production(self) -> None:\n",
    "        \"\"\"Load data to production.\"\"\"\n",
    "        self.logger.info(f\"Loading data to production {self.cfg.production_lake}\")\n",
    "```\n",
    "\n",
    "-   Extracted data from step 1 is loaded into a dedicated **staging** area\n",
    "    within [Google Cloud Storage (GCS)](https://cloud.google.com/storage). This\n",
    "    process serves as the initial raw data checkpoint, providing an\n",
    "    **immutable** storage layer for unprocessed data. This approach to storing\n",
    "    raw data helps maintain data integrity throughout the pipeline.\n",
    "-   The data is stored in a structured format, for instance, in the form of:\n",
    "\n",
    "    ```text\n",
    "    staging/raw_{table_name}/created_at={YYYY-MM-DD:HH:MM:SS:MS}`\n",
    "    ```\n",
    "\n",
    "    where\n",
    "\n",
    "    -   `staging` is the staging layer in GCS.\n",
    "    -   `raw_table_name` is the name of the table that you intend to store\n",
    "        later. Simply put, it is the name of the dataset.\n",
    "\n",
    "    This structure allows for easy tracking of the data's origin and timestamp,\n",
    "    adhering to the common\n",
    "    **[partitioning scheme](https://cloud.google.com/bigquery/docs/partitioned-tables)**\n",
    "    used in data storage. We can also add commit hash if need be, but as we\n",
    "    shall see shortly, if we have a versioning tool like DVC, we can use that to\n",
    "    maintain the data's lineage.\n",
    "\n",
    "-   Even though the data is stored such that we can easily reference the data's\n",
    "    origin and timestamp, there is a need to maintain a detailed record of the\n",
    "    data's **lineage**. This is where the **metadata** comes in. The metadata\n",
    "    contains information such as the data's origin, timestamp, and other\n",
    "    essential details such as the data's schema.\n",
    "\n",
    "    Furthermore, modern data versioning tools such as\n",
    "    [DVC (Data Version Control)](https://dvc.org) can be used to maintain a\n",
    "    detailed record of the data's lineage, ensuring that changes to the data can\n",
    "    be tracked and managed in a reproducible manner.\n",
    "\n",
    "What is the rationale in storing the data in GCS?\n",
    "\n",
    "-   **Raw Data Checkpoint**: GCS serves as a storage layer for raw, unprocessed\n",
    "    data. This creates a checkpoint where the data is unaltered and can be\n",
    "    reverted to if needed.\n",
    "\n",
    "-   **Flexibility**: Storing data in GCS provides flexibility in data formats\n",
    "    and allows for decoupling of storage and compute. It can serve various\n",
    "    downstream applications that might require raw data.\n",
    "\n",
    "-   **Cost-Effective**: GCS typically provides a more cost-effective solution\n",
    "    for storing large volumes of data, especially when long-term storage is\n",
    "    needed.\n",
    "\n",
    "-   **Immutable Storage Layer**: By providing an immutable storage layer, GCS\n",
    "    ensures that the original raw data remains unaltered, maintaining data\n",
    "    integrity.\n",
    "\n",
    "-   **Interoperability**: GCS can serve multiple environments and tools, not\n",
    "    just BigQuery, so it's a general-purpose storage solution.\n",
    "\n",
    "#### Step 3. Loading Data to Staging Warehouse\n",
    "\n",
    "Now, once we have the data in the staging GCS, we can load it to staging\n",
    "BigQuery. This is done using the following.\n",
    "\n",
    "```python\n",
    "class LoadToWarehouse(Load):\n",
    "    def load_to_staging(self) -> None:\n",
    "        \"\"\"Load data to staging.\"\"\"\n",
    "        self.logger.info(f\"Loading data to staging {self.cfg.staging_warehouse}\")\n",
    "\n",
    "    def load_to_production(self) -> None:\n",
    "        \"\"\"Load data to production.\"\"\"\n",
    "        self.logger.info(f\"Loading data to production {self.cfg.production_warehouse}\")\n",
    "```\n",
    "\n",
    "-   The data in the staging GCS is loaded into Google BigQuery for more advanced\n",
    "    processing and analysis. We are assuming the data is structured and ready\n",
    "    for loading into BigQuery.\n",
    "-   Data can be loaded using both write and append modes, allowing for\n",
    "    incremental refreshes.\n",
    "-   Metadata such as `created_at` and `updated_at` timestamps are added to\n",
    "    maintain a detailed record of the data's lineage.\n",
    "-   As BigQuery's primary key system may have limitations, one needs to be\n",
    "    careful to ensure that there are no **duplicate** records in the data.\n",
    "-   The path name of the data in GCS is used as the table name in BigQuery. For\n",
    "    instance, if the data is stored in the following path:\n",
    "\n",
    "    ```text\n",
    "    staging/raw_{table_name}/created_at={YYYY-MM-DD:HH:MM:SS:MS}`\n",
    "    ```\n",
    "\n",
    "    then the table name in BigQuery will be `staging/raw_{table_name}`.\n",
    "\n",
    "What is the rationale in storing the data in BigQuery, the staging analytics\n",
    "layer?\n",
    "\n",
    "-   **Advanced Processing & Analysis**: BigQuery is designed for performing\n",
    "    complex queries and analytics. Loading data into BigQuery allows you to\n",
    "    leverage its full analytical capabilities.\n",
    "\n",
    "-   **Optimized Query Performance**: BigQuery provides optimized query\n",
    "    performance, making it suitable for interactive and ad-hoc queries,\n",
    "    dashboards, and reports.\n",
    "\n",
    "#### Step 4. Data Validation After Extraction and Load\n",
    "\n",
    "-   Once the data is extracted and loaded into the staging area in GCS or\n",
    "    BigQuery, a preliminary data validation process is conducted.\n",
    "-   This may include checking for the presence and correctness of key fields,\n",
    "    ensuring the right data types, checking data ranges, verifying data\n",
    "    integrity, and so on.\n",
    "-   If the data fails the validation, appropriate error handling procedures\n",
    "    should be implemented. This may include logging the error, sending an alert,\n",
    "    or even stopping the pipeline based on the severity of the issue.\n",
    "\n",
    "Recall earlier in our `Load` base class, there is a `validator` in the\n",
    "constructor? This is where we can specify the validator to use for the data\n",
    "validation process.\n",
    "\n",
    "We can define a validation interface (an abstract class in Python) that will\n",
    "enforce the structure of all validators.\n",
    "\n",
    "```python\n",
    "class Validator(ABC):\n",
    "    @abstractmethod\n",
    "    def validate(self, data: Any) -> bool:\n",
    "        \"\"\"Validates the data. Returns True if valid, False otherwise.\"\"\"\n",
    "```\n",
    "\n",
    "Then we implement our own validator by inheriting from the `Validator`\n",
    "interface.\n",
    "\n",
    "```python\n",
    "class MySpecificValidator(Validator):\n",
    "    def validate(self, data: Any) -> bool:\n",
    "        \"\"\"Add logic here to check data's correctness, data types, etc.\"\"\"\n",
    "        return is_valid\n",
    "```\n",
    "\n",
    "Within the `Load` class, you can call the `validate` method of the provided\n",
    "`validator` instance at the appropriate stage of loading. Here's an example that\n",
    "adds a validation step after loading to staging:\n",
    "\n",
    "```python\n",
    "class Load(ABC):\n",
    "    def load_to_staging(self) -> None:\n",
    "        \"\"\"Load data to staging.\"\"\"\n",
    "        # Loading logic here...\n",
    "        self.logger.info(f\"Loading data to staging {self.cfg.staging_dir}\")\n",
    "\n",
    "        # Validate the data\n",
    "        if self.validator:\n",
    "            is_valid = self.validator.validate(data) # assuming data is what you want to validate\n",
    "            if not is_valid:\n",
    "                self.logger.error(\"Validation failed for staging data\")\n",
    "                # Additional error handling logic like raise etc.\n",
    "                return\n",
    "\n",
    "        self.logger.info(\"Load stage to staging complete\")\n",
    "```\n",
    "\n",
    "It's common in the industry to see a hybrid approach where basic validation is\n",
    "performed at the staging lake layer (GCS), followed by more validation once the\n",
    "data is loaded into staging warehouse layer (BigQuery). For example, some\n",
    "obvious bad data can be filtered out at the GCS layer, while more complex and\n",
    "specific validation can be done at the BigQuery layer.\n",
    "\n",
    "#### Step 5. Data Transformation\n",
    "\n",
    "-   In this step, the raw data from the staging area undergoes a series of\n",
    "    transformation processes to be refined into a format suitable for downstream\n",
    "    use cases, including analysis and machine learning model training. These\n",
    "    transformations might involve operations such as:\n",
    "\n",
    "    -   **Data Cleaning**: Identifying and correcting (or removing) errors and\n",
    "        inconsistencies in the data. This might include handling missing values,\n",
    "        eliminating duplicates, and dealing with outliers.\n",
    "\n",
    "    -   **Joining Data**: Combining related data from different sources or\n",
    "        tables to create a cohesive, unified dataset.\n",
    "\n",
    "    -   **Aggregating Data**: Grouping data by certain variables and calculating\n",
    "        aggregate measures (such as sums, averages, maximum or minimum values)\n",
    "        over each group.\n",
    "\n",
    "    -   **Structuring Data**: Formatting and organizing the data in a way that's\n",
    "        appropriate for the intended use cases. This might involve creating\n",
    "        certain derived variables, transforming data types, or reshaping the\n",
    "        data structure.\n",
    "\n",
    "-   It's important to note that the transformed data at this stage is intended\n",
    "    to be a high-quality, flexible data resource that can be leveraged across a\n",
    "    range of downstream use cases - not just for machine learning model training\n",
    "    and inference. For example, it might also be used for business reporting,\n",
    "    exploratory data analysis, or statistical studies.\n",
    "\n",
    "By maintaining a general-purpose transformed data layer, the pipeline ensures\n",
    "that a broad array of users and applications can benefit from the data cleaning\n",
    "and transformation efforts, enhancing overall data usability and efficiency\n",
    "within the organization.\n",
    "\n",
    "```python\n",
    "class Transformation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        cfg: Config,\n",
    "        logger: Logger,\n",
    "        metadata: Metadata,\n",
    "        validator: Validator,\n",
    "    ) -> None:\n",
    "        self.cfg = cfg\n",
    "        self.logger = logger\n",
    "        self.metadata = metadata\n",
    "        self.validator = validator\n",
    "\n",
    "    def clean_data(self, data: Any) -> Any:\n",
    "        \"\"\"Identify and correct errors and inconsistencies in the data.\"\"\"\n",
    "        self.logger.info(\"Logic for handling missing values, duplicates, outliers, etc.\")\n",
    "        return data\n",
    "\n",
    "    def join_data(self, data1: Any, data2: Any) -> Any:\n",
    "        \"\"\"Combine related data from different sources or tables.\"\"\"\n",
    "        self.logger.info(\"Logic for joining data from multiple sources\")\n",
    "        return joined_data\n",
    "\n",
    "    def aggregate_data(\n",
    "        self, data: Any, grouping_variables: List[str], aggregation_functions: Dict[Any, Any]\n",
    "    ) -> Any:\n",
    "        \"\"\"Group data and calculate aggregate measures.\"\"\"\n",
    "        self.logger.info(\"Logic for aggregating data\")\n",
    "        return aggregated_data\n",
    "\n",
    "    def structure_data(self, data: Any) -> Any:\n",
    "        \"\"\"Format and organize the data for intended use cases.\"\"\"\n",
    "        self.logger.info(\"Logic for creating derived variables, transforming data types, reshaping structure, etc.\")\n",
    "        return structured_data\n",
    "\n",
    "    def transform(self, data: Any) -> Any:\n",
    "        \"\"\"Execute the entire transformation process.\"\"\"\n",
    "        self.logger.info(\"Starting data transformation\")\n",
    "\n",
    "        data = self.clean_data(data)\n",
    "        # If more than one data source needs to be joined\n",
    "        # data = self.join_data(data1, data2)\n",
    "        data = self.aggregate_data(data, grouping_variables, aggregation_functions)\n",
    "        data = self.structure_data(data)\n",
    "\n",
    "        self.logger.info(\"Data transformation complete\")\n",
    "\n",
    "        if self.validator:\n",
    "            is_valid = self.validator.validate(data)\n",
    "            if not is_valid:\n",
    "                self.logger.error(\"Validation failed for transformed data\")\n",
    "                return # or raise\n",
    "\n",
    "        return data\n",
    "```\n",
    "\n",
    "#### Step 6. Data Validation After Transformation\n",
    "\n",
    "In step 5, we have another `validator` instance that validates the transformed\n",
    "data. The `validator` instance is passed to the `Transformation` class in the\n",
    "constructor.\n",
    "\n",
    "```python\n",
    "if self.validator:\n",
    "    is_valid = self.validator.validate(data)\n",
    "    if not is_valid:\n",
    "        self.logger.error(\"Validation failed for transformed data\")\n",
    "        return\n",
    "```\n",
    "\n",
    "-   After the data transformation process, another round of validation is\n",
    "    carried out on the transformed data.\n",
    "-   This may involve checking the output of the transformation against expected\n",
    "    results, ensuring the data structure conforms to the target schema, and\n",
    "    performing statistical checks (e.g., distributions, correlations, etc.).\n",
    "-   If the transformed data fails the validation, appropriate steps are taken\n",
    "    just like after extraction.\n",
    "\n",
    "By now, we should already be able to tell that the data validation process is an\n",
    "integral part of the data pipeline. It's not just a one-time check at the\n",
    "beginning of the pipeline, but rather a continuous process that occurs at\n",
    "multiple stages throughout the pipeline. Phew, so much work!\n",
    "\n",
    "#### Step 7. Load Transformed Data to Staging GCS and BigQuery\n",
    "\n",
    "-   After the data transformation and validation, the resulting data is loaded\n",
    "    back into the staging environment. This involves both Google Cloud Storage\n",
    "    (GCS) and BigQuery.\n",
    "\n",
    "    -   **Staging GCS**: The transformed data is saved back into a specific\n",
    "        location in the staging GCS. This provides a backup of the transformed\n",
    "        data and serves as an intermediate checkpoint before moving the data to\n",
    "        the production layer.\n",
    "\n",
    "    -   **Staging BigQuery**: The transformed data is also loaded into a\n",
    "        specific table in the staging area in BigQuery. Loading the transformed\n",
    "        data into BigQuery allows for quick and easy analysis and validation of\n",
    "        the transformed data, thanks to BigQuery's capabilities for handling\n",
    "        large-scale data and performing fast SQL-like queries.\n",
    "\n",
    "-   This step of loading the transformed data back into the staging GCS and\n",
    "    BigQuery is very similar to the earlier loading step. The `Load` class can\n",
    "    be reused for this step as well.\n",
    "\n",
    "#### Step 8. (Optional) Writing a DAG to Automate the Pipeline\n",
    "\n",
    "-   The whole step from 1 to 7 can be wrapped in a DAG.\n",
    "-   This means you can use things like Airflow to orchestrate the whole process.\n",
    "\n",
    "We can automate the code without a DAG as well, so why DAG? Here's some reasons.\n",
    "\n",
    "```{list-table} Why DAG?\n",
    ":header-rows: 1\n",
    ":widths: 25 75\n",
    ":name: ml-lifecycle-03-why-dag\n",
    "\n",
    "-   -   Feature\n",
    "    -   Description\n",
    "-   -   Scheduling and Automation\n",
    "    -   Airflow provides built-in scheduling options. You can define complex\n",
    "        schedules in a standard way, allowing tasks to be run at regular\n",
    "        intervals, on specific dates, or in response to specific triggers.\n",
    "        Managing scheduling in a custom Python script can be more\n",
    "        labor-intensive and error-prone.\n",
    "-   -   Parallel Execution and Resource Management\n",
    "    -   Airflow allows for parallel execution of tasks that don't depend on each\n",
    "        other. It can efficiently manage resources and distribute tasks across\n",
    "        different workers, something that can be complex and time-consuming to\n",
    "        implement in a custom Python pipeline.\n",
    "-   -   Monitoring and Logging\n",
    "    -   Airflow provides a user-friendly web interface that includes detailed\n",
    "        logs, visualizations of DAG runs, task status information, and more.\n",
    "        Building such comprehensive monitoring and logging capabilities into a\n",
    "        custom Python pipeline would require significant development effort.\n",
    "-   -   Error Handling and Retries\n",
    "    -   Airflow offers standard mechanisms for handling task failures, including\n",
    "        retries with backoff, notifications, etc. Implementing similar robust\n",
    "        error handling in a custom Python pipeline might require substantial\n",
    "        work.\n",
    "-   -   Integration with Various Tools\n",
    "    -   Airflow has a rich ecosystem of operators that facilitate integration\n",
    "        with various data sources, platforms, and tools. Implementing such\n",
    "        integrations manually in a custom Python script can be time-consuming\n",
    "        and less flexible.\n",
    "-   -   Scalability\n",
    "    -   Airflow is designed to run on distributed systems, making it easier to\n",
    "        scale up as data and processing requirements grow. Building scalability\n",
    "        into a custom Python pipeline might require extensive architectural\n",
    "        changes.\n",
    "```\n",
    "\n",
    "Airflow however is a complex tool, and if the use case is simple, it might be\n",
    "overkill - or one can argue if use case is simple, then the underlying DAG might\n",
    "be simple as well. One key thing of Airflow is the observability and monitoring\n",
    "capabilities it provides, which is crucial. Imagine a cronjob failing and you\n",
    "have no idea why, and you have to dig through logs to find out what happened.\n",
    "\n",
    "#### Step 9. Containerize the DAG\n",
    "\n",
    "Once your DAG or python code is ready, we can containerize it and deploy it.\n",
    "\n",
    "A templated Dockerfile can look like this:\n",
    "\n",
    "```dockerfile\n",
    "ARG PYTHON_VERSION=3.9\n",
    "ARG CONTEXT_DIR=.\n",
    "ARG HOME_DIR=/pipeline-dataops\n",
    "ARG VENV_DIR=/opt\n",
    "ARG VENV_NAME=venv\n",
    "\n",
    "FROM python:${PYTHON_VERSION}-slim-buster as builder\n",
    "\n",
    "ARG CONTEXT_DIR\n",
    "ARG HOME_DIR\n",
    "ARG VENV_DIR\n",
    "ARG VENV_NAME\n",
    "\n",
    "WORKDIR ${HOME_DIR}\n",
    "\n",
    "RUN apt-get update && \\\n",
    "    apt-get install -y --no-install-recommends \\\n",
    "    build-essential \\\n",
    "    python3-dev && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN python -m venv ${VENV_DIR}/${VENV_NAME}\n",
    "ENV PATH=\"${VENV_DIR}/${VENV_NAME}/bin:$PATH\"\n",
    "\n",
    "ARG REQUIREMENTS=requirements.txt\n",
    "ARG REQUIREMENTS_DEV=requirements_dev.txt\n",
    "COPY ./${CONTEXT_DIR}/${REQUIREMENTS} .\n",
    "COPY ./${CONTEXT_DIR}/${REQUIREMENTS_DEV} .\n",
    "\n",
    "RUN python3 -m pip install --upgrade pip && \\\n",
    "    python3 -m pip install --no-cache-dir -r ${REQUIREMENTS} && \\\n",
    "    python3 -m pip install --no-cache-dir -r ${REQUIREMENTS_DEV} && \\\n",
    "    pip install -U gaohn-common-utils && \\\n",
    "    pip install pydantic==2.0b3\n",
    "\n",
    "# This is the real runner for my app\n",
    "FROM python:${PYTHON_VERSION}-slim-buster as runner\n",
    "\n",
    "ARG CONTEXT_DIR\n",
    "ARG HOME_DIR\n",
    "ARG VENV_DIR\n",
    "ARG VENV_NAME\n",
    "\n",
    "# Copy from builder image\n",
    "COPY --from=builder ${VENV_DIR}/${VENV_NAME} ${VENV_DIR}/${VENV_NAME}\n",
    "COPY --from=builder ${HOME_DIR} ${HOME_DIR}\n",
    "\n",
    "# Set work dir again to the pipeline_training subdirectory\n",
    "# Set the working directory inside the Docker container\n",
    "WORKDIR ${HOME_DIR}\n",
    "\n",
    "RUN apt-get update && \\\n",
    "    apt-get install -y --no-install-recommends jq && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "ENV PATH=\"${VENV_DIR}/${VENV_NAME}/bin:$PATH\"\n",
    "\n",
    "ARG GIT_COMMIT_HASH\n",
    "ENV GIT_COMMIT_HASH=${GIT_COMMIT_HASH}\n",
    "\n",
    "# Copy the rest of the application's code\n",
    "COPY ${CONTEXT_DIR}/conf ${HOME_DIR}/conf\n",
    "COPY ${CONTEXT_DIR}/metadata ${HOME_DIR}/metadata\n",
    "COPY ${CONTEXT_DIR}/schema ${HOME_DIR}/schema\n",
    "COPY ${CONTEXT_DIR}/pipeline_dataops ${HOME_DIR}/pipeline_dataops\n",
    "COPY ${CONTEXT_DIR}/pipeline.py ${HOME_DIR}/pipeline.py\n",
    "COPY ${CONTEXT_DIR}/scripts/docker/entrypoint.sh ${HOME_DIR}/scripts/docker/entrypoint.sh\n",
    "\n",
    "RUN chmod -R +x ${HOME_DIR}/scripts/docker\n",
    "\n",
    "CMD [\"scripts/docker/entrypoint.sh\"]\n",
    "```\n",
    "\n",
    "#### Step 10. Deploy the DAG (Staging Environment)\n",
    "\n",
    "After containerizing the DAG, we can deploy it. For instance, we can deploy it\n",
    "to a Kubernetes cluster on a `CronJob` resource.\n",
    "\n",
    "We will not go into the details of how to deploy a DAG to a somewhere like a\n",
    "Kubernetes cluster here - it is out of scope and can be a whole topic on its\n",
    "own.\n",
    "\n",
    "#### Step 11. Trigger the DAG as part of a CI/CD pipeline\n",
    "\n",
    "```{list-table} CI/CD Pipeline\n",
    ":header-rows: 1\n",
    ":name: ml-lifecycle-03-ci-cd-pipeline\n",
    "\n",
    "-   -   Step\n",
    "    -   Description\n",
    "    -   Action\n",
    "    -   Rationale\n",
    "-   -   Version Control\n",
    "    -   All code related to data extraction, transformation, and loading (ETL),\n",
    "        as well as any related testing code and configuration files, is stored\n",
    "        in a version control system like DVC and Git.\n",
    "    -   The developer makes and commits the necessary code changes to the\n",
    "        version control system, such as Git.\n",
    "    -   Facilitates collaboration, versioning, and tracking changes. This is\n",
    "        usually the first trigger in the CI/CD pipeline.\n",
    "-   -   Trigger CI/CD Pipeline for Development\n",
    "    -   The commit automatically triggers the development Continuous\n",
    "        Integration/Continuous Deployment (CI/CD) pipeline.\n",
    "    -   The commit automatically triggers the development CI/CD pipeline.\n",
    "    -   Enables automated building and testing, ensuring that changes are\n",
    "        immediately evaluated for compatibility and correctness.\n",
    "-   -   Continuous Integration\n",
    "    -   When changes are pushed to the version control system, this triggers the\n",
    "        Continuous Integration process. Things like linting, type checking, unit\n",
    "        tests, etc. are run.\n",
    "    -   When changes are pushed to the version control system, this triggers the\n",
    "        Continuous Integration process. Tools such as GitHub Actions can be used\n",
    "        to automate this process.\n",
    "    -   The new code is merged with the main code base and automated tests are\n",
    "        run to ensure that the changes do not break existing functionality.\n",
    "-   -   Continuous Integration: Unit and Integration Tests\n",
    "    -   The code changes are subjected to unit tests and integration tests.\n",
    "    -   The code changes are subjected to unit tests (testing individual\n",
    "        components) and integration tests (testing interactions between\n",
    "        components).\n",
    "    -   Ensures that the code performs as expected at both the component and\n",
    "        system levels, minimizing the risk of introducing new bugs.\n",
    "-   -   Continuous Integration: Build Image of the DAG\n",
    "    -   Once the code level changes passed the unit and integration tests. An\n",
    "        image of the updated DAG, containing all necessary dependencies and\n",
    "        configurations, is built.\n",
    "    -   Once the code level changes passed the unit and integration tests, an\n",
    "        image of the updated DAG is built.\n",
    "    -   The image simplifies deployment and scaling by encapsulating the entire\n",
    "        application into a single deployable unit. At this stage, the image is\n",
    "        test-run to ensure it works as expected.\n",
    "-   -   Continuous Integration: System Tests\n",
    "    -   The whole Directed Acyclic Graph (DAG), packaged into an image, is\n",
    "        tested to ensure that the entire pipeline, with the updated\n",
    "        transformation logic, provides the correct output.\n",
    "    -   The whole DAG, packaged into an image, is tested.\n",
    "    -   Validates that the entire system functions correctly, confirming that\n",
    "        changes did not inadvertently disrupt other parts of the pipeline.\n",
    "        We usually do system test separately from unit and integration tests\n",
    "        because it might require more resources and time.\n",
    "-   -   Continuous Deployment: Push Image to (Staging) Artifacts Registry\n",
    "    -   The built image is pushed to a designated artifacts registry, such as\n",
    "        Docker Hub or a private registry.\n",
    "    -   The built image is pushed to a designated artifacts registry.\n",
    "    -   Stores the deployable image in a centralized location, making it easily\n",
    "        accessible for subsequent deployment stages. Allows for version control\n",
    "        and rollback capabilities of deployed images.\n",
    "-   -   Continuous Deployment: Deploy Image to Staging Environment\n",
    "    -   The image is deployed to the staging environment, where it is tested to\n",
    "        ensure that it functions as expected.\n",
    "    -   The image is deployed to the staging environment.\n",
    "    -   Validates that the image is deployable and performs as expected in a\n",
    "        production-like environment.\n",
    "-   -   Continuous Deployment: Performance Tests\n",
    "    -   The data pipelines are tested under simulated production load.\n",
    "    -   The data pipelines are tested under simulated production load.\n",
    "    -   Identifies any performance bottlenecks or issues that could affect the\n",
    "        data pipeline's performance in production.\n",
    "-   -   Trigger Message to Pub/Sub\n",
    "    -   After successful deployment in the staging environment, a\n",
    "        message is triggered to a Pub/Sub system to notify other services or\n",
    "        systems.\n",
    "    -   A message is sent to a designated Pub/Sub service, such as Google Cloud\n",
    "        Pub/Sub or Apache Kafka, to signify the completion of deployment or to\n",
    "        kick off subsequent processes such as deployment to production\n",
    "        environment.\n",
    "    -   Ensures downstream systems or services are notified of the pipeline's\n",
    "        status, facilitating automated workflows and integrations across\n",
    "        different parts of the infrastructure. In our example, the trigger will\n",
    "        lead us to deploy the application to the production environment since\n",
    "        the data pipeline is well validated and tested in the staging\n",
    "        environment.\n",
    "```\n",
    "\n",
    "### Production Layer\n",
    "\n",
    "#### Step 1. Triggering the Production Deployment Pipeline\n",
    "\n",
    "-   **Action**:\n",
    "    -   A success message from the development pipeline in the staging\n",
    "        environment is sent to Pub/Sub, triggering the CI/CD pipeline. The logic\n",
    "        can be as simple as if the staging pipeline is successful, then trigger\n",
    "        the production pipeline.\n",
    "    -   The production deployment pipeline is initiated.\n",
    "    -   A manual approval process typically confirms the deployment to\n",
    "        production.\n",
    "-   **Rationale**:\n",
    "    -   Enables automatic transition from development to production stages.\n",
    "    -   Ensures human oversight and control over what gets deployed.\n",
    "\n",
    "#### Step 2. CI/CD: Deploy Image to Production Environment\n",
    "\n",
    "Basically, the same steps as in the staging environment, but this time the image\n",
    "is deployed to the production environment.\n",
    "\n",
    "We can have some additional steps such as monitoring and feedback loops.\n",
    "\n",
    "##### Monitoring and Alerting\n",
    "\n",
    "This step will not be covered in details as it is out of scope for this post,\n",
    "but will be discussed in the later stages. Monitoring is a big thing in Machine\n",
    "Learning because not only do we monitor for system health, we also monitor for\n",
    "data quality and data drift.\n",
    "\n",
    "Once deployed, the data pipelines are continuously monitored to ensure they are\n",
    "functioning correctly. This can involve tracking metrics such as data quality,\n",
    "pipeline performance, and resource usage. Any issues that arise can trigger\n",
    "alerts for immediate response.\n",
    "\n",
    "**Action**:\n",
    "\n",
    "-   Implement ongoing monitoring for data quality and data drift.\n",
    "\n",
    "**Rationale**:\n",
    "\n",
    "-   Ensures continued adherence to quality standards.\n",
    "-   Quickly detects and alerts to any changes in the data distribution, which\n",
    "    could impact model performance or other downstream applications.\n",
    "\n",
    "##### Feedback Loop\n",
    "\n",
    "This refers to insights from monitoring and any errors encountered in production\n",
    "are fed back into the development process, leading to new iterations of\n",
    "development, testing, and deployment.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Typically, the movement of data from the staging layer to the production layer\n",
    "happens once the data has been cleaned, transformed, validated, and is deemed\n",
    "ready for use in downstream applications such as machine learning model\n",
    "training, analytics, reporting, etc. The transformed data is first validated to\n",
    "ensure that it meets the required quality standards. If the validation is\n",
    "successful, the data is moved to the production layer. The goal is to only\n",
    "expose clean, validated, and reliable data to end users or downstream\n",
    "applications.\n",
    "\n",
    "Once the data has passed both rounds of validation, it can be loaded into the\n",
    "production layer in both GCS and BigQuery. At this point, the data is ready for\n",
    "downstream use in tasks such as model training and inference.\n",
    "\n",
    "In the context of ML, these steps form the beginning part of our pipeline, where\n",
    "data is extracted, cleaned, and made ready for use in our ML models. Each step\n",
    "is designed to ensure the integrity and usability of the data, from extraction\n",
    "to querying for model training and inference.\n",
    "\n",
    "As a reminder, this is highly simplified and the actual process can be much more\n",
    "complex. For example, we simply assumed GCS and BigQuery, but in reality, you\n",
    "might have multiple data sources and destinations and even multiple data lakes\n",
    "and warehouses. The key is to understand the principles and adapt them to your\n",
    "needs.\n",
    "\n",
    "## The Evolution of Data Engineering (Don't Quote Me On This!)\n",
    "\n",
    "With the basic understanding of **Data Engineering** and its essential role in\n",
    "**Machine Learning**, it's important to recognize the evolution of data handling\n",
    "practices. Traditional **ETL (Extract, Transform, Load)** methodologies have\n",
    "long been the backbone of data pipeline design. They set the stage for\n",
    "collecting, processing, and storing data in a structured manner.\n",
    "\n",
    "However, the modern era of data-driven applications demands a more agile and\n",
    "responsive approach. This is where **DataOps**, encompassing principles of\n",
    "**Continuous Integration/Continuous Deployment (CI/CD)**, comes into play. The\n",
    "process builds on the ETL framework but now with automation, collaboration,\n",
    "monitoring, and quality assurance.\n",
    "\n",
    "In traditional ETL or ELT processes, the main focus is on extracting data from\n",
    "various sources, transforming it into the required format, and then loading it\n",
    "into a target system. These processes are typically batch-oriented and can be\n",
    "run on schedules or triggered manually.\n",
    "\n",
    "In a CI/CD DataOps pipeline, the focus expands to the entire data lifecycle and\n",
    "emphasizes automation, continuous integration, and continuous deployment. This\n",
    "means that the process not only includes the basic ETL or ELT steps but also\n",
    "involves:\n",
    "\n",
    "-   **Continuous Integration**: Automating the process of integrating code\n",
    "    changes from multiple contributors into a shared repository, often followed\n",
    "    by automated building and testing.\n",
    "-   **Continuous Deployment**: Automating the process of deploying the\n",
    "    integrated and tested code to production environments, ensuring that the\n",
    "    data pipeline remains stable and updated.\n",
    "-   **Monitoring and Alerting**: Keeping track of the performance and health of\n",
    "    the data pipeline, triggering alerts if anomalies or issues are detected.\n",
    "-   **Testing and Quality Assurance**: Embedding rigorous testing within the\n",
    "    pipeline to ensure data quality, integrity, and compliance with business\n",
    "    rules.\n",
    "\n",
    "## The ETL/ELT Framework\n",
    "\n",
    "### ETL (Extract, Transform, Load)\n",
    "\n",
    "**ETL** is a process in data handling that involves three main stages:\n",
    "\n",
    "1. **Extract**: Gathering data from various sources.\n",
    "2. **Transform**: Processing this data to fit the desired format, usually\n",
    "   outside the target system. This might include cleaning, aggregating,\n",
    "   filtering, etc.\n",
    "3. **Load**: Finally, loading the transformed data into the destination data\n",
    "   warehouse or database.\n",
    "\n",
    "### ELT (Extract, Load, Transform)\n",
    "\n",
    "**ELT** is a variant of ETL, but with a different order of operations:\n",
    "\n",
    "1. **Extract**: Gathering data from various sources.\n",
    "2. **Load**: Loading the raw data into the destination system.\n",
    "3. **Transform**: Performing transformations within the target system itself,\n",
    "   utilizing the processing capabilities of modern data warehouses.\n",
    "\n",
    "### ELTL (Extract, Load, Transform, Load)\n",
    "\n",
    "This combination could represent a two-step process:\n",
    "\n",
    "1. **Extract**: Gathering data from various sources.\n",
    "2. **Load**: Loading the raw data into a staging area or temporary storage.\n",
    "3. **Transform**: Performing transformations within this temporary storage.\n",
    "4. **Load**: Loading the transformed data into the final destination, such as a\n",
    "   data warehouse or database.\n",
    "\n",
    "This approach might be beneficial when working with massive datasets, allowing\n",
    "for an initial raw data consolidation, followed by transformation and final\n",
    "loading into the target system.\n",
    "\n",
    "### Intuition on When to Use ETL vs ELT\n",
    "\n",
    "In certain scenarios, companies opt for the ELT (Extract, Load, Transform)\n",
    "process, particularly when dealing with complex and unstructured data. During\n",
    "the **extraction** phase, data is collected from various sources and then\n",
    "immediately **loaded** or dumped into a **data lake**, which is a storage\n",
    "repository that holds a vast amount of raw data in its native format.\n",
    "\n",
    "This approach has the advantage of quickly making the data available, preserving\n",
    "its raw state for future use. However, this raw, unstructured data can become\n",
    "unwieldy, particularly when dealing with large volumes.\n",
    "\n",
    "When it's time to analyze or utilize the data, it must be **extracted** again\n",
    "from the data lake. This is followed by the **transformation** phase, where the\n",
    "data is processed and converted into a structured format suitable for analysis.\n",
    "\n",
    "While the ELT paradigm allows for greater flexibility and the ability to\n",
    "accommodate diverse data types, it can lead to inefficiencies when searching\n",
    "through large and unstructured data sets within the data lake. The process of\n",
    "extracting and transforming data from the data lake can be time-consuming and\n",
    "resource-intensive, particularly if the data needs to be combed through\n",
    "extensively.\n",
    "\n",
    "In essence, the ELT approach with a data lake can be both a boon and a\n",
    "challenge. It enables faster data ingestion and provides a flexible repository\n",
    "for raw data, but the subsequent handling and processing of that data might\n",
    "require significant effort, especially when dealing with large quantities of\n",
    "unstructured information.\n",
    "\n",
    "### ETL versus ELT\n",
    "\n",
    "Here's a table that breaks down the comparison, advantages, and disadvantages of\n",
    "both ETL and ELT.\n",
    "\n",
    "| Criteria                | ETL                                                                                      | ELT                                                                                              |\n",
    "| ----------------------- | ---------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------ |\n",
    "| **Basic Process**       | Extract, Transform, Load                                                                 | Extract, Load, Transform                                                                         |\n",
    "| **Data Latency**        | May introduce delays, affecting real-time analysis; near-real-time possible with tooling | Often reduces delays between data collection and availability                                    |\n",
    "| **Scalability**         | Can be scalable with proper architecture and parallel processing                         | Typically leverages modern data warehouses for scalability                                       |\n",
    "| **Flexibility**         | Less adaptable to changing requirements; can be mitigated with design                    | More adaptable to changes in data structure or requirements                                      |\n",
    "| **Pipeline Complexity** | May involve complex transformations, increasing development and maintenance efforts      | Might simplify some aspects of the pipeline, depending on tools and requirements                 |\n",
    "| **Accessibility**       | May require specialized skills, limiting accessibility                                   | Might allow more team participation, especially with common languages like SQL                   |\n",
    "| **Advantages**          | Suitable for complex transformations with structured data; control over transformation   | Flexibility, scalability, and potentially reduced latency; useful for unstructured data handling |\n",
    "| **Disadvantages**       | Potential bottlenecks; complexity; potential rigidity                                    | Might lead to inefficiencies in processing unstructured data; simplicity is context-dependent    |\n",
    "| **Use Case**            | When precise control over transformations and structured data processing is needed       | When handling diverse or unstructured data, or when flexibility and scalability are priorities   |\n",
    "\n",
    "### Sample ELTL Pipeline\n",
    "\n",
    "#### Extract\n",
    "\n",
    "In the extraction phase, data is pulled from various sources which could be\n",
    "structured, semi-structured or unstructured, and could be located in databases,\n",
    "data lakes, data warehouses, or external APIs. The key is to capture the\n",
    "necessary data without losing or modifying any of the original data during the\n",
    "process.\n",
    "\n",
    "#### Data Analysis\n",
    "\n",
    "Post extraction, data analysis provides insights into the nature and quality of\n",
    "the data. This stage involves examining the distribution of the data,\n",
    "identifying potential anomalies or outliers, and assessing the overall data\n",
    "quality. Techniques such as descriptive statistics and data visualization are\n",
    "commonly used.\n",
    "\n",
    "It is important to note that this process is generally applicable and not\n",
    "specific to machine learning. The outcome of this stage guides the\n",
    "decision-making process for subsequent data cleaning and transformation tasks,\n",
    "thereby setting up a solid foundation for downstream tasks such as reporting,\n",
    "analytics, or model training.\n",
    "\n",
    "#### Validate Raw\n",
    "\n",
    "Validation of raw data ensures that the extracted data meets the requirements\n",
    "and constraints for the subsequent stages. It involves checking for data\n",
    "completeness, consistency, and accuracy. This stage might include checking if\n",
    "all expected data has been extracted, if there are any unexpected null or\n",
    "missing values, and if the data aligns with known constraints (like a field that\n",
    "should always be positive).\n",
    "\n",
    "#### Load\n",
    "\n",
    "The load stage involves transferring the extracted data into a target system for\n",
    "storage. The target system can be a data warehouse, a data lake, or a specific\n",
    "database depending on the use case. The focus during this phase is on efficiency\n",
    "and reliability, ensuring that all data is accurately loaded without disrupting\n",
    "existing data or processes.\n",
    "\n",
    "#### Transform\n",
    "\n",
    "The transformation phase involves changing the raw data into a format that is\n",
    "suitable for downstream tasks. This may include cleaning operations (like\n",
    "handling missing values or outliers), integrating data from different sources,\n",
    "aggregating or summarizing data, and converting data types. Additionally,\n",
    "feature engineering for machine learning tasks often takes place in this stage.\n",
    "\n",
    "#### Validate Transformed\n",
    "\n",
    "After transformation, the data needs to be validated again to ensure it meets\n",
    "the specific requirements for downstream tasks. This might involve checking the\n",
    "data against predefined rules or statistical properties (like a specific\n",
    "distribution), checking for unexpected null or missing values after\n",
    "transformation, or comparing a sample of the transformed data against the\n",
    "expected output.\n",
    "\n",
    "#### Load Transformed\n",
    "\n",
    "After the transformed data has been validated, it can be loaded into the target\n",
    "system for storage. This might be a data warehouse, a data lake, or a specific\n",
    "database depending on the use case.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The choice between ETL and ELT depends on various factors like data volume,\n",
    "real-time requirements, team skills, technology stack, and the nature of the\n",
    "transformations. ETL might be more suitable for complex transformations with\n",
    "structured data, while ELT might be preferred for more flexible, scalable\n",
    "handling of diverse or unstructured data.\n",
    "\n",
    "It's crucial to acknowledge that both ETL and ELT can be implemented effectively\n",
    "or poorly, depending on the specific context, tools, and design principles\n",
    "applied. Neither approach is universally superior, and the decision should be\n",
    "based on a comprehensive understanding of the project's unique requirements and\n",
    "constraints.\n",
    "\n",
    "## Identify and Scope the Data Source\n",
    "\n",
    "### Intuition (What comes before Data Extraction?)\n",
    "\n",
    "As we have seen in the pipeline and subsequently, the ELT/ETL framework, the\n",
    "first step is data extraction. However, before we can extract data, we need to\n",
    "first identify the data source and scope it. This is a critical step in the\n",
    "pipeline, as it lays the foundation for the rest of the pipeline. If the data\n",
    "source is not correctly identified and scoped, it could lead to a lot of wasted\n",
    "time and effort down the line.\n",
    "\n",
    "In what follows, we will discuss the steps involved in identifying and scoping\n",
    "the data source, as well as the tools and methods for extracting data from the\n",
    "source.\n",
    "\n",
    "Consequently, the correct identification and meticulous scoping of the data\n",
    "source form the bedrock of the entire pipeline.\n",
    "\n",
    "In what follows, we will discuss the steps involved in identifying and scoping\n",
    "the data source.\n",
    "\n",
    "### Steps to Identify and Scope the Data Source\n",
    "\n",
    "#### A. Define the Type of Data\n",
    "\n",
    "-   **Action**: Determine whether the data is numerical, categorical,\n",
    "    time-series, text-based, images, or audio.\n",
    "-   **Rationale**: Facilitates the formulation of the appropriate strategy for\n",
    "    data collection.\n",
    "\n",
    "#### B. Locate the Data\n",
    "\n",
    "-   **Action**: Identify the location, such as databases (SQL or NoSQL), APIs,\n",
    "    log files, Excel or CSV files, etc.\n",
    "-   **Rationale**: Enables the selection of the suitable tools and methods for\n",
    "    extraction.\n",
    "\n",
    "#### C. Assess Accessibility and Compliance\n",
    "\n",
    "-   **Action**: Understand permissions, authentication, privacy concerns, and\n",
    "    restrictions on data extraction.\n",
    "-   **Rationale**: Ensures adherence to legal and organizational policies.\n",
    "\n",
    "#### D. Gauge the Data Volume\n",
    "\n",
    "-   **Action**: Determine the size of the dataset.\n",
    "-   **Rationale**: Influences the choice of tools for extraction and impacts the\n",
    "    entire ML model design process.\n",
    "\n",
    "#### E. Understand Data Characteristics\n",
    "\n",
    "-   **Action**: Recognize and address special characteristics, including\n",
    "    potential malformatting, privacy regulations, etc.\n",
    "-   **Rationale**: Facilitates proper processing, validation, and utilization of\n",
    "    the data.\n",
    "\n",
    "### Data Types in Machine Learning Systems\n",
    "\n",
    "Before we scope the data source, a logical question to first ask is, what\n",
    "_types_ of data are we dealing with? Knowing the data types will help us\n",
    "**understand the nature and structure of information that we need to obtain.**\n",
    "This understanding, in turn, informs our choice of **data sources** that are\n",
    "best suited to provide this specific type of data.\n",
    "\n",
    "For example, if we are working with time-series data, our data sources might be\n",
    "sensors, logs, or financial market feeds. If we are dealing with textual data,\n",
    "the sources might be documents, websites, or social media platforms. By first\n",
    "defining the data types, we align our subsequent exploration and selection of\n",
    "data sources with the inherent characteristics of the data we aim to analyze.\n",
    "This helps in ensuring compatibility and efficiency in the entire data\n",
    "acquisition and preparation process, forming a cohesive link between what type\n",
    "of data we need (data types) and where we can find it (data source).\n",
    "\n",
    "Here's a brief overview of the different types of data in the form of a\n",
    "graph/tree diagram:\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    Main[\"Main Types\"]\n",
    "    Structured[\"Structured Data\"]\n",
    "    SemiStructured[\"Semi-Structured Data\"]\n",
    "    Unstructured[\"Unstructured Data\"]\n",
    "\n",
    "    Main --> Structured\n",
    "    Main --> SemiStructured\n",
    "    Main --> Unstructured\n",
    "\n",
    "    Structured --> Numerical[\"Numerical Data\"]\n",
    "    Structured --> Categorical[\"Categorical Data\"]\n",
    "    Structured --> TimeSeries[\"Time-Series Data\"]\n",
    "    Structured --> Geospatial[\"Geospatial Data\"]\n",
    "    Structured --> Boolean[\"Boolean Data\"]\n",
    "\n",
    "    SemiStructured --> Multimodal[\"Multimodal Data\"]\n",
    "    SemiStructured --> Graph[\"Graph Data\"]\n",
    "    SemiStructured --> Mixed[\"Mixed Data Types\"]\n",
    "\n",
    "    Unstructured --> TextBased[\"Text-Based Data\"]\n",
    "    Unstructured --> Image[\"Image Data\"]\n",
    "    Unstructured --> Audio[\"Audio Data\"]\n",
    "    Unstructured --> Binary[\"Binary Data\"]\n",
    "    Unstructured --> Embeddings[\"Embeddings\"]\n",
    "\n",
    "    Numerical --> Continuous[\"Continuous Data\"]\n",
    "    Numerical --> Discrete[\"Discrete Data\"]\n",
    "\n",
    "    Categorical --> Nominal[\"Nominal Data\"]\n",
    "    Categorical --> Ordinal[\"Ordinal Data\"]\n",
    "```\n",
    "\n",
    "and the corresponding table:\n",
    "\n",
    "| Main Type                | Subtype          | Specific Types       | Description                                                                    |\n",
    "| ------------------------ | ---------------- | -------------------- | ------------------------------------------------------------------------------ |\n",
    "| **Structured Data**      | Numerical        | Continuous, Discrete | Continuous data can take any value, while Discrete data takes specific values. |\n",
    "|                          | Categorical      | Nominal, Ordinal     | Nominal data has no inherent order; Ordinal data has a meaningful order.       |\n",
    "|                          | Time-Series Data |                      | Data collected at specific time intervals.                                     |\n",
    "|                          | Geospatial Data  |                      | Information that includes geographical attributes.                             |\n",
    "|                          | Boolean Data     |                      | True/false or yes/no values.                                                   |\n",
    "| **Semi-Structured Data** | Multimodal       |                      | Combines data from multiple sources or types.                                  |\n",
    "|                          | Graph Data       |                      | Represents relationships using nodes and edges.                                |\n",
    "|                          | Mixed Data Types |                      | A combination of various data types.                                           |\n",
    "| **Unstructured Data**    | Text-Based Data  |                      | Unstructured textual information.                                              |\n",
    "|                          | Image Data       |                      | Visual information in a grid of pixels.                                        |\n",
    "|                          | Audio Data       |                      | Sound or speech data.                                                          |\n",
    "|                          | Binary Data      |                      | Data represented in a binary format.                                           |\n",
    "|                          | Embeddings       |                      | Representations of categorical, text, or complex data as continuous vectors    |\n",
    "\n",
    "### Data Sources in Machine Learning Systems\n",
    "\n",
    "Having identified the _types_ of data that our machine learning system will\n",
    "handle, we now turn our attention to the various **sources** from which this\n",
    "data can be obtained. Different data types require specific sources, both in\n",
    "terms of format compatibility and functional alignment. Here's an overview of\n",
    "various data sources, categorized by their characteristics and aligned with the\n",
    "types of data they typically provide:\n",
    "\n",
    "| **Category**                     | **Type**                    | **Examples/Details**                                                                                                                     |\n",
    "| -------------------------------- | --------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Databases**                    | Relational Databases (SQL)  | [MySQL](https://www.mysql.com/), [PostgreSQL](https://www.postgresql.org/), [MS SQL Server](https://www.microsoft.com/en-us/sql-server/) |\n",
    "|                                  | NoSQL Databases             | [MongoDB](https://www.mongodb.com/), [Cassandra](https://cassandra.apache.org/), [Redis](https://redis.io/)                              |\n",
    "| **File-Based Sources**           | Flat Files                  | CSV, Excel, TSV                                                                                                                          |\n",
    "|                                  | Binary Files                | Parquet, Avro                                                                                                                            |\n",
    "|                                  | Image and Video Files       | JPEG, PNG, MP4                                                                                                                           |\n",
    "|                                  | Text Files                  | TXT, PDF, DOC                                                                                                                            |\n",
    "| **Web Sources**                  | Web APIs                    | RESTful APIs, SOAP, [GraphQL](https://graphql.org/)                                                                                      |\n",
    "|                                  | Web Scraping                | HTML, XML                                                                                                                                |\n",
    "|                                  | Social Media                | Twitter, Facebook, Reddit                                                                                                                |\n",
    "| **Streaming Data Sources**       | Message Brokers             | [Kafka](https://kafka.apache.org/), [RabbitMQ](https://www.rabbitmq.com/)                                                                |\n",
    "|                                  | Real-Time Feeds             | Stock prices, sensor data                                                                                                                |\n",
    "| **Sensor Data**                  | IoT Devices                 | Smart devices, wearable tech                                                                                                             |\n",
    "|                                  | Industrial Sensors          | Temperature, pressure, humidity sensors                                                                                                  |\n",
    "| **Scientific Sources**           | Genomic Data                | DNA sequences, proteomics                                                                                                                |\n",
    "|                                  | Meteorological Data         | Weather stations, satellites                                                                                                             |\n",
    "| **Financial Data Sources**       | Stock Market Data           | Exchanges, trading platforms                                                                                                             |\n",
    "|                                  | Banking Transactions        | Credit card swipes, ATM transactions                                                                                                     |\n",
    "| **Healthcare Data Sources**      | Electronic Health Records   | Patient medical records                                                                                                                  |\n",
    "|                                  | Medical Imaging             | MRI, CT scans, X-rays                                                                                                                    |\n",
    "| **Government and Public Data**   | Census Data                 | Demographics, economics                                                                                                                  |\n",
    "|                                  | Legislation and Regulations | Law documents, policy papers                                                                                                             |\n",
    "| **Educational Data Sources**     | Academic Databases          | Research papers, thesis documents                                                                                                        |\n",
    "|                                  | Learning Management Systems | Student grades, course content                                                                                                           |\n",
    "| **Human-Generated Data Sources** | Surveys and Questionnaires  | Market research, feedback forms                                                                                                          |\n",
    "|                                  | Crowdsourcing Platforms     | [Amazon Mechanical Turk](https://www.mturk.com/)                                                                                         |\n",
    "| **Third-Party Data Providers**   | Commercial Data Providers   | Market trends, consumer habits                                                                                                           |\n",
    "|                                  | Open Data Repositories      | [Kaggle](https://www.kaggle.com/), [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)                           |\n",
    "\n",
    "This categorization of data sources resonates with our prior discussion on data\n",
    "types. By recognizing the intricate relationships between **databases**,\n",
    "**file-based sources**, **web sources**, **streaming data sources**, and others,\n",
    "with the data types we're interested in, we are better positioned to select the\n",
    "optimal sources for our machine learning system. Whether it's time-series data\n",
    "from real-time feeds, textual data from web scraping, or image data from\n",
    "specific file formats, our data source selection now aligns with the strategic\n",
    "goals and technical requirements of the system.\n",
    "\n",
    "## Data Formats in Machine Learning Systems\n",
    "\n",
    "### Intuition\n",
    "\n",
    "After understanding the various data types within the machine learning system,\n",
    "we arrive at the crucial aspect of data formats. Data formats play a vital role\n",
    "in determining how data is stored, processed, and communicated within the\n",
    "system. They also influence the efficiency of data retrieval and the ease of\n",
    "interoperability between different parts of the pipeline. Depending on the\n",
    "nature of the data, the chosen format can have a substantial impact on the\n",
    "performance and scalability of data-driven applications. In the following\n",
    "section, we'll delve into various data formats and their relevance to machine\n",
    "learning systems, highlighting the importance of selecting appropriate formats\n",
    "in alignment with specific requirements and constraints.\n",
    "\n",
    "In other words, once you scope the data source and data types, and manage to\n",
    "extract them, you need to store it in a format that is easy to work with.\n",
    "Storing data isn't straightforward because data can be of different types.\n",
    "\n",
    "Some questions to ask when choosing a data format:\n",
    "\n",
    "-   Where do you store the data? In a database? In a file system? In a key-value\n",
    "    store? We want it to be ideally cheap and fast to retrieve the data.\n",
    "-   How to store complex models so they can be loaded and run on different\n",
    "    devices (e.g. mobile phones, web browsers, etc.). In ML, it can be GPU, CPU,\n",
    "    etc.\n",
    "\n",
    "### Example: Sharding in Hugging Face\n",
    "\n",
    "**Sharding** refers to dividing a large dataset into smaller, more manageable\n",
    "parts or \"shards.\" Each shard can be processed independently, allowing for\n",
    "parallelism and more efficient utilization of resources. Sharding is\n",
    "particularly relevant when working with large-scale models that require\n",
    "extensive training data.\n",
    "\n",
    "For instance, if you were to train a model using Hugging Face's Transformers\n",
    "library on a vast corpus of text data, you might encounter challenges in loading\n",
    "and processing the entire dataset at once. By employing sharding, you could\n",
    "break down the corpus into smaller shards, each stored in a specific data format\n",
    "like TensorFlow's TFRecord or PyTorch's data loader format.\n",
    "\n",
    "Here's how sharding might be implemented in this scenario:\n",
    "\n",
    "1. **Divide the Data**: Split the entire corpus into smaller parts, each\n",
    "   representing a shard. The division could be based on logical segments like\n",
    "   chapters, documents, or fixed-size chunks.\n",
    "\n",
    "2. **Choose a Data Format**: Select an appropriate data format for the shards.\n",
    "   TFRecord is a common choice for TensorFlow, while PyTorch users might prefer\n",
    "   its native data handling.\n",
    "\n",
    "3. **Process Shards Independently**: Each shard can be loaded and processed\n",
    "   independently, allowing for parallel processing. This enables efficient data\n",
    "   handling, especially when using distributed computing resources.\n",
    "\n",
    "4. **Assemble Results**: After processing the individual shards, the results can\n",
    "   be assembled to form the complete dataset or model parameters.\n",
    "\n",
    "This approach leverages data sharding and specific data formats to provide a\n",
    "scalable and flexible method for working with extensive datasets in Hugging\n",
    "Face. It's an illustrative example of how data formats, coupled with effective\n",
    "data engineering practices, can profoundly impact the efficiency and scalability\n",
    "of machine learning workflows.\n",
    "\n",
    "Moral of the story is, you cannot just store data in any format. You need to\n",
    "think you multiple aspects of the data and the system before choosing a data\n",
    "format. Most of the times, we want **efficient** and **scalable** data formats.\n",
    "\n",
    "Certainly! While the initial example provided is a straightforward way to store\n",
    "multimodal data (images and text), it may lack some rigor and scalability. In a\n",
    "real-world setting, particularly for large-scale e-commerce platforms, a more\n",
    "robust approach would be necessary.\n",
    "\n",
    "### Example: Multimodal Data Storage for E-Commerce\n",
    "\n",
    "In e-commerce platforms, product pages often contain rich multimedia\n",
    "information, including images and corresponding textual descriptions. Storing\n",
    "and retrieving this information efficiently requires careful design.\n",
    "\n",
    "One robust approach might involve:\n",
    "\n",
    "1. **Storing Images in a Binary Format**: Rather than embedding the raw image\n",
    "   tensor within a data structure, it's often more efficient to store the image\n",
    "   in a binary format (e.g., JPEG, PNG) and keep a reference to its location\n",
    "   (e.g., file path or URL).\n",
    "\n",
    "2. **Utilizing a Database for Textual Information**: The textual information,\n",
    "   including descriptions and metadata, can be stored in a relational database.\n",
    "   This approach provides scalable storage and efficient query capabilities.\n",
    "\n",
    "3. **Creating a Unified Schema**: A unified schema or data model could\n",
    "   encapsulate both the image references and the corresponding textual data.\n",
    "   This schema acts as a bridge between the two data types, allowing them to be\n",
    "   treated as a cohesive unit.\n",
    "\n",
    "Here's an example code snippet that reflects this design:\n",
    "\n",
    "```python title=\"Sample Data Schema Encoding Image and Text\"\n",
    "sample_data_schema = {\n",
    "    \"product_id\": 123,\n",
    "    \"image_url\": \"https://path/to/image.jpg\",\n",
    "    \"description\": \"This is a picture of a cat.\",\n",
    "    \"additional_metadata\": { ... }  # Additional textual or numerical information.\n",
    "}\n",
    "```\n",
    "\n",
    "and in tabular form:\n",
    "\n",
    "| Field Name            | Data Type       | Description                                                                               |\n",
    "| --------------------- | --------------- | ----------------------------------------------------------------------------------------- |\n",
    "| `product_id`          | Integer         | A unique identifier for the product.                                                      |\n",
    "| `image_url`           | String (URL)    | The URL or file path to the product's image.                                              |\n",
    "| `description`         | String (Text)   | The textual description of the product.                                                   |\n",
    "| `additional_metadata` | Dictionary/JSON | Additional textual or numerical information, such as categories, tags, or specifications. |\n",
    "\n",
    "In this approach, the `\"image_url\"` field stores a reference to the location of\n",
    "the image, and the `\"description\"` field contains the textual description. The\n",
    "additional metadata can encapsulate other relevant information, such as\n",
    "categories, tags, or product specifications.\n",
    "\n",
    "This design offers several advantages:\n",
    "\n",
    "-   **Scalability**: By storing images in a binary format and using database\n",
    "    storage for text, this approach can scale to handle large product catalogs.\n",
    "-   **Efficiency**: Leveraging specialized storage mechanisms for different data\n",
    "    types ensures that retrieval and updates are efficient.\n",
    "-   **Flexibility**: A unified schema provides a consistent way to represent and\n",
    "    manipulate text-image pairs, while still allowing for customization and\n",
    "    extension as needed.\n",
    "\n",
    "Overall, this example demonstrates a more rigorous and practical approach to\n",
    "storing and managing multimodal data in a context such as an e-commerce\n",
    "platform. It illustrates the interplay between data formats, storage mechanisms,\n",
    "and data modeling in handling complex, multifaceted information.\n",
    "\n",
    "### Data Formats\n",
    "\n",
    "We will describe a few choices of data formats below.\n",
    "\n",
    "#### Data Serialization vs Data Deserialization\n",
    "\n",
    "The process of transforming data structures or object states into a format that\n",
    "can be saved (e.g., in a file like JSON) and later rebuilt in the same or a\n",
    "different computing environment is known as serialization. The opposite process,\n",
    "called deserialization, involves retrieving data from the stored formats. In\n",
    "simpler terms, serialization refers to storing data, while deserialization\n",
    "refers to accessing data from the saved formats.\n",
    "\n",
    "In other words, **storing data** is called **serialization**, and **retrieving\n",
    "data from the stored formats** is called **deserialization**.\n",
    "\n",
    "#### JSON\n",
    "\n",
    "[**JSON**](https://www.json.org/json-en.html), which stands for JavaScript\n",
    "Object Notation, is a lightweight data-interchange format that uses a key-value\n",
    "pair paradigm. It is human-readable, easy to parse, and simple to generate,\n",
    "making it an ideal choice for data exchange between a server and a client in\n",
    "machine learning applications. JSON's structure allows for easy storage in\n",
    "databases and can represent a wide variety of data types, including strings,\n",
    "numbers, booleans, objects, and arrays.\n",
    "\n",
    "```json title=\"example.json\"\n",
    "{\n",
    "    \"name\": \"John\",\n",
    "    \"age\": 30,\n",
    "    \"cars\": [\n",
    "        { \"name\": \"Ford\", \"models\": [\"Fiesta\", \"Focus\", \"Mustang\"] },\n",
    "        { \"name\": \"BMW\", \"models\": [\"320\", \"X3\", \"X5\"] },\n",
    "        { \"name\": \"Fiat\", \"models\": [\"500\", \"Panda\"] }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "While JSON has many advantages, it does have some drawbacks, such as increased\n",
    "storage requirements due to its text-based nature. However, its simplicity and\n",
    "ease of use have made it one of the most popular data formats in machine\n",
    "learning and other applications.\n",
    "\n",
    "In addition to the key-value pair structure, JSON also supports nesting of\n",
    "objects and arrays, which allows for more complex data representation. This\n",
    "makes JSON a versatile choice for a variety of use cases, from simple\n",
    "configuration files to complex machine learning model inputs and outputs.\n",
    "\n",
    "Furthermore, JSON has extensive support in many programming languages, with\n",
    "built-in libraries or third-party packages available for parsing and generating\n",
    "JSON data. This widespread support makes it a convenient choice for developers\n",
    "working with machine learning systems and data pipelines.\n",
    "\n",
    "In summary, JSON's human-readable format, easy parsing, support for complex data\n",
    "structures, and widespread language support make it an excellent choice for data\n",
    "exchange and storage in machine learning applications, despite its increased\n",
    "storage requirements compared to binary formats.\n",
    "\n",
    "#### Row and Columnar Formats\n",
    "\n",
    "##### Concept of Row-major vs Column-major order\n",
    "\n",
    "Row-major and column-major order describe two ways to store multi-dimensional\n",
    "arrays in linear memory. In row-major order, the elements of a multi-dimensional\n",
    "array are stored row by row, whereas in column-major order, the elements are\n",
    "stored column by column.\n",
    "\n",
    "##### Examples of Row-major vs Column-major order\n",
    "\n",
    "In row-major order, the elements of each row of a matrix are stored together in\n",
    "contiguous memory locations, with the elements of successive rows appearing\n",
    "consecutively in memory. For example, consider a 3x2 matrix:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{A} = \\begin{bmatrix}\n",
    "    1 & 2 \\\\\n",
    "    3 & 4 \\\\\n",
    "    5 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In row-major order, the elements are stored in memory as:\n",
    "\n",
    "```python\n",
    "[1, 2, 3, 4, 5, 6]\n",
    "```\n",
    "\n",
    "In contrast, in column-major order, the elements of each column are stored\n",
    "together in contiguous memory locations, with the elements of successive columns\n",
    "appearing consecutively in memory. For the same matrix, the column-major order\n",
    "would be:\n",
    "\n",
    "```python\n",
    "[1, 3, 5, 2, 4, 6]\n",
    "```\n",
    "\n",
    "Row-major and column-major order can make a difference in performance when\n",
    "accessing multi-dimensional arrays, especially for large arrays. For example,\n",
    "when accessing elements of a row in row-major order, consecutive elements are\n",
    "likely to be cached together, which can improve access time. Similarly, when\n",
    "accessing elements of a column in column-major order, consecutive elements are\n",
    "likely to be cached together, which can improve performance.\n",
    "\n",
    "#### Pros and cons of Row-major vs Column-major order\n",
    "\n",
    "##### Row-major order\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "-   It is the default order used in many programming languages, including C and\n",
    "    C++.\n",
    "-   It can be more intuitive for humans to understand, as rows are typically\n",
    "    used to represent entities (e.g., students, observations) and columns are\n",
    "    used to represent attributes (e.g., grades, measurements).\n",
    "-   When iterating over the elements of a matrix row-by-row, row-major order\n",
    "    ensures that the elements accessed are contiguous in memory, which can\n",
    "    improve cache locality and reduce the number of cache misses.\n",
    "-   Many linear algebra libraries, such as BLAS and LAPACK, use row-major order\n",
    "    by default.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "-   When iterating over the elements of a matrix column-by-column, row-major\n",
    "    order can lead to poor cache locality and a higher number of cache misses.\n",
    "    This is because consecutive elements in the same column are not necessarily\n",
    "    contiguous in memory.\n",
    "-   When transposing a matrix, row-major order requires copying the entire\n",
    "    matrix into a new block of memory in column-major order, which can be costly\n",
    "    for large matrices.\n",
    "-   Some hardware architectures may be optimized for column-major order, leading\n",
    "    to lower performance for row-major order.\n",
    "\n",
    "##### Column-major order\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "-   Column-major order is used by default in some programming languages, such as\n",
    "    Fortran.\n",
    "-   When iterating over the elements of a matrix column-by-column, column-major\n",
    "    order ensures that the elements accessed are contiguous in memory, which can\n",
    "    improve cache locality and reduce the number of cache misses.\n",
    "-   Some hardware architectures, such as GPUs, are optimized for column-major\n",
    "    order, leading to potentially better performance.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "-   Column-major order can be less intuitive for humans to understand, as it is\n",
    "    not the standard representation used in many fields.\n",
    "-   When iterating over the elements of a matrix row-by-row, column-major order\n",
    "    can lead to poor cache locality and a higher number of cache misses. This is\n",
    "    because consecutive elements in the same row are not necessarily contiguous\n",
    "    in memory.\n",
    "-   Many linear algebra libraries, such as BLAS and LAPACK, use row-major order\n",
    "    by default, so using column-major order may require additional memory copies\n",
    "    or transpositions.\n",
    "\n",
    "Overall, the choice between row-major and column-major order depends on the\n",
    "specific use case and hardware architecture. For performance-critical\n",
    "applications, it may be worth experimenting with both orders to see which yields\n",
    "better results.\n",
    "\n",
    "#### Modern Row and Columnar Formats\n",
    "\n",
    "| Library | Order for Multidimensional Arrays              |\n",
    "| ------- | ---------------------------------------------- |\n",
    "| NumPy   | Row-Major Order                                |\n",
    "| MATLAB  | Column-Major Order                             |\n",
    "| OpenGL  | Column-Major Order                             |\n",
    "| CUDA    | Column-Major Order                             |\n",
    "| OpenCV  | Row-Major Order                                |\n",
    "| Eigen   | Supports both Row-Major and Column-Major Order |\n",
    "| CSV     | Row-Major Order                                |\n",
    "| Parquet | Column-Major Order                             |\n",
    "\n",
    "Column-major formats are better for accessing specific columns of large datasets\n",
    "with many features, while row-major formats are better for faster data writes\n",
    "when adding new individual examples to data. Row-major formats are better for a\n",
    "lot of writes, while column-major formats are better for a lot of column-based\n",
    "reads.\n",
    "\n",
    "When you have a dataset with many features, storing the data in a column-major\n",
    "format is more efficient because it allows for direct access to individual\n",
    "columns without having to scan through all the other data in the rows. This\n",
    "means that when you need to extract a specific subset of columns from the\n",
    "dataset, you can do so more efficiently because the system doesn't need to read\n",
    "through all the other data in the rows to access the desired columns.\n",
    "\n",
    "In contrast, with a row-major format, the data for each row is stored together\n",
    "in memory, meaning that to access a specific column, you have to read through\n",
    "all the other columns in the row before you get to the desired column. This can\n",
    "be especially inefficient when dealing with large datasets with many features,\n",
    "as the system has to read through a lot of data to extract the desired subset of\n",
    "columns.\n",
    "\n",
    "For example, consider a dataset of ride-sharing transactions with 1,000\n",
    "features, but you only need to extract four specific columns: time, location,\n",
    "distance, and price. With a column-major format, you can directly access these\n",
    "columns, whereas with a row-major format, you have to read through all the other\n",
    "996 columns in each row before getting to the desired four columns. This can be\n",
    "slow and inefficient, especially if you need to access the subset of columns\n",
    "frequently or if the dataset is very large.\n",
    "\n",
    "In summary, storing data in a column-major format is more efficient for datasets\n",
    "with many features because it allows for direct access to individual columns,\n",
    "which can significantly speed up data retrieval and processing.\n",
    "\n",
    "#### Examples in code (Python) of Row-major vs Column-major order and its effect on performance\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Callable, TypeVar, Any\n",
    "\n",
    "F = TypeVar(\"F\", bound=Callable[..., Any])\n",
    "\n",
    "\n",
    "def timer(func: F) -> F:\n",
    "    \"\"\"Timer decorator.\"\"\"\n",
    "\n",
    "    def wrapper(*args: Any, **kwargs: Any) -> Any:\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"{func.__name__} took {elapsed_time:.4f} seconds to execute.\")\n",
    "        # print(f\"{func.__name__} took {elapsed_time / 60:.4f} minutes to execute.\")\n",
    "        # print(f\"{func.__name__} took {elapsed_time / 60 / 60:.4f} hours to execute.\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@timer\n",
    "def traverse_dataframe_by_row(df: pd.DataFrame) -> None:\n",
    "    for col in df.columns:\n",
    "        for _ in df[col]:\n",
    "            pass\n",
    "\n",
    "\n",
    "@timer\n",
    "def traverse_dataframe_by_column(df: pd.DataFrame) -> None:\n",
    "    num_rows = df.shape[0]\n",
    "    for row_idx in range(num_rows):\n",
    "        for _ in df.iloc[row_idx]:\n",
    "            pass\n",
    "\n",
    "\n",
    "df = pd.DataFrame(np.random.rand(5000, 5000))\n",
    "print(df.shape)\n",
    "\n",
    "traverse_dataframe_by_row(df)\n",
    "traverse_dataframe_by_column(df)\n",
    "\n",
    "\n",
    "# Row-major traversal (C-like order)\n",
    "df_np = df.to_numpy()\n",
    "df_np = np.array(df_np, order=\"C\")  # Row-major traversal (C-like order)\n",
    "n_rows, n_cols = df_np.shape\n",
    "\n",
    "\n",
    "@timer\n",
    "def traverse_numpy_by_row(array: np.ndarray) -> None:\n",
    "    for row_idx in range(n_rows):\n",
    "        for col_idx in range(n_cols):\n",
    "            _ = array[row_idx, col_idx]\n",
    "\n",
    "\n",
    "@timer\n",
    "def traverse_numpy_by_column(array: np.ndarray) -> None:\n",
    "    for col_idx in range(n_cols):\n",
    "        for row_idx in range(n_rows):\n",
    "            _ = array[row_idx, col_idx]\n",
    "\n",
    "\n",
    "traverse_numpy_by_row(df_np)\n",
    "traverse_numpy_by_column(df_np)\n",
    "\n",
    "df_np_col = np.array(df_np, order=\"F\")  # Column-major traversal (Fortran-like order)\n",
    "\n",
    "traverse_numpy_by_row(df_np_col)\n",
    "traverse_numpy_by_column(df_np_col)\n",
    "```\n",
    "\n",
    "### Text vs Binary Formats\n",
    "\n",
    "CSV and JSON are text files, while Parquet files are binary files. Text files\n",
    "are human-readable, while binary files are only readable by programs that can\n",
    "interpret the raw bytes. Binary files contain only 0s and 1s and are more\n",
    "compact than text files. Binary files can save space compared to text files; for\n",
    "example, storing the number 1000000 requires 7 bytes in a text file and only 4\n",
    "bytes in a binary file as int32. Parquet files are more efficient than text\n",
    "files in terms of storage and processing speed. For example, AWS recommends\n",
    "using the Parquet format because it consumes up to 6x less storage and is up to\n",
    "2x faster to unload in Amazon S3 compared to text formats.\n",
    "\n",
    "!!! example \"Text vs Binary\" For example, if you want to store the number\n",
    "$1000000$, and if you store it in text file it takes 7 characters (1, 0, 0, 0,\n",
    "0, 0, 0), taking up 7 bytes of storage if 1 character is 1 byte. But if you\n",
    "store it in binary format as `int32`, then it takes 32 bits, which is 4 bytes.\n",
    "\n",
    "## Data Storage in Machine Learning Systems\n",
    "\n",
    "Once the data source is scoped and well-defined, before we even start extracting\n",
    "the data, we need to know what kind of data we are dealing with and **how** and\n",
    "**where** we are going to store the extracted data.\n",
    "\n",
    "Determining the **storage format** is critical. Will the data be stored in its\n",
    "raw form, or does it need to be processed and converted into a different format\n",
    "like CSV, JSON, or Parquet? The chosen data format can have significant\n",
    "implications on storage costs, access speed, and compatibility with your data\n",
    "processing tools.\n",
    "\n",
    "The **storage location** is equally important. Depending on the volume of the\n",
    "data, your budget, and security requirements, you might opt for on-premises\n",
    "servers, cloud storage, or even a hybrid solution. Cloud storage, like Google\n",
    "Cloud Storage, Amazon S3, or Azure Blob Storage, offer scalable and secure\n",
    "solutions. However, you need to consider data privacy regulations and compliance\n",
    "requirements when deciding where to store the data.\n",
    "\n",
    "You should also consider how the data will be organized. Will it be stored in a\n",
    "structured database like MySQL, a NoSQL database like MongoDB, or a distributed\n",
    "file system like Hadoop HDFS? The data's nature, the need for scalability, and\n",
    "the types of queries you'll be running, all factor into this decision.\n",
    "\n",
    "Finally, the choice of **storage technology** also depends on the **data\n",
    "operations** you anticipate. For instance, if your data needs frequent updates,\n",
    "a database might be more suitable. If your data is largely static but needs to\n",
    "be read frequently, a file system might be a better choice.\n",
    "\n",
    "Your data storage decisions can greatly impact the efficiency of your data\n",
    "operations and the overall success of your machine learning project. Hence,\n",
    "careful planning and consideration are required in this stage.\n",
    "\n",
    "### Data Storage Options\n",
    "\n",
    "... list table of different data storage options for different use cases.\n",
    "\n",
    "As we see shortly in the next few sections, once the data source is defined,\n",
    "before we even start extracting the data, we need to know what kind of data we\n",
    "are dealing with and **how** and **where** we are going to store it.\n",
    "\n",
    "### Data Lake\n",
    "\n",
    "### Data Warehouse\n",
    "\n",
    "### Data Lakehouse\n",
    "\n",
    "### Delta Lake\n",
    "\n",
    "### SQL vs NoSQL\n",
    "\n",
    "### Vector Database (A High-dimensional Playground for Large Language Models)\n",
    "\n",
    "[see here](https://learn.microsoft.com/en-us/semantic-kernel/memories/vector-db)\n",
    "\n",
    "A **vector database** is an ingenious data storage system that capitalizes on\n",
    "the properties of vectors — mathematical objects that possess magnitude and\n",
    "direction. The high-dimensional vectors stored in these databases embody the\n",
    "features or attributes of data, which could range from text, images, audio, and\n",
    "video to even more complex structures.\n",
    "\n",
    "#### Transformation and Embeddings\n",
    "\n",
    "The crucial task of converting raw data to their vector representations\n",
    "(embeddings) is typically achieved by utilizing machine learning models, word\n",
    "embedding algorithms, or feature extraction techniques.\n",
    "\n",
    "For instance, a movie review text can be represented as a high-dimensional\n",
    "vector via word embedding techniques like Word2Vec or GloVe. Similarly, an image\n",
    "can be transformed into a vector representation using deep learning models like\n",
    "convolutional neural networks (CNNs).\n",
    "\n",
    "#### The Power of Similarity Search\n",
    "\n",
    "Vector databases deviate from the conventional way databases work. Rather than\n",
    "retrieving data based on exact matches or predefined criteria, vector databases\n",
    "empower users to conduct searches based on vector similarity. This facilitates\n",
    "the retrieval of data that bears semantic or contextual similarity to the query\n",
    "data, even if they don't share exact keyword matches.\n",
    "\n",
    "Consider this example: Given an image of a cat, a vector database can find\n",
    "images that are visually similar (e.g., other images of cats, or perhaps images\n",
    "of small, furry animals), even if \"cat\" isn't explicitly tagged or described in\n",
    "the metadata of those images.\n",
    "\n",
    "#### The Working Mechanism\n",
    "\n",
    "Here's how the magic happens: A query vector, which symbolizes your search\n",
    "criterion, is used to scour the database for the most similar vectors. This\n",
    "query vector can be either generated from the same data type as the stored\n",
    "vectors (image for image, text for text, etc.) or from different types.\n",
    "\n",
    "A similarity measure, such as cosine similarity or Euclidean distance, is then\n",
    "employed to calculate the proximity between the query vector and stored vectors.\n",
    "The result is a ranked list of vectors — and their corresponding raw data — that\n",
    "have the highest similarity to the query.\n",
    "\n",
    "#### Use Cases: From NLP to Recommendation Systems\n",
    "\n",
    "The potential applications for vector databases are wide-ranging. They can be\n",
    "utilized in natural language processing, computer vision, recommendation\n",
    "systems, and any domain requiring a deep understanding and matching of data\n",
    "semantics.\n",
    "\n",
    "For example, a large language model (LLM) like GPT-3 can be complemented with a\n",
    "vector database to generate more relevant and coherent text. Let's say you want\n",
    "the LLM to write a blog post about the latest trends in artificial intelligence.\n",
    "While the model can generate text based on the prompt, it may lack the most\n",
    "recent information or context about the subject matter.\n",
    "\n",
    "This is where a vector database comes into play. You could maintain a vector\n",
    "database with the latest information, articles, and papers about AI trends. When\n",
    "you prompt the LLM to write the blog post, you could use a query to pull the\n",
    "most relevant and recent vectors from the database, and feed this information\n",
    "into the model along with your prompt. This would guide the model to generate\n",
    "text that is not only contextually accurate but also up-to-date with current\n",
    "information.\n",
    "\n",
    "Keep in mind, though, that building and maintaining such a vector database\n",
    "requires careful consideration of your data update strategy, storage\n",
    "requirements, and search efficiency, among other things.\n",
    "\n",
    "#### The New Kid in Town\n",
    "\n",
    "As the world of data continues to expand in volume and complexity, the need for\n",
    "intelligent and efficient databases becomes more apparent. Vector databases,\n",
    "with their high-dimensional storage and similarity-based search capabilities,\n",
    "provide a promising solution to manage and make sense of the deluge of data in\n",
    "various application areas.\n",
    "\n",
    "## Batch Processing vs. Stream Processing (TODO as not familiar with stream processing)\n",
    "\n",
    "For real-time or near-real-time ML applications, traditional batch processing of\n",
    "ETL might not be suitable. Instead, stream processing frameworks like Apache\n",
    "Kafka or Apache Flink allow for continuous data processing and may be used as\n",
    "alternatives or complements to ETL.\n",
    "\n",
    "## References and Further Readings\n",
    "\n",
    "-   Huyen, Chip. \"Chapter 3. Data Engineering Fundamentals.\" In Designing\n",
    "    Machine Learning Systems: An Iterative Process for Production-Ready\n",
    "    Applications, O'Reilly Media, Inc., 2022.\n",
    "-   Kleppmann, Martin. \"Chapter 2. Data Models and Query Languages.\" In\n",
    "    Designing Data-Intensive Applications. Beijing: O'Reilly, 2017.\n",
    "-   [Microsoft: What is a Vector DB?](https://learn.microsoft.com/en-us/semantic-kernel/memories/vector-db)\n",
    "-   [Machine Learning System Design Interview](https://bytebytego.com/intro/machine-learning-system-design-interview)\n",
    "-   [Madewithml: Data Engineering](https://madewithml.com/courses/mlops/data-stack/)\n",
    "-   [Google: CI/CD Pipeline for Data Processing](https://cloud.google.com/architecture/cicd-pipeline-for-data-processing)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "mystnb": {
   "number_source_lines": true
  },
  "source_map": [
   16
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}