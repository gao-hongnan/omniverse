{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95cd2c0",
   "metadata": {},
   "source": [
    "# Stage 6. Model Evaluation (MLOps)\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Brain_Dump-red)\n",
    "![Tag](https://img.shields.io/badge/Level-Beginner-green)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "\n",
    "Once the model has been trained, it's crucial to evaluate its performance. This\n",
    "evaluation involves various techniques:\n",
    "\n",
    "-   **Out-of-Fold Predictions**: In k-fold cross-validation, the model makes\n",
    "    predictions on the validation set in each fold of the process. These\n",
    "    predictions are \"out of fold\" because they are made on data that the model\n",
    "    hasn't seen during training. Collecting the out-of-fold predictions from\n",
    "    each fold and comparing them to the actual targets can give a more robust\n",
    "    estimate of the model's performance.\n",
    "\n",
    "-   **Holdout Evaluation**: This is a simpler form of model validation, where\n",
    "    the dataset is split into two sets: a training set and a validation (or\n",
    "    holdout) set. The model is trained on the training set and evaluated on the\n",
    "    holdout set. The key is to ensure that the model never sees the validation\n",
    "    data during training, which gives us an unbiased estimate of the model's\n",
    "    performance on unseen data.\n",
    "\n",
    "-   **Bias-Variance Tradeoff**: This is a fundamental concept in machine\n",
    "    learning that refers to the tradeoff between a model's ability to fit\n",
    "    training data (bias) and its ability to generalize to unseen data\n",
    "    (variance). An optimal model should strike a balance between the two, which\n",
    "    is typically achieved through techniques like regularization and\n",
    "    hyperparameter tuning. This is usually not a separate step in the model\n",
    "    evaluation, but a principle that guides the entire model building process.\n",
    "\n",
    "-   **Model Metrics Evaluation**: Use the appropriate evaluation metrics (as\n",
    "    decided in the Model Training stage) to measure the model's performance.\n",
    "    This could include metrics like accuracy, precision, recall, F1 score,\n",
    "    AUC-ROC for classification problems; and Mean Absolute Error (MAE), Mean\n",
    "    Squared Error (MSE), Root Mean Squared Error (RMSE) for regression problems,\n",
    "    among others.\n",
    "\n",
    "-   **Model Interpretability Evaluation**: If applicable, evaluate how\n",
    "    interpretable the model is. Some models, like decision trees or linear\n",
    "    regression, are quite interpretable (i.e., we can understand how they make\n",
    "    predictions), but others, like deep neural networks, are more like \"black\n",
    "    boxes\". Depending on the application, a model's interpretability may be very\n",
    "    important.\n",
    "\n",
    "These evaluation techniques allow us to get a more complete picture of how well\n",
    "the model is likely to perform on unseen data.\n",
    "\n",
    "## Bias and Variance\n",
    "\n",
    "See\n",
    "[mlxtend's bias-variance decomp](https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/).\n",
    "\n",
    "1. Draw `num_rounds` bootstrap samples from the training set. Each bootstrap\n",
    "   sample `X_boot` and `y_boot` is the same size as the original training set\n",
    "   and is drawn with replacement.\n",
    "\n",
    "2. Fit the model to each bootstrap sample and make predictions on the test set.\n",
    "   This gives you a matrix of predictions, `all_pred`, with `num_rounds` rows\n",
    "   (one for each round of bootstrapping) and `n_test` columns (one for each\n",
    "   example in the test set).\n",
    "\n",
    "3. Calculate the main predictions. If the loss is '0-1_loss', this is the mode\n",
    "   of the predictions in `all_pred` for each test example (i.e., the most common\n",
    "   prediction across bootstrap rounds). If the loss is 'mse', this is the mean\n",
    "   of the predictions in `all_pred` for each test example.\n",
    "\n",
    "4. Now calculate the three quantities of interest:\n",
    "\n",
    "    - The average expected loss is the average loss across all bootstrap rounds\n",
    "      and all test examples. For '0-1_loss', this is the mean number of times\n",
    "      the prediction in each round is not equal to the true test label. For\n",
    "      'mse', this is the mean of the squared differences between the prediction\n",
    "      in each round and the true test label.\n",
    "\n",
    "    - The average bias is the loss of the main predictions. For '0-1_loss', this\n",
    "      is the number of times the main prediction is not equal to the true test\n",
    "      label, averaged over all test examples. For 'mse', this is the mean of the\n",
    "      squared differences between the main prediction and the true test label.\n",
    "\n",
    "    - The average variance is the average variance of the predictions across all\n",
    "      bootstrap rounds and all test examples. For '0-1_loss', this is the mean\n",
    "      number of times the prediction in each round is not equal to the main\n",
    "      prediction. For 'mse', this is the mean of the squared differences between\n",
    "      the prediction in each round and the main prediction.\n",
    "\n",
    "The average expected loss, average bias, and average variance are then returned.\n",
    "\n",
    "This method provides a way to quantify the bias and variance of a model, giving\n",
    "insight into the model's tendency to overfit (high variance) or underfit (high\n",
    "bias) the data. By using bootstrapping, it simulates the scenario of having\n",
    "multiple different samples from the population and provides an estimate of how\n",
    "the model's predictions would vary across these different samples.\n",
    "\n",
    "## Model Calibration\n",
    "\n",
    "See chip\n",
    "\n",
    "## References and Further Readings\n",
    "\n",
    "-   [Fast.ai](https://www.fast.ai/)\n",
    "-   [Kaggle](https://www.kaggle.com/)\n",
    "-   [Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning](https://sebastianraschka.com/pdf/manuscripts/model-eval.pdf)\n",
    "-   Huyen, Chip. \"Chapter 6. Model Development and Offline Evaluation.\" In\n",
    "    Designing Machine Learning Systems: An Iterative Process for\n",
    "    Production-Ready Applications, O'Reilly Media, Inc., 2022.\n",
    "-   [Madewithml](https://madewithml.com/)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "mystnb": {
   "number_source_lines": true
  },
  "source_map": [
   16
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}