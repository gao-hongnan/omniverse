{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd93ccc",
   "metadata": {},
   "source": [
    "# Stage 8. Model Serving (MLOps)\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Brain_Dump-red)\n",
    "![Tag](https://img.shields.io/badge/Level-Beginner-green)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "\n",
    "Model serving is the process of making the trained machine learning models\n",
    "available for inference. It involves loading the model into memory and setting\n",
    "up an API or a service to accept data inputs, perform predictions, and return\n",
    "the results.\n",
    "\n",
    "-   **Deployment**: Models are deployed in a suitable environment that can be a\n",
    "    server, a cloud service, or even an edge device. Depending on the use case,\n",
    "    the deployment can be a batch scoring job that runs at regular intervals, or\n",
    "    a real-time scoring service that needs to respond immediately to incoming\n",
    "    requests.\n",
    "\n",
    "-   **Scalability**: The model serving infrastructure should be able to handle\n",
    "    varying loads. It should be able to scale up during high load times and\n",
    "    scale down when the load is low to efficiently use resources.\n",
    "\n",
    "-   **Latency**: For real-time applications, the model serving infrastructure\n",
    "    should have low latency to deliver predictions within an acceptable time\n",
    "    frame.\n",
    "\n",
    "## Finite vs. Unbounded Feature Space, Precurser to Deployment Strategies\n",
    "\n",
    "The concept of finite vs. unbounded feature space refers to the dimensionality\n",
    "and potential variations in the input data that a model is expected to handle.\n",
    "\n",
    "-   **Finite Feature Space**: In a finite feature space, the possible inputs a\n",
    "    model might encounter are limited and well-defined. For example, a model\n",
    "    predicting whether an email is spam or not might look at a finite set of\n",
    "    features like the length of the email, the number of capitalized words, etc.\n",
    "    Since the feature space is finite and well understood, you can often batch\n",
    "    process the inputs and generate predictions in large groups. This approach\n",
    "    is often more computationally efficient.\n",
    "\n",
    "-   **Unbounded Feature Space**: Conversely, in an unbounded feature space, the\n",
    "    potential inputs are not strictly limited. For example, a model that\n",
    "    generates responses to open-ended human language input operates in an\n",
    "    unbounded feature space because the possible variations in language are\n",
    "    virtually limitless. In such cases, real-time prediction is often necessary\n",
    "    because the model needs to handle a constant stream of unique, unpredictable\n",
    "    inputs.\n",
    "\n",
    "The nature of your feature space has a significant impact on your deployment\n",
    "strategy. If you're working with a finite feature space and there's no urgent\n",
    "need for real-time predictions, batch scoring can be a good approach. But if\n",
    "you're dealing with an unbounded feature space or if your use case requires\n",
    "immediate predictions (such as recommendation systems, chatbots, etc.), then\n",
    "you'll need to set up an infrastructure that can handle real-time scoring.\n",
    "\n",
    "It's important to note that these are not strict rules but guidelines. The\n",
    "ultimate decision would depend on various factors like business requirements,\n",
    "computational resources, model complexity, etc.\n",
    "\n",
    "### The Distinction Between Finite and Unbounded Feature Space\n",
    "\n",
    "When we say \"finite feature space\", we don't necessarily mean that the possible\n",
    "values for a feature are finite, but rather that the set of _types_ of features\n",
    "we are considering is finite and well-defined.\n",
    "\n",
    "For instance, in the email spam detection example, we might decide to consider\n",
    "features such as the length of the email, the number of capitalized words, the\n",
    "presence of specific keywords, etc. This set of features forms a \"finite feature\n",
    "space\" because we have a specific, finite list of feature types we are\n",
    "considering.\n",
    "\n",
    "Within each of these feature types, of course, the specific values can vary\n",
    "widely. The length of an email could range from 1 word to thousands of words, so\n",
    "in that sense, the range of potential feature values can be quite broad.\n",
    "However, it is still a finite feature space because we are only considering a\n",
    "specific, finite set of feature types.\n",
    "\n",
    "In contrast, an \"unbounded feature space\" might refer to a situation where we\n",
    "can't easily enumerate all the potential types of features in advance, or where\n",
    "new types of features may continually arise. An example might be a natural\n",
    "language processing task where the input could be any arbitrary text string, and\n",
    "we might need to consider an essentially infinite set of potential features\n",
    "(every possible word or phrase that could appear in the text).\n",
    "\n",
    "So to summarize, in a finite feature space, we have a specific, finite set of\n",
    "feature types that we are considering, even though the specific values for each\n",
    "feature can vary widely. In an unbounded feature space, we can't easily\n",
    "enumerate all the potential feature types in advance, and new types of features\n",
    "may continually arise.\n",
    "\n",
    "The choice between batch processing and real-time processing usually depends\n",
    "more on the specific use case requirements (e.g., how quickly a prediction is\n",
    "needed) rather than the nature of the feature space. However, in some cases, an\n",
    "unbounded feature space might make batch processing more challenging, as the\n",
    "data preprocessing and feature extraction steps could be more complex and\n",
    "time-consuming.\n",
    "\n",
    "## Batch Features, Online Features, and Streaming Features\n",
    "\n",
    "Let's clarify the distinction between \"streaming features\", \"online features\",\n",
    "and \"batch features\" and incorporate them into an example. I'll use the case of\n",
    "a recommendation system as it's a common area where all these feature types come\n",
    "into play.\n",
    "\n",
    "-   **Batch Features:** Batch features are derived from historical data that is\n",
    "    processed at regular intervals, typically in a batch mode. For a\n",
    "    recommendation system, batch features could be item embeddings which\n",
    "    represent items (like products, movies, articles, etc.) in a\n",
    "    multi-dimensional space. These embeddings are usually precomputed using\n",
    "    historical data (like past user interactions with items) and are updated\n",
    "    periodically.\n",
    "\n",
    "-   **Online Features:** Online features are a broader category that includes\n",
    "    any feature used for real-time predictions. This can include both batch\n",
    "    features (like the item embeddings mentioned above) that are stored and\n",
    "    fetched when needed for a prediction, as well as real-time features that are\n",
    "    derived from the current context.\n",
    "\n",
    "-   **Streaming Features:** Streaming features are specifically derived from\n",
    "    streaming data. In a recommendation system, these might be real-time user\n",
    "    activity data, such as the sequence of items a user is currently viewing or\n",
    "    interacting with in a session.\n",
    "\n",
    "Let's consider a concrete example. Say we are running an e-commerce website and\n",
    "want to recommend products to users in real-time.\n",
    "\n",
    "-   When a user starts a browsing session on the site, we can track their\n",
    "    activity in real-time, such as which products they are viewing, clicking on,\n",
    "    or adding to their cart. These form our streaming features.\n",
    "-   At the same time, we also have item embeddings (batch features) precomputed\n",
    "    for all products in our inventory, which we can fetch in real-time when\n",
    "    needed for a prediction.\n",
    "-   Together, the item embeddings and the real-time user activity data\n",
    "    constitute our online features, which are used for making the real-time\n",
    "    product recommendations.\n",
    "\n",
    "So in this scenario, the item embeddings are batch features, which are a subset\n",
    "of online features, used for online prediction. However, they are not streaming\n",
    "features, as they are not computed from streaming data. The user's real-time\n",
    "activity data are both streaming features and online features. This example\n",
    "should provide a good context to understand the different types of features\n",
    "involved in a real-time recommendation system.\n",
    "\n",
    "## Serving Strategies\n",
    "\n",
    "Model serving is the process of using a trained machine learning model to make\n",
    "predictions in a production environment. There are several common ways to serve\n",
    "a model, and the best method often depends on the specific use case. Here are\n",
    "some of the most common methods of model serving.\n",
    "\n",
    "### Batch Serving/Inference (Asynchronous)\n",
    "\n",
    "#### Definition\n",
    "\n",
    "Batch inference refers to the process of making predictions on a large set of\n",
    "inputs at once. This is usually performed when there isn't a need for real-time\n",
    "predictions, and when predictions can be made in advance and stored for later\n",
    "use.\n",
    "\n",
    "_Batch prediction_ is when predictions are generated periodically or whenever\n",
    "triggered. The predictions are stored somewhere, such as in SQL tables or an\n",
    "in-memory data‐ base, and retrieved as needed. For example, Netflix might\n",
    "generate movie recommen‐ dations for all of its users every four hours, and the\n",
    "precomputed recommendations are fetched and shown to users when they log on to\n",
    "Netflix. Batch prediction is also known as _asynchronous prediction_:\n",
    "predictions are generated asynchronously with\n",
    "requests[^huyen-chip-model-deployment-and-prediction-service].\n",
    "\n",
    "#### Visualization (Batch Features only)\n",
    "\n",
    "<figure markdown>\n",
    "  ![Batch Serving](../assets/batch-serving-chip-huyen.png)\n",
    "  <figcaption>A simplified architecture for batch prediction. Image Credit: Huyen, Chip. \"Chapter 7.\n",
    "Model Deployment and Prediction Service.\" In Designing Machine Learning Systems:\n",
    "An Iterative Process for Production-Ready Applications, O'Reilly Media,\n",
    "Inc., 2022.\n",
    " </figcaption>\n",
    "</figure>\n",
    "\n",
    "#### Example\n",
    "\n",
    "Batch serving is particularly suitable for tasks where the data doesn't change\n",
    "rapidly and real-time predictions are not required. Here are a few examples:\n",
    "\n",
    "1. **User content recommendation**: Recommending content to users based on their\n",
    "   viewing history is a common use case for batch serving. For example, a\n",
    "   streaming service like Netflix or Spotify could use batch serving to generate\n",
    "   personalized content recommendations for all users overnight, based on their\n",
    "   viewing or listening history up to the end of the previous day.\n",
    "\n",
    "2. **Predictive maintenance**: Another use case is predicting equipment failures\n",
    "   or maintenance needs based on sensor data. If the equipment doesn't require\n",
    "   real-time monitoring, the sensor data could be batch processed overnight to\n",
    "   predict the probability of equipment failure in the next few days or weeks.\n",
    "\n",
    "3. **Marketing campaign planning**: In marketing, batch serving could be used to\n",
    "   segment the customer base and predict the expected response to different\n",
    "   marketing campaigns. These predictions could be used to plan marketing\n",
    "   activities for the next few days or weeks.\n",
    "\n",
    "However, for new users who do not yet have a viewing history, the system might\n",
    "have to make generic recommendations based on their explicitly stated interests\n",
    "or demographic information. Also, for certain popular combinations of input\n",
    "features, caching could indeed be a valuable strategy to increase serving speed,\n",
    "even in a real-time serving setup. This is sometimes referred to as \"hot data\n",
    "caching\" or \"frequently used data caching\".\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "-   The model generates and caches predictions, which enables very fast\n",
    "    inference for users. This is because the model doesn't have to process each\n",
    "    input individually in real-time; instead, it retrieves the precomputed\n",
    "    results from the database.\n",
    "\n",
    "-   The model doesn't need to be hosted as a live service since it's never used\n",
    "    in real-time. This can reduce the resource requirements and complexity of\n",
    "    the deployment infrastructure.\n",
    "\n",
    "#### Disadvantages\n",
    "\n",
    "-   The predictions can become stale or outdated if the user develops new\n",
    "    interests that aren’t captured by the old data on which the current\n",
    "    predictions are based. In other words, the model's predictions won't adapt\n",
    "    quickly to changes in user behavior or other factors.\n",
    "\n",
    "-   The input feature space must be finite because we need to generate all the\n",
    "    predictions before they're needed for real-time. This requirement limits the\n",
    "    applicability of batch serving to certain use cases. For example, it might\n",
    "    not be suitable for scenarios where the potential inputs are diverse or\n",
    "    unpredictable, such as natural language processing tasks.\n",
    "\n",
    "#### Setting up a Batch Serving System\n",
    "\n",
    "If you want to store predictions even when you don't know in advance what input\n",
    "the users will provide, you can design your system to capture and store the\n",
    "predictions along with the corresponding user inputs. Here's a general approach:\n",
    "\n",
    "1. Set up a data storage system: Choose a suitable data storage solution such as\n",
    "   a relational database, NoSQL database, or a data lake to store the\n",
    "   predictions and associated user inputs. Select a storage solution that aligns\n",
    "   with your system requirements in terms of scalability, query capabilities,\n",
    "   and data retention policies.\n",
    "\n",
    "2. Define a data schema: Determine the structure of the data you want to store.\n",
    "   Define the necessary fields to capture the user inputs and the corresponding\n",
    "   predictions. For example, you might include fields such as user ID,\n",
    "   timestamp, input features, and predicted output.\n",
    "\n",
    "3. Capture user inputs: Within your API or application, capture the user inputs\n",
    "   as they interact with the system. Extract the relevant information from the\n",
    "   user's request, such as feature values or any contextual data you need for\n",
    "   prediction.\n",
    "\n",
    "4. Generate predictions: Pass the user inputs to your pre-trained model to\n",
    "   generate predictions. Retrieve the output from the model for the given\n",
    "   inputs.\n",
    "\n",
    "5. Store the data: Once you have the user inputs and the corresponding\n",
    "   predictions, store them in your chosen data storage system. Serialize the\n",
    "   data in a format that can be stored, such as JSON or CSV, and save it in the\n",
    "   appropriate database table or collection.\n",
    "\n",
    "6. Retrieve predictions: At a later time, you can query the data storage system\n",
    "   to retrieve and analyze the stored predictions. You can use SQL queries,\n",
    "   NoSQL queries, or other methods supported by your chosen storage solution to\n",
    "   filter, aggregate, or perform further analysis on the stored data.\n",
    "\n",
    "By implementing this approach, you can store the predictions alongside the user\n",
    "inputs, enabling you to review and analyze the predictions later, gain insights,\n",
    "track performance, and potentially improve your models or system based on the\n",
    "collected data.\n",
    "\n",
    "### Real-Time Serving/Inference (Online with only Batch Features)\n",
    "\n",
    "#### Definition\n",
    "\n",
    "Real-time inference involves making predictions on the fly as soon as a request\n",
    "comes in. This approach is utilized when there's a need for immediate\n",
    "prediction.\n",
    "\n",
    "#### Visualization (Batch Features only)\n",
    "\n",
    "<figure markdown>\n",
    "  ![Online Serving](../assets/online-serving-chip-huyen.png)\n",
    "  <figcaption>A simplified architecture for online prediction that only uses batch features. Image Credit: Huyen, Chip. \"Chapter 7.\n",
    "Model Deployment and Prediction Service.\" In Designing Machine Learning Systems:\n",
    "An Iterative Process for Production-Ready Applications, O'Reilly Media,\n",
    "Inc., 2022.\n",
    " </figcaption>\n",
    "</figure>\n",
    "\n",
    "Say we have a recommender system recommending credit card (items) to users. The\n",
    "precomputer embeddings of credit card features may be static, but the user\n",
    "features may be different for each user. So, the user features are streaming\n",
    "features, and the credit card embeddings are batch features where we can store\n",
    "them in a database and retrieve them when needed to say, compute the cosine\n",
    "similarity between the user features and the credit card embeddings.\n",
    "\n",
    "#### Visualization (Online Features)\n",
    "\n",
    "I do not have a diagram yet, but for instance, the users' real-time\n",
    "activity/features are online features, and the credit card embeddings are batch\n",
    "features.\n",
    "\n",
    "#### Example\n",
    "\n",
    "-   **Fraud detection**: A fraud detection system is a great example of a\n",
    "    real-time inference application. In such a case, the model needs to evaluate\n",
    "    transactions in real-time to prevent fraudulent activities.\n",
    "-   **Real-time personalization**: In e-commerce, models might be used to\n",
    "    personalize the shopping experience by recommending products or offers in\n",
    "    real-time, based on the user's current activity on the site.\n",
    "-   **Chatbots and virtual assistants**: In conversational AI, models need to\n",
    "    generate responses to user inputs in real-time.\n",
    "-   **Autocomplete**: In search engines, models might be used to generate\n",
    "    autocomplete suggestions in real-time as the user types in the search box.\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "-   Provides immediate predictions, which can enhance the user experience and\n",
    "    provide immediate feedback.\n",
    "-   More suitable for systems with an unbounded feature space, where inputs can\n",
    "    change and evolve over time.\n",
    "\n",
    "#### Disadvantages\n",
    "\n",
    "-   Requires more computational resources and infrastructure to handle real-time\n",
    "    requests.\n",
    "-   More complex to implement and manage due to the need to handle potentially\n",
    "    high volumes of real-time requests.\n",
    "-   Requires real-time monitoring to ensure the system is functioning correctly.\n",
    "\n",
    "### Streaming Inference\n",
    "\n",
    "#### Definition\n",
    "\n",
    "Streaming inference refers to the process of making predictions on a continuous\n",
    "stream of incoming data. This method is useful when the data is constantly\n",
    "updating and predictions need to be made as soon as the new data arrives.\n",
    "\n",
    "#### Visualization\n",
    "\n",
    "<figure markdown>\n",
    "  ![Streaming Serving](../assets/online-serving-chip-huyen.png)\n",
    "  <figcaption>A simplified architecture for online prediction that uses both batch features and streaming features. Image Credit: Huyen, Chip. \"Chapter 7.\n",
    "Model Deployment and Prediction Service.\" In Designing Machine Learning Systems:\n",
    "An Iterative Process for Production-Ready Applications, O'Reilly Media,\n",
    "Inc., 2022.\n",
    " </figcaption>\n",
    "</figure>\n",
    "\n",
    "#### Example\n",
    "\n",
    "Streaming inference would be ideal for real-time monitoring of workplace safety\n",
    "based on continuous inputs from various sensors.\n",
    "\n",
    "Imagine a manufacturing plant where safety is crucial. In this environment,\n",
    "there might be various sensors distributed throughout the facility, continuously\n",
    "monitoring factors like temperature, noise levels, vibration, toxic gas\n",
    "concentrations, and other environmental variables. There could also be cameras\n",
    "monitoring the physical activities of workers, ensuring that safety protocols\n",
    "are being followed.\n",
    "\n",
    "A machine learning model could be set up to process this constant stream of\n",
    "sensor data in real time. Using streaming inference, the model would analyze\n",
    "each incoming piece of data as it arrives, looking for patterns or anomalies\n",
    "that might indicate a safety hazard.\n",
    "\n",
    "For example, the model might predict a risk of equipment failure if it detects\n",
    "unusual vibration patterns from a specific machine. Or it might alert to a\n",
    "potential safety violation if it recognizes through video analysis that a worker\n",
    "isn't wearing the required protective gear.\n",
    "\n",
    "By using streaming inference in this way, the system can detect and respond to\n",
    "potential safety issues as soon as they arise, rather than waiting to analyze\n",
    "batches of data after the fact. This real-time response could prevent accidents,\n",
    "protect workers, and maintain productivity.\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "-   Can handle continuous streams of data and provide real-time predictions.\n",
    "-   Particularly suitable for systems where the data is constantly updating and\n",
    "    you need to make predictions as soon as new data arrives.\n",
    "\n",
    "#### Disadvantages\n",
    "\n",
    "-   Requires infrastructure and resources to handle continuous streams of data.\n",
    "-   More complex to implement and manage than batch or real-time inference.\n",
    "-   Also requires real-time monitoring to ensure the system is functioning\n",
    "    correctly.\n",
    "\n",
    "### Hybrid Serving\n",
    "\n",
    "However, online prediction and batch prediction don’t have to be mutually\n",
    "exclusive. One hybrid solution is that you precompute predictions for popular\n",
    "queries, then generate predictions online for less popular\n",
    "queries[^huyen-chip-model-deployment-and-prediction-service].\n",
    "\n",
    "A real-world example of this hybrid solution could be an e-commerce\n",
    "recommendation system.\n",
    "\n",
    "In this system, we could utilize embeddings to represent each product. An\n",
    "embedding is a way of representing a product in a high-dimensional space such\n",
    "that similar products are closer to each other. These embeddings can be\n",
    "pre-computed using historical data and used as batch features.\n",
    "\n",
    "For popular products, you can precompute the \"nearest neighbors\" in the\n",
    "embedding space using batch prediction. This means that for each popular\n",
    "product, you identify a set of other products that are closest to it in the\n",
    "embedding space, i.e., the most similar products. These become your precomputed\n",
    "recommendations for that product.\n",
    "\n",
    "Now, when a customer views a popular product, the system can instantly return\n",
    "the precomputed recommendations without needing to perform any online\n",
    "calculations. This approach helps to reduce the computation load and improve\n",
    "response times for the majority of customer interactions.\n",
    "\n",
    "On the other hand, for less popular products or unique customer queries, you\n",
    "might not have precomputed recommendations. In this case, you need to perform\n",
    "online prediction. When a customer views a less common product, the system would\n",
    "calculate its position in the embedding space and find the nearest neighbors in\n",
    "real-time, thereby generating personalized recommendations.\n",
    "\n",
    "In this way, embeddings can be used in both batch and online prediction\n",
    "settings, enabling a hybrid approach that balances computational efficiency with\n",
    "the ability to handle diverse customer needs.\n",
    "\n",
    "### On-Device Inference\n",
    "\n",
    "#### Definition\n",
    "\n",
    "On-device inference refers to a machine learning model making predictions on the\n",
    "device itself, rather than on a server. This method is often used when there's a\n",
    "need for real-time predictions without relying on a stable internet connection.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Voice recognition models on smartphones, like Siri or Google Assistant, are\n",
    "classic examples of on-device inference. These models need to respond instantly\n",
    "to voice commands, and running the model on the device itself reduces latency.\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "-   Provides immediate predictions without needing a server or internet\n",
    "    connection.\n",
    "-   Can help to preserve user privacy as no data needs to be sent to a server.\n",
    "-   Suitable for applications where the device might not always have a reliable\n",
    "    internet connection.\n",
    "\n",
    "#### Disadvantages\n",
    "\n",
    "-   Limited by the device's computational resources, so models may need to be\n",
    "    smaller and more efficient, potentially reducing their accuracy.\n",
    "-   Can be more difficult to update and maintain models across multiple devices.\n",
    "-   Implementing and managing models across a range of different device types\n",
    "    and operating systems can add complexity.\n",
    "\n",
    "## Batch Processing does not mean Predicting all Possible Inputs in Advance\n",
    "\n",
    "There might be confusion around the batch processing and real-time processing.\n",
    "\n",
    "In the context of machine learning model serving, batch processing doesn't mean\n",
    "predicting all possible inputs in advance. Instead, it means that predictions\n",
    "for multiple inputs are made together in one batch, rather than one at a time.\n",
    "\n",
    "For example, in a recommendation system, users' activities might be collected\n",
    "over a day, and at the end of the day, recommendations for all users are\n",
    "generated together in a batch process and stored in a database. This doesn't\n",
    "require knowing in advance what the users' activities will be.\n",
    "\n",
    "On the other hand, real-time processing means that the model makes a prediction\n",
    "for a single input immediately when it is received, without waiting for more\n",
    "inputs to process together in a batch.\n",
    "\n",
    "The choice between batch processing and real-time processing often depends on\n",
    "the specific requirements of the use case.\n",
    "\n",
    "1. **Batch Processing**: If the application does not require immediate\n",
    "   responses, and computational resources are limited, batch processing can be a\n",
    "   good choice. Batch processing is computationally efficient because it allows\n",
    "   the machine learning system to process large amounts of data at once, taking\n",
    "   full advantage of the parallel processing capabilities of modern hardware.\n",
    "\n",
    "2. **Real-time Processing**: If the application requires immediate responses\n",
    "   (e.g., fraud detection, autonomous vehicles, etc.), real-time processing is\n",
    "   necessary. Real-time processing might be more computationally intensive since\n",
    "   predictions need to be made immediately for each individual input.\n",
    "\n",
    "So, when I mentioned the feature space (finite or unbounded) earlier, it was\n",
    "more about how difficult it might be to engineer features for batch or real-time\n",
    "processing. For finite feature spaces, feature engineering might be simpler,\n",
    "potentially making it easier to use batch processing. But for unbounded feature\n",
    "spaces, feature engineering can be more complex and computationally intensive,\n",
    "potentially making real-time processing more challenging.\n",
    "\n",
    "But ultimately, the choice between batch and real-time depends more on the\n",
    "specific requirements of the application and the computational resources\n",
    "available.\n",
    "\n",
    "## From Batch Prediction to Online Prediction\n",
    "\n",
    "One thing that resonates well from Chip's chapter on model\n",
    "serving[^huyen-chip-model-deployment-and-prediction-service] is that people\n",
    "coming from a traditional modelling or more academic background tend to be\n",
    "accustomed to online prediction as a natural way of transitioning from training\n",
    "to serving.\n",
    "\n",
    "This is likely how most people interact with their models while prototyping.\n",
    "This is also likely easier to do for most companies when first deploying a\n",
    "model. You export your model, upload the exported model to Amazon SageMaker or\n",
    "Google App Engine, and get back an exposed endpoint. Now, if you send a request\n",
    "that contains an input to that endpoint, it will send back a prediction\n",
    "generated on that input[^huyen-chip-model-deployment-and-prediction-service].\n",
    "\n",
    "In other words, I trained a nice model, I wrap it in a nice API like FastAPI,\n",
    "and I deploy it to a cloud service like AWS or GCP. Now, I can send requests to\n",
    "the API and get back predictions. This is online prediction. Rarely do we\n",
    "\"precompute\" predictions in advance and store them in a database for later use.\n",
    "\n",
    "She listed out the pros and cons with online prediction, and I think it's worth\n",
    "to go over her content in details.\n",
    "\n",
    "## Unifying Batch Pipeline and Streaming Pipeline\n",
    "\n",
    "See the section on Chip's\n",
    "book[^huyen-chip-model-deployment-and-prediction-service].\n",
    "\n",
    "## Model Compression\n",
    "\n",
    "See the section on Chip's\n",
    "book[^huyen-chip-model-deployment-and-prediction-service].\n",
    "\n",
    "## Machine Learning on the Cloud and on the Edge\n",
    "\n",
    "See the section on Chip's\n",
    "book[^huyen-chip-model-deployment-and-prediction-service].\n",
    "\n",
    "## References and Further Readings\n",
    "\n",
    "-   Huyen, Chip. \"Chapter 7. Model Deployment and Prediction Service.\" In\n",
    "    Designing Machine Learning Systems: An Iterative Process for\n",
    "    Production-Ready Applications, O'Reilly Media, Inc., 2022.\n",
    "-   [Madewithml: Serving](https://madewithml.com/courses/mlops/api/)\n",
    "\n",
    "[^huyen-chip-model-deployment-and-prediction-service]:\n",
    "    Huyen, Chip. \"Chapter 7. Model Deployment and Prediction Service.\" In\n",
    "    Designing Machine Learning Systems: An Iterative Process for\n",
    "    Production-Ready Applications, O'Reilly Media, Inc., 2022."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "mystnb": {
   "number_source_lines": true
  },
  "source_map": [
   16
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}