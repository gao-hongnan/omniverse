{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f909d2",
   "metadata": {},
   "source": [
    "# Stage 5.2. Metric Selection\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Brain_Dump-red)\n",
    "![Tag](https://img.shields.io/badge/Level-Beginner-green)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "\n",
    "## Metric Selection\n",
    "\n",
    "Before beginning model training, it's crucial to define the metrics by which the\n",
    "model's performance will be evaluated. These metrics will guide the optimization\n",
    "process and provide a quantitative measure of the model's quality.\n",
    "\n",
    "The appropriate metric depends on the type of machine learning task, the data,\n",
    "and the specific requirements of the project:\n",
    "\n",
    "-   **Classification**: Here, we're predicting categorical outcomes. Common\n",
    "    metrics include Accuracy (percentage of correct predictions), Precision\n",
    "    (proportion of true positive predictions out of all positive predictions),\n",
    "    Recall (proportion of true positive predictions out of all actual\n",
    "    positives), F1-Score (harmonic mean of precision and recall), and AUC-ROC\n",
    "    (area under the Receiver Operating Characteristic curve, representing the\n",
    "    model's ability to distinguish between classes).\n",
    "\n",
    "    Do be aware of the differences of threshold invariant and scale invariant\n",
    "    metrics, for instance, AUC-ROC is threshold invariant and scale invariant,\n",
    "    an ideal metric when looking at the model's ability to distinguish between\n",
    "    classes.\n",
    "\n",
    "    A last point is some metrics can be misleading, for instance, accuracy can\n",
    "    be misleading when the classes are imbalanced, and including PR-AUC may be a\n",
    "    better choice.\n",
    "\n",
    "-   **Regression**: In regression problems, we're predicting continuous values.\n",
    "    Metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean\n",
    "    Squared Error (RMSE), and R-squared are typically used.\n",
    "\n",
    "    MAE measures the average magnitude of errors in a set of predictions,\n",
    "    without considering their direction. MSE and RMSE are similar to MAE but\n",
    "    amplify the impact of large errors. R-squared (coefficient of determination)\n",
    "    explains how much of the dependent variable's variation is explained by the\n",
    "    independent variable(s).\n",
    "\n",
    "-   **Ranking**: In ranking problems, we're interested in the order of\n",
    "    predictions. The Normalized Discounted Cumulative Gain (NDCG) measures the\n",
    "    quality of a ranking, taking into account the positions of the relevant\n",
    "    items. Precision at K is another commonly used metric, providing the\n",
    "    proportion of relevant items among the top K items.\n",
    "\n",
    "Remember that the choice of a metric should align with the business objective.\n",
    "For instance, if false positives and false negatives have a different cost, you\n",
    "might want to optimize for Precision or Recall rather than overall Accuracy.\n",
    "\n",
    "Another thing to consider is whether you are more interested in the ranking of\n",
    "predictions (which prediction is ranked highest) or in the absolute values of\n",
    "predictions (predicting the exact value). Depending on this, you might choose to\n",
    "optimize for AUC-ROC or Log Loss for a classification problem, or for RMSE or\n",
    "Mean Absolute Percentage Error (MAPE) for a regression problem.\n",
    "\n",
    "Finally, when comparing different models, it's important to use the same metric\n",
    "for a fair comparison. If one model is optimized for Accuracy and another for\n",
    "F1-Score, they might perform differently when evaluated using a single, common\n",
    "metric.\n",
    "\n",
    "It's not uncommon to use multiple metrics to evaluate a model's performance, as\n",
    "they can each highlight different aspects of the model's behavior. For instance,\n",
    "in a binary classification problem, accuracy might not be a good metric if the\n",
    "classes are imbalanced; precision and recall or the F1-score might be more\n",
    "informative.\n",
    "\n",
    "### Benefit Structure\n",
    "\n",
    "The \"benefit structure\" or \"cost-benefit analysis\" is an approach to model\n",
    "evaluation that goes beyond standard metrics like accuracy, precision, and\n",
    "recall. It involves assigning specific costs and benefits to different types of\n",
    "errors and correct decisions the model makes, based on their real-world\n",
    "implications. This analysis can be especially crucial in fields like healthcare,\n",
    "where different types of errors can have drastically different consequences.\n",
    "\n",
    "Let's illustrate this with an example of cancer detection.\n",
    "\n",
    "Suppose we are building a machine learning model to detect whether a patient has\n",
    "cancer based on various diagnostic tests. The prediction can either be positive\n",
    "(cancer detected) or negative (no cancer detected). However, these predictions\n",
    "can either be correct or incorrect, leading to four possible outcomes:\n",
    "\n",
    "-   **True Positive (TP)**: The model correctly identifies a cancer patient.\n",
    "-   **True Negative (TN)**: The model correctly identifies a healthy patient.\n",
    "-   **False Positive (FP)**: The model incorrectly identifies a healthy patient\n",
    "    as having cancer.\n",
    "-   **False Negative (FN)**: The model incorrectly identifies a cancer patient\n",
    "    as healthy.\n",
    "\n",
    "We can construct a table like this:\n",
    "\n",
    "| -                         | Cancer (actual)                                     | No Cancer (actual)                            |\n",
    "| ------------------------- | --------------------------------------------------- | --------------------------------------------- |\n",
    "| **Cancer (predicted)**    | TP (Benefit: early treatment, higher survival rate) | FP (Cost: unnecessary worry, further tests)   |\n",
    "| **No Cancer (predicted)** | FN (Cost: delayed treatment, lower survival rate)   | TN (Benefit: peace of mind, no further tests) |\n",
    "\n",
    "The benefits and costs associated with each outcome can be estimated:\n",
    "\n",
    "-   **True Positive (TP)**: Early detection of cancer can lead to early\n",
    "    treatment, which significantly increases the survival rate. We might assign\n",
    "    a high benefit value here, say +100.\n",
    "\n",
    "-   **True Negative (TN)**: Correctly identifying a patient without cancer can\n",
    "    provide peace of mind and avoid unnecessary further tests. We might assign a\n",
    "    moderate benefit value here, say +10.\n",
    "\n",
    "-   **False Positive (FP)**: A false positive result can lead to unnecessary\n",
    "    worry and further tests for the patient. We could assign a moderate cost\n",
    "    value here, say -20.\n",
    "\n",
    "-   **False Negative (FN)**: A false negative is very harmful because it might\n",
    "    lead to delayed treatment, significantly decreasing the survival rate.\n",
    "    Therefore, we could assign a high cost value here, say -200.\n",
    "\n",
    "Using this benefit structure, we can evaluate the model not just on how often\n",
    "it's right or wrong, but also on the real-world impact of its predictions.\n",
    "\n",
    "A model with a higher overall benefit score (i.e., a weighted sum of TP, TN, FP,\n",
    "FN using the assigned benefit/cost values) would be preferred over one with\n",
    "lower benefit score, even if the latter has higher accuracy.\n",
    "\n",
    "In conclusion, benefit structure allows for a more nuanced evaluation of model\n",
    "performance by considering the specific implications of each type of prediction\n",
    "error. This type of analysis is particularly relevant in critical\n",
    "decision-making contexts, such as medical diagnosis, credit scoring, or fraud\n",
    "detection.\n",
    "\n",
    "### Calibration\n",
    "\n",
    "Calibration in the context of machine learning models refers to how well the\n",
    "model's predicted probabilities of outcomes match the actual outcomes'\n",
    "frequencies in the real world. A well-calibrated model means that if the model\n",
    "predicts an event with a probability of $p$, that event should indeed occur\n",
    "approximately $p$ percent of the time. For example, if a model predicts that\n",
    "there's a $70 \\%$ chance of rain tomorrow, then, ideally, it should rain 70 out\n",
    "of 100 similar days with that prediction.\n",
    "\n",
    "```{admonition} See Also\n",
    ":class: seealso\n",
    "\n",
    "- [Why model calibration matters and how to achieve it](https://www.unofficialgoogledatascience.com/2021/04/why-model-calibration-matters-and-how.html).\n",
    "```\n",
    "\n",
    "#### Why Does Calibration Matter?\n",
    "\n",
    "The output of classification models can often be interpreted as the probability\n",
    "that a given instance belongs to the positive class. For example, in binary\n",
    "classification, a model might predict a value of 0.7 for a certain instance,\n",
    "which can be interpreted as a 70% probability that this instance belongs to the\n",
    "positive class.\n",
    "\n",
    "However, these predicted probabilities are not always accurate. For example, a\n",
    "model might predict a probability of 70% for a set of instances, but only 60% of\n",
    "them actually belong to the positive class. This discrepancy between the\n",
    "predicted probabilities and the true probabilities is a sign that the model is\n",
    "not well-calibrated.\n",
    "\n",
    "A well-calibrated model is one where, for all instances where it predicts a\n",
    "probability of P, the proportion of those instances that are positive is\n",
    "actually P. For example, if we gather all instances where the model predicts a\n",
    "70% probability, 70% of those instances should indeed be positive. Calibration\n",
    "is particularly important in cases where the predicted probabilities are used to\n",
    "make decisions that depend not just on the class labels, but also on the\n",
    "certainty of the prediction.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Let's consider a model that predicts whether a patient has a certain disease\n",
    "(the positive class) or not (the negative class) based on some diagnostic tests.\n",
    "Let's assume we have a set of 1000 patients and the model makes predictions on\n",
    "this set.\n",
    "\n",
    "We could compile the model's predictions and the actual outcomes into a table\n",
    "like this:\n",
    "\n",
    "| Predicted Probability | Number of Predictions | Actual Positives |\n",
    "| --------------------- | --------------------- | ---------------- |\n",
    "| 0.1-0.2               | 200                   | 30               |\n",
    "| 0.2-0.3               | 180                   | 50               |\n",
    "| 0.3-0.4               | 150                   | 60               |\n",
    "| 0.4-0.5               | 120                   | 65               |\n",
    "| 0.5-0.6               | 100                   | 70               |\n",
    "| 0.6-0.7               | 90                    | 65               |\n",
    "| 0.7-0.8               | 80                    | 65               |\n",
    "| 0.8-0.9               | 50                    | 45               |\n",
    "| 0.9-1.0               | 30                    | 30               |\n",
    "\n",
    "In this table, we've binned the predictions into ranges like 0.1-0.2, 0.2-0.3,\n",
    "etc. Then we count the number of predictions in each bin and the number of\n",
    "actual positives in each bin.\n",
    "\n",
    "Ideally, the proportion of actual positives in each bin should match the\n",
    "predicted probability. For example, in the 0.3-0.4 bin, the model predicts a\n",
    "probability of 30% to 40%, and the actual proportion of positives is 60/150 =\n",
    "40%, which is well within the range.\n",
    "\n",
    "But if we look at the 0.7-0.8 bin, the model predicts a 70% to 80% probability,\n",
    "but the actual proportion of positives is 65/80 = 81.25%. This is a sign that\n",
    "the model is not well-calibrated for this range of probabilities, as the actual\n",
    "positive rate is higher than the predicted probability.\n",
    "\n",
    "In practice, we often visualize this data with a calibration plot, where the\n",
    "x-axis represents the predicted probabilities and the y-axis represents the\n",
    "actual positive rates. A well-calibrated model would lie along the line y = x,\n",
    "while deviations from this line indicate miscalibration.\n",
    "\n",
    "#### Calibrating Models\n",
    "\n",
    "There are several methods to calibrate a model after it has been trained. One of\n",
    "the most popular methods is Platt Scaling, which fits a logistic regression\n",
    "model to the model's scores. This logistic regression model can adjust the\n",
    "predicted probabilities to make them more accurate. Another popular method is\n",
    "Isotonic Regression, which fits a piecewise-constant non-decreasing function to\n",
    "the model's scores.\n",
    "\n",
    "```{admonition} See Also\n",
    ":class: seealso\n",
    "\n",
    "-   [1.16. Probability calibration (Scikit-Learn)](https://scikit-learn.org/stable/modules/calibration.html)\n",
    "```\n",
    "\n",
    "#### Calibration and Evaluation (Brier + AUROC combo)\n",
    "\n",
    "Once a model is calibrated, we need a way to evaluate how well-calibrated it is.\n",
    "This can be done using calibration plots, also known as reliability diagrams. In\n",
    "a calibration plot, the x-axis represents the predicted probabilities and the\n",
    "y-axis represents the proportion of instances that are positive. A\n",
    "well-calibrated model will have its calibration plot close to the diagonal.\n",
    "\n",
    "Another common way to evaluate calibration is using the Brier score. The Brier\n",
    "score is a metric that combines calibration and refinement. The Brier score is\n",
    "given by:\n",
    "\n",
    "$$\n",
    "BS = \\frac{1}{N}\\sum_{t=1}^{N}(f_t-o_t)^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "-   $N$ is the total number of instances,\n",
    "-   $f_t$ is the predicted probability for instance $t$,\n",
    "-   $o_t$ is the true outcome for instance $t$ (1 for positive and 0 for\n",
    "    negative).\n",
    "\n",
    "The Brier score ranges from 0 for a perfect model to 1 for a constantly wrong\n",
    "model. However, the Brier score alone is not sufficient to evaluate a model, as\n",
    "it does not account for the model's ability to discriminate between classes. For\n",
    "this reason, it is often used in combination with the AUROC (Area Under the\n",
    "Receiver Operating Characteristic curve) metric.\n",
    "\n",
    "While the AUROC measures how well the model can distinguish between classes, it\n",
    "does not take into account the accuracy of the predicted probabilities. Thus,\n",
    "using the Brier score and the AUROC together can provide a more complete\n",
    "evaluation of a model's performance. By optimizing these two metrics\n",
    "simultaneously, we can obtain a model that not only discriminates well between\n",
    "classes, but also provides accurate predicted probabilities."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "mystnb": {
   "number_source_lines": true
  },
  "source_map": [
   16
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}