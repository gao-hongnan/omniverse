{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4a37ae",
   "metadata": {},
   "source": [
    "# Stage 5.3. Experiment Tracking And Versioning\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Brain_Dump-red)\n",
    "![Tag](https://img.shields.io/badge/Level-Beginner-green)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "\n",
    "During the iterative process of model development and training, keeping track of\n",
    "the numerous experiments run, their parameters, and outcomes is a crucial step.\n",
    "As models are trained, tested, tweaked, and retrained, it becomes increasingly\n",
    "complex to manage and compare these various experiments.\n",
    "\n",
    "This is where experiment tracking comes into play. By tracking each experiment,\n",
    "data scientists can easily compare the results of different models,\n",
    "configurations, hyperparameters, and even completely different approaches. Also\n",
    "it enables error analysis, debugging, and model improvement.\n",
    "\n",
    "## Experiment Tracking\n",
    "\n",
    "What do we track then? Can we track everything? Well if you can, that's great,\n",
    "but it's often better and more concise to track things that matter the most for\n",
    "monitoring and **_debugging_**. Here are some common metrics to track in\n",
    "experiment tracking:\n",
    "\n",
    "```{list-table} Some Common Metrics to Track in Experiment Tracking\n",
    ":header-rows: 1\n",
    ":name: ml-lifecycle-experiment-tracking-metrics\n",
    "\n",
    "-   -   Aspect\n",
    "    -   Description\n",
    "-   -   Model Architecture\n",
    "    -   The type of model used, including the number of layers, activation\n",
    "        functions, and other architectural details.\n",
    "-   -   Hyperparameters\n",
    "    -   The settings and hyperparameters used for each model.\n",
    "-   -   Evaluation Metrics\n",
    "    -   How each model performed according to the selected evaluation metrics.\n",
    "-   -   Feature Importance\n",
    "    -   Which features were most influential in the model's predictions.\n",
    "-   -   System Metrics\n",
    "    -   Resource usage, training time, and other system-level metrics such as\n",
    "        what GPU was used, how much memory was used, etc.\n",
    "-   -   Gradient Norm (Global and Per Layer), Activation Distribution and Norms,\n",
    "        Weight Distribution etc\n",
    "    -   These are important indicators of how well the model is training, and\n",
    "        can be used to diagnose issues like vanishing gradients, exploding\n",
    "        gradients, etc. And for example if your embedding layer keep exploding,\n",
    "        you may want to initialize the weights with smaller values.\n",
    "```\n",
    "\n",
    "It's also worth noting that several tools can facilitate experiment tracking,\n",
    "such as MLflow, TensorBoard, and Weights & Biases. By adopting such tools, teams\n",
    "can create a central repository of experiments that enable collaboration and\n",
    "reproducibility. It becomes easier to revisit old experiments, share findings\n",
    "with team members, and ultimately make more informed decisions about which\n",
    "models and configurations to move forward with.\n",
    "\n",
    "If you can, tracking as many key indicators and metrics as possible is a good\n",
    "way to help debug model issues (i.e. why is model diverging at the 100th step)\n",
    "and memory leak issues (i.e. why is the CUDA memory usage increasing over time).\n",
    "\n",
    "## Reproducibility\n",
    "\n",
    "To ensure that your machine learning experiments are reproducible, you should\n",
    "keep track of the following components:\n",
    "\n",
    "1. **Code**\n",
    "2. **Data**\n",
    "3. **Model config, artifacts and metadata**\n",
    "4. **Environment**\n",
    "5. **Seeding**\n",
    "\n",
    "### Model Versioning, Code Versioning, and Data Versioning\n",
    "\n",
    "In addition to tracking experiments, it's also important to version the models,\n",
    "code, and data used in those experiments. This ensures that the results of an\n",
    "experiment can be reproduced at a later time, even if the code, data, or\n",
    "environment have changed.\n",
    "\n",
    "Now it is worth mentioning that tracking model is the key, and since model is a\n",
    "combination of code and data, it is important to track the code and data as\n",
    "well[^chip-chapter6]. Tracking code can be easily done via version control\n",
    "systems like Git, and tracking data can be done via data versioning tools like\n",
    "DVC.\n",
    "\n",
    "Below we see some pseudo code on how to track the code, data, and model\n",
    "artifacts.\n",
    "\n",
    "#### 1. Code versioning\n",
    "\n",
    "Use a version control system like **Git** to keep track of your codebase. Git\n",
    "allows you to track changes in your code over time and manage different\n",
    "versions. To log the exact commit hash of your codebase when logging your MLflow\n",
    "run, you can use the following code snippet:\n",
    "\n",
    "```python\n",
    "import subprocess\n",
    "\n",
    "commit_hash = (\n",
    "    subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).decode(\"utf-8\").strip()\n",
    ")\n",
    "mlflow.log_param(\"commit_hash\", commit_hash)\n",
    "```\n",
    "\n",
    "By logging the commit hash, you can always refer back to the exact version of\n",
    "the code used for a specific run, ensuring reproducibility.\n",
    "\n",
    "#### 2. Data versioning\n",
    "\n",
    "For data versioning, you can use a tool like **DVC (Data Version Control)**. DVC\n",
    "is designed to handle large data files, models, and metrics, and it integrates\n",
    "well with Git. DVC helps you track changes in your data files and manage\n",
    "different versions.\n",
    "\n",
    "When you start a new MLflow run, log the DVC version or metadata of the input\n",
    "data used in the experiment. This way, you can always retrieve the exact version\n",
    "of the data used for a specific run, ensuring reproducibility.\n",
    "\n",
    "See [Data Management Tutorial](https://dvc.org/doc/start/data-management) for\n",
    "more insights.\n",
    "\n",
    "Important points to consider.\n",
    "\n",
    "-   gitignore will be created automatically in data folder once you dvc add.\n",
    "-   After successfully pushing the data to remote, how do you \"retrieve them\"?\n",
    "-   If you are in the same repository, you can just pull the data from remote.\n",
    "\n",
    "The idea is to use dvc checkout to switch between different versions of your\n",
    "data files, as tracked by DVC. When you use dvc checkout, you provide a Git\n",
    "commit hash or tag. DVC will then update your working directory with the data\n",
    "files that were tracked at that specific Git commit.\n",
    "\n",
    "Here are the steps to use dvc checkout with a Git commit hash:\n",
    "\n",
    "-   Make sure you have the latest version of your repository and DVC remote by\n",
    "    running git pull and dvc pull.\n",
    "-   Switch to the desired Git commit by running git checkout `<commit-hash>`.\n",
    "-   Run dvc checkout to update your data files to the version tracked at the\n",
    "    specified commit.\n",
    "\n",
    "Remember that dvc checkout only updates the data files tracked by DVC. To switch\n",
    "between code versions, you'll still need to use git checkout.\n",
    "\n",
    "```bash\n",
    "git checkout <commit_hash>\n",
    "dvc checkout # in this commit hash\n",
    "dvc pull\n",
    "```\n",
    "\n",
    "#### 3. Model artifacts and metadata\n",
    "\n",
    "You have already logged the artifacts (model, vectorizer, config, log files)\n",
    "using `mlflow.log_artifact()`. You can also log additional metadata related to\n",
    "the artifacts as you have done with additional_metadata. This should be\n",
    "sufficient for keeping track of the artifacts associated with each run.\n",
    "\n",
    "#### Recovering a run\n",
    "\n",
    "1. Check the commit hashes for the code and data used in the run.\n",
    "2. Checkout the code and data versions using the commit hashes.\n",
    "\n",
    "```bash\n",
    "git checkout <commit_hash>\n",
    "pip install -r requirements.txt\n",
    "python main.py train\n",
    "# once done\n",
    "git checkout main\n",
    "```\n",
    "\n",
    "By combining code versioning with Git, data versioning with DVC, and logging\n",
    "artifacts and metadata with MLflow, you can ensure that your machine learning\n",
    "experiments are reproducible. This means that you can always go back and\n",
    "reproduce the results of a specific experiment, even if the code, data, or\n",
    "environment have changed but is it always the case? We see that in the next\n",
    "section.\n",
    "\n",
    "### Seeding\n",
    "\n",
    "I won't go into too much on this, but beyond versioning, one must ensure\n",
    "aggresive seeding in their code base, especially in non-deterministic operations\n",
    "like training deep learning models. This is however not so simple, even with\n",
    "aggresive seeding, the same code might produce slightly different results if\n",
    "trained on a different hardware. Furthermore, a common mistake in the resumption\n",
    "of training is not saving the rng states, and since dataloaders in frameworks\n",
    "like pytorch will shuffle (if set to true) on each epoch, it may come as a shock\n",
    "that resuming training will produce different results.\n",
    "\n",
    "Just have a look at the below seeding functions I use for single node single GPU\n",
    "training:\n",
    "\n",
    "```python\n",
    "def configure_deterministic_mode() -> None:\n",
    "    # fmt: off\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    torch.backends.cudnn.benchmark        = False\n",
    "    torch.backends.cudnn.deterministic    = True\n",
    "    torch.backends.cudnn.enabled          = False\n",
    "\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    # fmt: on\n",
    "    warnings.warn(\n",
    "        \"Deterministic mode is activated. This will negatively impact performance and may cause increase in CUDA memory footprint.\",\n",
    "        category=UserWarning,\n",
    "        stacklevel=2,\n",
    "    )\n",
    "\n",
    "\n",
    "def seed_all(\n",
    "    seed: int = 1992,\n",
    "    seed_torch: bool = True,\n",
    "    set_torch_deterministic: bool = True,\n",
    ") -> int:\n",
    "    # fmt: off\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)       # set PYTHONHASHSEED env var at fixed value\n",
    "    np.random.default_rng(seed)                    # numpy pseudo-random generator\n",
    "    random.seed(seed)                              # python's built-in pseudo-random generator\n",
    "\n",
    "    if seed_torch:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)           # pytorch (both CPU and CUDA)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.enabled = False\n",
    "\n",
    "        if set_torch_deterministic:\n",
    "            configure_deterministic_mode()\n",
    "    # fmt: on\n",
    "    return seed\n",
    "```\n",
    "\n",
    "And seeding in distributed training is much more complex, so to fully ensure a\n",
    "reproducible training, one must not only ensure good versioning practices, but\n",
    "also ensure the seeding mechanism is in place.\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "When training machine learning models, each combination of dataset and model\n",
    "requires a unique set of hyperparameters, which act as configurable variables.\n",
    "Determining the optimal hyperparameters involves conducting multiple experiments\n",
    "where different sets are tested with the model, a process known as\n",
    "hyperparameter tuning. Essentially, this involves training the model repeatedly,\n",
    "each time with a different configuration of hyperparameters. This tuning can be\n",
    "performed manually or through automated methods[^aws-hyperparameter-tuning].\n",
    "\n",
    "Regardless of the chosen method, it's crucial to systematically record the\n",
    "outcomes of these experiments. This typically involves using statistical\n",
    "measures, like the loss function, to evaluate and identify which hyperparameters\n",
    "yield the most effective results. Hyperparameter tuning is a critical component\n",
    "of model development and requires significant computational resources.\n",
    "\n",
    "There's many methods, for example, random search, grid search, Bayesian\n",
    "optimization, and more recently, evolutionary algorithms. The choice of method\n",
    "really depends on many factors, such as the size of the search space, the\n",
    "computational resources available, and the time constraints. What is important\n",
    "is that the hyperparameters obey the same rules as the training phase, and no\n",
    "data leakage should occur.\n",
    "\n",
    "## References and Further Readings\n",
    "\n",
    "-   Huyen, Chip. \"Chapter 6. Model Development and Offline Evaluation.\" In\n",
    "    Designing Machine Learning Systems: An Iterative Process for\n",
    "    Production-Ready Applications, O'Reilly Media, Inc., 2022.\n",
    "-   [What is Hyperparameter Tuning?](https://aws.amazon.com/what-is/hyperparameter-tuning/)\n",
    "\n",
    "[^chip-chapter6]:\n",
    "    Huyen, Chip. \"Chapter 6. Model Development and Offline Evaluation.\" In\n",
    "    Designing Machine Learning Systems: An Iterative Process for\n",
    "    Production-Ready Applications, O'Reilly Media, Inc., 2022.\n",
    "\n",
    "[^aws-hyperparameter-tuning]:\n",
    "    [What is Hyperparameter Tuning?](https://aws.amazon.com/what-is/hyperparameter-tuning/)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "mystnb": {
   "number_source_lines": true
  },
  "source_map": [
   16
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}