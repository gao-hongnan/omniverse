{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c505144b",
   "metadata": {},
   "source": [
    "# Stage 3.3. Extract, Transform, Load (ETL)\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Brain_Dump-red)\n",
    "![Tag](https://img.shields.io/badge/Level-Beginner-green)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "\n",
    "## The Evolution of Data Engineering (Don't Quote Me On This!)\n",
    "\n",
    "With the basic understanding of **Data Engineering** and its essential role in\n",
    "**Machine Learning**, it's important to recognize the evolution of data handling\n",
    "practices. Traditional **ETL (Extract, Transform, Load)** methodologies have\n",
    "long been the backbone of data pipeline design. They set the stage for\n",
    "collecting, processing, and storing data in a structured manner.\n",
    "\n",
    "However, the modern era of data-driven applications demands a more agile and\n",
    "responsive approach. This is where **DataOps**, encompassing principles of\n",
    "**Continuous Integration/Continuous Deployment (CI/CD)**, comes into play. The\n",
    "process builds on the ETL framework but now with automation, collaboration,\n",
    "monitoring, and quality assurance.\n",
    "\n",
    "In traditional ETL or ELT processes, the main focus is on extracting data from\n",
    "various sources, transforming it into the required format, and then loading it\n",
    "into a target system. These processes are typically batch-oriented and can be\n",
    "run on schedules or triggered manually.\n",
    "\n",
    "In a CI/CD DataOps pipeline, the focus expands to the entire data lifecycle and\n",
    "emphasizes automation, continuous integration, and continuous deployment. This\n",
    "means that the process not only includes the basic ETL or ELT steps but also\n",
    "involves:\n",
    "\n",
    "-   **Continuous Integration**: Automating the process of integrating code\n",
    "    changes from multiple contributors into a shared repository, often followed\n",
    "    by automated building and testing.\n",
    "-   **Continuous Deployment**: Automating the process of deploying the\n",
    "    integrated and tested code to production environments, ensuring that the\n",
    "    data pipeline remains stable and updated.\n",
    "-   **Monitoring and Alerting**: Keeping track of the performance and health of\n",
    "    the data pipeline, triggering alerts if anomalies or issues are detected.\n",
    "-   **Testing and Quality Assurance**: Embedding rigorous testing within the\n",
    "    pipeline to ensure data quality, integrity, and compliance with business\n",
    "    rules.\n",
    "\n",
    "## The ETL/ELT Framework\n",
    "\n",
    "### ETL (Extract, Transform, Load)\n",
    "\n",
    "**ETL** is a process in data handling that involves three main stages:\n",
    "\n",
    "1. **Extract**: Gathering data from various sources.\n",
    "2. **Transform**: Processing this data to fit the desired format, usually\n",
    "   outside the target system. This might include cleaning, aggregating,\n",
    "   filtering, etc.\n",
    "3. **Load**: Finally, loading the transformed data into the destination data\n",
    "   warehouse or database.\n",
    "\n",
    "### ELT (Extract, Load, Transform)\n",
    "\n",
    "**ELT** is a variant of ETL, but with a different order of operations:\n",
    "\n",
    "1. **Extract**: Gathering data from various sources.\n",
    "2. **Load**: Loading the raw data into the destination system.\n",
    "3. **Transform**: Performing transformations within the target system itself,\n",
    "   utilizing the processing capabilities of modern data warehouses.\n",
    "\n",
    "### ELTL (Extract, Load, Transform, Load)\n",
    "\n",
    "This combination could represent a two-step process:\n",
    "\n",
    "1. **Extract**: Gathering data from various sources.\n",
    "2. **Load**: Loading the raw data into a staging area or temporary storage.\n",
    "3. **Transform**: Performing transformations within this temporary storage.\n",
    "4. **Load**: Loading the transformed data into the final destination, such as a\n",
    "   data warehouse or database.\n",
    "\n",
    "## Step 0. Project Scope\n",
    "\n",
    "As usual, defining the scope of your project is the first step in any DataOps\n",
    "pipeline. The scope of your project will determine the type of data you need to\n",
    "extract, the tools you'll use to extract it, and the type of data you'll need to\n",
    "store and process.\n",
    "\n",
    "## Step 1. **E**xtract (Data Extraction and Defining the Data Source)\n",
    "\n",
    "Data extraction refers to the process of retrieving or gathering data from\n",
    "various sources, such as websites, APIs, databases, or files. This process often\n",
    "involves techniques like web scraping, API requests, or database queries. Data\n",
    "extraction is usually the first step in the broader process of data collection.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The key of this step is to scope the **data source**.\n",
    "\n",
    "This means identifying the data you need to extract and the sources where you\n",
    "can find it. You can extract data from various sources, such as web pages, APIs,\n",
    "databases, and public/private datasets. You can also extract data in real-time\n",
    "from streaming sources, such as social media feeds, or log files.\n",
    "\n",
    "For example, let's say you are a movie streaming provider and you want to build\n",
    "a movie recommendation system. You need to extract data about movies and users.\n",
    "You can extract this data from various sources such as IMDb, Rotten Tomatoes, as\n",
    "well as your internal database of user interactions in order to build your\n",
    "recommendation system. This step is non-trivial in practice because no one is\n",
    "going to give you a dataset with all the cleaned data you need magically.\n",
    "\n",
    "### Definition\n",
    "\n",
    "[**Data extraction**](https://en.wikipedia.org/wiki/Data_extraction) is the act\n",
    "or process of retrieving data out of (usually unstructured or poorly\n",
    "[structured](https://en.wikipedia.org/wiki/Unstructured_data)) data sources\n",
    "(such as websites, APIs) for further data processing or data storage (data\n",
    "migration).\n",
    "\n",
    "This version of the definition **does not entail loading the extracted data into\n",
    "an intermediate storage**. Instead, we are only interested in identifying the\n",
    "key points listed in the next section.\n",
    "\n",
    "### Template\n",
    "\n",
    "#### **What** (raw) data do I need to extract (relevant to my project)?\n",
    "\n",
    "-   _Describe the types of data required for your project (e.g., text data,\n",
    "    images, time-series data, etc.)._\n",
    "-   _What information do you need from the data (e.g., sentiment, keywords,\n",
    "    trends, etc.)?_\n",
    "\n",
    "#### **Where** can I find this data? This means finding all potential data sources\n",
    "\n",
    "-   _List potential sources where the required data can be found (e.g.,\n",
    "    websites, APIs, databases, public or private datasets, etc.)._\n",
    "-   _Are there any restrictions or limitations on accessing the data (e.g.,\n",
    "    licensing, API rate limits, etc.)?_\n",
    "\n",
    "#### **What** format is the data in? Is it in a format that I can use?\n",
    "\n",
    "-   _Describe the format of the data in each source (e.g., JSON, CSV, XML, plain\n",
    "    text, etc.)._\n",
    "-   _If the data format is not directly usable, what conversion or\n",
    "    transformation is needed?_\n",
    "\n",
    "#### **How** do I access and extract the data?\n",
    "\n",
    "-   _List the methods to access the data (e.g., web scraping, API requests,\n",
    "    database queries, file downloads, etc.)._\n",
    "-   _Are there any authentication or authorization requirements to access the\n",
    "    data (e.g., API keys, login credentials, etc.)?_\n",
    "\n",
    "#### **What** tools do I need to extract the data?\n",
    "\n",
    "-   _For each data access method, list the appropriate tools or libraries (e.g.,\n",
    "    Scrapy for web scraping, Requests for API requests, etc.)._\n",
    "-   _Are there any specific configurations or setups needed for using these\n",
    "    tools (e.g., user agents, request headers, etc.)?_\n",
    "\n",
    "### Workflow\n",
    "\n",
    "Data collection involves gathering raw data from various sources and making it\n",
    "available for further processing in your DataOps pipeline. We see below for some\n",
    "common techniques and tools for data extraction, along with their advantages and\n",
    "disadvantages.\n",
    "\n",
    "| Description (What)                                                                                                                                  | Data Sources (Where)                              | Data Formats                                     | Access & Extraction (How) | Tools and Libraries                                                                                                                                                                                        | Requirements / Configurations                                                                      |\n",
    "| --------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------- | ------------------------------------------------ | ------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- |\n",
    "| Extract **text data**, **images**, and other content from web pages, such as movie reviews, product listings, or news articles.                     | Websites, Web forums, Blogs                       | HTML, CSS, JavaScript                            | Web Scraping              | [Scrapy](https://scrapy.org/), [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/), [Selenium](https://www.selenium.dev/), [lxml](https://lxml.de/)                                           | User agents, Request headers, Handling AJAX requests, Managing cookies, Captcha solving            |\n",
    "| Retrieve data from various **APIs** provided by platforms like **Twitter**, **Reddit**, news organizations, government data, or other services.     | APIs (Twitter, Reddit, IMDb, etc.)                | JSON, XML, CSV                                   | API Requests              | [Requests](https://docs.python-requests.org/en/master/), [Tweepy (Twitter)](https://www.tweepy.org/), [PRAW (Reddit)](https://praw.readthedocs.io/en/stable/), [Pandas](https://pandas.pydata.org/)        | API keys, Access tokens, Rate limits, Pagination, Error handling                                   |\n",
    "| Access large datasets like **Common Crawl**, **Wikipedia dumps**, or proprietary data from private sources, including text, images, and multimedia. | Public/Private Datasets (Common Crawl, Wikipedia) | CSV, TSV, Excel, JSON, XML, Images, Audio, Video | Downloading datasets      | [Common Crawl API](https://commoncrawl.org/the-data/get-started/), [WikiExtractor](https://github.com/attardi/wikiextractor), [Pandas](https://pandas.pydata.org/), [OpenCV (Images)](https://opencv.org/) | Licensing, Data access restrictions, Data storage and management, File format conversion           |\n",
    "| Collect data in **real-time** from **streaming sources**, such as social media feeds, or log files.                                                 | Social media feeds, IoT devices, Log files        | JSON, XML, CSV, Binary, Text                     | Data Streaming            | [Apache Kafka](https://kafka.apache.org/), [Amazon Kinesis](https://aws.amazon.com/kinesis/), [Google Cloud Pub/Sub](https://cloud.google.com/pubsub)                                                      | Real-time processing, Stream configuration, Handling data partitioning and replication             |\n",
    "| Extract data from **structured** or **semi-structured databases**, such as **relational databases**, **NoSQL databases**, or **data warehouses**.   | Databases (SQL, NoSQL, Data warehouses)           | SQL, NoSQL, CSV, TSV, JSON, XML                  | Database queries          | [SQL queries](https://en.wikipedia.org/wiki/SQL), [SQLAlchemy](https://www.sqlalchemy.org/), [PyMySQL (MySQL)](https://pypi.org/project/PyMySQL/), [psycopg2 (PostgreSQL)](https://www.psycopg.org/)       | Authentication, Connection strings, Query optimization, Managing database connections and sessions |\n",
    "\n",
    "After collecting the data, it's important to **ingest** it into your **DataOps\n",
    "pipeline** for further **processing**, such as **storage**, **preprocessing**,\n",
    "and **transformation**. Data ingestion can be done in **real-time** or **batch**\n",
    "mode, depending on the specific requirements of your project.\n",
    "\n",
    "### An Example Of Data Extraction For Collecting Large Volumes Of Text Data\n",
    "\n",
    "In this example, we will illustrate the process of data extraction for a ChatGPT\n",
    "equivalent for South East Asian languages using the data collection template as\n",
    "a guide.\n",
    "\n",
    "| Description (What data)                                                                                                                        | Data Sources (Where)                                                    | Data Formats                                     | Access & Extraction (How)                                                      | Tools and Libraries                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Requirements / Configurations                                                                                                                                                                      |\n",
    "| ---------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------- | ------------------------------------------------ | ------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Large volumes of **text data** in South East Asian languages (e.g., Indonesian, Malay, Thai, Vietnamese, Filipino, etc.) from diverse sources. | - Common Crawl<br>- APIs<br>- Public/Private Datasets<br>- Web Scraping | - WARC<br>- JSON<br>- CSV, JSON, XML, Plain text | - Common Crawl API<br>- API Requests<br>- Accessing datasets<br>- Web Scraping | - [warcio](https://github.com/webrecorder/warcio) or [WARC Tools](https://github.com/internetarchive/warc-tools)<br>- [Requests](https://docs.python-requests.org/en/master/)<br>- [Tweepy (Twitter)](https://www.tweepy.org/)<br>- [PRAW (Reddit)](https://praw.readthedocs.io/en/stable/)<br>- [Pandas](https://pandas.pydata.org/)<br>- [Scrapy](https://scrapy.org/)<br>- [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/)<br>- [Selenium](https://www.selenium.dev/) | - Processing and extraction of WARC files<br>- API authentication and rate limit handling<br>- Adhering to website usage policies and robots.txt<br>- Tools for processing various dataset formats |\n",
    "\n",
    "#### **What** (raw) data do I need to extract (relevant to my project)?\n",
    "\n",
    "-   Large volumes of **text data** in South East Asian languages (e.g.,\n",
    "    Indonesian, Malay, Thai, Vietnamese, Filipino, etc.).\n",
    "-   Data should come from diverse sources to capture different styles, contexts,\n",
    "    and domains (e.g., news articles, social media, forums, websites, etc.).\n",
    "\n",
    "#### **Where** can I find this data? This means finding all potential data sources\n",
    "\n",
    "-   [**Common Crawl**](https://commoncrawl.org/): A large and frequently updated\n",
    "    dataset of web crawl data that includes text in various languages, including\n",
    "    South East Asian languages.\n",
    "-   **APIs**: Platforms like **Twitter**, **Reddit**, and news organizations\n",
    "    that have content in South East Asian languages.\n",
    "-   **Public/Private Datasets**: Language-specific datasets, government\n",
    "    datasets, and domain-specific datasets containing text in South East Asian\n",
    "    languages.\n",
    "-   **Web Scraping**: Target websites and forums with a high volume of content\n",
    "    in South East Asian languages.\n",
    "\n",
    "#### **What** format is the data in? Is it in a format that I can use?\n",
    "\n",
    "-   Common Crawl data comes in **WARC** format, which will require processing\n",
    "    and extraction to obtain usable text.\n",
    "-   API data is typically in **JSON** format, which can be easily parsed and\n",
    "    processed.\n",
    "-   Datasets can come in various formats, such as **CSV**, **JSON**, **XML**, or\n",
    "    plain text. Ensure that you have the appropriate tools to process each\n",
    "    format.\n",
    "\n",
    "#### **How** do I access the data?\n",
    "\n",
    "-   **Common Crawl**: Access data through the\n",
    "    [Common Crawl API](https://commoncrawl.org/the-data/get-started/) or\n",
    "    download WARC files directly from\n",
    "    [Amazon S3](https://aws.amazon.com/datasets/common-crawl/).\n",
    "-   **APIs**: Request data using platform-specific APIs (e.g., Twitter API,\n",
    "    Reddit API, etc.) with proper authentication and rate limit handling.\n",
    "-   **Public/Private Datasets**: Access datasets through their respective\n",
    "    providers (e.g., download links, dataset repositories, etc.).\n",
    "-   **Web Scraping**: Crawl and scrape target websites using web scraping tools\n",
    "    and libraries, while adhering to the site's robots.txt and usage policies.\n",
    "\n",
    "#### **What** tools do I need to extract the data?\n",
    "\n",
    "-   **Common Crawl**: Use libraries like\n",
    "    [warcio](https://github.com/webrecorder/warcio) or\n",
    "    [WARC Tools](https://github.com/internetarchive/warc-tools) to process WARC\n",
    "    files.\n",
    "-   **APIs**: Use libraries like\n",
    "    [Requests](https://docs.python-requests.org/en/master/),\n",
    "    [Tweepy (Twitter)](https://www.tweepy.org/), and\n",
    "    [PRAW (Reddit)](https://praw.readthedocs.io/en/stable/) to interact with\n",
    "    APIs.\n",
    "-   **Public/Private Datasets**: Use data processing libraries (e.g.,\n",
    "    [Pandas](https://pandas.pydata.org/) for CSV,\n",
    "    [JSON](https://docs.python.org/3/library/json.html) for JSON) to handle\n",
    "    various dataset formats.\n",
    "-   **Web Scraping**: Use web scraping libraries like\n",
    "    [Scrapy](https://scrapy.org/),\n",
    "    [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/), and\n",
    "    [Selenium](https://www.selenium.dev/) to scrape target websites.\n",
    "\n",
    "## Step 2. **L**oad (Data Ingestion and Destination)\n",
    "\n",
    "Data ingestion refers to the process of loading the extracted data into a\n",
    "storage solution, such as a data warehouse, data lake, or data lakehouse. This\n",
    "step is crucial because it makes the data available for further processing and\n",
    "analysis in your DataOps pipeline.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The key to this step is to choose an appropriate **destination** for storing the\n",
    "extracted data. The choice of destination depends on your project requirements,\n",
    "the type and size of the data, and your budget constraints. Different\n",
    "destinations have their own advantages and limitations, so it's essential to\n",
    "choose the right one for your specific needs.\n",
    "\n",
    "In our case, since we're dealing with large volumes of text data, a cloud-based\n",
    "storage solution like [Google BigQuery](https://cloud.google.com/bigquery) (a\n",
    "data warehouse) or [Google Cloud Storage](https://cloud.google.com/storage) (a\n",
    "data lake) would be suitable.\n",
    "\n",
    "A side note is that these large text data is used for downstream pre-training\n",
    "task, and as such, say we are using SLURM cluster on AWS, naturally we would\n",
    "also need a highly efficient shared file system like\n",
    "[Amazon EFS](https://aws.amazon.com/efs/) or\n",
    "[Amazon FSx for Lustre](https://aws.amazon.com/fsx/lustre/) that plays well with\n",
    "High Performance Computing (HPC) workloads. If you plan to use say FSx, then\n",
    "stick to AWS for the data lake as well as S3 and FSx are integrated well enough\n",
    "for one to dump data to S3 and then \"mount\" it to FSx.\n",
    "\n",
    "### Definition\n",
    "\n",
    "[**Data loading**](https://en.wikipedia.org/wiki/Extract,_transform,_load#Load)\n",
    "is the process of importing extracted data into a target storage system, such as\n",
    "a [database](https://en.wikipedia.org/wiki/Database),\n",
    "[data warehouse](https://en.wikipedia.org/wiki/Data_warehouse),\n",
    "[data lake](https://en.wikipedia.org/wiki/Data_lake), or\n",
    "[data lakehouse](https://databricks.com/glossary/data-lakehouse).\n",
    "\n",
    "In the context of\n",
    "[**ETL**](https://en.wikipedia.org/wiki/Extract,_transform,_load) (Extract,\n",
    "Transform, Load), this process typically involves transforming, validating, and\n",
    "cleaning the extracted data to ensure its compatibility, consistency, and\n",
    "accuracy within the target storage system. However, in this version, we will\n",
    "adopt an approach more similar to\n",
    "[**ELT**](https://en.wikipedia.org/wiki/Extract,_load,_transform) (Extract,\n",
    "Load, Transform), where the data is loaded into the destination storage system\n",
    "(i.e., data lake) without any transformations or cleaning.\n",
    "\n",
    "Loading consists of two main steps:\n",
    "\n",
    "1. **Store** the extracted raw data temporarily on a local machine, on-premises\n",
    "   server, or cloud-based storage solution.\n",
    "2. **Ingest** the raw data directly into the destination storage solution for\n",
    "   further processing and analysis in your DataOps pipeline.\n",
    "\n",
    "### Template\n",
    "\n",
    "To successfully load the extracted data, consider the following key points that\n",
    "helps one to choose the right destination storage for their data.\n",
    "\n",
    "#### **What** are my project requirements?\n",
    "\n",
    "-   _What are the main goals and objectives of the project?_\n",
    "-   _What kind of processing, analysis, or transformations will be performed on\n",
    "    the data?_\n",
    "-   _Do I need real-time processing and analytics, or is batch processing\n",
    "    sufficient?_\n",
    "\n",
    "#### **Where** should I store the extracted data?\n",
    "\n",
    "-   _Choose an appropriate storage solution based on your project requirements,\n",
    "    the type and size of the data, and your budget constraints._\n",
    "-   _Consider the scalability, performance, and cost of the storage solution._\n",
    "-   _Ensure the storage solution is compatible with your data processing and\n",
    "    analysis tools._\n",
    "\n",
    "#### **How** do I load the extracted data into the destination storage?\n",
    "\n",
    "-   _Select an appropriate method for loading the data (e.g., batch loading,\n",
    "    real-time streaming, etc.)._\n",
    "-   _Choose the right tools or libraries for loading the data._\n",
    "-   _Ensure the loaded data is organized and structured in a way that\n",
    "    facilitates efficient querying and analysis._\n",
    "\n",
    "#### **What** considerations should I keep in mind when loading the data?\n",
    "\n",
    "-   _Consider data security and privacy, especially if you're dealing with\n",
    "    sensitive or personal information._\n",
    "-   _Ensure data integrity by validating, cleaning, and transforming the data._\n",
    "-   _Monitor and optimize the performance of the data loading process._\n",
    "\n",
    "#### **What** type of data am I dealing with?\n",
    "\n",
    "-   _Is the data structured, semi-structured, or unstructured?_\n",
    "-   _What is the size and complexity of the data?_\n",
    "\n",
    "#### **What** are my budget constraints?\n",
    "\n",
    "-   _What is the total budget for data storage?_\n",
    "-   _How much will it cost to store the data for the duration of the project,\n",
    "    considering factors like storage capacity, data transfer, and data access\n",
    "    costs?_\n",
    "-   _Are there any cost-saving options or discounts available, such as long-term\n",
    "    storage plans or usage-based pricing?_\n",
    "\n",
    "#### **What** are the performance requirements?\n",
    "\n",
    "-   _How fast do I need to access and query the data?_\n",
    "-   _Are there any specific latency, throughput, or concurrency requirements?_\n",
    "-   _Does the storage solution provide sufficient performance for my use case?_\n",
    "\n",
    "#### **What** are the scalability and flexibility requirements?\n",
    "\n",
    "-   _Will the data volume grow over time? If so, how much?_\n",
    "-   _Does the storage solution support scaling storage capacity and performance\n",
    "    as needed?_\n",
    "-   _Can the storage solution handle changes in data types, formats, or schemas\n",
    "    over time?_\n",
    "\n",
    "#### **What** are the data security and compliance requirements?\n",
    "\n",
    "-   _What are the security requirements for the data, such as encryption, access\n",
    "    control, and auditing?_\n",
    "-   _Does the storage solution meet any regulatory or compliance requirements\n",
    "    that apply to my project or industry (e.g., GDPR, HIPAA, etc.)?_\n",
    "\n",
    "### Workflow\n",
    "\n",
    "Here's an overview of some common techniques and tools for data loading, along\n",
    "with their advantages and disadvantages.\n",
    "\n",
    "| Description (What)                                                                                                                        | Destination (Where)                                                           | Access & Ingestion (How)           | Tools and Libraries                                                                                                                                                                                                                                                                                                                                                                                                                                 | Considerations                                                                                                                           |\n",
    "| ----------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------- | ---------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Load extracted data into a **relational database** for structured data storage, querying, and analysis.                                   | SQL Databases (MySQL, PostgreSQL, Oracle, MS SQL Server)                      | Batch loading, Real-time streaming | [SQLAlchemy](https://www.sqlalchemy.org/), [PyMySQL (MySQL)](https://pypi.org/project/PyMySQL/), [psycopg2 (PostgreSQL)](https://www.psycopg.org/)                                                                                                                                                                                                                                                                                                  | Data schema, Data normalization, Indexing, Transactions, Constraints, Backup and recovery                                                |\n",
    "| Load extracted data into a **NoSQL database** for flexible, schema-less storage and querying of semi-structured or unstructured data.     | NoSQL Databases (MongoDB, Cassandra, Couchbase, Redis)                        | Batch loading, Real-time streaming | [PyMongo (MongoDB)](https://pymongo.readthedocs.io/en/stable/), [Cassandra Driver (Cassandra)](https://github.com/datastax/python-driver), [Couchbase SDK (Couchbase)](https://docs.couchbase.com/python-sdk/current/hello-world/start-using-sdk.html)                                                                                                                                                                                              | Data model, Horizontal scaling, Partitioning, Replication, Consistency, Backup and recovery                                              |\n",
    "| Ingest extracted data into a **data warehouse** for large-scale storage, querying, and analysis of structured or semi-structured data.    | Data Warehouses (Google BigQuery, Amazon Redshift, Snowflake)                 | Batch loading, Real-time streaming | [Google Cloud SDK (BigQuery)](https://cloud.google.com/bigquery/docs/reference/libraries), [Amazon Redshift SDK](https://aws.amazon.com/sdk-for-python/), [Snowflake Python Connector](https://docs.snowflake.com/en/user-guide/python-connector.html)                                                                                                                                                                                              | Data storage format, Data partitioning, Indexing, Data compression, Security, Backup and recovery                                        |\n",
    "| Store extracted data in a **data lake** for cost-effective, scalable storage and processing of raw, unstructured or semi-structured data. | Data Lakes (Amazon S3, Google Cloud Storage, Azure Blob Storage, Hadoop HDFS) | Batch loading, Real-time streaming | [Boto3 (Amazon S3)](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html), [Google Cloud SDK (Google Cloud Storage)](https://cloud.google.com/storage/docs/reference/libraries), [Azure SDK (Azure Blob Storage)](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python), [Hadoop HDFS SDK (Hadoop HDFS)](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsPythonGuide.html) | Data storage format, Data partitioning, Data compression, Security, Backup and recovery, Data lake organization and governance           |\n",
    "| Load extracted data into a **data lakehouse** for a hybrid solution that combines the best features of data warehouses and data lakes.    | Data Lakehouses (Databricks, Delta Lake)                                      | Batch loading, Real-time streaming | [Databricks SDK (Databricks)](https://docs.databricks.com/dev-tools/api/latest/index.html), [Delta Lake SDK (Delta Lake)](https://delta.io/)                                                                                                                                                                                                                                                                                                        | Data storage format, Data partitioning, Indexing, Data compression, Security, Backup and recovery, Data lake organization and governance |\n",
    "\n",
    "### Common Data Destinations\n",
    "\n",
    "| Destination        | Description                                                                                                                                                                                                                                      | Examples                                                                                                                                                                                                                                                         |\n",
    "| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Data Warehouse** | A centralized storage solution optimized for structured or semi-structured data, designed for fast querying and analytics. Data warehouses typically use SQL-based languages for querying and are ideal for large-scale, long-term data storage. | [Google BigQuery](https://cloud.google.com/bigquery), [Amazon Redshift](https://aws.amazon.com/redshift/), [Snowflake](https://www.snowflake.com/)                                                                                                               |\n",
    "| **Data Lake**      | A storage solution that can store large volumes of raw, unprocessed data in its native format, typically optimized for cost-effective storage. Data lakes can store structured, semi-structured, and unstructured data.                          | [Google Cloud Storage](https://cloud.google.com/storage), [Amazon S3](https://aws.amazon.com/s3/), [Microsoft Azure Data Lake Storage](https://azure.microsoft.com/en-us/services/storage/data-lake-storage/)                                                    |\n",
    "| **Data Lakehouse** | A hybrid storage solution that combines the best features of data warehouses and data lakes, providing a unified platform for storing, processing, and analyzing structured, semi-structured, and unstructured data.                             | [Databricks Delta Lake](https://databricks.com/product/delta-lake-on-databricks), [Apache Hudi](https://hudi.apache.org/), [Snowflake](https://www.snowflake.com/) (when combined with data lake storage solutions like [Amazon S3](https://aws.amazon.com/s3/)) |\n",
    "\n",
    "### Data Lake vs. Data Warehouse\n",
    "\n",
    "In the\n",
    "[Data Engineering Guide](https://madewithml.com/courses/mlops/data-stack/#data-warehouse)\n",
    "by Goku Mohandas, he explains that storing raw data in **data lakes** has become\n",
    "popular due to cost-effective storage and cloud-based Software as a Service\n",
    "(SaaS) management options. Data lakes can store **unstructured** and\n",
    "**unprocessed** data without the need for justifying its storage for downstream\n",
    "applications. When there is a need to process and transform the data, it can be\n",
    "moved to a **data warehouse**, where these operations can be performed more\n",
    "efficiently. Data warehouses are optimized for fast querying and analytics and\n",
    "are typically used for storing **structured** or **semi-structured** data.\n",
    "\n",
    "What this really means is that, no matter what kind of data is being\n",
    "**extracted** from the **source**, it can be **loaded and stored into** a **data\n",
    "lake** first as a copy. Then, when the data needs to be **processed and\n",
    "transformed** for downstream applications, it can be **moved** to a **data\n",
    "warehouse** for further processing and analysis.\n",
    "\n",
    "### Monitoring and Optimizing\n",
    "\n",
    "Once the data is loaded into the destination storage, it's essential to monitor\n",
    "the performance of the data loading process and optimize it as needed. This can\n",
    "help ensure that the data is always up-to-date, accurate, and available for\n",
    "further processing and analysis.\n",
    "\n",
    "Some tips for monitoring and optimizing the data loading process include:\n",
    "\n",
    "-   Regularly check the performance metrics of the data loading process, such as\n",
    "    data load times, error rates, and data throughput.\n",
    "-   Identify and resolve any bottlenecks or issues that may be impacting the\n",
    "    performance of the data loading process.\n",
    "-   Optimize the data loading process by adjusting the batch size, parallelizing\n",
    "    the process, or using incremental loading.\n",
    "-   Monitor the storage utilization and performance of the destination storage\n",
    "    system to ensure it meets the requirements of your DataOps pipeline.\n",
    "-   Leverage built-in tools, third-party solutions, or custom scripts for\n",
    "    monitoring and optimization\n",
    "\n",
    "## Step 3. Transform (Data Processing and Transformation)\n",
    "\n",
    "Data transformation is the process of converting, cleaning, and enriching the\n",
    "raw data that has been loaded into your storage solution to make it more\n",
    "suitable for analysis and reporting. This step is essential for improving data\n",
    "quality, reducing inconsistencies, and ensuring that the data is in a format\n",
    "that can be easily understood and consumed by your downstream analytics tools\n",
    "and processes.\n",
    "\n",
    "However, as\n",
    "[Goku puts it](https://madewithml.com/courses/mlops/data-stack/#transform), the\n",
    "_transform_ here is not necessarily the same as the _preprocessing_ step in a\n",
    "machine learning task. A typical machine learning pipeline involves steps such\n",
    "as **one-hot encoding**, **feature scaling**, and **feature extraction**. In\n",
    "contrast, the _transform_ step in the DataOps pipeline reflects more about the\n",
    "**business logic** of the data.\n",
    "\n",
    "Why? This is because we want the data at this stage to be as agnostic as\n",
    "possible so that downstream tasks are not only restricted to machine learning.\n",
    "For example, we may want to extract data from the data lake and load it into a\n",
    "data warehouse for further processing and analysis. In this case, the data needs\n",
    "to be transformed in a way that's suitable for the downstream task (and not\n",
    "necessarily for machine learning).\n",
    "\n",
    "As a result, our DataOps workflows are not specific to any particular downstream\n",
    "application so the transformation must be globally relevant.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The key to this step is to understand the structure and nature of your data and\n",
    "then apply the necessary transformations to make it more valuable and useful for\n",
    "your analysis. The specific transformations you apply will depend on your\n",
    "project requirements, the type of data you're working with, and the desired\n",
    "output format.\n",
    "\n",
    "In cases where large volumes of text data are being processed, some common\n",
    "agnostic transformations applicable to various downstream tasks include:\n",
    "\n",
    "-   **Cleaning**: Removing obvious errors, inconsistencies, or irrelevant\n",
    "    information.\n",
    "-   **De-duplication**: Removing duplicate records.\n",
    "\n",
    "### Definition\n",
    "\n",
    "[**Data transformation**](https://en.wikipedia.org/wiki/Data_transformation) is\n",
    "the process of converting, cleaning, and enriching raw data into a format that\n",
    "is more suitable for analysis, reporting, and other data-driven tasks. This step\n",
    "typically involves applying various data processing techniques, such as\n",
    "filtering, sorting, aggregation, and joining, to modify and enhance the data's\n",
    "structure, quality, and consistency.\n",
    "\n",
    "These operations typically include:\n",
    "\n",
    "-   **Filtering:** Removing unwanted data, such as duplicates, irrelevant\n",
    "    information, or incomplete records.\n",
    "-   **Aggregating:** Combining data from multiple sources or records to create\n",
    "    summary statistics, averages, or other measures of central tendency.\n",
    "-   **Normalizing:** Scaling or standardizing data to a common range or\n",
    "    distribution, making it easier to compare and analyze.\n",
    "\n",
    "In the context of\n",
    "[**ETL**](https://en.wikipedia.org/wiki/Extract,_transform,_load) (Extract,\n",
    "Transform, Load) and\n",
    "[**ELT**](https://en.wikipedia.org/wiki/Extract,_load,_transform) (Extract,\n",
    "Load, Transform), data transformation is an essential step that prepares the\n",
    "data for loading into a target storage system or for further processing and\n",
    "analysis in your DataOps pipeline.\n",
    "\n",
    "### Template\n",
    "\n",
    "#### **What** are my project requirements?\n",
    "\n",
    "-   _What are the main goals and objectives of the project?_\n",
    "-   _What kind of processing, analysis, or transformations will be performed on\n",
    "    the data?_\n",
    "-   _What is the desired output format or structure of the transformed data?_\n",
    "\n",
    "#### **What** type of data am I dealing with?\n",
    "\n",
    "-   _Is the data structured, semi-structured, or unstructured?_\n",
    "-   _What is the size and complexity of the data?_\n",
    "-   _Are there any specific data quality or consistency issues that need to be\n",
    "    addressed?_\n",
    "\n",
    "#### **What** are the data processing techniques and tools I should use?\n",
    "\n",
    "-   _Select appropriate data processing techniques based on your project\n",
    "    requirements and the type of data you're working with._\n",
    "-   _Choose the right tools or libraries for implementing the data processing\n",
    "    techniques._\n",
    "-   _Ensure the tools and libraries are compatible with your data storage\n",
    "    solution and analytics tools._\n",
    "\n",
    "#### **What** are the performance requirements?\n",
    "\n",
    "-   _How fast do I need to process and transform the data?_\n",
    "-   _Are there any specific latency, throughput, or concurrency requirements?_\n",
    "-   _Does the data processing solution provide sufficient performance for my use\n",
    "    case?_\n",
    "\n",
    "#### **What** are the scalability and flexibility requirements?\n",
    "\n",
    "-   _Will the data volume grow over time? If so, how much?_\n",
    "-   _Does the data processing solution support scaling processing capacity and\n",
    "    performance as needed?_\n",
    "-   _How well does the data processing solution handle changes in data types,\n",
    "    formats, or schemas over time?_\n",
    "\n",
    "### Workflow\n",
    "\n",
    "| Technique (What)                                                            | Description                                                                                                          | Input (Where)                        | Output (How)        | Tools and Libraries              | Considerations                                                                               |\n",
    "| --------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | ------------------------------------ | ------------------- | -------------------------------- | -------------------------------------------------------------------------------------------- |\n",
    "| [**Filtering**](<https://en.wikipedia.org/wiki/Filter_(signal_processing)>) | Remove unwanted or irrelevant data based on specified criteria or conditions.                                        | Raw data from **storage solution**   | **Filtered data**   | Pandas, NumPy, Dask              | Criteria for filtering, handling missing or incomplete data, **data quality**                |\n",
    "| [**Sorting**](https://en.wikipedia.org/wiki/Sorting_algorithm)              | Organize the data based on one or more attributes, either in ascending or descending order.                          | Filtered or unfiltered data          | **Sorted data**     | Pandas, NumPy, Dask              | Sorting criteria, handling missing or incomplete data, sorting algorithms and **complexity** |\n",
    "| [**Aggregation**](https://en.wikipedia.org/wiki/Aggregate_data)             | Combine and summarize data based on specific attributes, such as sum, average, count, or other statistical measures. | Filtered or unfiltered data          | **Aggregated data** | Pandas, NumPy, Dask              | Aggregation functions, handling missing or incomplete data, **data granularity**             |\n",
    "| [**Joining**](<https://en.wikipedia.org/wiki/Join_(SQL)>)                   | Merge data from multiple sources or tables based on a common attribute or key.                                       | Data from multiple sources or tables | **Joined data**     | Pandas, SQLAlchemy, Apache Spark | **Join types**, joining keys, handling missing or incomplete data, join **performance**      |\n",
    "\n",
    "## Batch Processing vs. Stream Processing\n",
    "\n",
    "For real-time or near-real-time ML applications, traditional batch processing of\n",
    "ETL might not be suitable. Instead, stream processing frameworks like Apache\n",
    "Kafka or Apache Flink allow for continuous data processing and may be used as\n",
    "alternatives or complements to ETL.\n",
    "\n",
    "## References and Further Readings\n",
    "\n",
    "-   [Data Stack for Machine Learning](https://madewithml.com/courses/mlops/data-stack)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "mystnb": {
   "number_source_lines": true
  },
  "source_map": [
   16
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}