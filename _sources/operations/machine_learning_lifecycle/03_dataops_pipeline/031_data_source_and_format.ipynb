{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9f74ee",
   "metadata": {},
   "source": [
    "# Stage 3.1. Data Source and Formats\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Brain_Dump-red)\n",
    "![Tag](https://img.shields.io/badge/Level-Beginner-green)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "\n",
    "## Identify and Scope the Data Source\n",
    "\n",
    "In this stage, we identify and scope the data source. This involves determining\n",
    "the type of data, locating the data, assessing accessibility and compliance,\n",
    "gauging the data volume, and understanding data characteristics.\n",
    "\n",
    "### Intuition (What comes before Data Extraction?)\n",
    "\n",
    "As we have seen in the pipeline and in a later section, the ELT/ETL framework,\n",
    "the first step is data extraction. However, before we can extract data, we need\n",
    "to first identify the data source and scope it.\n",
    "\n",
    "In what follows, we will discuss the steps involved in identifying and scoping\n",
    "the data source, as well as the tools and methods for extracting data from the\n",
    "source.\n",
    "\n",
    "### Steps to Identify and Scope the Data Source\n",
    "\n",
    "```{list-table} Table of Steps to Identify and Scope the Data Source\n",
    ":header-rows: 1\n",
    ":name: ml-lifecycle-03-steps-identify-scope-data-source\n",
    "\n",
    "-   -   Step\n",
    "    -   Action\n",
    "    -   Rationale\n",
    "-   -   Define the Type of Data\n",
    "    -   Determine whether the data is numerical, categorical, time-series,\n",
    "        text-based, images, or audio.\n",
    "    -   This can affect the model design and the choice of data sources.\n",
    "-   -   Locate the Data\n",
    "    -   Identify the location, such as databases (SQL or NoSQL), APIs, log\n",
    "        files, Excel or CSV files, etc.\n",
    "    -   Enables the selection of the suitable tools and methods for extraction.\n",
    "-   -   Assess Accessibility and Compliance\n",
    "    -   Understand permissions, authentication, privacy concerns, and\n",
    "        restrictions on data extraction.\n",
    "    -   Ensures adherence to legal and organizational policies.\n",
    "-   -   Gauge the Data Volume\n",
    "    -   Determine the size of the dataset.\n",
    "    -   Influences the choice of tools for extraction and storage. This is\n",
    "        important because large dataset need to be stored in a way that is\n",
    "        efficient and scalable.\n",
    "-   -   Understand Data Characteristics\n",
    "    -   Recognize and address special characteristics, for example, if you are\n",
    "        collecting images of apples for classification, you need to be sure\n",
    "        that the images have say, in RGB format, and not in grayscale.\n",
    "    -   Facilitates proper processing, validation, and utilization of the data.\n",
    "```\n",
    "\n",
    "### Data Types in Machine Learning Systems\n",
    "\n",
    "Before we scope the data source, a logical question to first ask is, what\n",
    "_types_ of data are we dealing with? Knowing the data types will help us\n",
    "**understand the nature and structure of information that we need to obtain.**\n",
    "This understanding, in turn, informs our choice of **data sources** that are\n",
    "best suited to provide this specific type of data.\n",
    "\n",
    "For example, if we are working with time-series data, our data sources might be\n",
    "sensors, logs, or financial market feeds. If we are dealing with textual data,\n",
    "the sources might be documents, websites, or social media platforms.\n",
    "\n",
    "Here's a brief overview of the different types of data in the form of a table.\n",
    "\n",
    "| Main Type                | Subtype          | Specific Types       | Description                                                                    |\n",
    "| ------------------------ | ---------------- | -------------------- | ------------------------------------------------------------------------------ |\n",
    "| **Structured Data**      | Numerical        | Continuous, Discrete | Continuous data can take any value, while Discrete data takes specific values. |\n",
    "|                          | Categorical      | Nominal, Ordinal     | Nominal data has no inherent order; Ordinal data has a meaningful order.       |\n",
    "|                          | Time-Series Data |                      | Data collected at specific time intervals.                                     |\n",
    "|                          | Geospatial Data  |                      | Information that includes geographical attributes.                             |\n",
    "|                          | Boolean Data     |                      | True/false or yes/no values.                                                   |\n",
    "| **Semi-Structured Data** | Multimodal       |                      | Combines data from multiple sources or types.                                  |\n",
    "|                          | Graph Data       |                      | Represents relationships using nodes and edges.                                |\n",
    "|                          | Mixed Data Types |                      | A combination of various data types.                                           |\n",
    "| **Unstructured Data**    | Text-Based Data  |                      | Unstructured textual information.                                              |\n",
    "|                          | Image Data       |                      | Visual information in a grid of pixels.                                        |\n",
    "|                          | Audio Data       |                      | Sound or speech data.                                                          |\n",
    "|                          | Binary Data      |                      | Data represented in a binary format.                                           |\n",
    "|                          | Embeddings       |                      | Representations of categorical, text, or complex data as continuous vectors    |\n",
    "\n",
    "### Data Sources in Machine Learning Systems\n",
    "\n",
    "Having identified the _types_ of data that our machine learning system will\n",
    "handle, we now turn our attention to the various **sources** from which this\n",
    "data can be obtained. Different data types require specific sources, both in\n",
    "terms of format compatibility and functional alignment. Here's an overview of\n",
    "various data sources, categorized by their characteristics and aligned with the\n",
    "types of data they typically provide:\n",
    "\n",
    "| **Category**                     | **Type**                    | **Examples/Details**                                                                                                                     |\n",
    "| -------------------------------- | --------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Databases**                    | Relational Databases (SQL)  | [MySQL](https://www.mysql.com/), [PostgreSQL](https://www.postgresql.org/), [MS SQL Server](https://www.microsoft.com/en-us/sql-server/) |\n",
    "|                                  | NoSQL Databases             | [MongoDB](https://www.mongodb.com/), [Cassandra](https://cassandra.apache.org/), [Redis](https://redis.io/)                              |\n",
    "| **File-Based Sources**           | Flat Files                  | CSV, Excel, TSV                                                                                                                          |\n",
    "|                                  | Binary Files                | Parquet, Avro                                                                                                                            |\n",
    "|                                  | Image and Video Files       | JPEG, PNG, MP4                                                                                                                           |\n",
    "|                                  | Text Files                  | TXT, PDF, DOC                                                                                                                            |\n",
    "| **Web Sources**                  | Web APIs                    | RESTful APIs, SOAP, [GraphQL](https://graphql.org/)                                                                                      |\n",
    "|                                  | Web Scraping                | HTML, XML                                                                                                                                |\n",
    "|                                  | Social Media                | Twitter, Facebook, Reddit                                                                                                                |\n",
    "| **Streaming Data Sources**       | Message Brokers             | [Kafka](https://kafka.apache.org/), [RabbitMQ](https://www.rabbitmq.com/)                                                                |\n",
    "|                                  | Real-Time Feeds             | Stock prices, sensor data                                                                                                                |\n",
    "| **Sensor Data**                  | IoT Devices                 | Smart devices, wearable tech                                                                                                             |\n",
    "|                                  | Industrial Sensors          | Temperature, pressure, humidity sensors                                                                                                  |\n",
    "| **Scientific Sources**           | Genomic Data                | DNA sequences, proteomics                                                                                                                |\n",
    "|                                  | Meteorological Data         | Weather stations, satellites                                                                                                             |\n",
    "| **Financial Data Sources**       | Stock Market Data           | Exchanges, trading platforms                                                                                                             |\n",
    "|                                  | Banking Transactions        | Credit card swipes, ATM transactions                                                                                                     |\n",
    "| **Healthcare Data Sources**      | Electronic Health Records   | Patient medical records                                                                                                                  |\n",
    "|                                  | Medical Imaging             | MRI, CT scans, X-rays                                                                                                                    |\n",
    "| **Government and Public Data**   | Census Data                 | Demographics, economics                                                                                                                  |\n",
    "|                                  | Legislation and Regulations | Law documents, policy papers                                                                                                             |\n",
    "| **Educational Data Sources**     | Academic Databases          | Research papers, thesis documents                                                                                                        |\n",
    "|                                  | Learning Management Systems | Student grades, course content                                                                                                           |\n",
    "| **Human-Generated Data Sources** | Surveys and Questionnaires  | Market research, feedback forms                                                                                                          |\n",
    "|                                  | Crowdsourcing Platforms     | [Amazon Mechanical Turk](https://www.mturk.com/)                                                                                         |\n",
    "| **Third-Party Data Providers**   | Commercial Data Providers   | Market trends, consumer habits                                                                                                           |\n",
    "|                                  | Open Data Repositories      | [Kaggle](https://www.kaggle.com/), [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)                           |\n",
    "\n",
    "## Data Formats in Machine Learning Systems\n",
    "\n",
    "In other words, once you scope the data source and data types, and manage to\n",
    "extract them, you need to store it in a format that is easy to work with. By\n",
    "easy I mean that the data should be easily accessible, scalable, and efficient\n",
    "to work with. As such, storing data isn't straightforward because data can be of\n",
    "different types and one must be experienced or knowledgeable enough to know what\n",
    "storage to use when storing data of a particular type.\n",
    "\n",
    "Some questions to ask when choosing a data format:\n",
    "\n",
    "-   Where do you store the data? In a database? In a file system? In a key-value\n",
    "    store? We want it to be ideally cheap and fast to retrieve the data.\n",
    "-   How to store complex models so they can be loaded and run on different\n",
    "    devices (e.g. mobile phones, web browsers, etc.). In ML, it can be GPU, CPU,\n",
    "    etc.\n",
    "\n",
    "### Distributed Data Parallelism And Data Sharding\n",
    "\n",
    "Sometimes data is too large to fit into a single machine's memory. In such\n",
    "cases, we can use distributed data parallelism (DDP) and data sharding to\n",
    "distribute the data across multiple machines. In DDP the model and trainer are\n",
    "replicated across multiple instances/nodes/ranks and each instance processes a\n",
    "different subset of the data. In this case, data sharding is often necessary in\n",
    "DDP to ensure that each process (or GPU) gets a unique subset of the data during\n",
    "each training iteration. Each process needs to handle a different portion of the\n",
    "data to ensure diversity in learning across replicas and prevent data\n",
    "redundancy, which can skew the learning process.\n",
    "\n",
    "In practice, data loaders that are DDP-aware (like those in PyTorch)\n",
    "automatically shard the dataset across the available GPUs. For instance, if you\n",
    "have a dataset of 1000 samples and 10 GPUs, each GPU might be assigned 100\n",
    "unique samples per iteration. This distribution ensures that all the data gets\n",
    "utilized without overlap between GPUs (their sampler ensures that each GPU gets\n",
    "a unique subset of the data).\n",
    "\n",
    "### An Example on Multimodal Data Storage For E-Commerce\n",
    "\n",
    "In e-commerce platforms, product pages often contain rich multimedia\n",
    "information, including images and corresponding textual descriptions. Storing\n",
    "and retrieving this information efficiently can involve the following.\n",
    "\n",
    "1. **Storing Images in a Binary Format**: Rather than embedding the raw image\n",
    "   tensor within a data structure, it's often more efficient to store the image\n",
    "   in a binary format (e.g., JPEG, PNG) and keep a reference to its location\n",
    "   (e.g., file path or URL).\n",
    "\n",
    "2. **Utilizing a Database for Textual Information**: The textual information,\n",
    "   including descriptions and metadata, can be stored in a relational database.\n",
    "   This approach provides scalable storage and efficient query capabilities.\n",
    "\n",
    "3. **Creating a Unified Schema**: A unified schema or data model could\n",
    "   encapsulate both the image references and the corresponding textual data.\n",
    "   This schema acts as a bridge between the two data types, allowing them to be\n",
    "   treated as a cohesive unit.\n",
    "\n",
    "Consider the below code snippet:\n",
    "\n",
    "```python title=\"Sample Data Schema Encoding Image and Text\"\n",
    "sample_data_schema = {\n",
    "    \"product_id\": 123,\n",
    "    \"image_url\": \"https://path/to/image.jpg\",\n",
    "    \"description\": \"This is a picture of a cat.\",\n",
    "    \"additional_metadata\": { ... }  # Additional textual or numerical information.\n",
    "}\n",
    "```\n",
    "\n",
    "and in tabular form:\n",
    "\n",
    "| Field Name            | Data Type       | Description                                                                               |\n",
    "| --------------------- | --------------- | ----------------------------------------------------------------------------------------- |\n",
    "| `product_id`          | Integer         | A unique identifier for the product.                                                      |\n",
    "| `image_url`           | String (URL)    | The URL or file path to the product's image.                                              |\n",
    "| `description`         | String (Text)   | The textual description of the product.                                                   |\n",
    "| `additional_metadata` | Dictionary/JSON | Additional textual or numerical information, such as categories, tags, or specifications. |\n",
    "\n",
    "In this approach, the `\"image_url\"` field stores a reference to the location of\n",
    "the image, and the `\"description\"` field contains the textual description. The\n",
    "additional metadata can encapsulate other relevant information, such as\n",
    "categories, tags, or product specifications.\n",
    "\n",
    "This design offers several advantages:\n",
    "\n",
    "-   **Scalability**: By storing images in a binary format and using database\n",
    "    storage for text, this approach can scale to handle large product catalogs.\n",
    "-   **Efficiency**: Leveraging specialized storage mechanisms for different data\n",
    "    types ensures that retrieval and updates are efficient.\n",
    "\n",
    "### Data Formats\n",
    "\n",
    "We will describe a few choices of data formats below.\n",
    "\n",
    "#### Data Serialization vs Data Deserialization\n",
    "\n",
    "The process of transforming data structures or object states into a format that\n",
    "can be saved (e.g., in a file like JSON) and later rebuilt in the same or a\n",
    "different computing environment is known as serialization. The opposite process,\n",
    "called deserialization, involves retrieving data from the stored formats. In\n",
    "simpler terms, serialization refers to storing data, while deserialization\n",
    "refers to accessing data from the saved formats.\n",
    "\n",
    "In other words, **storing data** is called **serialization**, and **retrieving\n",
    "data from the stored formats** is called **deserialization**.\n",
    "\n",
    "#### JSON\n",
    "\n",
    "[**JSON**](https://www.json.org/json-en.html), which stands for JavaScript\n",
    "Object Notation, is a lightweight data-interchange format that uses a key-value\n",
    "pair paradigm. It is human-readable, easy to parse, and simple to generate,\n",
    "making it an ideal choice for data exchange between a server and a client in\n",
    "machine learning applications. JSON's structure allows for easy storage in\n",
    "databases and can represent a wide variety of data types, including strings,\n",
    "numbers, booleans, objects, and arrays.\n",
    "\n",
    "```json title=\"example.json\"\n",
    "{\n",
    "    \"name\": \"John\",\n",
    "    \"age\": 30,\n",
    "    \"cars\": [\n",
    "        { \"name\": \"Ford\", \"models\": [\"Fiesta\", \"Focus\", \"Mustang\"] },\n",
    "        { \"name\": \"BMW\", \"models\": [\"320\", \"X3\", \"X5\"] },\n",
    "        { \"name\": \"Fiat\", \"models\": [\"500\", \"Panda\"] }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "While JSON has many advantages, it does have some drawbacks, such as increased\n",
    "storage requirements due to its text-based nature. However, its simplicity and\n",
    "ease of use have made it one of the most popular data formats in machine\n",
    "learning and other applications.\n",
    "\n",
    "In addition to the key-value pair structure, JSON also supports nesting of\n",
    "objects and arrays, which allows for more complex data representation. This\n",
    "makes JSON a versatile choice for a variety of use cases, from simple\n",
    "configuration files to complex machine learning model inputs and outputs.\n",
    "\n",
    "Furthermore, JSON has extensive support in many programming languages, with\n",
    "built-in libraries or third-party packages available for parsing and generating\n",
    "JSON data.\n",
    "\n",
    "In summary, JSON's human-readable format, easy parsing, support for complex data\n",
    "structures, and widespread language support make it an excellent choice for data\n",
    "exchange and storage in machine learning applications, despite its increased\n",
    "storage requirements compared to binary formats.\n",
    "\n",
    "#### Row and Columnar Formats\n",
    "\n",
    "##### Concept of Row-major vs Column-major order\n",
    "\n",
    "Row-major and column-major order describe two ways to store multi-dimensional\n",
    "arrays in linear memory. In row-major order, the elements of a multi-dimensional\n",
    "array are stored row by row, whereas in column-major order, the elements are\n",
    "stored column by column.\n",
    "\n",
    "##### Examples of Row-major vs Column-major order\n",
    "\n",
    "In row-major order, the elements of each row of a matrix are stored together in\n",
    "contiguous memory locations, with the elements of successive rows appearing\n",
    "consecutively in memory. For example, consider a 3x2 matrix:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{A} = \\begin{bmatrix}\n",
    "    1 & 2 \\\\\n",
    "    3 & 4 \\\\\n",
    "    5 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In row-major order, the elements are stored in memory as:\n",
    "\n",
    "```python\n",
    "[1, 2, 3, 4, 5, 6]\n",
    "```\n",
    "\n",
    "In contrast, in column-major order, the elements of each column are stored\n",
    "together in contiguous memory locations, with the elements of successive columns\n",
    "appearing consecutively in memory. For the same matrix, the column-major order\n",
    "would be:\n",
    "\n",
    "```python\n",
    "[1, 3, 5, 2, 4, 6]\n",
    "```\n",
    "\n",
    "Row-major and column-major order can make a difference in performance when\n",
    "accessing multi-dimensional arrays, especially for large arrays. For example,\n",
    "when accessing elements of a row in row-major order, consecutive elements of the\n",
    "row are likely to be cached together, which can improve access time. Similarly,\n",
    "when accessing elements of a column in column-major order, consecutive elements\n",
    "in the column are likely to be cached together, which can improve performance.\n",
    "\n",
    "#### Pros and cons of Row-major vs Column-major order\n",
    "\n",
    "##### Row-major order\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "-   It is the default order used in many programming languages, including C and\n",
    "    C++.\n",
    "-   It can be more intuitive for humans to understand, as rows are typically\n",
    "    used to represent entities (e.g., students, observations) and columns are\n",
    "    used to represent attributes (e.g., grades, measurements).\n",
    "-   When iterating over the elements of a matrix row-by-row, row-major order\n",
    "    ensures that the elements accessed are contiguous in memory, which can\n",
    "    improve cache locality and reduce the number of cache misses.\n",
    "-   Many linear algebra libraries, such as BLAS and LAPACK, use row-major order\n",
    "    by default.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "-   When iterating over the elements of a matrix column-by-column, row-major\n",
    "    order can lead to poor cache locality and a higher number of cache misses.\n",
    "    This is because consecutive elements in the same column are not necessarily\n",
    "    contiguous in memory.\n",
    "-   When transposing a matrix, row-major order requires copying the entire\n",
    "    matrix into a new block of memory in column-major order, which can be costly\n",
    "    for large matrices.\n",
    "-   Some hardware architectures may be optimized for column-major order, leading\n",
    "    to lower performance for row-major order.\n",
    "\n",
    "##### Column-major order\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "-   Column-major order is used by default in some programming languages, such as\n",
    "    Fortran.\n",
    "-   When iterating over the elements of a matrix column-by-column, column-major\n",
    "    order ensures that the elements accessed are contiguous in memory, which can\n",
    "    improve cache locality and reduce the number of cache misses.\n",
    "-   Some hardware architectures, such as GPUs, are optimized for column-major\n",
    "    order, leading to potentially better performance.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "-   Column-major order can be less intuitive for humans to understand, as it is\n",
    "    not the standard representation used in many fields.\n",
    "-   When iterating over the elements of a matrix row-by-row, column-major order\n",
    "    can lead to poor cache locality and a higher number of cache misses. This is\n",
    "    because consecutive elements in the same row are not necessarily contiguous\n",
    "    in memory.\n",
    "-   Many linear algebra libraries, such as BLAS and LAPACK, use row-major order\n",
    "    by default, so using column-major order may require additional memory copies\n",
    "    or transpositions.\n",
    "\n",
    "Overall, the choice between row-major and column-major order depends on the\n",
    "specific use case and hardware architecture.\n",
    "\n",
    "#### Modern Row and Columnar Formats\n",
    "\n",
    "| Library | Order for Multidimensional Arrays              |\n",
    "| ------- | ---------------------------------------------- |\n",
    "| NumPy   | Row-Major Order                                |\n",
    "| MATLAB  | Column-Major Order                             |\n",
    "| OpenGL  | Column-Major Order                             |\n",
    "| CUDA    | Column-Major Order                             |\n",
    "| OpenCV  | Row-Major Order                                |\n",
    "| Eigen   | Supports both Row-Major and Column-Major Order |\n",
    "| CSV     | Row-Major Order                                |\n",
    "| Parquet | Column-Major Order                             |\n",
    "\n",
    "Column-major formats are better for accessing specific columns of large datasets\n",
    "with many features, while row-major formats are better for faster data writes\n",
    "when adding new individual examples to data. Row-major formats are better for a\n",
    "lot of writes, while column-major formats are better for a lot of column-based\n",
    "reads.\n",
    "\n",
    "When you have a dataset with many features, storing the data in a column-major\n",
    "format is more efficient because it allows for direct access to individual\n",
    "columns without having to scan through all the other data in the rows. This\n",
    "means that when you need to extract a specific subset of columns from the\n",
    "dataset, you can do so more efficiently because the system doesn't need to read\n",
    "through all the other data in the rows to access the desired columns.\n",
    "\n",
    "In contrast, with a row-major format, the data for each row is stored together\n",
    "in memory, meaning that to access a specific column, you have to read through\n",
    "all the other columns in the row before you get to the desired column. This can\n",
    "be especially inefficient when dealing with large datasets with many features,\n",
    "as the system has to read through a lot of data to extract the desired subset of\n",
    "columns.\n",
    "\n",
    "For example, consider a dataset of ride-sharing transactions with 1,000\n",
    "features, but you only need to extract four specific columns: time, location,\n",
    "distance, and price. With a column-major format, you can directly access these\n",
    "columns, whereas with a row-major format, you have to read through all the other\n",
    "996 columns in each row before getting to the desired four columns. This can be\n",
    "slow and inefficient, especially if you need to access the subset of columns\n",
    "frequently or if the dataset is very large.\n",
    "\n",
    "In summary, storing data in a column-major format is more efficient for datasets\n",
    "with many features because it allows for direct access to individual columns,\n",
    "which can significantly speed up data retrieval and processing.\n",
    "\n",
    "#### Examples in code (Python) of Row-major vs Column-major order and its effect on performance\n",
    "\n",
    "```python\n",
    "import functools\n",
    "import time\n",
    "from typing import Any, Callable\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def timer(func: Callable[..., Any]) -> Callable[..., Any]:\n",
    "    \"\"\"Timer decorator.\"\"\"\n",
    "\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args: Any, **kwargs: Any) -> Any:\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"{func.__name__} took {elapsed_time:.4f} seconds to execute.\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@timer\n",
    "def traverse_dataframe_by_row(df: pd.DataFrame) -> None:\n",
    "    for col in df.columns:\n",
    "        for _ in df[col]:\n",
    "            pass\n",
    "\n",
    "\n",
    "@timer\n",
    "def traverse_dataframe_by_column(df: pd.DataFrame) -> None:\n",
    "    num_rows = df.shape[0]\n",
    "    for row_idx in range(num_rows):\n",
    "        for _ in df.iloc[row_idx]:\n",
    "            pass\n",
    "\n",
    "\n",
    "df = pd.DataFrame(np.random.rand(5000, 5000))\n",
    "print(df.shape)\n",
    "\n",
    "traverse_dataframe_by_row(df)\n",
    "traverse_dataframe_by_column(df)\n",
    "\n",
    "\n",
    "# Row-major traversal (C-like order)\n",
    "df_np = df.to_numpy()\n",
    "df_np = np.array(df_np, order=\"C\")  # Row-major traversal (C-like order)\n",
    "n_rows, n_cols = df_np.shape\n",
    "\n",
    "\n",
    "@timer\n",
    "def traverse_numpy_by_row(array: npt.NDArray[np.floating[Any]]) -> None:\n",
    "    for row_idx in range(n_rows):\n",
    "        for col_idx in range(n_cols):\n",
    "            _ = array[row_idx, col_idx]\n",
    "\n",
    "\n",
    "@timer\n",
    "def traverse_numpy_by_column(array: npt.NDArray[np.floating[Any]]) -> None:\n",
    "    for col_idx in range(n_cols):\n",
    "        for row_idx in range(n_rows):\n",
    "            _ = array[row_idx, col_idx]\n",
    "\n",
    "\n",
    "traverse_numpy_by_row(df_np)\n",
    "traverse_numpy_by_column(df_np)\n",
    "\n",
    "df_np_col = np.array(df_np, order=\"F\")  # Column-major traversal (Fortran-like order)\n",
    "\n",
    "traverse_numpy_by_row(df_np_col)\n",
    "traverse_numpy_by_column(df_np_col)\n",
    "```\n",
    "\n",
    "### Text vs Binary Formats\n",
    "\n",
    "CSV and JSON are text files, while Parquet files are binary files. Text files\n",
    "are human-readable, while binary files are only readable by programs that can\n",
    "interpret the raw bytes. Binary files contain only 0s and 1s and are more\n",
    "compact than text files. Binary files can save space compared to text files; for\n",
    "example, storing the number 1000000 requires 7 bytes in a text file and only 4\n",
    "bytes in a binary file as int32. Parquet files are more efficient than text\n",
    "files in terms of storage and processing speed. For example, AWS recommends\n",
    "using the Parquet format because it consumes up to 6x less storage and is up to\n",
    "2x faster to unload in Amazon S3 compared to text formats.\n",
    "\n",
    "For example, if you want to store the number $1000000$, and if you store it in\n",
    "text file it takes 7 characters (1, 0, 0, 0, 0, 0, 0), taking up 7 bytes of\n",
    "storage if 1 character is 1 byte. But if you store it in binary format as\n",
    "`int32`, then it takes 32 bits, which is 4 bytes.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "Once the data source is scoped and well-defined, before we even start extracting\n",
    "the data, we need to know what kind of data we are dealing with and **how** and\n",
    "**where** we are going to store the extracted data.\n",
    "\n",
    "Determining the **storage format** is critical. Will the data be stored in its\n",
    "raw form, or does it need to be processed and converted into a different format\n",
    "like CSV, JSON, or Parquet? The chosen data format can have significant\n",
    "implications on storage costs, access speed, and compatibility with your data\n",
    "processing tools.\n",
    "\n",
    "The **storage location** is equally important. Depending on the volume of the\n",
    "data, your budget, and security requirements, you might opt for on-premises\n",
    "servers, cloud storage, or even a hybrid solution. Cloud storage, like Google\n",
    "Cloud Storage, Amazon S3, or Azure Blob Storage, offer scalable and secure\n",
    "solutions. However, you need to consider data privacy regulations and compliance\n",
    "requirements when deciding where to store the data.\n",
    "\n",
    "You should also consider how the data will be organized. Will it be stored in a\n",
    "structured database like MySQL, a NoSQL database like MongoDB, or a distributed\n",
    "file system like Hadoop HDFS? The data's nature, the need for scalability, and\n",
    "the types of queries you'll be running, all factor into this decision.\n",
    "\n",
    "Finally, the choice of **storage technology** also depends on the **data\n",
    "operations** you anticipate. For instance, if your data needs frequent updates,\n",
    "a database might be more suitable. If your data is largely static but needs to\n",
    "be read frequently, a file system might be a better choice.\n",
    "\n",
    "## References and Further Readings\n",
    "\n",
    "-   Huyen, Chip. \"Chapter 3. Data Engineering Fundamentals.\" In Designing\n",
    "    Machine Learning Systems: An Iterative Process for Production-Ready\n",
    "    Applications, O'Reilly Media, Inc., 2022.\n",
    "-   Kleppmann, Martin. \"Chapter 2. Data Models and Query Languages.\" In\n",
    "    Designing Data-Intensive Applications. Beijing: O'Reilly, 2017."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "mystnb": {
   "number_source_lines": true
  },
  "source_map": [
   16
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}