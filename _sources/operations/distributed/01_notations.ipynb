{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad463628",
   "metadata": {},
   "source": [
    "# Notations\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Brain_Dump-red)\n",
    "![Tag](https://img.shields.io/badge/Level-Beginner-green)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "\n",
    "To simplify notations, we assume that each node has the same number of GPUs so\n",
    "that we can have less indexes to keep track of. In particular, we assume the\n",
    "following notations. We can refer to\n",
    "[CS336](https://github.com/stanford-cs336/spring2024-assignment2-systems) for\n",
    "more information.\n",
    "\n",
    "## Process\n",
    "\n",
    "-   $P$ (Worker/Number of Processes): is the number of processes across the\n",
    "    cluster of nodes;\n",
    "    -   $p$: is the index of a process, $p \\in [0, P)$;\n",
    "    -   A **process** is a program in execution, characterized by a unique\n",
    "        process ID and its own independent set of system resources. It possesses\n",
    "        its own memory space, including code, runtime memory, and system\n",
    "        resources. Processes are isolated from each other, ensuring that the\n",
    "        actions (or failures) of one process don't directly affect another. They\n",
    "        can communicate with each other through various inter-process\n",
    "        communication mechanisms.\n",
    "    -   It is an instance of a program that’s participating in the distributed\n",
    "        training. In this assignment, each worker will have a single process, so\n",
    "        we’ll use worker, process, and worker process interchangeably. However,\n",
    "        a worker may use multiple processes (e.g., to load data for training),\n",
    "        so these terms are not always equivalent in practice.\n",
    "    -   In the context of distributed computing and deep learning, a process\n",
    "        typically refers to an instance of a training algorithm running on a\n",
    "        computational unit (like a CPU core or GPU).\n",
    "\n",
    "## Nodes\n",
    "\n",
    "-   $N$ (Number of Nodes): is the number of nodes in the cluster;\n",
    "    -   $n$: is the index of a node in the cluster, $n \\in [0, N)$;\n",
    "    -   A **node** is a physical machine with its own operating system and\n",
    "        system resources. It can have multiple CPUs and GPUs. For example,\n",
    "        `g4dn.4xlarge` is a node type in AWS EC2 that has 2 GPUs.\n",
    "\n",
    "## Local And Global World Size\n",
    "\n",
    "-   $G$ (Local World Size): is the number of GPUs per node or the number of\n",
    "    processes per node or **local world size**;\n",
    "\n",
    "    -   $g$: is the index of a GPU in a node, $g \\in [0, G)$;\n",
    "    -   A **GPU** is a computational unit that can perform parallel computation\n",
    "        on tensors. It has its own memory space, including code, runtime memory,\n",
    "        and system resources. **_Consequently, we often collapse the notion of a\n",
    "        GPU and a process together, and use the two terms interchangeably_**.\n",
    "\n",
    "-   $W$: is the world size;\n",
    "\n",
    "    -   The **world size** refers to the total number of application\n",
    "        **processes** $P$ running across the cluster of nodes $N$. So $W = P$.\n",
    "    -   Since in the context of distributed deep learning training, the process\n",
    "        and the GPU are often used interchangeably where each GPU runs one\n",
    "        process , we can also say that the world size $W$ is the total number of\n",
    "        GPUs across the cluster of nodes $N$.\n",
    "\n",
    "        $$\n",
    "        W = N \\times G\n",
    "        $$\n",
    "\n",
    "## Local And Global Rank\n",
    "\n",
    "-   Global Rank (often denoted just as Rank) $R_g \\in [0, W-1]$: is the global\n",
    "    rank of a process across the cluster of nodes;\n",
    "\n",
    "    -   $R_g = n \\times G + g$ where $n$ is the index of a node and $g$ is the\n",
    "        index of a GPU in that particular node $n$;\n",
    "    -   The global rank $R_g$ is the unique identifier of a process across the\n",
    "        cluster of nodes. It is used to identify a process in the collective\n",
    "        communication operations.\n",
    "    -   In the context of distributed deep learning training, the global rank\n",
    "        $R_g$ is the unique identifier of a GPU across the cluster of nodes. It\n",
    "        is used to identify a GPU in the collective communication operations.\n",
    "\n",
    "-   Local Rank $R_{l} \\in [0, L-1]$: is the local rank of a process within a\n",
    "    node;\n",
    "    -   $R_{l} = g$;\n",
    "    -   The local rank $R_{l}$ is the unique identifier of a process within a\n",
    "        node. It is used to identify a process in the intra-node collective\n",
    "        communication operations.\n",
    "    -   In the context of distributed deep learning training, the local rank\n",
    "        $R_l$ is the unique identifier of a GPU within a node. It is used to\n",
    "        identify a GPU in the intra-node collective communication operations.\n",
    "\n",
    "## An Example To Illustrate The Notations\n",
    "\n",
    "To illustrate the terminology defined above, consider the case where a DDP\n",
    "application is launched on two nodes, each of which has four GPUs. We would then\n",
    "like each process to span one GPUs each. The mapping of processes to nodes is\n",
    "shown in the figure below:\n",
    "\n",
    "```{figure} ./assets/distributed-1.png\n",
    "---\n",
    "name: distributed-1\n",
    "---\n",
    "\n",
    "2 nodes with 4 GPUs each and each process spans 1 GPU.\n",
    "```\n",
    "\n",
    "More concretely, we have the following notations:\n",
    "\n",
    "-   $N = 2$: the number of nodes in the cluster;\n",
    "-   $G = 4$: the number of GPUs per node;\n",
    "-   $W = N \\times G = 8$: the world size, i.e., the total number of GPUs across\n",
    "    the cluster of nodes;\n",
    "\n",
    "Now in node $n=0$, we have the following notations:\n",
    "\n",
    "-   GPU $0$ corresponds to $R_g = 0$ and $R_l = 0$;\n",
    "-   GPU $1$ corresponds to $R_g = 1$ and $R_l = 1$;\n",
    "-   GPU $2$ corresponds to $R_g = 2$ and $R_l = 2$;\n",
    "-   GPU $3$ corresponds to $R_g = 3$ and $R_l = 3$;\n",
    "-   The local ranks of the processes in node $n=0$ are $R_{l} \\in [0, 3]$;\n",
    "-   The global ranks of the processes in node $n=0$ are $R_{g} \\in [0, 3]$;\n",
    "\n",
    "Now in node $n=1$, we have the following notations:\n",
    "\n",
    "-   GPU $0$ corresponds to $R_g = 4$ and $R_l = 0$;\n",
    "-   GPU $1$ corresponds to $R_g = 5$ and $R_l = 1$;\n",
    "-   GPU $2$ corresponds to $R_g = 6$ and $R_l = 2$;\n",
    "-   GPU $3$ corresponds to $R_g = 7$ and $R_l = 3$;\n",
    "-   The local ranks of the processes in node $n=1$ are $R_{l} \\in [0, 3]$;\n",
    "-   The global ranks of the processes in node $n=1$ are $R_{g} \\in [4, 7]$;\n",
    "\n",
    "To illustrate the terminology defined above, consider the case where a DDP\n",
    "application is launched on two nodes, each of which has four GPUs. We would then\n",
    "like each process to span two GPUs each. The mapping of processes to nodes is\n",
    "shown in the figure below:\n",
    "\n",
    "```{figure} ./assets/distributed-2.png\n",
    "---\n",
    "name: distributed-2\n",
    "---\n",
    "\n",
    "2 nodes with 4 GPUs each and each process spans 2 GPUs.\n",
    "```\n",
    "\n",
    "More concretely, for this scenario:\n",
    "\n",
    "-   $N = 2$: the number of nodes in the cluster.\n",
    "-   $G = 4$: the number of GPUs per node.\n",
    "-   Since each process spans two GPUs, we will have half the number of processes\n",
    "    compared to GPUs in each node. Let's call the number of processes in each\n",
    "    node $P_{node}$.\n",
    "-   $P_{node} = \\frac{G}{2} = 2$: number of processes per node.\n",
    "-   $W = N \\times P_{node} = 4$: the world size, i.e., the total number of\n",
    "    processes across the cluster of nodes.\n",
    "\n",
    "**For node $n=0$**:\n",
    "\n",
    "-   Process $0$ spans GPU $0$ and GPU $1$:\n",
    "\n",
    "    -   Global rank: $R_g = 0$\n",
    "    -   Local rank: $R_l = 0$\n",
    "\n",
    "-   Process $1$ spans GPU $2$ and GPU $3$:\n",
    "\n",
    "    -   Global rank: $R_g = 1$\n",
    "    -   Local rank: $R_l = 1$\n",
    "\n",
    "-   The local ranks of the processes in node $n=0$ are $R_{l} \\in [0, 1]$.\n",
    "-   The global ranks of the processes in node $n=0$ are $R_{g} \\in [0, 1]$.\n",
    "\n",
    "**For node $n=1$**:\n",
    "\n",
    "-   Process $2$ spans GPU $0$ and GPU $1$:\n",
    "\n",
    "    -   Global rank: $R_g = 2$\n",
    "    -   Local rank: $R_l = 0$\n",
    "\n",
    "-   Process $3$ spans GPU $2$ and GPU $3$:\n",
    "\n",
    "    -   Global rank: $R_g = 3$\n",
    "    -   Local rank: $R_l = 1$\n",
    "\n",
    "-   The local ranks of the processes in node $n=1$ are $R_{l} \\in [0, 1]$.\n",
    "-   The global ranks of the processes in node $n=1$ are $R_{g} \\in [2, 3]$.\n",
    "\n",
    "In this scenario, while the number of GPUs in each node remains unchanged, the\n",
    "concept of process has been expanded to span two GPUs. This implies that the\n",
    "computation associated with each process is now distributed across two GPUs on\n",
    "the same node. Such a configuration can be useful for models that are too large\n",
    "to fit in the memory of a single GPU or for scenarios where inter-GPU\n",
    "communication within the same node is more efficient than across nodes.\n",
    "\n",
    "## References and Further Readings\n",
    "\n",
    "-   [Definitions of Distributed Training in PyTorch](https://pytorch.org/docs/stable/elastic/run.html#definitions)\n",
    "-   [PyTorch: DDP Toy Example](https://github.com/pytorch/examples/blob/main/distributed/ddp/README.md)\n",
    "-   [Distributed Computing with PyTorch](https://shivgahlout.github.io/2021-05-18-distributed-computing/)\n",
    "-   [CS336: Language Modeling from Scratch](https://github.com/stanford-cs336/spring2024-assignment2-systems/blob/master/cs336_spring2024_assignment2_systems.pdf)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "mystnb": {
   "number_source_lines": true
  },
  "source_map": [
   16
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}