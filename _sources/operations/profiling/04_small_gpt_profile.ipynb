{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Profile GPT Small Time And Memory\n","\n","[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n","[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n","[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n","![Tag](https://img.shields.io/badge/Tag-Brain_Dump-red)\n","![Tag](https://img.shields.io/badge/Level-Beginner-green)\n","\n","\n","```{contents}\n",":local:\n","```"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T07:54:00.934349Z","iopub.status.busy":"2024-08-12T07:54:00.933852Z","iopub.status.idle":"2024-08-12T07:54:41.684984Z","shell.execute_reply":"2024-08-12T07:54:41.683923Z","shell.execute_reply.started":"2024-08-12T07:54:00.934304Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 24.6.1 requires cubinlinker, which is not installed.\n","cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cudf 24.6.1 requires ptxcompiler, which is not installed.\n","cuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n","dask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n","apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n","apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n","apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\n","beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.3 which is incompatible.\n","cudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\n","jupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","pointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\n","spaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n","spopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\n","ydata-profiling 4.6.4 requires matplotlib<3.9,>=3.2, but you have matplotlib 3.9.0 which is incompatible.\n","ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\n","ydata-profiling 4.6.4 requires seaborn<0.13,>=0.10.1, but you have seaborn 0.13.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install -q omniverse==0.0.63"]},{"cell_type":"markdown","metadata":{},"source":["## Common Functions\n","\n","This module include GPT model definitions as well as some common config."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T07:54:41.687461Z","iopub.status.busy":"2024-08-12T07:54:41.687148Z","iopub.status.idle":"2024-08-12T07:54:43.664671Z","shell.execute_reply":"2024-08-12T07:54:43.663453Z","shell.execute_reply.started":"2024-08-12T07:54:41.687434Z"},"trusted":true},"outputs":[],"source":["from __future__ import annotations\n","\n","from typing import Literal, Tuple, cast\n","\n","import torch\n","from pydantic import BaseModel\n","from torch import nn\n","\n","from omnivault.modules.activation import GELU, SoftmaxStable\n","from omnivault.transformer.modules.layers.normalization import RMSNorm\n","\n","__tagged__ = \"This code tags to `30d963e` of cs336-stanford-spring2024-assignment1-gpt-from-scratch.\"\n","__reference__ = [\"https://github.com/marcelroed/spring2024-assignment2-systems/blob/master/writeup.pdf\"]\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","class General(BaseModel):\n","    batch_size: int = 16\n","    seed: int = 20230310\n","\n","\n","class GPTConfig(BaseModel):\n","    approximate: Literal[\"tanh\"] | None = None\n","    activation_name: Literal[\"gelu\"] = \"gelu\"\n","    d_model: int\n","    d_ff: int | None = None\n","    num_heads: int\n","    context_length: int\n","    attn_pdrop: float = 0.0\n","    resid_pdrop: float = 0.0\n","    bias: bool = False\n","    vocab_size: int\n","    num_blocks: int\n","    token_position_pdrop: float = 0.0\n","    weight_tie: bool = False\n","\n","\n","class PositionwiseFeedForward(nn.Module):\n","    def __init__(\n","        self,\n","        d_model: int,\n","        d_ff: int | None = None,\n","        bias: bool = False,\n","        activation_name: Literal[\"gelu\"] = \"gelu\",\n","        dropout: float = 0.0,\n","    ) -> None:\n","        super().__init__()\n","\n","        self.d_model = d_model\n","        self.d_ff = d_ff or 4 * d_model\n","        self.bias = bias  # bias False in this exercise\n","        self.activation_name = activation_name\n","        self.dropout = dropout\n","\n","        self.ffn = nn.ModuleDict(\n","            {\n","                # incoming `B x T x D` and we are interested in `T x D` so weight is `D x d_ff`\n","                # so that `Z @ W1 -> (T x D) @ (D x d_ff)`\n","                \"context_fc\": nn.Linear(in_features=self.d_model, out_features=self.d_ff, bias=self.bias),\n","                \"activation\": self.activation,\n","                # apply dropout after activation for random lights out\n","                \"dropout\": nn.Dropout(p=self.dropout, inplace=False),\n","                # incoming is Z @ W1 -> T x d_ff -> (T x d_ff) @ (d_ff x D) project back to D\n","                \"context_projection\": nn.Linear(in_features=self.d_ff, out_features=self.d_model, bias=self.bias),\n","            }\n","        )\n","\n","    @property\n","    def activation(self) -> nn.Module:\n","        if self.activation_name == \"gelu\":\n","            activation = GELU(approximate=None)  # no approx using tanh\n","        else:\n","            raise ValueError(f\"Unsupported activation: {self._activation}\")\n","        return activation\n","\n","    def forward(self, z: torch.Tensor) -> torch.Tensor:\n","        # fmt: off\n","        z = self.ffn[\"context_fc\"](z)           # Z @ W1 = [B, T, D] @ [D, d_ff] = [B, T, d_ff]\n","        z = self.ffn[\"activation\"](z)           # \\sigma(Z @ W1) = [B, T, d_ff]\n","        z = self.ffn[\"dropout\"](z)              # \\dropout(\\sigma(Z @ W1)) = [B, T, d_ff]\n","        z = self.ffn[\"context_projection\"](z)   # \\dropout(\\sigma(Z @ W1)) @ W2 = [B, T, d_ff] @ [d_ff, D] = [B, T, D]\n","        # fmt: on\n","        return z\n","\n","\n","class ScaledDotProductAttention(nn.Module):\n","    def __init__(self, dropout: float = 0.0) -> None:\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(\n","        self,\n","        query: torch.Tensor,\n","        key: torch.Tensor,\n","        value: torch.Tensor,\n","        mask: torch.BoolTensor | None = None,\n","    ) -> Tuple[torch.Tensor, torch.Tensor]:\n","        # fmt: off\n","        T, d_q = query.size(-2), query.size(-1)\n","\n","        attention_scores  = torch.matmul(query, key.transpose(dim0=-2, dim1=-1)) / torch.sqrt(torch.tensor(d_q).float())        # Q @ K.T = [B, H, T, d_q] @ [B, H, d_q, T] = [B, H, T, T]\n","\n","        if mask is not None:\n","            mask = mask[:, :, :T, :T] # type: ignore[assignment]\n","            attention_scores  = attention_scores.masked_fill(mask == 1, float(\"-inf\")) if mask is not None else attention_scores    # [B, H, T, T]\n","\n","        softmax           = SoftmaxStable(dim=-1)\n","        attention_weights = softmax(attention_scores)               # [B, H, T, T]\n","        attention_weights = self.dropout(attention_weights)         # [B, H, T, T]\n","\n","        context_vector    = torch.matmul(attention_weights, value)  # [B, H, T, T] @ [B, H, T, d_v] = [B, H, T, d_v]\n","        # fmt: on\n","        return context_vector, attention_weights\n","\n","\n","class CausalMultiHeadSelfAttention(nn.Module):\n","    context_vector: torch.Tensor\n","    attention_weights: torch.Tensor\n","\n","    def __init__(\n","        self,\n","        d_model: int,\n","        num_heads: int,\n","        context_length: int,\n","        attn_pdrop: float = 0.0,  # pdrop means prob of dropout\n","        resid_pdrop: float = 0.0,\n","        bias: bool = False,\n","    ) -> None:\n","        super().__init__()\n","\n","        assert d_model % num_heads == 0\n","\n","        self.d_model = d_model\n","        self.H = num_heads\n","        self.context_length = context_length\n","        self.attn_pdrop = attn_pdrop\n","        self.resid_pdrop = resid_pdrop\n","        self.bias = bias\n","\n","        self.W_Q = nn.Linear(in_features=self.d_model, out_features=self.d_model, bias=self.bias)\n","        self.W_K = nn.Linear(in_features=self.d_model, out_features=self.d_model, bias=self.bias)\n","        self.W_V = nn.Linear(in_features=self.d_model, out_features=self.d_model, bias=self.bias)\n","\n","        # alias of W_O\n","        self.context_projection = nn.Linear(in_features=self.d_model, out_features=self.d_model, bias=self.bias)\n","\n","        # regularization\n","        self.resid_dropout = nn.Dropout(self.resid_pdrop)\n","\n","        self.attention = ScaledDotProductAttention(dropout=self.attn_pdrop)\n","\n","        # causal mask to ensure that attention is only applied to the left in the input sequence\n","        # register buffer cause not learnable weights\n","        self.register_buffer(\n","            \"causal_mask\",\n","            torch.triu(\n","                torch.ones((self.context_length, self.context_length)).bool(),\n","                diagonal=1,\n","            ).view(1, 1, self.context_length, self.context_length),\n","        )\n","\n","    def forward(self, *, z: torch.Tensor) -> torch.Tensor:\n","        B, T, D = z.size()\n","\n","        # fmt: off\n","        Q: torch.Tensor = self.W_Q(z).contiguous() # Z @ W_Q = [B, T, D] @ [D, D] = [B, T, D]\n","        K: torch.Tensor = self.W_K(z).contiguous() # Z @ W_K = [B, T, D] @ [D, D] = [B, T, D]\n","        V: torch.Tensor = self.W_V(z).contiguous() # Z @ W_V = [B, T, D] @ [D, D] = [B, T, D]\n","\n","        Q = Q.view(B, T, self.H, D // self.H).transpose(dim0=1, dim1=2) # [B, T, D] -> [B, T, H, D // H] -> [B, H, T, D//H]\n","        K = K.view(B, T, self.H, D // self.H).transpose(dim0=1, dim1=2)\n","        V = V.view(B, T, self.H, D // self.H).transpose(dim0=1, dim1=2)\n","\n","        # Now pass them to self attention\n","        self.context_vector, self.attention_weights = self.attention(query=Q, key=K, value=V, mask=self.causal_mask) # ([B, H, T, D // H], [B, H, T, T])\n","        assert isinstance(self.context_vector, torch.Tensor) # do this for type hint in IDE\n","\n","        # Now context vector is shape [B, H, T, D // H] but we want [B, T, D] to matmul with W_O/context_projection\n","        self.context_vector = self.context_vector.transpose(dim0=1, dim1=2).contiguous().view(B, T, D) # merge all heads together\n","        # fmt: on\n","\n","        projected_context_vector: torch.Tensor = self.resid_dropout(\n","            self.context_projection(self.context_vector)  # [B, T, D] @ [D, D] = [B, T, D]\n","        )\n","        return projected_context_vector\n","\n","\n","class GPTBlock(nn.Module):\n","    def __init__(\n","        self,\n","        config: GPTConfig,\n","    ) -> None:\n","        super().__init__()\n","\n","        self.rmns_1 = RMSNorm(d_model=config.d_model, eps=1e-5)\n","        self.attn = CausalMultiHeadSelfAttention(\n","            d_model=config.d_model,\n","            num_heads=config.num_heads,\n","            context_length=config.context_length,\n","            attn_pdrop=config.attn_pdrop,\n","            resid_pdrop=config.resid_pdrop,\n","            bias=config.bias,\n","        )\n","        self.rmns_2 = RMSNorm(d_model=config.d_model, eps=1e-5)\n","        self.ffn = PositionwiseFeedForward(\n","            d_model=config.d_model,\n","            d_ff=config.d_ff,\n","            bias=config.bias,\n","            activation_name=config.activation_name,\n","            dropout=config.resid_pdrop,\n","        )\n","\n","    def forward(self, z: torch.Tensor) -> torch.Tensor:\n","        z = z + self.attn(z=self.rmns_1(z))\n","        z = z + self.ffn(self.rmns_2(z))\n","        return z\n","\n","\n","class GPT(nn.Module):\n","    def __init__(self, config: GPTConfig) -> None:\n","        super().__init__()\n","\n","        self.config = config\n","        self.d_model = config.d_model\n","        self.num_blocks = config.num_blocks\n","        self.vocab_size = config.vocab_size\n","\n","        self.blocks = nn.ModuleList([GPTBlock(config=config) for _ in range(self.num_blocks)])\n","\n","        self.backbone = nn.ModuleDict(\n","            dict(  # noqa: C408\n","                token_embeddings=nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.d_model),\n","                position_embeddings=nn.Embedding(num_embeddings=config.context_length, embedding_dim=self.d_model),\n","                dropout=nn.Dropout(p=config.token_position_pdrop),\n","                layers=self.blocks,\n","                ln_final=RMSNorm(d_model=self.d_model, eps=1e-5),\n","            )\n","        )\n","        self.head = nn.Linear(in_features=self.d_model, out_features=self.vocab_size, bias=config.bias)\n","\n","        self.apply(self._init_weights)\n","\n","        context_projections = \"context_projection.weight\"\n","        # apply special scaled init to the residual projections, per GPT-2 paper\n","        for parameter_name, parameter in self.named_parameters():\n","            # NOTE: W_O is also projection but I did not have foresight to name it as such.\n","            if parameter_name.endswith(context_projections):\n","                mean = 0.0\n","                std_dev = 0.02 / torch.sqrt(torch.tensor(2 * config.num_blocks, dtype=torch.float))\n","                torch.nn.init.normal_(parameter, mean=mean, std=std_dev)\n","\n","        if config.weight_tie:\n","            self.backbone.token_embeddings.weight = self.head.weight\n","\n","    def crop_context_length(self, context_length: int) -> None:\n","        # NOTE: conveniently took Karpathy's implementation here for cropping\n","        assert context_length <= self.config.context_length\n","        self.config.context_length = context_length  # update config\n","\n","        self.backbone.position_embeddings.weight = nn.Parameter(\n","            self.backbone.position_embeddings.weight[:context_length]\n","        )\n","        for block in self.backbone.layers:\n","            if hasattr(block.attn, \"causal_mask\"):\n","                block.attn.causal_mask = block.attn.causal_mask[:, :, :context_length, :context_length]\n","\n","            # update context length attribute in MultiHeadSelfAttention\n","            block.attn.context_length = context_length\n","\n","    def _init_weights(self, module: nn.Module) -> None:\n","        normal_init_modules = (nn.Linear, nn.Embedding)\n","        if isinstance(module, normal_init_modules):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if hasattr(module, \"bias\") and module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","\n","    def forward(self, in_indices: torch.LongTensor) -> torch.FloatTensor:\n","        device = in_indices.device\n","\n","        B, T = in_indices.size()\n","\n","        positions = torch.arange(0, T, dtype=torch.long, device=device)  # [T]\n","        token_embeddings = self.backbone.token_embeddings(in_indices)  # [B, T, D]\n","        positional_embeddings = self.backbone.position_embeddings(positions)  # [T, D]\n","        # fmt: off\n","        positional_embeddings = positional_embeddings.unsqueeze(0) # .expand(B, -1, -1) # [B, T, D]\n","        # fmt: on\n","\n","        z = self.backbone.dropout(token_embeddings + positional_embeddings)  # [B, T, D]\n","\n","        for block in self.backbone.layers:\n","            z = block(z)  # [B, T, D]\n","\n","        z = self.backbone.ln_final(z)  # [B, T, D]\n","\n","        logits = self.head(z)  # [B, T, V]\n","        return cast(torch.FloatTensor, logits)  # [B, T, V]\n","\n","\n","def initialize_model(\n","    config: GPTConfig,\n","    device: str = \"cuda\",\n",") -> GPT:\n","    if config.d_ff is None:\n","        config.d_ff = 4 * config.d_model\n","\n","    model = GPT(config)\n","    return model.to(device)\n","\n","\n","def get_random_batch(\n","    batch_size: int,\n","    context_length: int,\n","    vocab_size: int,\n","    device: str = \"cuda\",\n",") -> Tuple[torch.Tensor, torch.Tensor]:\n","    inputs = torch.randint(  # [B, T]\n","        0,\n","        vocab_size,\n","        (batch_size, context_length),\n","        dtype=torch.long,\n","        device=device,\n","    )\n","\n","    targets = torch.randint(  # [B, T]\n","        0,\n","        vocab_size,\n","        (batch_size, context_length),\n","        dtype=torch.long,\n","        device=device,\n","    )\n","    return inputs, targets"]},{"cell_type":"markdown","metadata":{},"source":["## Main Profiling Code"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T08:51:42.093943Z","iopub.status.busy":"2024-08-12T08:51:42.093602Z","iopub.status.idle":"2024-08-12T08:51:42.117247Z","shell.execute_reply":"2024-08-12T08:51:42.116218Z","shell.execute_reply.started":"2024-08-12T08:51:42.093917Z"},"trusted":true},"outputs":[],"source":["from __future__ import annotations\n","\n","import logging\n","import socket\n","import sys\n","from contextlib import nullcontext\n","from datetime import datetime\n","from typing import Any, Iterable, Tuple\n","\n","import torch\n","from torch import nn\n","from torch._C._profiler import _ExperimentalConfig\n","from torch.autograd.profiler import record_function\n","from torch.profiler import ProfilerActivity, profile, record_function\n","\n","from omnivault.modules.loss import CrossEntropyLoss\n","from omnivault.utils.reproducibility.seed import seed_all\n","\n","seed_all(42, True, False)\n","\n","\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n","    handlers=[logging.StreamHandler(sys.stdout)],\n","    force=True,\n",")\n","logger = logging.getLogger(__name__)\n","TIME_FORMAT_STR: str = \"%b_%d_%H_%M_%S\"\n","\n","\n","def profile_one_step(\n","    model: nn.Module,\n","    batch: Tuple[torch.Tensor, torch.Tensor],\n","    optimizer: torch.optim.Optimizer,\n","    criterion: CrossEntropyLoss,\n","    enable_backward: bool,\n","    enable_optimizer: bool,\n","    mixed_precision: bool = False,\n",") -> None:\n","    context = torch.autocast(\"cuda\", dtype=torch.bfloat16) if mixed_precision else nullcontext()\n","    inputs, targets = batch[0], batch[1]  # typically this doesn't need to be under context.\n","    with context:  # type: ignore[attr-defined]\n","        with record_function(name=\"forward_pass\"):\n","            logits = model(inputs)\n","            loss = criterion(logits, targets)\n","\n","        if enable_backward:\n","            with record_function(name=\"backward_pass\"):\n","                loss.backward()\n","            if enable_optimizer:\n","                with record_function(name=\"optimizer\"):\n","                    optimizer.step()\n","                    optimizer.zero_grad(set_to_none=True)\n","\n","\n","def run_warmup(\n","    model: nn.Module,\n","    batch: Tuple[torch.Tensor, torch.Tensor],\n","    optimizer: torch.optim.Optimizer,\n","    criterion: CrossEntropyLoss,\n",") -> None:\n","    inputs, targets = batch[0], batch[1]\n","    logits = model(inputs)\n","    loss = criterion(logits, targets)\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad(set_to_none=True)\n","    torch.cuda.synchronize()\n","\n","\n","def trace_handler(prof: torch.profiler.profile) -> None:\n","    # Prefix for file names.\n","    host_name = socket.gethostname()\n","    timestamp = datetime.now().strftime(TIME_FORMAT_STR)\n","    file_prefix = f\"{host_name}_{timestamp}\"\n","\n","    # Construct the trace file.\n","    prof.export_chrome_trace(f\"{file_prefix}.json.gz\")\n","\n","    # Construct the memory timeline file.\n","    prof.export_memory_timeline(f\"{file_prefix}.html\", device=\"cuda:0\")\n","    prof.export_stacks(\"lm_profiler_stacks.txt\", \"self_cuda_time_total\")\n","\n","\n","def run_profiler(\n","    model: nn.Module,\n","    batch: Tuple[torch.Tensor, torch.Tensor],\n","    optimizer: torch.optim.Optimizer,\n","    criterion: CrossEntropyLoss,\n","    enable_backward: bool,\n","    enable_optimizer: bool,\n","    mixed_precision: bool = False,\n","    profile_steps: int = 5,\n","    *,\n","    activities: Iterable[ProfilerActivity] | None = None,\n","    profile_memory: bool = False,\n","    with_stack: bool = True,\n","    record_shapes: bool = True,\n","    with_flops: bool = False,\n","    **profile_kwargs: Any,\n",") -> torch.profiler.profile:\n","    # profile_memory requires with_stack and record_shapes, hence we override these if profile_memory is True\n","    # See torch.profiler.profiler._memory_profile\n","    if profile_memory:\n","        logger.warning(\n","            \"`profile_memory` requires `with_stack` and `record_shapes`, these will be enabled since `profile_memory` is True\"\n","        )\n","    with_stack = with_stack or profile_memory\n","    record_shapes = record_shapes or profile_memory\n","\n","    # experimental config is needed to export stacks: see https://github.com/pytorch/pytorch/issues/100253\n","    experimental_config = _ExperimentalConfig(verbose=True) if with_stack else None\n","\n","    if profile_memory:\n","        torch.cuda.memory._record_memory_history(max_entries=1_000_000)\n","\n","    with profile(\n","        activities=activities,  # [torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA]\n","        experimental_config=experimental_config,\n","        record_shapes=record_shapes,\n","        profile_memory=profile_memory,\n","        with_stack=with_stack,\n","        with_flops=with_flops,\n","        **profile_kwargs,\n","    ) as prof:\n","        for _ in range(profile_steps):\n","            profile_one_step(\n","                model=model,\n","                batch=batch,\n","                optimizer=optimizer,\n","                criterion=criterion,\n","                enable_backward=enable_backward,\n","                enable_optimizer=enable_optimizer,\n","                mixed_precision=mixed_precision,\n","            )\n","            prof.step()\n","\n","    if profile_memory:\n","        torch.cuda.memory._dump_snapshot('memory_snapshot.pickle')\n","        torch.cuda.memory._record_memory_history(enabled=None)\n","    return prof  # type: ignore[no-any-return]\n"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T08:51:47.236435Z","iopub.status.busy":"2024-08-12T08:51:47.235801Z","iopub.status.idle":"2024-08-12T08:51:50.125255Z","shell.execute_reply":"2024-08-12T08:51:50.124472Z","shell.execute_reply.started":"2024-08-12T08:51:47.236401Z"},"trusted":true},"outputs":[],"source":["gpt_small_config = GPTConfig(\n","    context_length=128,\n","    vocab_size=10_000,\n","    d_model=768,\n","    num_blocks=12,\n","    num_heads=12,\n",")\n","general = General()\n","\n","seed_all(general.seed, True, False)\n","\n","batch = get_random_batch(\n","    batch_size=general.batch_size,\n","    context_length=gpt_small_config.context_length,\n","    vocab_size=gpt_small_config.vocab_size,\n",")\n","\n","model = GPT(gpt_small_config).to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n","criterion = CrossEntropyLoss()"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T08:48:51.912474Z","iopub.status.busy":"2024-08-12T08:48:51.912117Z","iopub.status.idle":"2024-08-12T08:49:06.092182Z","shell.execute_reply":"2024-08-12T08:49:06.091300Z","shell.execute_reply.started":"2024-08-12T08:48:51.912445Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["STAGE:2024-08-12 08:48:52 34:34 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n","STAGE:2024-08-12 08:48:53 34:34 ActivityProfilerController.cpp:318] Completed Stage: Collection\n","STAGE:2024-08-12 08:48:53 34:34 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"]}],"source":["run_warmup(model=model, batch=batch, optimizer=optimizer, criterion=criterion)\n","\n","profiled = run_profiler(\n","    model=model,\n","    batch=batch,\n","    optimizer=optimizer,\n","    criterion=criterion,\n","    enable_backward=True,\n","    enable_optimizer=True,\n","    mixed_precision=False,\n","    profile_steps=5,\n","    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n","    profile_memory=False,\n","    with_stack=True,\n","    record_shapes=True,\n","    with_flops=False,\n",")"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T08:27:50.729369Z","iopub.status.busy":"2024-08-12T08:27:50.728980Z","iopub.status.idle":"2024-08-12T08:27:59.970235Z","shell.execute_reply":"2024-08-12T08:27:59.969316Z","shell.execute_reply.started":"2024-08-12T08:27:50.729338Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n","-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                                          backward_pass        36.08%     643.198ms        36.09%     643.418ms     128.684ms       0.000us         0.00%       5.000us       1.000us             5  \n","                                       cudaLaunchKernel        34.75%     619.425ms        34.75%     619.425ms      80.654us       0.000us         0.00%       0.000us       0.000us          7680  \n","                                           forward_pass         3.08%      54.873ms        18.44%     328.673ms      65.735ms       0.000us         0.00%     355.509ms      71.102ms             5  \n","      autograd::engine::evaluate_function: DivBackward0         0.34%       5.976ms        11.06%     197.217ms     458.644us       0.000us         0.00%      57.947ms     134.760us           430  \n","                                              aten::mul         1.23%      21.851ms         8.95%     159.555ms     112.363us      83.133ms         7.15%      83.133ms      58.544us          1420  \n","                                  cudaDeviceSynchronize         8.42%     150.048ms         8.42%     150.048ms     150.048ms       0.000us         0.00%       0.000us       0.000us             1  \n","                                           DivBackward0         0.21%       3.775ms         6.89%     122.752ms     285.470us       0.000us         0.00%      47.889ms     111.370us           430  \n","                                              aten::div         1.22%      21.803ms         6.68%     119.108ms      87.259us      50.880ms         4.38%      50.880ms      37.275us          1365  \n","                                               aten::mm         1.38%      24.547ms         6.10%     108.698ms      99.268us     757.305ms        65.15%     757.305ms     691.603us          1095  \n","       autograd::engine::evaluate_function: MmBackward0         0.20%       3.608ms         5.07%      90.308ms     247.419us       0.000us         0.00%     507.949ms       1.392ms           365  \n","-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","Self CPU time total: 1.783s\n","Self CUDA time total: 1.162s\n","\n","-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n","-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                                               aten::mm         1.38%      24.547ms         6.10%     108.698ms      99.268us     757.305ms        65.15%     757.305ms     691.603us          1095  \n","       autograd::engine::evaluate_function: MmBackward0         0.20%       3.608ms         5.07%      90.308ms     247.419us       0.000us         0.00%     507.949ms       1.392ms           365  \n","                                            MmBackward0         0.26%       4.547ms         4.86%      86.700ms     237.534us       0.000us         0.00%     507.949ms       1.392ms           365  \n","                                           forward_pass         3.08%      54.873ms        18.44%     328.673ms      65.735ms       0.000us         0.00%     355.509ms      71.102ms             5  \n","                                           aten::matmul         0.30%       5.300ms         3.74%      66.703ms     137.532us       0.000us         0.00%     269.186ms     555.023us           485  \n","                                           aten::linear         0.15%       2.613ms         2.51%      44.805ms     122.753us       0.000us         0.00%     237.179ms     649.805us           365  \n","                                 sgemm_128x128x8_TN_vec         0.00%       0.000us         0.00%       0.000us       0.000us     184.272ms        15.85%     184.272ms       1.474ms           125  \n","                                maxwell_sgemm_128x64_nn         0.00%       0.000us         0.00%       0.000us       0.000us     181.512ms        15.62%     181.512ms     427.087us           425  \n","                                maxwell_sgemm_128x64_tn         0.00%       0.000us         0.00%       0.000us       0.000us     166.766ms        14.35%     166.766ms     397.062us           420  \n","                                 sgemm_128x128x8_NT_vec         0.00%       0.000us         0.00%       0.000us       0.000us      94.916ms         8.17%      94.916ms       1.460ms            65  \n","-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","Self CPU time total: 1.783s\n","Self CUDA time total: 1.162s\n","\n"]}],"source":["profiled.export_stacks(\"lm_profiler_stacks.txt\", \"self_cuda_time_total\")\n","print(profiled.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n","print(profiled.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n","profiled.export_chrome_trace(\"trace.json\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T08:51:50.903683Z","iopub.status.busy":"2024-08-12T08:51:50.902607Z","iopub.status.idle":"2024-08-12T08:52:25.698890Z","shell.execute_reply":"2024-08-12T08:52:25.698021Z","shell.execute_reply.started":"2024-08-12T08:51:50.903638Z"},"trusted":true},"outputs":[],"source":["run_warmup(model=model, batch=batch, optimizer=optimizer, criterion=criterion)\n","\n","profiled = run_profiler(\n","    model=model,\n","    batch=batch,\n","    optimizer=optimizer,\n","    criterion=criterion,\n","    enable_backward=True,\n","    enable_optimizer=True,\n","    mixed_precision=False,\n","    profile_steps=5,\n","    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n","    profile_memory=True,\n","    with_stack=True,\n","    record_shapes=True,\n","    with_flops=False,\n","    schedule=torch.profiler.schedule(wait=0, warmup=0, active=1, repeat=3),\n","    on_trace_ready=trace_handler,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["You can plug your `memory_snapshot.pickle` into https://pytorch.org/memory_viz."]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-08-12T08:18:06.700480Z","iopub.status.busy":"2024-08-12T08:18:06.699347Z","iopub.status.idle":"2024-08-12T08:18:07.369177Z","shell.execute_reply":"2024-08-12T08:18:07.368215Z","shell.execute_reply.started":"2024-08-12T08:18:06.700447Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n","-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","                                              aten::mul         1.81%       5.259ms         2.62%       7.591ms      26.729us      16.628ms         7.17%      16.628ms      58.549us          24 b          24 b       3.21 Gb       3.21 Gb           284  \n","                                           forward_pass         4.13%      11.992ms        12.06%      34.999ms      34.999ms       0.000us         0.00%      70.702ms      70.702ms         440 b         244 b       2.15 Gb      -1.94 Gb             1  \n","                                              aten::div         1.63%       4.720ms         2.37%       6.889ms      25.234us      10.180ms         4.39%      10.180ms      37.289us           8 b           8 b       1.99 Gb       1.99 Gb           273  \n","                                               aten::mm         1.94%       5.618ms         3.01%       8.729ms      39.858us     150.881ms        65.08%     150.881ms     688.954us           0 b           0 b       1.71 Gb       1.71 Gb           219  \n","                                            aten::empty         0.72%       2.082ms         0.72%       2.082ms      13.788us       0.000us         0.00%       0.000us       0.000us          96 b          96 b       1.17 Gb       1.17 Gb           151  \n","                                           MulBackward0         0.10%     297.000us         0.90%       2.600ms      53.061us       0.000us         0.00%       5.903ms     120.469us           0 b           0 b       1.14 Gb      24.00 Mb            49  \n","                                           aten::matmul         0.32%     925.000us         2.77%       8.047ms      82.959us       0.000us         0.00%      53.449ms     551.021us           0 b           0 b       1.13 Gb           0 b            97  \n","                                            MmBackward0         0.38%       1.100ms         2.77%       8.050ms     110.274us       0.000us         0.00%     101.425ms       1.389ms           0 b           0 b       1.00 Gb           0 b            73  \n","                                           DivBackward0         0.30%     867.000us         2.32%       6.722ms      78.163us       0.000us         0.00%       9.584ms     111.442us           0 b           0 b    1020.20 Mb    -882.00 Mb            86  \n","                                       aten::empty_like         0.14%     419.000us         0.56%       1.627ms      13.446us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     864.00 Mb      24.00 Mb           121  \n","                                            aten::clone         0.25%     726.000us         1.85%       5.358ms      44.650us       0.000us         0.00%       5.261ms      43.842us           0 b           0 b     864.00 Mb     -24.00 Mb           120  \n","                                              aten::neg         0.35%       1.002ms         0.55%       1.589ms      24.828us       3.204ms         1.38%       3.204ms      50.062us           0 b           0 b     804.14 Mb     804.14 Mb            64  \n","                                           aten::linear         0.12%     353.000us         1.87%       5.432ms      74.411us       0.000us         0.00%      47.933ms     656.616us           0 b           0 b     726.12 Mb      30.00 Mb            73  \n","                                              aten::pow         0.47%       1.376ms         0.82%       2.391ms      38.565us       1.737ms         0.75%       2.377ms      38.339us           0 b           0 b     588.00 Mb     588.00 Mb            62  \n","                                              aten::bmm         0.61%       1.767ms         0.83%       2.409ms      33.458us       7.221ms         3.11%       7.221ms     100.292us           0 b           0 b     576.00 Mb     576.00 Mb            72  \n","                                              aten::add         0.47%       1.369ms         0.67%       1.945ms      25.933us       2.466ms         1.06%       2.466ms      32.880us           0 b           0 b     510.20 Mb     510.20 Mb            75  \n","                                              aten::exp         0.15%     433.000us         0.22%     632.000us      25.280us       2.020ms         0.87%       2.020ms      80.800us           0 b           0 b     510.12 Mb     510.12 Mb            25  \n","                                          aten::reshape         0.41%       1.199ms         2.11%       6.133ms      15.807us       0.000us         0.00%       3.612ms       9.309us           0 b           0 b     504.00 Mb           0 b           388  \n","                                    aten::empty_strided         0.25%     723.000us         0.25%     723.000us       3.394us       0.000us         0.00%       0.000us       0.000us         300 b         300 b     397.07 Mb     397.07 Mb           213  \n","                                    aten::_foreach_sqrt         0.11%     310.000us         0.34%     981.000us     981.000us       1.601ms         0.69%       1.601ms       1.601ms           0 b           0 b     397.07 Mb           0 b             1  \n","                                           BmmBackward0         0.10%     285.000us         0.75%       2.185ms      91.042us       0.000us         0.00%       4.825ms     201.042us           0 b           0 b     360.00 Mb           0 b            24  \n","                                           ErfBackward0         0.07%     207.000us         0.60%       1.739ms     144.917us       0.000us         0.00%       6.193ms     516.083us           0 b           0 b     336.00 Mb      -1.08 Gb            12  \n","                                      aten::masked_fill         0.11%     325.000us         0.71%       2.070ms      86.250us       0.000us         0.00%       2.533ms     105.542us           0 b           0 b     288.00 Mb           0 b            24  \n","                                              aten::erf         0.06%     180.000us         0.09%     254.000us      21.167us       1.142ms         0.49%       1.142ms      95.167us           0 b           0 b     288.00 Mb     288.00 Mb            12  \n","                                            aten::zeros         0.03%      84.000us         0.20%     589.000us      39.267us       0.000us         0.00%     438.000us      29.200us           0 b           0 b     251.80 Mb           0 b            15  \n","                                              aten::sub         0.08%     239.000us         0.11%     331.000us      25.462us     984.000us         0.42%     984.000us      75.692us           0 b           0 b     222.12 Mb     222.12 Mb            13  \n","                                           ExpBackward0         0.03%      89.000us         0.15%     430.000us      33.077us       0.000us         0.00%       1.173ms      90.231us           0 b           0 b     222.12 Mb           0 b            13  \n","                                           SubBackward0         0.01%      40.000us         0.12%     361.000us      27.769us       0.000us         0.00%     880.000us      67.692us           0 b           0 b     222.12 Mb           0 b            13  \n","                                           MaxBackward0         0.02%      47.000us         0.38%       1.113ms      85.615us       0.000us         0.00%     521.000us      40.077us           0 b           0 b     222.12 Mb           0 b            13  \n","               aten::value_selecting_reduction_backward         0.04%     108.000us         0.36%       1.042ms      80.154us       0.000us         0.00%     511.000us      39.308us           0 b           0 b     222.12 Mb           0 b            13  \n","                                           PowBackward0         0.09%     265.000us         0.97%       2.820ms     112.800us       0.000us         0.00%       1.976ms      79.040us           0 b          -8 b     174.00 Mb    -276.00 Mb            25  \n","                                    UnsafeViewBackward0         0.16%     473.000us         0.76%       2.216ms      16.662us       0.000us         0.00%       1.032ms       7.759us           0 b           0 b     144.00 Mb      12.00 Mb           133  \n","                                          ViewBackward0         0.16%     464.000us         0.75%       2.170ms      16.316us       0.000us         0.00%     792.000us       5.955us           0 b           0 b     144.00 Mb      12.00 Mb           133  \n","                                    MaskedFillBackward0         0.07%     205.000us         0.39%       1.136ms      94.667us       0.000us         0.00%       1.057ms      88.083us           0 b           0 b     144.00 Mb      24.00 Mb            12  \n","                                        GatherBackward0         0.00%       6.000us         0.03%      88.000us      88.000us       0.000us         0.00%     145.000us     145.000us           0 b           0 b      78.12 Mb           0 b             1  \n","                                  aten::gather_backward         0.00%       8.000us         0.03%      82.000us      82.000us       0.000us         0.00%     145.000us     145.000us           0 b           0 b      78.12 Mb           0 b             1  \n","                                        aten::new_zeros         0.00%       7.000us         0.01%      40.000us      40.000us       0.000us         0.00%     136.000us     136.000us           0 b           0 b      78.12 Mb           0 b             1  \n","                                        aten::new_empty         0.00%       3.000us         0.00%      13.000us      13.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      78.12 Mb           0 b             1  \n","                                       aten::contiguous         0.01%      21.000us         0.18%     513.000us      42.750us       0.000us         0.00%     432.000us      36.000us           0 b           0 b      72.00 Mb           0 b            12  \n","                                     EmbeddingBackward0         0.00%       6.000us         0.04%     108.000us      54.000us       0.000us         0.00%     214.000us     107.000us           0 b           0 b      29.67 Mb           0 b             2  \n","                               aten::embedding_backward         0.00%       5.000us         0.04%     102.000us      51.000us       0.000us         0.00%     214.000us     107.000us           0 b           0 b      29.67 Mb           0 b             2  \n","                         aten::embedding_dense_backward         0.01%      29.000us         0.03%      97.000us      48.500us     161.000us         0.07%     214.000us     107.000us           0 b           0 b      29.67 Mb           0 b             2  \n","autograd::engine::evaluate_function: EmbeddingBackwa...         0.01%      18.000us         0.04%     126.000us      63.000us       0.000us         0.00%     214.000us     107.000us           0 b           0 b      23.30 Mb      -6.38 Mb             2  \n","                                          aten::resize_         0.01%      34.000us         0.01%      34.000us      11.333us       0.000us         0.00%       0.000us       0.000us           0 b           0 b       6.38 Mb       6.38 Mb             3  \n","                                        aten::embedding         0.01%      36.000us         0.05%     156.000us      78.000us       0.000us         0.00%      80.000us      40.000us           0 b           0 b       6.38 Mb           0 b             2  \n","                                     aten::index_select         0.02%      50.000us         0.04%     111.000us      55.500us      80.000us         0.03%      80.000us      40.000us           0 b           0 b       6.38 Mb           0 b             2  \n","       autograd::engine::evaluate_function: MmBackward0         0.32%     921.000us         3.09%       8.971ms     122.890us       0.000us         0.00%     101.425ms       1.389ms           0 b           0 b       4.92 Mb   -1020.12 Mb            73  \n","                                              aten::sum         0.99%       2.874ms         1.35%       3.923ms      34.412us       4.123ms         1.78%       4.123ms      36.167us           0 b           0 b       4.23 Mb       4.23 Mb           114  \n","                                              aten::max         0.15%     442.000us         0.19%     544.000us      41.846us       1.390ms         0.60%       1.390ms     106.923us           0 b           0 b       3.40 Mb       3.40 Mb            13  \n","      autograd::engine::evaluate_function: SubBackward0         0.06%     175.000us         0.32%     921.000us      70.846us       0.000us         0.00%       1.718ms     132.154us           0 b           0 b       1.13 Mb    -222.12 Mb            13  \n","-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n","Self CPU time total: 290.215ms\n","Self CUDA time total: 231.841ms\n","\n"]}],"source":["print(profiled.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=50))"]},{"cell_type":"markdown","metadata":{},"source":["## References And Further Readings\n","\n","-   [PyTorch Benchmarking Tutorial](https://pytorch.org/tutorials/recipes/recipes/benchmark.html)\n","-   [CUDA Mode Notes - Lecture 001 by Christian J. Mills](https://christianjmills.com/posts/cuda-mode-notes/lecture-001/)\n","-   [GitHub Repository: Spring2024 Assignment2 Systems by Marcel Roed](https://github.com/marcelroed/spring2024-assignment2-systems/tree/master)\n","-   [PyTorch Profiler Recipe](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)\n","-   [PyTorch Torchtune GitHub Repository](https://github.com/pytorch/torchtune)\n","-   [Understanding GPU Memory - Part 1](https://pytorch.org/blog/understanding-gpu-memory-1/)\n","-   [Understanding GPU Memory - Part 2](https://pytorch.org/blog/understanding-gpu-memory-2/)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
