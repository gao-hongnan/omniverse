{"cells":[{"cell_type":"markdown","metadata":{},"source":["# How To Fine-Tune Decoder-Only Models For Sequence Classification Using Last Token Pooling?\n","\n","[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n","[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n","[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n","![Tag](https://img.shields.io/badge/Tag-Brain_Dump-red)\n","![Tag](https://img.shields.io/badge/Level-Beginner-green)\n","[![Code](https://img.shields.io/badge/View-Code-blue?style=flat-square&logo=github)](https://github.com/gao-hongnan/omniverse/tree/main/omnivault/transformer)\n","\n","```{contents}\n",":local:\n","```\n","\n","Firstly, if you have not read my\n","[Generative Pre-trained Transformers (GPT) series](https://www.gaohongnan.com/influential/generative_pretrained_transformer/03_concept.html),\n","please have a read first to establish some basic understand on what a\n","decoder-only model entails."]},{"cell_type":"markdown","metadata":{},"source":["## Dependencies\n","\n","```bash\n","pip install -U omniverse==0.0.63\n","```"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T08:29:38.504311Z","iopub.status.busy":"2024-08-25T08:29:38.503924Z","iopub.status.idle":"2024-08-25T08:30:17.949638Z","shell.execute_reply":"2024-08-25T08:30:17.948553Z","shell.execute_reply.started":"2024-08-25T08:29:38.504263Z"},"trusted":true},"outputs":[],"source":["# %pip install omniverse"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T08:37:54.618879Z","iopub.status.busy":"2024-08-25T08:37:54.618103Z","iopub.status.idle":"2024-08-25T08:37:54.628775Z","shell.execute_reply":"2024-08-25T08:37:54.627782Z","shell.execute_reply.started":"2024-08-25T08:37:54.618839Z"},"trusted":true},"outputs":[],"source":["from __future__ import annotations\n","\n","import logging\n","from collections import Counter, OrderedDict\n","from typing import Any, Dict, List, Tuple, TypedDict, overload\n","\n","import numpy as np\n","import pandas as pd\n","import psutil\n","import torch\n","from datasets import load_dataset\n","from rich.pretty import pprint\n","from scipy.special import softmax\n","from sklearn.metrics import (\n","    accuracy_score,\n","    auc,\n","    average_precision_score,\n","    brier_score_loss,\n","    confusion_matrix,\n","    f1_score,\n","    log_loss,\n","    precision_recall_curve,\n","    precision_score,\n","    recall_score,\n","    roc_auc_score,\n","    roc_curve,\n",")\n","from torch import nn\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm.notebook import tqdm  # Use notebook version for better UI in notebooks\n","from transformers import (\n","    DataCollatorWithPadding,\n","    EvalPrediction,\n","    GPT2ForSequenceClassification,\n","    GPT2Tokenizer,\n","    PreTrainedModel,\n","    PreTrainedTokenizer,\n","    PreTrainedTokenizerBase,\n","    PreTrainedTokenizerFast,\n","    Trainer,\n","    TrainingArguments,\n",")\n","from transformers.trainer_utils import EvalPrediction\n","\n","from omnivault.transformer.config.decoder import (\n","    AddNormConfig,\n","    DecoderBlockConfig,\n","    DecoderConfig,\n","    MultiHeadedAttentionConfig,\n","    PositionwiseFeedForwardConfig,\n",")\n","from omnivault.transformer.modules.attention.core import MultiHeadedAttention, ScaledDotProductAttention\n","from omnivault.transformer.modules.layers.addnorm import AddNorm\n","from omnivault.transformer.modules.layers.mlp import PositionwiseFeedForward\n","from omnivault.utils.reproducibility.seed import seed_all\n","from omnivault.utils.torch_utils.model_utils import total_trainable_parameters\n"]},{"cell_type":"markdown","metadata":{},"source":["## Setting Up"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T08:30:23.764720Z","iopub.status.busy":"2024-08-25T08:30:23.764009Z","iopub.status.idle":"2024-08-25T08:30:23.777133Z","shell.execute_reply":"2024-08-25T08:30:23.776133Z","shell.execute_reply.started":"2024-08-25T08:30:23.764679Z"},"trusted":true},"outputs":[{"data":{"text/plain":["2024"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["seed_all(seed=2024, seed_torch=True, set_torch_deterministic=False)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T08:30:24.645445Z","iopub.status.busy":"2024-08-25T08:30:24.645049Z","iopub.status.idle":"2024-08-25T08:30:24.652043Z","shell.execute_reply":"2024-08-25T08:30:24.650968Z","shell.execute_reply.started":"2024-08-25T08:30:24.645406Z"},"trusted":true},"outputs":[],"source":["LOGGER = logging.getLogger(__name__)\n","LOGGER.setLevel(logging.INFO)\n","handler = logging.StreamHandler()\n","formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","handler.setFormatter(formatter)\n","LOGGER.addHandler(handler)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T08:30:26.483785Z","iopub.status.busy":"2024-08-25T08:30:26.483088Z","iopub.status.idle":"2024-08-25T08:30:26.521514Z","shell.execute_reply":"2024-08-25T08:30:26.520556Z","shell.execute_reply.started":"2024-08-25T08:30:26.483740Z"},"trusted":true},"outputs":[],"source":["DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","MAX_LENGTH = 64\n","PADDING = \"longest\"\n","BATCH_SIZE = 32\n","TRUNCATION = True\n","RETURN_TENSORS = \"pt\""]},{"cell_type":"markdown","metadata":{},"source":["## Dataset"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['sentence', 'labels'],\n","    num_rows: 2264\n","})"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["dataset = load_dataset('financial_phrasebank', 'sentences_allagree', trust_remote_code=True)[\"train\"]\n","dataset = dataset.rename_column(\"label\", \"labels\")\n","dataset"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">303</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1391</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">570</span><span style=\"font-weight: bold\">}</span>\n","</pre>\n"],"text/plain":["\u001b[1m{\u001b[0m\u001b[1;36m0\u001b[0m: \u001b[1;36m303\u001b[0m, \u001b[1;36m1\u001b[0m: \u001b[1;36m1391\u001b[0m, \u001b[1;36m2\u001b[0m: \u001b[1;36m570\u001b[0m\u001b[1m}\u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["def count_labels(labels: List[int]) -> Dict[int, int]:\n","    label_counts = Counter(labels)\n","    ordered_label_counts = OrderedDict(sorted(label_counts.items()))\n","    return dict(ordered_label_counts)\n","\n","\n","sentences_allagree = dataset['sentence']\n","labels_allagree = dataset['labels']\n","\n","label_counts = count_labels(labels_allagree)\n","pprint(label_counts)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["train_valid_split = dataset.train_test_split(test_size=0.1, shuffle=True, stratify_by_column='labels')\n","train_dataset = train_valid_split['train']\n","valid_dataset = train_valid_split['test']"]},{"cell_type":"markdown","metadata":{},"source":["We create our own `Dataset` just for understanding!"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["train_df = train_dataset.to_pandas()\n","valid_df = valid_dataset.to_pandas()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class FinancialDataset(Dataset):\n","    def __init__(self, df: pd.DataFrame, tokenizer: PreTrainedTokenizer, **tokenizer_kwargs: Any) -> None:\n","        self.tokenizer = tokenizer\n","        self.tokenizer_kwargs = tokenizer_kwargs\n","        self.inputs = df[\"sentence\"].tolist()\n","        self.labels = df[\"labels\"].tolist()\n","\n","    def __len__(self) -> int:\n","        return len(self.labels)\n","\n","    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n","        input_ids = self.tokenizer.encode(text=self.inputs[index], **self.tokenizer_kwargs).long()\n","        labels = torch.tensor(self.labels[index]).long()\n","        return {\n","            \"input_ids\": input_ids,\n","            \"labels\": labels,\n","        }"]},{"cell_type":"markdown","metadata":{},"source":["We will create the causal mask in the collator."]},{"cell_type":"markdown","metadata":{},"source":["## Tokenizer"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'bos_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'eos_token'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'unk_token'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span><span style=\"font-weight: bold\">}</span>\n","</pre>\n"],"text/plain":["\u001b[1m{\u001b[0m\u001b[32m'bos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'eos_token'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'<|endoftext|>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'unk_token'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'<|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m\u001b[1m}\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPT2Tokenizer</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name_or_path</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt2'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">vocab_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50257</span>, <span style=\"color: #808000; text-decoration-color: #808000\">model_max_length</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span>, <span style=\"color: #808000; text-decoration-color: #808000\">is_fast</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">padding_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">truncation_side</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'right'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">special_tokens</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'bos_token'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'eos_token'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'unk_token'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'pad_token'</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;|endoftext|&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">}</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #808000; text-decoration-color: #808000\">clean_up_tokenization_spaces</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,  </span><span style=\"color: #808000; text-decoration-color: #808000\">added_tokens_decoder</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50256</span><span style=\"color: #000000; text-decoration-color: #000000\">: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AddedToken</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">\"&lt;|endoftext|&gt;\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">rstrip</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">lstrip</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">single_word</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">normalized</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #808000; text-decoration-color: #808000\">special</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>,\n","<span style=\"font-weight: bold\">}</span>\n","</pre>\n"],"text/plain":["\u001b[1;35mGPT2Tokenizer\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname_or_path\u001b[0m=\u001b[32m'gpt2'\u001b[0m, \u001b[33mvocab_size\u001b[0m=\u001b[1;36m50257\u001b[0m, \u001b[33mmodel_max_length\u001b[0m=\u001b[1;36m1024\u001b[0m, \u001b[33mis_fast\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mpadding_side\u001b[0m=\u001b[32m'right'\u001b[0m, \u001b[33mtruncation_side\u001b[0m=\u001b[32m'right'\u001b[0m, \u001b[33mspecial_tokens\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'bos_token'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32m|endoftext|\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'eos_token'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'<|endoftext|>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'unk_token'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'<|endoftext|>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'pad_token'\u001b[0m\u001b[39m: \u001b[0m\u001b[32m'<|endoftext|>'\u001b[0m\u001b[1;39m}\u001b[0m\u001b[39m, \u001b[0m\u001b[33mclean_up_tokenization_spaces\u001b[0m\u001b[39m=\u001b[0m\u001b[3;92mTrue\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,  \u001b[0m\u001b[33madded_tokens_decoder\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m{\u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1;36m50256\u001b[0m\u001b[39m: \u001b[0m\u001b[1;35mAddedToken\u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m\"<|endoftext|\u001b[0m\u001b[32m>\u001b[0m\u001b[32m\"\u001b[0m, \u001b[33mrstrip\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mlstrip\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33msingle_word\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mnormalized\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mspecial\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m,\n","\u001b[1m}\u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","pprint(tokenizer.special_tokens_map)\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","pprint(tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["## Data Collator And DataLoader"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["train_dataset = FinancialDataset(train_df, tokenizer=tokenizer, max_length=3, padding=PADDING, truncation=TRUNCATION, return_tensors=RETURN_TENSORS)\n","valid_dataset = FinancialDataset(valid_df, tokenizer=tokenizer, max_length=3, padding=PADDING, truncation=TRUNCATION, return_tensors=RETURN_TENSORS)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def construct_dummy_batch_causal_masks(batch_size: int, seq_len: int) -> torch.BoolTensor:\n","    \"\"\"Broadcast future mask from shape (L, L) to (B, L, L) then (B, 1, L, L).\"\"\"\n","    # Create a lower triangular mask for a single sequence\n","    future_mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool), diagonal=0).to(torch.bool)\n","    future_mask = future_mask.contiguous()\n","    # broadcast future mask from shape (L, L) to (B, L, L)\n","    causal_masks = future_mask.unsqueeze(0).expand(batch_size, -1, -1)\n","    # broadcast future mask from shape (B, L, L) to (B, 1, L, L)\n","    causal_masks = causal_masks.unsqueeze(1)\n","    return torch.BoolTensor(causal_masks)\n","\n","def collate_for_unidirectional(\n","    batch: List[Dict[str, torch.Tensor]],\n",") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n","    max_length = max(item[\"input_ids\"].size(1) for item in batch) # 25\n","    input_ids = torch.zeros((len(batch), max_length), dtype=torch.long)\n","    labels = torch.zeros(len(batch), dtype=torch.long)\n","\n","    # do padding manually\n","    for index, item in enumerate(batch):\n","        seq_len = item[\"input_ids\"].size(1)\n","        input_ids[index, :seq_len] = item[\"input_ids\"]\n","        labels[index] = item[\"labels\"]\n","\n","    batch_size, seq_len = input_ids.size()\n","\n","    causal_masks = construct_dummy_batch_causal_masks(batch_size, seq_len)\n","    return input_ids, labels, causal_masks"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47117</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">351</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">262</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3732</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">680</span><span style=\"font-weight: bold\">]])</span>\n","</pre>\n"],"text/plain":["\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m47117\u001b[0m,   \u001b[1;36m351\u001b[0m,   \u001b[1;36m262\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m   \u001b[1;36m37\u001b[0m,  \u001b[1;36m3732\u001b[0m,   \u001b[1;36m680\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">])</span>\n","</pre>\n"],"text/plain":["\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[[</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,  <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,  <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,  <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">]]]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[[[</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,  <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,  <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,  <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">]]]])</span>\n","</pre>\n"],"text/plain":["\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[3;92mTrue\u001b[0m, \u001b[3;91mFalse\u001b[0m, \u001b[3;91mFalse\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m[\u001b[0m \u001b[3;92mTrue\u001b[0m,  \u001b[3;92mTrue\u001b[0m, \u001b[3;91mFalse\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m[\u001b[0m \u001b[3;92mTrue\u001b[0m,  \u001b[3;92mTrue\u001b[0m,  \u001b[3;92mTrue\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[3;92mTrue\u001b[0m, \u001b[3;91mFalse\u001b[0m, \u001b[3;91mFalse\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m[\u001b[0m \u001b[3;92mTrue\u001b[0m,  \u001b[3;92mTrue\u001b[0m, \u001b[3;91mFalse\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m[\u001b[0m \u001b[3;92mTrue\u001b[0m,  \u001b[3;92mTrue\u001b[0m,  \u001b[3;92mTrue\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["seed_all(seed=2024, seed_torch=True, set_torch_deterministic=False)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_for_unidirectional)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=collate_for_unidirectional)\n","\n","for batch in train_dataloader:\n","    input_ids, labels, causal_masks = batch\n","    pprint(input_ids)\n","    pprint(labels)\n","    pprint(causal_masks)\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["## Model Architecture"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["class DecoderForSequenceClassificationConfig(DecoderConfig):\n","    num_labels: int\n","    head_bias: bool = False\n","    pre_head_pooling: bool = True\n","\n","\n","class GPTPretrainedModel(nn.Module):\n","    def _init_weights(self, module: nn.Module) -> None:\n","        normal_init_modules = (nn.Linear, nn.Embedding)\n","        if isinstance(module, normal_init_modules):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if hasattr(module, \"bias\") and module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","\n","\n","class GPTDecoderBlock(nn.Module):\n","    \"\"\"GPTDecoderBlock focuses on masked self-attention and feed-forward layers.\n","\n","    The architecture follows the GPT-style decoder, which only has masked\n","    self-attention and position-wise feed-forward layers, omitting the\n","    encoder-decoder cross-attention.\n","    \"\"\"\n","\n","    def __init__(self, config: DecoderConfig) -> None:\n","        super().__init__()\n","        self.masked_self_attention_mha = MultiHeadedAttention(\n","            **config.decoder_block.masked_self_attention_mha.model_dump(mode=\"python\")\n","        )\n","        self.feed_forward = PositionwiseFeedForward(**config.decoder_block.feed_forward.model_dump(mode=\"python\"))\n","        self.add_norm_1 = AddNorm(**config.decoder_block.add_norm_1.model_dump(mode=\"python\"))\n","        self.add_norm_2 = AddNorm(**config.decoder_block.add_norm_2.model_dump(mode=\"python\"))\n","\n","    def forward(self, z: torch.Tensor, causal_masks: torch.BoolTensor) -> torch.Tensor:\n","        \"\"\"\n","        Parameters\n","        ----------\n","        z:              Input sequence.\n","                        type:  torch.Tensor\n","                        shape: (B, S or T, D)\n","\n","        Returns\n","        -------\n","        z:              Output tensor after masked self-attention and feed-forward layers.\n","                        type:  torch.Tensor\n","                        shape: (B, S or T, D)\n","        \"\"\"\n","        z = self.add_norm_1(\n","            z,\n","            lambda z: self.masked_self_attention_mha(query=z, key=z, value=z, mask=causal_masks),\n","        )\n","        z = self.add_norm_2(z, self.feed_forward)\n","        return z\n","\n","\n","class GPTBackbone(GPTPretrainedModel):\n","    def __init__(self, config: DecoderConfig) -> None:\n","        super().__init__()\n","        self.d_model: int = config.d_model\n","        self.tok_embed: nn.Embedding = nn.Embedding(config.vocab_size, config.d_model)\n","        self.pos_embed: nn.Parameter = nn.Parameter(torch.zeros(1, config.context_length, config.d_model))\n","        self.decoder_blocks: nn.ModuleList = nn.ModuleList(\n","            [GPTDecoderBlock(config) for _ in range(config.num_decoder_blocks)]\n","        )  # PyTorch did not make ModuleList a proper container, maybe open a PR to make it inherit Generic[T]???\n","\n","        self.dropout: nn.Dropout = nn.Dropout(config.dropout)\n","        self.layer_norm: nn.LayerNorm = nn.LayerNorm(config.d_model)\n","\n","        self.apply(self._init_weights)\n","\n","        context_projections = (\"context_projection.weight\", \"W_O.weight\")\n","        # apply special scaled init to the residual projections, per GPT-2 paper\n","        for parameter_name, parameter in self.named_parameters():\n","            # NOTE: W_O is also projection but I did not have foresight to name it as such.\n","            if parameter_name.endswith(context_projections):\n","                mean = 0.0\n","                std_dev = 0.02 / torch.sqrt(torch.tensor(2 * config.num_decoder_blocks, dtype=torch.float))\n","                torch.nn.init.normal_(parameter, mean=mean, std=std_dev)\n","\n","    def forward(\n","        self, input_tokens: torch.LongTensor, *, causal_masks: torch.BoolTensor\n","    ) -> torch.FloatTensor:\n","        seq_len: int = input_tokens.size(1)  # note seq_len <= context_length in decoder\n","        causal_masks = causal_masks.to(input_tokens.device)  # type: ignore[assignment]\n","\n","        z = self.tok_embed(input_tokens)  # TODO: * math.sqrt(self.d_model) for better optimization landscape\n","        z = z + self.pos_embed[:, :seq_len, :]\n","        z = self.dropout(z)\n","\n","        for decoder_block in self.decoder_blocks:\n","            z = decoder_block(z, causal_masks=causal_masks)\n","\n","        z = self.layer_norm(z)\n","        return z\n","\n","class LastTokenPooling(nn.Module):\n","    def __init__(self, pre_head_pooling: bool = True) -> None:\n","        super().__init__()\n","        self.pre_head_pooling = pre_head_pooling\n","\n","    @overload\n","    def forward(self, last_hidden_state: torch.Tensor, logits: None = None) -> torch.Tensor: ...\n","\n","    @overload\n","    def forward(self, last_hidden_state: None, logits: torch.Tensor) -> torch.Tensor: ...\n","\n","    def forward(self,  last_hidden_state: torch.Tensor | None = None, logits: torch.Tensor | None = None) -> torch.Tensor:\n","        \"\"\"Forward pass for the pooling layer.\n","\n","        Parameters\n","        ----------\n","        last_hidden_state:  Hidden state of the last layer.\n","                            type:  torch.Tensor\n","                            shape: (B, T, D)\n","        logits:             Logits from the last layer.\n","                            type:  torch.Tensor\n","                            shape: (B, T, C)\n","\n","        Notes\n","        -----\n","        In both cases, we will slice the `T` dimension to get the last token's\n","        hidden state or logits. For example, if `last_hidden_state` is provided,\n","        then we have `[B, T, D] -> [B, D]` and if `logits` is provided, then we\n","        have `[B, T, C] -> [B, C]`.\n","        \"\"\"\n","        if self.pre_head_pooling:\n","            assert last_hidden_state is not None, \"last_hidden_state must be provided when pre_head is True\"\n","            pooled_hidden_state = last_hidden_state[:, -1, :]\n","            return pooled_hidden_state\n","        else:\n","            assert logits is not None, \"logits must be provided when pre_head is False\"\n","            pooled_logits = logits[:, -1, :]\n","            return pooled_logits\n","\n","class GPTForSequenceClassification(GPTPretrainedModel):\n","    def __init__(self, config: DecoderForSequenceClassificationConfig) -> None:\n","        super().__init__()\n","        self.config = config\n","\n","        self.backbone = GPTBackbone(config)\n","        self.pooler = LastTokenPooling(pre_head_pooling=config.pre_head_pooling)\n","        self.head = nn.Linear(config.d_model, config.num_labels, bias=config.head_bias)\n","\n","        self.apply(self._init_weights)\n","\n","        # apply special scaled init to the residual projections, per GPT-2 paper\n","        for parameter_name, parameter in self.named_parameters():\n","            if parameter_name.endswith(\"context_projection.weight\"):\n","                mean = 0.0\n","                std_dev = 0.02 / torch.sqrt(torch.tensor(2 * config.num_decoder_blocks, dtype=torch.float))\n","                torch.nn.init.normal_(parameter, mean=mean, std=std_dev)\n","\n","    def forward(\n","        self,\n","        input_tokens: torch.LongTensor,\n","        *,  # force keyword only arguments to prevent errors\n","        causal_masks: torch.BoolTensor,\n","    ) -> torch.FloatTensor:\n","        \"\"\"\n","        Notations\n","        ---------\n","        B:      Batch size\n","        S or L: Source sequence length\n","        T or L: Target sequence length\n","        D:      Embedding dimension\n","        C:      Vocabulary size (Class size)\n","\n","        Parameters\n","        ----------\n","        input_tokens:           Input sequence.\n","                                type:  torch.Tensor\n","                                shape: (B, T)\n","        causal_masks:           Future mask.\n","                                type:  torch.BoolTensor\n","                                shape: (B, 1, T, T)\n","\n","        Variables\n","        ---------\n","        z:                      Input sequence after token and position embedding.\n","                                type:  torch.Tensor\n","                                shape: (B, T, D)\n","        causal_masks:           Target mask.\n","                                type:  torch.BoolTensor\n","                                shape: (B, 1, T, T)\n","        logits:                 Output logits.\n","                                type:  torch.FloatTensor\n","                                shape: (B, T, C)\n","        pooled_logits:          Pooled logits.\n","                                type:  torch.FloatTensor\n","                                shape: (B, C)\n","        \"\"\"\n","\n","        backbone_last_layer_hidden_state = self.backbone(input_tokens, causal_masks=causal_masks)\n","\n","        if self.config.pre_head_pooling:\n","            pooled_hidden_state = self.pooler(backbone_last_layer_hidden_state)\n","            pooled_logits = self.head(pooled_hidden_state)\n","        else:\n","            logits = self.head(backbone_last_layer_hidden_state)\n","            pooled_logits = self.pooler(logits)\n","        return pooled_logits\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["model_config = DecoderForSequenceClassificationConfig(\n","    d_model=32,\n","    vocab_size=tokenizer.vocab_size,\n","    context_length=MAX_LENGTH,\n","    num_decoder_blocks=1,\n","    dropout=0.0,\n","    decoder_block=DecoderBlockConfig(\n","        masked_self_attention_mha=MultiHeadedAttentionConfig(\n","            attention=ScaledDotProductAttention(), d_model=32, H=1, dropout=0.0\n","        ),\n","        feed_forward=PositionwiseFeedForwardConfig(\n","            d_model=32, d_ff=32 * 2, activation=nn.GELU(approximate=\"tanh\"), dropout=0.0, bias=True\n","        ),\n","        add_norm_1=AddNormConfig(feature_dim=32, dropout=0.0),\n","        add_norm_2=AddNormConfig(feature_dim=32, dropout=0.0),\n","    ),\n","    num_labels=3,\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["model = GPTForSequenceClassification(model_config).to(DEVICE)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPTForSequenceClassification</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>backbone<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPTBackbone</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>tok_embed<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Embedding</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50257</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>decoder_blocks<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ModuleList</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPTDecoderBlock</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">(</span>masked_self_attention_mha<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MultiHeadedAttention</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>W_Q<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>W_K<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>W_V<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>W_O<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>attention<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScaledDotProductAttention</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">(</span>feed_forward<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PositionwiseFeedForward</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>ffn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ModuleDict</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"font-weight: bold\">(</span>context_fc<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"font-weight: bold\">(</span>activation<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GELU</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">approximate</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'tanh'</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"font-weight: bold\">(</span>context_projection<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">(</span>add_norm_1<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AddNorm</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>layer_norm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-05</span>, <span style=\"color: #808000; text-decoration-color: #808000\">elementwise_affine</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">(</span>add_norm_2<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AddNorm</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>layer_norm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-05</span>, <span style=\"color: #808000; text-decoration-color: #808000\">elementwise_affine</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>layer_norm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-05</span>, <span style=\"color: #808000; text-decoration-color: #808000\">elementwise_affine</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>pooler<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LastTokenPooling</span><span style=\"font-weight: bold\">()</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>head<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"font-weight: bold\">)</span>\n","</pre>\n"],"text/plain":["\u001b[1;35mGPTForSequenceClassification\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mbackbone\u001b[1m)\u001b[0m: \u001b[1;35mGPTBackbone\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0mtok_embed\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m50257\u001b[0m, \u001b[1;36m32\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0mdecoder_blocks\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mGPTDecoderBlock\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m(\u001b[0mmasked_self_attention_mha\u001b[1m)\u001b[0m: \u001b[1;35mMultiHeadedAttention\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mW_Q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m32\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m32\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mW_K\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m32\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m32\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mW_V\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m32\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m32\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mW_O\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m32\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m32\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mattention\u001b[1m)\u001b[0m: \u001b[1;35mScaledDotProductAttention\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│   │   │   \u001b[0m\u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m(\u001b[0mfeed_forward\u001b[1m)\u001b[0m: \u001b[1;35mPositionwiseFeedForward\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mffn\u001b[1m)\u001b[0m: \u001b[1;35mModuleDict\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│   │   │   \u001b[0m\u001b[1m(\u001b[0mcontext_fc\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m32\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m64\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   │   \u001b[0m\u001b[1m(\u001b[0mactivation\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'tanh'\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   │   \u001b[0m\u001b[1m(\u001b[0mcontext_projection\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m64\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m32\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   │   \u001b[0m\u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m(\u001b[0madd_norm_1\u001b[1m)\u001b[0m: \u001b[1;35mAddNorm\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mlayer_norm\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m32\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m(\u001b[0madd_norm_2\u001b[1m)\u001b[0m: \u001b[1;35mAddNorm\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mlayer_norm\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m32\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│     \u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0mlayer_norm\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m32\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m  \u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mpooler\u001b[1m)\u001b[0m: \u001b[1;35mLastTokenPooling\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mhead\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m32\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m3\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[1m)\u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["pprint(model)"]},{"cell_type":"markdown","metadata":{},"source":["## Dry Run\n","\n","In our dry run, we make the following assumptions:\n","\n","- `batch_size = 2` which means we have $2$ samples in a batch.\n","- `MAX_LEN = 3` which means the context length $T$ is $3$.\n","- `d_model = 4` which means the model dimension is $4$ for hidden layers.\n","- Consequently the final output dimension of the backbone is $\\mathcal{B} \\times T \\times D \\rightarrow 2\\times 3 \\times 4$."]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPTForSequenceClassification</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>backbone<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPTBackbone</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>tok_embed<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Embedding</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50257</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>decoder_blocks<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ModuleList</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GPTDecoderBlock</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">(</span>masked_self_attention_mha<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MultiHeadedAttention</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>W_Q<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>W_K<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>W_V<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>W_O<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>attention<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScaledDotProductAttention</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">(</span>feed_forward<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PositionwiseFeedForward</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>ffn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ModuleDict</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"font-weight: bold\">(</span>context_fc<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"font-weight: bold\">(</span>activation<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GELU</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">approximate</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'tanh'</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"font-weight: bold\">(</span>context_projection<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">(</span>add_norm_1<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AddNorm</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>layer_norm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-05</span>, <span style=\"color: #808000; text-decoration-color: #808000\">elementwise_affine</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">(</span>add_norm_2<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AddNorm</span><span style=\"font-weight: bold\">(</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">(</span>layer_norm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-05</span>, <span style=\"color: #808000; text-decoration-color: #808000\">elementwise_affine</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>dropout<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dropout</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">p</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">inplace</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>layer_norm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LayerNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-05</span>, <span style=\"color: #808000; text-decoration-color: #808000\">elementwise_affine</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">)</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>pooler<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LastTokenPooling</span><span style=\"font-weight: bold\">()</span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>head<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>\n","<span style=\"font-weight: bold\">)</span>\n","</pre>\n"],"text/plain":["\u001b[1;35mGPTForSequenceClassification\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mbackbone\u001b[1m)\u001b[0m: \u001b[1;35mGPTBackbone\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0mtok_embed\u001b[1m)\u001b[0m: \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m50257\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0mdecoder_blocks\u001b[1m)\u001b[0m: \u001b[1;35mModuleList\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mGPTDecoderBlock\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m(\u001b[0mmasked_self_attention_mha\u001b[1m)\u001b[0m: \u001b[1;35mMultiHeadedAttention\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mW_Q\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mW_K\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mW_V\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mW_O\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mattention\u001b[1m)\u001b[0m: \u001b[1;35mScaledDotProductAttention\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│   │   │   \u001b[0m\u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m(\u001b[0mfeed_forward\u001b[1m)\u001b[0m: \u001b[1;35mPositionwiseFeedForward\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mffn\u001b[1m)\u001b[0m: \u001b[1;35mModuleDict\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│   │   │   \u001b[0m\u001b[1m(\u001b[0mcontext_fc\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m8\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   │   \u001b[0m\u001b[1m(\u001b[0mactivation\u001b[1m)\u001b[0m: \u001b[1;35mGELU\u001b[0m\u001b[1m(\u001b[0m\u001b[33mapproximate\u001b[0m=\u001b[32m'tanh'\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   │   \u001b[0m\u001b[1m(\u001b[0mcontext_projection\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m8\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   │   \u001b[0m\u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m(\u001b[0madd_norm_1\u001b[1m)\u001b[0m: \u001b[1;35mAddNorm\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mlayer_norm\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m(\u001b[0madd_norm_2\u001b[1m)\u001b[0m: \u001b[1;35mAddNorm\u001b[0m\u001b[1m(\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m(\u001b[0mlayer_norm\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│     \u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0mdropout\u001b[1m)\u001b[0m: \u001b[1;35mDropout\u001b[0m\u001b[1m(\u001b[0m\u001b[33mp\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m, \u001b[33minplace\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0mlayer_norm\u001b[1m)\u001b[0m: \u001b[1;35mLayerNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m, \u001b[33melementwise_affine\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m  \u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mpooler\u001b[1m)\u001b[0m: \u001b[1;35mLastTokenPooling\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mhead\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m3\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\n","\u001b[1m)\u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["seed_all(seed=2024, seed_torch=True, set_torch_deterministic=False)\n","\n","dry_run_model_config = DecoderForSequenceClassificationConfig(\n","    d_model=4,\n","    vocab_size=tokenizer.vocab_size,\n","    context_length=MAX_LENGTH,\n","    num_decoder_blocks=1,\n","    dropout=0.0,\n","    decoder_block=DecoderBlockConfig(\n","        masked_self_attention_mha=MultiHeadedAttentionConfig(\n","            attention=ScaledDotProductAttention(), d_model=4, H=1, dropout=0.0\n","        ),\n","        feed_forward=PositionwiseFeedForwardConfig(\n","            d_model=4, d_ff=4 * 2, activation=nn.GELU(approximate=\"tanh\"), dropout=0.0, bias=True\n","        ),\n","        add_norm_1=AddNormConfig(feature_dim=4, dropout=0.0),\n","        add_norm_2=AddNormConfig(feature_dim=4, dropout=0.0),\n","    ),\n","    num_labels=3,\n",")\n","\n","dry_run_model = GPTForSequenceClassification(dry_run_model_config).to(DEVICE)\n","pprint(dry_run_model)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["seed_all(seed=2024, seed_torch=True, set_torch_deterministic=False)\n","\n","batch = next(iter(train_dataloader))\n","input_ids, labels, causal_masks = batch"]},{"cell_type":"markdown","metadata":{},"source":["First, we see the input ids, labels and causal masks to be of the below format."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">47117</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">351</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">262</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span>   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3732</span>,   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">680</span><span style=\"font-weight: bold\">]])</span>\n","</pre>\n"],"text/plain":["\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m47117\u001b[0m,   \u001b[1;36m351\u001b[0m,   \u001b[1;36m262\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m   \u001b[1;36m37\u001b[0m,  \u001b[1;36m3732\u001b[0m,   \u001b[1;36m680\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'Relations with the'</span>\n","</pre>\n"],"text/plain":["\u001b[32m'Relations with the'\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">])</span>\n","</pre>\n"],"text/plain":["\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[[</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,  <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,  <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,  <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">]]]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[[[</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,  <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │     </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,  <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,  <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">]]]])</span>\n","</pre>\n"],"text/plain":["\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[3;92mTrue\u001b[0m, \u001b[3;91mFalse\u001b[0m, \u001b[3;91mFalse\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m[\u001b[0m \u001b[3;92mTrue\u001b[0m,  \u001b[3;92mTrue\u001b[0m, \u001b[3;91mFalse\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m[\u001b[0m \u001b[3;92mTrue\u001b[0m,  \u001b[3;92mTrue\u001b[0m,  \u001b[3;92mTrue\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[3;92mTrue\u001b[0m, \u001b[3;91mFalse\u001b[0m, \u001b[3;91mFalse\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m[\u001b[0m \u001b[3;92mTrue\u001b[0m,  \u001b[3;92mTrue\u001b[0m, \u001b[3;91mFalse\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │     \u001b[0m\u001b[1m[\u001b[0m \u001b[3;92mTrue\u001b[0m,  \u001b[3;92mTrue\u001b[0m,  \u001b[3;92mTrue\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["pprint(input_ids)\n","pprint(tokenizer.decode(input_ids[0].tolist(), skip_special_tokens=True))\n","pprint(labels)\n","pprint(causal_masks)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0732</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3017</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.6063</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2314</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.6302</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.6116</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0369</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.9445</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0674</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2399</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4665</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2940</span><span style=\"font-weight: bold\">]]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span>\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7063</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4734</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.6868</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5071</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.5821</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1785</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6935</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.2899</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │    </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0365</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7257</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.8955</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5847</span><span style=\"font-weight: bold\">]]])</span>\n","</pre>\n"],"text/plain":["\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m1.0732\u001b[0m,  \u001b[1;36m0.3017\u001b[0m, \u001b[1;36m-1.6063\u001b[0m,  \u001b[1;36m0.2314\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.6302\u001b[0m,  \u001b[1;36m1.6116\u001b[0m, \u001b[1;36m-0.0369\u001b[0m, \u001b[1;36m-0.9445\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.0674\u001b[0m,  \u001b[1;36m1.2399\u001b[0m, \u001b[1;36m-1.4665\u001b[0m,  \u001b[1;36m0.2940\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.7063\u001b[0m,  \u001b[1;36m0.4734\u001b[0m, \u001b[1;36m-1.6868\u001b[0m,  \u001b[1;36m0.5071\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.5821\u001b[0m,  \u001b[1;36m1.1785\u001b[0m,  \u001b[1;36m0.6935\u001b[0m, \u001b[1;36m-1.2899\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │    \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0365\u001b[0m, \u001b[1;36m-0.7257\u001b[0m, \u001b[1;36m-0.8955\u001b[0m,  \u001b[1;36m1.5847\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">])</span>\n","</pre>\n"],"text/plain":["\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["dry_run_backbone = dry_run_model.backbone\n","dry_run_backbone_last_layer_hidden_state = dry_run_backbone(input_ids, causal_masks=causal_masks)\n","dry_run_backbone_last_layer_hidden_state = dry_run_backbone_last_layer_hidden_state.detach().cpu()\n","pprint(dry_run_backbone_last_layer_hidden_state)\n","pprint(dry_run_backbone_last_layer_hidden_state.shape)"]},{"cell_type":"markdown","metadata":{},"source":["Here indeed the output of the backbone is of shape `[2, 3, 4]`. More concretely,\n","we have for the first sequence/example in the batch to be `[47117,   351,   262]`\n","with the underlying text to be `'Relations with the'` and the corresponding\n","label to be `0`. Now you see there are 3 tokens in the sequence, it is \n","normal because if we do autoregressive modelling, we need to predict the next\n","token given the previous tokens. However, when we move on to sequence level\n","classification, we actually want to predict the label for the entire sequence\n","and not just say, given the first token, predict the second token and so on.\n","Fundamentally, the backbone is not designed for this task. Currently, the\n","backbone outputs the hidden states for each token in the sequence. \n","\n","For example,\n","\n","```python\n","[ 1.0732,  0.3017, -1.6063,  0.2314] # -> token embedding for `Relations`\n","[-0.6302,  1.6116, -0.0369, -0.9445] # -> token embedding for `with`\n","[-0.0674,  1.2399, -1.4665,  0.2940] # -> token embedding for `the`\n","```"]},{"cell_type":"markdown","metadata":{},"source":["We introduce the idea of pooling the hidden states to get a single representation\n","for the entire sequence. You can think of it as transforming the hidden states\n","of all 3 tokens in the sequence to 1 single sentence/sequence representation.\n","\n","```python\n","[-0.0674,  1.2399, -1.4665,  0.2940] # -> pooled embedding for `Relations with the`\n","```\n","\n","However, in decoder only models, we do not have the `[CLS]` token to pool the\n","hidden states. However, recall the causal mask format for the first sequence.\n","\n","```python\n","[ True, False, False]\n","[ True,  True, False]\n","[ True,  True,  True]\n","```\n","\n","Oh, so the last token in the sequence is the one that is not masked - which\n","defaults to _cross attention_ since it has information of _every token_ in the\n","sequence. So, we can simply pool the last token to get the sequence\n","representation."]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0674</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2399</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.4665</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2940</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0365</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.7257</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.8955</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5847</span><span style=\"font-weight: bold\">]])</span>\n","</pre>\n"],"text/plain":["\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-0.0674\u001b[0m,  \u001b[1;36m1.2399\u001b[0m, \u001b[1;36m-1.4665\u001b[0m,  \u001b[1;36m0.2940\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0365\u001b[0m, \u001b[1;36m-0.7257\u001b[0m, \u001b[1;36m-0.8955\u001b[0m,  \u001b[1;36m1.5847\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">])</span>\n","</pre>\n"],"text/plain":["\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["dry_run_pooler = dry_run_model.pooler\n","dry_run_pooler_output = dry_run_pooler(dry_run_backbone_last_layer_hidden_state)\n","dry_run_pooler_output = dry_run_pooler_output.detach().cpu()\n","pprint(dry_run_pooler_output)\n","pprint(dry_run_pooler_output.shape)"]},{"cell_type":"markdown","metadata":{},"source":["And we got the pooled representation for the first sequence. Earlier I \n","commented this to be the pooled embedding for the sequence `Relations with the`.\n","However, to be more pedantic, it is merely the embedding for the last token in the sequence and because of the last token being aware of all tokens in the sequence, it can be considered as the pooled embedding for the entire sequence.\n","\n","```python\n","[-0.0674,  1.2399, -1.4665,  0.2940] \n","```\n","\n","So we went from `[3, 4]` to `[1, 4]` by pooling the last token. This is a lossy\n","compression but is good enough. If you have done encoder pooling before, you\n","will figure that there are many ways to \"better\" pool the hidden states. For\n","example, you can mean pool, max pool, etc. However, in decoder only models, we\n","can only make well use of the last token so our pooling is limited to that, unless\n","you swap the causal attention to cross attention, which people do that to benefit\n","from the large number of parameters in the decoder.\n","\n","Anothing thing is HuggingFace defaults the last token pooling to _after the head layer_.\n","We offer the option to pool _before the head layer_ as well and the results\n","should be similar."]},{"cell_type":"markdown","metadata":{},"source":["Lastly, we pass the pooled embeddings to a linear layer to get the logits for\n","the classification task. For our current example we got the logits to be:\n","\n","```python\n","[0.0287,  0.0123,  0.0312]\n","```"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0287</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0123</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0312</span><span style=\"font-weight: bold\">]</span>,\n","<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">[</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0237</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-0.0079</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0117</span><span style=\"font-weight: bold\">]])</span>\n","</pre>\n"],"text/plain":["\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0287\u001b[0m,  \u001b[1;36m0.0123\u001b[0m,  \u001b[1;36m0.0312\u001b[0m\u001b[1m]\u001b[0m,\n","\u001b[2;32m│   │   \u001b[0m\u001b[1m[\u001b[0m \u001b[1;36m0.0237\u001b[0m, \u001b[1;36m-0.0079\u001b[0m,  \u001b[1;36m0.0117\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["head = dry_run_model.head\n","logits = head(dry_run_pooler_output)\n","logits = logits.detach().cpu()\n","pprint(logits)"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.0048)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=6, eta_min=0.0)\n","num_epochs = 6"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["train_dataset = FinancialDataset(train_df, tokenizer=tokenizer, max_length=MAX_LENGTH, padding=PADDING, truncation=TRUNCATION, return_tensors=RETURN_TENSORS)\n","valid_dataset = FinancialDataset(valid_df, tokenizer=tokenizer, max_length=MAX_LENGTH, padding=PADDING, truncation=TRUNCATION, return_tensors=RETURN_TENSORS)\n","train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_for_unidirectional, pin_memory=True)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_for_unidirectional, pin_memory=True)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["def train_one_epoch(\n","    model: nn.Module,\n","    dataloader: DataLoader,\n","    optimizer: torch.optim.Optimizer,\n","    criterion: nn.Module,\n","    device: torch.device,\n",") -> Tuple[float, float]:\n","    model.train()\n","    total_loss = 0.0\n","    correct_predictions = 0\n","\n","    for batch in tqdm(dataloader, desc=\"Training\", leave=True):\n","        input_ids, labels, causal_masks = (x.to(device) for x in batch)\n","        optimizer.zero_grad()\n","        outputs = model(input_tokens=input_ids, causal_masks=causal_masks)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        _, preds = torch.max(outputs, dim=1)\n","        correct_predictions += torch.sum(preds == labels).item()\n","\n","    avg_loss = total_loss / len(dataloader)\n","    accuracy = correct_predictions / len(dataloader.dataset)\n","    return avg_loss, accuracy\n","\n","\n","def validate_one_epoch(\n","    model: nn.Module, dataloader: DataLoader, criterion: nn.Module, device: torch.device\n",") -> Tuple[float, float]:\n","    model.eval()\n","    val_loss = 0.0\n","    val_correct_predictions = 0\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc=\"Validation\", leave=True):\n","            input_ids, labels, causal_masks = (x.to(device) for x in batch)\n","            outputs = model(input_tokens=input_ids, causal_masks=causal_masks)\n","            loss = criterion(outputs, labels)\n","\n","            val_loss += loss.item()\n","            _, preds = torch.max(outputs, dim=1)\n","            val_correct_predictions += torch.sum(preds == labels).item()\n","\n","    avg_val_loss = val_loss / len(dataloader)\n","    val_accuracy = val_correct_predictions / len(dataloader.dataset)\n","    return avg_val_loss, val_accuracy\n","\n","\n","def train_model(\n","    model: nn.Module,\n","    train_dataloader: DataLoader,\n","    valid_dataloader: DataLoader,\n","    criterion: nn.Module,\n","    optimizer: torch.optim.Optimizer,\n","    scheduler: torch.optim.lr_scheduler._LRScheduler,\n","    num_epochs: int,\n","    device: torch.device,\n",") -> None:\n","    for epoch in range(num_epochs):\n","        train_loss, train_accuracy = train_one_epoch(model, train_dataloader, optimizer, criterion, device)\n","        scheduler.step()\n","        val_loss, val_accuracy = validate_one_epoch(model, valid_dataloader, criterion, device)\n","\n","        LOGGER.info(\n","            \"Epoch %d/%d - Training loss: %.4f, Training accuracy: %.4f - Validation loss: %.4f, Validation accuracy: %.4f\",\n","            epoch + 1, num_epochs, train_loss, train_accuracy, val_loss, val_accuracy\n","        )"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"52437f4f047f49c4a69b8c6602f40b2d","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/64 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e2fea829376b4b7aa2ddf1a3c3ac9dea","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/8 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024-07-20 21:12:02,603 - __main__ - INFO - Epoch 1/6 - Training loss: 0.7196, Training accuracy: 0.6966 - Validation loss: 0.5611, Validation accuracy: 0.7841\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d3bb05df766c4fb99d4bf1dccfdc1ced","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/64 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"745f74f7c7414fcdb2656011c1740e19","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/8 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024-07-20 21:12:03,551 - __main__ - INFO - Epoch 2/6 - Training loss: 0.4483, Training accuracy: 0.7973 - Validation loss: 0.5083, Validation accuracy: 0.7974\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38669b78d298453f96ba82118fb25ea5","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/64 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b10d667f7d824edb8d469a3a4a061d61","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/8 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024-07-20 21:12:04,381 - __main__ - INFO - Epoch 3/6 - Training loss: 0.2279, Training accuracy: 0.9116 - Validation loss: 0.6285, Validation accuracy: 0.7930\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4d46b34078f94e3d8461d88be523f735","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/64 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3be668e128d943d6b9b7e79e0908bf6f","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/8 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024-07-20 21:12:05,294 - __main__ - INFO - Epoch 4/6 - Training loss: 0.1285, Training accuracy: 0.9607 - Validation loss: 0.4113, Validation accuracy: 0.8722\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"90ce7f7e192b4ee4a0e729be94392a32","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/64 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1c0e3c1e6f84b0cb4c5e1a6e7c2a82c","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/8 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024-07-20 21:12:06,251 - __main__ - INFO - Epoch 5/6 - Training loss: 0.0344, Training accuracy: 0.9936 - Validation loss: 0.4243, Validation accuracy: 0.8943\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d1f39bd1cb6f40e3a570f8b940cba43d","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/64 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23d747e5e5ab42a7821a714ef37b33ad","version_major":2,"version_minor":0},"text/plain":["Validation:   0%|          | 0/8 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024-07-20 21:12:07,058 - __main__ - INFO - Epoch 6/6 - Training loss: 0.0220, Training accuracy: 0.9961 - Validation loss: 0.4257, Validation accuracy: 0.9031\n"]}],"source":["train_model(model, train_dataloader, valid_dataloader, criterion, optimizer, scheduler, num_epochs, DEVICE)"]},{"cell_type":"markdown","metadata":{},"source":["The results are just decent, but you can see the model is learning. Tuning \n","decoder only models need some experimentation."]},{"cell_type":"markdown","metadata":{},"source":["## Using HuggingFace"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T08:30:34.822106Z","iopub.status.busy":"2024-08-25T08:30:34.821195Z","iopub.status.idle":"2024-08-25T08:30:34.828032Z","shell.execute_reply":"2024-08-25T08:30:34.826941Z","shell.execute_reply.started":"2024-08-25T08:30:34.822065Z"},"trusted":true},"outputs":[],"source":["class Batch(TypedDict):\n","    sentence: List[str]\n","    labels: List[int]\n","\n","\n","class TokenizedBatch(TypedDict):\n","    input_ids: List[int]\n","    attention_mask: List[int]\n","    labels: List[int]\n","\n","\n","def preprocess_function(batch: Batch, **kwargs: Any) -> TokenizedBatch:\n","    return tokenizer(batch[\"sentence\"], **kwargs)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T08:30:36.768226Z","iopub.status.busy":"2024-08-25T08:30:36.767477Z","iopub.status.idle":"2024-08-25T08:30:58.235318Z","shell.execute_reply":"2024-08-25T08:30:58.234156Z","shell.execute_reply.started":"2024-08-25T08:30:36.768187Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01da28b3e3d74e64954e7c08304d79c9","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.04k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c49ca662c73644d3ad452c099d797f0b","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/8.88k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"37644b38c94a4f2a90f5ff14508dd3f2","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/682k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4dd213efcb9e4290b47462dbbfcae4d7","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/2264 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c5572d562d114e0eafe3e1dc875e0df6","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad981c03f579492abeb9078a19493fde","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e62d46f63b0a45dd87d32d93ed7ed599","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5911c80c1ca44b20b108c04c8525408a","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"678bea5492544e71942cb1d530ad351f","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c622fbab2847411aa6f1ab7e65f694d8","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=4):   0%|          | 0/2037 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b2e54448d984c468dbd6517dc4801b3","version_major":2,"version_minor":0},"text/plain":["Map (num_proc=4):   0%|          | 0/227 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\", trust_remote_code=True)[\"train\"]\n","dataset = dataset.rename_column(\"label\", \"labels\")\n","\n","train_valid_split = dataset.train_test_split(test_size=0.1, shuffle=True, stratify_by_column=\"labels\")\n","\n","train_dataset = train_valid_split[\"train\"]\n","valid_dataset = train_valid_split[\"test\"]\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.pad_token_id = tokenizer.eos_token_id\n","tokenizer.padding_side = \"left\"\n","\n","tokenized_train_dataset = train_dataset.map(\n","    preprocess_function,\n","    fn_kwargs={\"truncation\": TRUNCATION, \"padding\": PADDING, \"max_length\": MAX_LENGTH},\n","    batched=True,\n","    num_proc=psutil.cpu_count(logical=True),\n","    batch_size=1000,\n",").remove_columns([\"sentence\"])\n","\n","tokenized_valid_dataset = valid_dataset.map(\n","    preprocess_function,\n","    fn_kwargs={\"truncation\": TRUNCATION, \"padding\": PADDING, \"max_length\": MAX_LENGTH},\n","    batched=True,\n","    num_proc=psutil.cpu_count(logical=True),\n","    batch_size=1000,\n",").remove_columns([\"sentence\"])\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T08:42:22.644951Z","iopub.status.busy":"2024-08-25T08:42:22.644565Z","iopub.status.idle":"2024-08-25T08:42:23.202231Z","shell.execute_reply":"2024-08-25T08:42:23.201438Z","shell.execute_reply.started":"2024-08-25T08:42:22.644915Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n","label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n","num_labels = len(id2label)\n","\n","base_model = GPT2ForSequenceClassification.from_pretrained(\n","    \"gpt2\",\n","    id2label=id2label,\n","    label2id=label2id,\n","    num_labels=num_labels,\n","    problem_type=\"single_label_classification\",\n",")\n","base_model.config.pad_token_id = tokenizer.pad_token_id"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T08:31:11.313580Z","iopub.status.busy":"2024-08-25T08:31:11.312742Z","iopub.status.idle":"2024-08-25T08:31:11.320220Z","shell.execute_reply":"2024-08-25T08:31:11.319501Z","shell.execute_reply.started":"2024-08-25T08:31:11.313536Z"},"trusted":true},"outputs":[{"data":{"text/plain":["124.442112"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["total_trainable_parameters(base_model) / 1e6"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T08:42:26.029646Z","iopub.status.busy":"2024-08-25T08:42:26.029213Z","iopub.status.idle":"2024-08-25T08:42:26.058929Z","shell.execute_reply":"2024-08-25T08:42:26.057911Z","shell.execute_reply.started":"2024-08-25T08:42:26.029597Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}],"source":["training_args = TrainingArguments(\n","    do_eval=True,\n","    do_predict=False,\n","    do_train=True,\n","    warmup_ratio=0.1,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=5,\n","    report_to=\"none\",\n","    output_dir = './artifacts',\n","    overwrite_output_dir=True,\n","    gradient_accumulation_steps=1,\n","    logging_steps=25,\n","    evaluation_strategy='epoch',\n","    eval_steps=25,\n","    save_strategy=\"epoch\",\n","    save_steps=25,\n","    load_best_model_at_end=True,\n","    metric_for_best_model='accuracy',\n","    lr_scheduler_type='cosine',\n","    weight_decay=0.01,\n","    save_total_limit=2,\n","    seed=42,\n","    data_seed=42,\n","    half_precision_backend=\"auto\",\n","    optim=\"adamw_torch\",\n","    label_smoothing_factor=0.0,\n","    max_grad_norm = 1.0,\n",")\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T08:40:37.554646Z","iopub.status.busy":"2024-08-25T08:40:37.554224Z","iopub.status.idle":"2024-08-25T08:40:37.566815Z","shell.execute_reply":"2024-08-25T08:40:37.565848Z","shell.execute_reply.started":"2024-08-25T08:40:37.554607Z"},"trusted":true},"outputs":[],"source":["def compute_metrics_for_single_label_classification(eval_prediction: EvalPrediction) -> Dict[str, float | List[float]]:\n","    logits, labels = eval_prediction.predictions, eval_prediction.label_ids\n","    probs = softmax(logits, axis=-1)\n","\n","    num_classes = logits.shape[1]\n","    preds = np.argmax(probs, axis=1)\n","\n","    metrics = {\n","        \"eval_log_loss\": log_loss(labels, probs),\n","        \"eval_accuracy\": accuracy_score(labels, preds),\n","        \"eval_precision_macro\": precision_score(labels, preds, average=\"macro\", zero_division=0),\n","        \"eval_recall_macro\": recall_score(labels, preds, average=\"macro\", zero_division=0),\n","        \"eval_f1_score_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n","        \"eval_precision_micro\": precision_score(labels, preds, average=\"micro\", zero_division=0),\n","        \"eval_recall_micro\": recall_score(labels, preds, average=\"micro\", zero_division=0),\n","        \"eval_f1_score_micro\": f1_score(labels, preds, average=\"micro\", zero_division=0),\n","        \"eval_confusion_matrix\": confusion_matrix(labels, preds).tolist(),\n","        \"eval_roc_auc\": roc_auc_score(labels, probs, multi_class=\"ovr\"),\n","        # \"eval_pr_auc\": average_precision_score(labels, probs, average=\"macro\"),\n","    }\n","\n","    if num_classes == 2:\n","        metrics[\"eval_brier_score\"] = brier_score_loss(labels, probs[:, 1], pos_label=1)\n","    else:\n","        brier_scores = [brier_score_loss(labels == i, probs[:, i]) for i in range(num_classes)]\n","        metrics[\"eval_brier_score\"] = np.mean(brier_scores)\n","\n","    if num_classes > 2:\n","        for class_index in range(num_classes):\n","            fpr, tpr, _ = roc_curve(labels == class_index, probs[:, class_index])\n","            roc_auc = auc(fpr, tpr)\n","            precision, recall, _ = precision_recall_curve(labels == class_index, probs[:, class_index])\n","            pr_auc = auc(recall, precision)\n","            metrics[f\"eval_roc_auc_class_{class_index}\"] = roc_auc\n","            metrics[f\"eval_pr_auc_class_{class_index}\"] = pr_auc\n","\n","    return metrics\n"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T08:42:28.531470Z","iopub.status.busy":"2024-08-25T08:42:28.531080Z","iopub.status.idle":"2024-08-25T08:44:34.436581Z","shell.execute_reply":"2024-08-25T08:44:34.435587Z","shell.execute_reply.started":"2024-08-25T08:42:28.531431Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1275' max='1275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1275/1275 02:05, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Log Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision Macro</th>\n","      <th>Recall Macro</th>\n","      <th>F1 Score Macro</th>\n","      <th>Precision Micro</th>\n","      <th>Recall Micro</th>\n","      <th>F1 Score Micro</th>\n","      <th>Confusion Matrix</th>\n","      <th>Roc Auc</th>\n","      <th>Brier Score</th>\n","      <th>Roc Auc Class 0</th>\n","      <th>Pr Auc Class 0</th>\n","      <th>Roc Auc Class 1</th>\n","      <th>Pr Auc Class 1</th>\n","      <th>Roc Auc Class 2</th>\n","      <th>Pr Auc Class 2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.654800</td>\n","      <td>0.575435</td>\n","      <td>0.575435</td>\n","      <td>0.740088</td>\n","      <td>0.612724</td>\n","      <td>0.596449</td>\n","      <td>0.599999</td>\n","      <td>0.740088</td>\n","      <td>0.740088</td>\n","      <td>0.740088</td>\n","      <td>[[9, 5, 16], [0, 125, 15], [12, 11, 34]]</td>\n","      <td>0.876099</td>\n","      <td>0.113814</td>\n","      <td>0.860237</td>\n","      <td>0.379456</td>\n","      <td>0.942775</td>\n","      <td>0.963399</td>\n","      <td>0.825284</td>\n","      <td>0.608643</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.478300</td>\n","      <td>0.219012</td>\n","      <td>0.219012</td>\n","      <td>0.929515</td>\n","      <td>0.909526</td>\n","      <td>0.901044</td>\n","      <td>0.903625</td>\n","      <td>0.929515</td>\n","      <td>0.929515</td>\n","      <td>0.929515</td>\n","      <td>[[27, 3, 0], [0, 137, 3], [5, 5, 47]]</td>\n","      <td>0.979735</td>\n","      <td>0.038522</td>\n","      <td>0.991540</td>\n","      <td>0.935628</td>\n","      <td>0.987603</td>\n","      <td>0.991196</td>\n","      <td>0.960062</td>\n","      <td>0.933540</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.158000</td>\n","      <td>0.221089</td>\n","      <td>0.221089</td>\n","      <td>0.938326</td>\n","      <td>0.906788</td>\n","      <td>0.942398</td>\n","      <td>0.921905</td>\n","      <td>0.938326</td>\n","      <td>0.938326</td>\n","      <td>0.938326</td>\n","      <td>[[30, 0, 0], [2, 133, 5], [4, 3, 50]]</td>\n","      <td>0.986326</td>\n","      <td>0.031093</td>\n","      <td>0.993739</td>\n","      <td>0.949336</td>\n","      <td>0.993103</td>\n","      <td>0.995271</td>\n","      <td>0.972136</td>\n","      <td>0.952987</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.212800</td>\n","      <td>0.227551</td>\n","      <td>0.227551</td>\n","      <td>0.942731</td>\n","      <td>0.914779</td>\n","      <td>0.944779</td>\n","      <td>0.927814</td>\n","      <td>0.942731</td>\n","      <td>0.942731</td>\n","      <td>0.942731</td>\n","      <td>[[30, 0, 0], [1, 134, 5], [4, 3, 50]]</td>\n","      <td>0.989035</td>\n","      <td>0.033271</td>\n","      <td>0.994755</td>\n","      <td>0.961793</td>\n","      <td>0.994745</td>\n","      <td>0.996582</td>\n","      <td>0.977606</td>\n","      <td>0.959318</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.101200</td>\n","      <td>0.212625</td>\n","      <td>0.212625</td>\n","      <td>0.942731</td>\n","      <td>0.915999</td>\n","      <td>0.930785</td>\n","      <td>0.923062</td>\n","      <td>0.942731</td>\n","      <td>0.942731</td>\n","      <td>0.942731</td>\n","      <td>[[28, 0, 2], [1, 135, 4], [3, 3, 51]]</td>\n","      <td>0.990026</td>\n","      <td>0.029624</td>\n","      <td>0.994755</td>\n","      <td>0.962396</td>\n","      <td>0.994828</td>\n","      <td>0.996647</td>\n","      <td>0.980495</td>\n","      <td>0.961750</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=1275, training_loss=0.3979669969222125, metrics={'train_runtime': 125.2594, 'train_samples_per_second': 81.311, 'train_steps_per_second': 10.179, 'total_flos': 332666429276160.0, 'train_loss': 0.3979669969222125, 'epoch': 5.0})"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["trainer = Trainer(\n","    model=base_model,\n","    args=training_args,\n","    train_dataset=tokenized_train_dataset,\n","    eval_dataset=tokenized_valid_dataset,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics_for_single_label_classification,\n",")\n","\n","trainer.train()\n"]},{"cell_type":"markdown","metadata":{},"source":["## References And Further Readings\n","\n","-   https://arxiv.org/pdf/2401.00368\n","-   https://arxiv.org/pdf/2201.10005"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
