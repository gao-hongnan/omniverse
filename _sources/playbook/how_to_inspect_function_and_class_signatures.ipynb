{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Inspect Function and Class Signatures in Python\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "[![Code](https://img.shields.io/badge/View-Code-blue?style=flat-square&logo=github)](https://github.com/gao-hongnan/omniverse/blob/main/omnivault/utils/inspector/core.py)\n",
    "![Tag](https://img.shields.io/badge/Tag-Brain_Dump-red)\n",
    "\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, GPT2LMHeadModel, TrainingArguments\n",
    "from dataclasses import field, make_dataclass\n",
    "\n",
    "import inspect\n",
    "from inspect import Signature, Parameter\n",
    "from typing import Any, Callable, Dict, Set, Optional, _GenericAlias, List, Type, Union, get_type_hints, overload, Tuple\n",
    "from pydantic import BaseModel\n",
    "from rich.pretty import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Don't need to use `getfullargspec` as it seems to be to be more of an old remnant of the `getargspec` method. See [here](https://github.com/joblib/joblib/issues/1164).\n",
    "- how about if the class is subclass of another, so u need to get all arg of original class too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Hypothetical Function, Child and Parent Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParentClass:\n",
    "    \"\"\"This is the parent class.\"\"\"\n",
    "\n",
    "    parent_class_attr = 'a parent class attribute'\n",
    "\n",
    "    def __init__(self, parent_instance_attr: str) -> None:\n",
    "        self.parent_instance_attr = parent_instance_attr\n",
    "\n",
    "    def parent_method(self) -> str:\n",
    "        \"\"\"This is a method in the parent class.\"\"\"\n",
    "        return \"Parent method called\"\n",
    "\n",
    "class ChildClass(ParentClass):\n",
    "    \"\"\"This is a subclass of ParentClass.\"\"\"\n",
    "\n",
    "    # Class attribute\n",
    "    class_attr = 'a class attribute'\n",
    "\n",
    "    # Private and protected attributes\n",
    "    _protected_attr = 'a protected attribute'\n",
    "    __private_attr = 'a private attribute'\n",
    "\n",
    "    def __init__(self, instance_attr: str, parent_instance_attr: str) -> None:\n",
    "        \"\"\"Initialize the instance.\"\"\"\n",
    "        super().__init__(parent_instance_attr)\n",
    "        # Instance attribute\n",
    "        self.instance_attr = instance_attr\n",
    "\n",
    "    @property\n",
    "    def read_only_attr(self) -> str:\n",
    "        \"\"\"This is a read-only attribute.\"\"\"\n",
    "        return 'You can read me, but you cannot change me.'\n",
    "\n",
    "    def instance_method(self, arg: str) -> str:\n",
    "        \"\"\"This is an instance method.\"\"\"\n",
    "        return f'Instance method called with argument: {arg}'\n",
    "\n",
    "    @classmethod\n",
    "    def class_method(cls, arg: str) -> str:\n",
    "        \"\"\"This is a class method.\"\"\"\n",
    "        return f'Class method called with argument: {arg}'\n",
    "\n",
    "    @staticmethod\n",
    "    def static_method(arg: str) -> str:\n",
    "        \"\"\"This is a static method.\"\"\"\n",
    "        return f'Static method called with argument: {arg}'\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Return a string representation of the instance.\"\"\"\n",
    "        return f'MyClass(instance_attr={self.instance_attr})'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_child = ChildClass(instance_attr='an instance attribute', parent_instance_attr='a parent instance attribute')\n",
    "class_child = ChildClass\n",
    "\n",
    "instance_parent = ParentClass(parent_instance_attr='a parent instance attribute')\n",
    "class_parent = ParentClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(a: int, b: str, c: List[int], d: Tuple[str, str], e: Union[int, str], **kwargs: Any) -> str:\n",
    "    return a, b, c, d, e, kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect All Members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "@overload\n",
    "def get_members_of_function_or_method(\n",
    "    func_or_class: Type[object], predicate: Optional[Callable[[Any], bool]] = None\n",
    ") -> List[Tuple[str, Any]]:\n",
    "    ...\n",
    "\n",
    "\n",
    "@overload\n",
    "def get_members_of_function_or_method(\n",
    "    func_or_class: Callable[..., Any], predicate: Optional[Callable[[Any], bool]] = None\n",
    ") -> List[Tuple[str, Any]]:\n",
    "    ...\n",
    "\n",
    "\n",
    "def get_members_of_function_or_method(\n",
    "    func_or_class: Union[Type[object], Callable[..., Any]], predicate: Optional[Callable[[Any], bool]] = None\n",
    ") -> List[Tuple[str, Any]]:\n",
    "    return inspect.getmembers(func_or_class, predicate)\n",
    "\n",
    "def loop_through_members(members: List[Tuple[str, Any]], filter: Optional[str] = None) -> None:\n",
    "    if filter is not None:\n",
    "        members = [member for member in members if filter in member[0]]\n",
    "    for member in members:\n",
    "        name, value = member\n",
    "        print(f'{name}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial goal is to get all signatures and type annotations of a class or function. We can use the `inspect` module to achieve this. The `getmembers` function returns all members of a class or module. We can then filter out the functions and classes and inspect their signatures.\n",
    "\n",
    "However, for our purpose, it may be overkill since it retrieves all members\n",
    "within a module, the scope is very broad, for example, inspecting just the `func`\n",
    "defined will also return all `__globals__`, which may not be what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__annotations__: {'a': <class 'int'>, 'b': <class 'str'>, 'c': typing.List[int], 'd': typing.Tuple[str, str], 'e': typing.Union[int, str], 'kwargs': typing.Any, 'return': <class 'str'>}\n",
      "__call__: <method-wrapper '__call__' of function object at 0x2a7d5b310>\n",
      "__class__: <class 'function'>\n",
      "__closure__: None\n",
      "__code__: <code object func at 0x2a9741be0, file \"/var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_16129/2139551385.py\", line 1>\n",
      "__defaults__: None\n",
      "__delattr__: <method-wrapper '__delattr__' of function object at 0x2a7d5b310>\n",
      "__dict__: {}\n",
      "__dir__: <built-in method __dir__ of function object at 0x2a7d5b310>\n",
      "__doc__: None\n",
      "__eq__: <method-wrapper '__eq__' of function object at 0x2a7d5b310>\n",
      "__format__: <built-in method __format__ of function object at 0x2a7d5b310>\n",
      "__ge__: <method-wrapper '__ge__' of function object at 0x2a7d5b310>\n",
      "__get__: <method-wrapper '__get__' of function object at 0x2a7d5b310>\n",
      "__getattribute__: <method-wrapper '__getattribute__' of function object at 0x2a7d5b310>\n",
      "__globals__: {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', \"get_ipython().run_line_magic('pip', 'install -U -q langchain')\", 'from transformers import Trainer, GPT2LMHeadModel, TrainingArguments\\nfrom dataclasses import field, make_dataclass\\n\\nimport inspect\\nfrom inspect import Signature, Parameter\\nfrom typing import Any, Callable, Dict, Set, Optional, List, Type, Union, get_type_hints, overload, Tuple\\nfrom pydantic import BaseModel\\nfrom rich.pretty import pprint', 'class ParentClass:\\n    \"\"\"This is the parent class.\"\"\"\\n\\n    parent_class_attr = \\'a parent class attribute\\'\\n\\n    def __init__(self, parent_instance_attr: str) -> None:\\n        self.parent_instance_attr = parent_instance_attr\\n\\n    def parent_method(self) -> str:\\n        \"\"\"This is a method in the parent class.\"\"\"\\n        return \"Parent method called\"\\n\\nclass ChildClass(ParentClass):\\n    \"\"\"This is a subclass of ParentClass.\"\"\"\\n\\n    # Class attribute\\n    class_attr = \\'a class attribute\\'\\n\\n    # Private and protected attributes\\n    _protected_attr = \\'a protected attribute\\'\\n    __private_attr = \\'a private attribute\\'\\n\\n    def __init__(self, instance_attr: str, parent_instance_attr: str) -> None:\\n        \"\"\"Initialize the instance.\"\"\"\\n        super().__init__(parent_instance_attr)\\n        # Instance attribute\\n        self.instance_attr = instance_attr\\n\\n    @property\\n    def read_only_attr(self) -> str:\\n        \"\"\"This is a read-only attribute.\"\"\"\\n        return \\'You can read me, but you cannot change me.\\'\\n\\n    def instance_method(self, arg: str) -> str:\\n        \"\"\"This is an instance method.\"\"\"\\n        return f\\'Instance method called with argument: {arg}\\'\\n\\n    @classmethod\\n    def class_method(cls, arg: str) -> str:\\n        \"\"\"This is a class method.\"\"\"\\n        return f\\'Class method called with argument: {arg}\\'\\n\\n    @staticmethod\\n    def static_method(arg: str) -> str:\\n        \"\"\"This is a static method.\"\"\"\\n        return f\\'Static method called with argument: {arg}\\'\\n\\n    def __str__(self) -> str:\\n        \"\"\"Return a string representation of the instance.\"\"\"\\n        return f\\'MyClass(instance_attr={self.instance_attr})\\'', \"instance_child = ChildClass(instance_attr='an instance attribute', parent_instance_attr='a parent instance attribute')\\nclass_child = ChildClass\\n\\ninstance_parent = ParentClass(parent_instance_attr='a parent instance attribute')\\nclass_parent = ParentClass\", 'def func(a: int, b: str, c: List[int], d: Tuple[str, str], e: Union[int, str], **kwargs: Any) -> str:\\n    return a, b, c, d, e, kwargs', \"@overload\\ndef get_members_of_function_or_method(\\n    func_or_class: Type[object], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    ...\\n\\n\\n@overload\\ndef get_members_of_function_or_method(\\n    func_or_class: Callable[..., Any], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    ...\\n\\n\\ndef get_members_of_function_or_method(\\n    func_or_class: Union[Type[object], Callable[..., Any]], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    return inspect.getmembers(func_or_class, predicate)\\n\\ndef loop_through_members(members: List[Tuple[str, Any]], filter: Optional[str] = None) -> None:\\n    if filter is not None:\\n        members = [member for member in members if filter in member[0]]\\n    for member in members:\\n        name, value = member\\n        print(f'{name}: {value}')\", 'func_all_members = get_members_of_function_or_method(func, predicate=None)\\nloop_through_members(func_all_members)', \"loop_through_members(func_all_members, filter='__annotations__')\", 'class_child_all_members = get_members_of_function_or_method(class_child, predicate=None)\\nloop_through_members(class_child_all_members)', 'instance_child_all_members = get_members_of_function_or_method(instance_child, predicate=None)\\nloop_through_members(instance_child_all_members)', 'trainer_all_members = get_members_of_function_or_method(Trainer, predicate=None)\\nloop_through_members(trainer_all_members)', \"child_class_methods_using_dict = list(ChildClass.__dict__.keys())\\npprint(sorted(child_class_methods_using_dict))\\n\\nassert 'parent_method' not in child_class_methods_using_dict\\nassert 'read_only_attr' in child_class_methods_using_dict\\nassert 'class_method' in child_class_methods_using_dict\", 'pprint(instance_child.__class__.__dict__.keys() == ChildClass.__dict__.keys())', \"child_class_methods_using_vars = list(vars(ChildClass).keys())\\npprint(sorted(child_class_methods_using_vars))\\n\\nassert 'parent_method' not in child_class_methods_using_vars\\nassert 'read_only_attr' in child_class_methods_using_vars\\nassert 'class_method' in child_class_methods_using_vars\\n\\nassert set(child_class_methods_using_dict) == set(child_class_methods_using_vars)\", \"child_class_methods_using_dir = list(dir(ChildClass))\\npprint(sorted(child_class_methods_using_dir))\\n\\nassert 'parent_method' in child_class_methods_using_dir\\nassert 'read_only_attr' in child_class_methods_using_dir\\nassert 'class_method' in child_class_methods_using_dir\", 'predicate = inspect.isroutine\\nchild_class_methods_using_getmembers = list(get_members_of_function_or_method(ChildClass, predicate=predicate))\\n\\npprint(sorted(child_class_methods_using_getmembers))', 'predicate = inspect.isroutine\\nGPT2LMHeadModel_methods_using_getmembers = list(get_members_of_function_or_method(GPT2LMHeadModel, predicate=predicate))\\n\\npprint(sorted(GPT2LMHeadModel_methods_using_getmembers))', 'inspect.getmro(GPT2LMHeadModel)', 'func_sig: Signature = inspect.signature(func)\\npprint(func_sig.parameters)\\npprint(func_sig.return_annotation)', 'ChildClass.__bases__, GPT2LMHeadModel.__bases__[0].__bases__[0].__bases__', 'def get_base_classes(cls: Type[Any], include_self: bool = False) -> Set[Type[Any]]:\\n    \"\"\"\\n    Get the base classes of a class and all its base classes.\\n    \"\"\"\\n    return set(cls.__mro__[0:-1] if include_self else cls.__mro__[1:-1])\\n\\npprint(get_base_classes(GPT2LMHeadModel, include_self=True))', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\n\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True) -> Dict[str, Any]:\\n    namespace: Dict[str, Any] = {}\\n    # Use the helper function to get all base classes if include_bases is True\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in classes_to_inspect:\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n                namespace[name] = (get_default(param), type_hints.get(name))\\n\\n    ConfigClass = type(f\"{cls.__name__}Config\", (object,), namespace)\\n    return namespace, ConfigClass\\n\\nnamespace, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(namespace)\\npprint(config)', 'get_type_hints(get_type_hints)', 'get_type_hints(func)', 'get_type_hints(Trainer)', 'get_type_hints(Trainer.__init__)', 'get_type_hints(func)', 'get_type_hints(no_type_hints)', 'def no_type_hints(a, b, c, d, e, **kwargs):\\n    return a, b, c, d, e, kwargs', 'get_type_hints(no_type_hints)', 'get_type_hints(no_type_hints), inspect.signature(no_type_hints)', \"for name, value in inspect.getmembers(func):\\n    print(f'{name}: {value}')\", 'for name, value in inspect.signature(func).parameters.items():\\n    print(name, value.annotation)', 'for name, value in inspect.signature(func).parameters.items():\\n    print(value.default)', 'for name, value in inspect.signature(func).parameters.items():\\n    print(value.default)\\n    print(value.default is inspect.Parameter.empty)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, field(default=default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    ConfigClass = make_dataclass(f\"{cls.__name__}Config\", fields)\\n    ConfigClass.__annotations__.update(annotations)\\n    return fields, ConfigClass\\n\\nfields, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)\\npprint(config)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, field(default=default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    ConfigClass = make_dataclass(f\"{cls.__name__}Config\", fields)\\n    ConfigClass.__annotations__.update(annotations)\\n    return fields, ConfigClass\\n\\nfields, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\n# pprint(fields)\\npprint(config)', 'type(config)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, field(default=default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    ConfigClass = make_dataclass(f\"{cls.__name__}Config\", fields)\\n    # ConfigClass.__annotations__.update(annotations)\\n    return fields, ConfigClass\\n\\nfields, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\n# pprint(fields)\\npprint(config)', 'type(config)', \"from dataclasses import dataclass\\n\\n@dataclass\\nclass MyClass:\\n    a: int\\n    b: str\\n\\n# Using make_dataclass\\nAnotherClass = make_dataclass('AnotherClass', [('c', int), ('d', str)])\\n\\n# Checking their types\\nprint(type(MyClass))  # <class 'type'>\\nprint(type(AnotherClass))  # <class 'type'>\", 'inspect.signature(config).parameters', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, field(default=default_value)))\\n\\n    fields = required_fields + optional_fields\\n    namespace = {name: annotation for name, annotation in annotations.items()}\\n\\n    if as_dataclass:\\n        ConfigClass = make_dataclass(f\"{cls.__name__}Config\", fields)\\n    else:\\n        # Create a normal class, manually setting __annotations__ and defaults\\n        ConfigClass = type(f\"{cls.__name__}Config\", (object,), namespace)\\n        for name, _, default in optional_fields:\\n            setattr(ConfigClass, name, default)\\n        for name, _ in required_fields:\\n            if name not in namespace:  # Avoid overwriting defaults\\n                setattr(ConfigClass, name, None)\\n\\n    ConfigClass.__annotations__ = annotations\\n    return namespace, fields, ConfigClass\\n\\nnamespace, fields, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(namespace)\\npprint(config)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, field(default=default_value)))\\n\\n    fields = required_fields + optional_fields\\n    namespace = {name: annotation for name, annotation in annotations.items()}\\n\\n    if as_dataclass:\\n        ConfigClass = make_dataclass(f\"{cls.__name__}Config\", fields)\\n    else:\\n        # Create a normal class, manually setting __annotations__ and defaults\\n        ConfigClass = type(f\"{cls.__name__}Config\", (object,), namespace)\\n        for name, _, default in optional_fields:\\n            setattr(ConfigClass, name, default)\\n        for name, _ in required_fields:\\n            if name not in namespace:  # Avoid overwriting defaults\\n                setattr(ConfigClass, name, None)\\n\\n    ConfigClass.__annotations__ = annotations\\n\\n    if as_dataclass:\\n        return namespace, fields, ConfigClass\\n\\n\\n    fields = [(name, annotation) for name, annotation, _ in fields]\\n    return namespace, fields, ConfigClass\\n\\n\\nnamespace, fields, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(namespace)\\npprint(config)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, field(default=default_value)))\\n\\n    fields = required_fields + optional_fields\\n    namespace = {name: annotation for name, annotation in annotations.items()}\\n\\n    if as_dataclass:\\n        ConfigClass = make_dataclass(f\"{cls.__name__}Config\", fields)\\n    else:\\n        # Create a normal class, manually setting __annotations__ and defaults\\n        ConfigClass = type(f\"{cls.__name__}Config\", (object,), namespace)\\n        for name, _, default in optional_fields:\\n            setattr(ConfigClass, name, default)\\n        for name, _ in required_fields:\\n            if name not in namespace:  # Avoid overwriting defaults\\n                setattr(ConfigClass, name, None)\\n\\n    ConfigClass.__annotations__ = annotations\\n\\n    if as_dataclass:\\n        return namespace, fields, ConfigClass\\n\\n    pprint(fields)\\n    fields = [(name, annotation) for name, annotation, _ in fields]\\n    return namespace, fields, ConfigClass\\n\\n\\nnamespace, fields, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(namespace)\\npprint(config)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, (type_hint, default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations\\n\\n\\nfields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)\\npprint(annotations)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, (type_hint, default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations\\n\\n\\nfields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, (type_hint, default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations\\n\\n\\nfields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\nprint(fields)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, (type_hint, default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations\\n\\n\\nfields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)', 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    pass\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        for name, type_hint, default in fields:\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(\\n    cls: Type[Any], include_bases: bool = True\\n) -> Tuple[List[Union[Tuple[str, Tuple[Any, Any]], Tuple[str, Any, ellipsis]]], Dict[str, Any]]: # noqa: F821\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, (type_hint, default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(\\n    cls: Type[Any], include_bases: bool = True\\n) -> Tuple[List[Union[Tuple[str, Tuple[Any, Any]], Tuple[str, Any, Any]]], Dict[str, Any]]: # noqa: F821\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, (type_hint, default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations', 'fields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)', 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        for name, type_hint, default in fields:\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        for name, (type_hint, default) in fields:\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        for name, field_tuple in fields:\\n            type_hint, default = field_tuple\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        print(fields)\\n        for name, field_tuple in fields:\\n            type_hint, default = field_tuple\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(\\n    cls: Type[Any], include_bases: bool = True\\n) -> Tuple[List[Union[Tuple[str, Tuple[Any, Any]], Tuple[str, Any, Any]]], Dict[str, Any]]: # noqa: F821\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, default_value))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations', 'fields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)', 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        print(fields)\\n        for name, field_tuple in fields:\\n            type_hint, default = field_tuple\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        print(fields)\\n        for name, type_hint, default in fields:\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', 'fields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)\\nfor field in fields:\\n    print(field)', 'fields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)\\nfor field in fields:\\n    print(field)\\n    assert len(field) == 3', 'from typing import Any, List, Tuple\\nimport typing\\n\\ndef type_hint_to_str(type_hint: Any) -> str:\\n    \"\"\"\\n    Convert a type hint into its string representation.\\n    \"\"\"\\n    if hasattr(type_hint, \\'__name__\\'):\\n        return type_hint.__name__\\n    elif hasattr(type_hint, \\'_name\\') and type_hint._name is not None:\\n        return str(type_hint._name)\\n    elif type(type_hint) == typing._GenericAlias:  # For Python 3.8+\\n        # Handles complex types, e.g., List[int], Union[str, int]\\n        origin = type_hint_to_str(type_hint.__origin__)\\n        args = \\', \\'.join(type_hint_to_str(arg) for arg in type_hint.__args__)\\n        return f\"{origin}[{args}]\"\\n    else:\\n        # Fallback for unhandled types\\n        return str(type_hint)\\n\\ndef create_config_class_str(fields: List[Tuple[str, Any, Any]]) -> str:\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        for name, type_hint, default in fields:\\n            type_hint_str = type_hint_to_str(type_hint)\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint_str}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint_str} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\n# Example usage:\\n# Replace \\'fields\\' with your actual fields data\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', 'from transformers import Trainer, GPT2LMHeadModel, TrainingArguments\\nfrom dataclasses import field, make_dataclass\\n\\nimport inspect\\nfrom inspect import Signature, Parameter\\nfrom typing import Any, Callable, Dict, Set, Optional, _GenericAlias, List, Type, Union, get_type_hints, overload, Tuple\\nfrom pydantic import BaseModel\\nfrom rich.pretty import pprint', 'inspect.getmro(GPT2LMHeadModel)', 'reversed(inspect.getmro(GPT2LMHeadModel))', 'list(reversed(inspect.getmro(GPT2LMHeadModel)))', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_field_annotations(func_or_method: Callable[..., Any]) -> Tuple[List[Tuple[str, Any, Any]], Dict[str, Any]]:\\n    if not inspect.isroutine(func_or_method):\\n        raise ValueError(\"Expected a function or method\")\\n\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    try:\\n        sig: Signature = inspect.signature(func_or_method)\\n        type_hints: Dict[str, Any] = get_type_hints(func_or_method)\\n    except ValueError:\\n        raise ValueError(\"Object does not support signature or type hints extraction.\") from None\\n\\n    for name, param in sig.parameters.items():\\n        if name == \"self\":\\n            continue\\n\\n        type_hint = type_hints.get(name, Any)\\n        annotations[name] = type_hint\\n        if param.default is param.empty:\\n            required_fields.append((name, type_hint, Ellipsis))\\n        else:\\n            default_value = param.default\\n            optional_fields.append((name, type_hint, default_value))\\n\\n    fields = required_fields + optional_fields\\n    return fields, annotations\\n\\n\\n# TODO: Tuple[str, Any, Any] should be Tuple[str, Any, ellipsis]\\ndef get_constructor_field_annotations(\\n    cls: Type[Any], include_bases: bool = True\\n) -> Tuple[List[Tuple[str, Any, Any]], Dict[str, Any]]:\\n    fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            class_fields, class_annotations = get_field_annotations(c.__init__)\\n            # Update fields and annotations with those from the current class,\\n            # avoiding duplicates.\\n            for field in class_fields:\\n                if field[0] not in annotations:\\n                    fields.append(field)  # noqa: PERF401\\n            annotations.update(class_annotations)\\n\\n    return fields, annotations', 'fields, annotations = get_constructor_field_annotations(TrainingArguments, include_bases=False)\\npprint(fields)\\nfor field in fields:\\n    print(field)\\n    assert len(field) == 3', 'fields, annotations = get_constructor_field_annotations(TrainingArguments, include_bases=False)\\npprint(fields)\\nfor field in fields:\\n    assert len(field) == 3', 'fields, annotations = get_constructor_field_annotations(TrainingArguments, include_bases=False)\\npprint(fields)\\nfor field in fields:\\n    assert len(field) == 3\\n    \\nassert get_field_annotations(TrainingArguments.__init__) == (fields, annotations)', 'def type_hint_to_str(type_hint: Any) -> str:\\n    \"\"\"\\n    Convert a type hint into its string representation.\\n    \"\"\"\\n    if hasattr(type_hint, \\'__name__\\'):\\n        return type_hint.__name__\\n    elif hasattr(type_hint, \\'_name\\') and type_hint._name is not None:\\n        return str(type_hint._name)\\n    elif type(type_hint) == _GenericAlias:  # For Python 3.8+\\n        # Handles complex types, e.g., List[int], Union[str, int]\\n        origin = type_hint_to_str(type_hint.__origin__)\\n        args = \\', \\'.join(type_hint_to_str(arg) for arg in type_hint.__args__)\\n        return f\"{origin}[{args}]\"\\n    else:\\n        # Fallback for unhandled types\\n        return str(type_hint)\\n\\ndef create_config_class_str(fields: List[Tuple[str, Any, Any]]) -> str:\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        for name, type_hint, default in fields:\\n            type_hint_str = type_hint_to_str(type_hint)\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint_str}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint_str} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\n# Example usage:\\n# Replace \\'fields\\' with your actual fields data\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', \"get_ipython().run_line_magic('pip', 'install -U -q langchain')\", 'from transformers import Trainer, GPT2LMHeadModel, TrainingArguments\\nfrom dataclasses import field, make_dataclass\\n\\nimport inspect\\nfrom inspect import Signature, Parameter\\nfrom typing import Any, Callable, Dict, Set, Optional, _GenericAlias, List, Type, Union, get_type_hints, overload, Tuple\\nfrom pydantic import BaseModel\\nfrom rich.pretty import pprint', 'class ParentClass:\\n    \"\"\"This is the parent class.\"\"\"\\n\\n    parent_class_attr = \\'a parent class attribute\\'\\n\\n    def __init__(self, parent_instance_attr: str) -> None:\\n        self.parent_instance_attr = parent_instance_attr\\n\\n    def parent_method(self) -> str:\\n        \"\"\"This is a method in the parent class.\"\"\"\\n        return \"Parent method called\"\\n\\nclass ChildClass(ParentClass):\\n    \"\"\"This is a subclass of ParentClass.\"\"\"\\n\\n    # Class attribute\\n    class_attr = \\'a class attribute\\'\\n\\n    # Private and protected attributes\\n    _protected_attr = \\'a protected attribute\\'\\n    __private_attr = \\'a private attribute\\'\\n\\n    def __init__(self, instance_attr: str, parent_instance_attr: str) -> None:\\n        \"\"\"Initialize the instance.\"\"\"\\n        super().__init__(parent_instance_attr)\\n        # Instance attribute\\n        self.instance_attr = instance_attr\\n\\n    @property\\n    def read_only_attr(self) -> str:\\n        \"\"\"This is a read-only attribute.\"\"\"\\n        return \\'You can read me, but you cannot change me.\\'\\n\\n    def instance_method(self, arg: str) -> str:\\n        \"\"\"This is an instance method.\"\"\"\\n        return f\\'Instance method called with argument: {arg}\\'\\n\\n    @classmethod\\n    def class_method(cls, arg: str) -> str:\\n        \"\"\"This is a class method.\"\"\"\\n        return f\\'Class method called with argument: {arg}\\'\\n\\n    @staticmethod\\n    def static_method(arg: str) -> str:\\n        \"\"\"This is a static method.\"\"\"\\n        return f\\'Static method called with argument: {arg}\\'\\n\\n    def __str__(self) -> str:\\n        \"\"\"Return a string representation of the instance.\"\"\"\\n        return f\\'MyClass(instance_attr={self.instance_attr})\\'', \"instance_child = ChildClass(instance_attr='an instance attribute', parent_instance_attr='a parent instance attribute')\\nclass_child = ChildClass\\n\\ninstance_parent = ParentClass(parent_instance_attr='a parent instance attribute')\\nclass_parent = ParentClass\", 'def func(a: int, b: str, c: List[int], d: Tuple[str, str], e: Union[int, str], **kwargs: Any) -> str:\\n    return a, b, c, d, e, kwargs', \"@overload\\ndef get_members_of_function_or_method(\\n    func_or_class: Type[object], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    ...\\n\\n\\n@overload\\ndef get_members_of_function_or_method(\\n    func_or_class: Callable[..., Any], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    ...\\n\\n\\ndef get_members_of_function_or_method(\\n    func_or_class: Union[Type[object], Callable[..., Any]], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    return inspect.getmembers(func_or_class, predicate)\\n\\ndef loop_through_members(members: List[Tuple[str, Any]], filter: Optional[str] = None) -> None:\\n    if filter is not None:\\n        members = [member for member in members if filter in member[0]]\\n    for member in members:\\n        name, value = member\\n        print(f'{name}: {value}')\", 'func_all_members = get_members_of_function_or_method(func, predicate=None)\\nloop_through_members(func_all_members)'], '_oh': {18: (<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel'>, <class 'transformers.modeling_utils.PreTrainedModel'>, <class 'torch.nn.modules.module.Module'>, <class 'transformers.modeling_utils.ModuleUtilsMixin'>, <class 'transformers.generation.utils.GenerationMixin'>, <class 'transformers.utils.hub.PushToHubMixin'>, <class 'transformers.integrations.peft.PeftAdapterMixin'>, <class 'object'>), 20: ((<class '__main__.ParentClass'>,), (<class 'torch.nn.modules.module.Module'>, <class 'transformers.modeling_utils.ModuleUtilsMixin'>, <class 'transformers.generation.utils.GenerationMixin'>, <class 'transformers.utils.hub.PushToHubMixin'>, <class 'transformers.integrations.peft.PeftAdapterMixin'>)), 23: {}, 24: {'a': <class 'int'>, 'b': <class 'str'>, 'c': typing.List[int], 'd': typing.Tuple[str, str], 'e': typing.Union[int, str], 'kwargs': typing.Any, 'return': <class 'str'>}, 25: {}, 26: {'model': typing.Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module, NoneType], 'args': typing.Optional[transformers.training_args.TrainingArguments], 'data_collator': typing.Optional[DataCollator], 'train_dataset': typing.Optional[torch.utils.data.dataset.Dataset], 'eval_dataset': typing.Union[torch.utils.data.dataset.Dataset, typing.Dict[str, torch.utils.data.dataset.Dataset], NoneType], 'tokenizer': typing.Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase], 'model_init': typing.Optional[typing.Callable[[], transformers.modeling_utils.PreTrainedModel]], 'compute_metrics': typing.Optional[typing.Callable[[transformers.trainer_utils.EvalPrediction], typing.Dict]], 'callbacks': typing.Optional[typing.List[transformers.trainer_callback.TrainerCallback]], 'optimizers': typing.Tuple[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR], 'preprocess_logits_for_metrics': typing.Optional[typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor]]}, 27: {'a': <class 'int'>, 'b': <class 'str'>, 'c': typing.List[int], 'd': typing.Tuple[str, str], 'e': typing.Union[int, str], 'kwargs': typing.Any, 'return': <class 'str'>}, 30: {}, 31: ({}, <Signature (a, b, c, d, e, **kwargs)>), 38: <class 'type'>, 40: <class 'type'>, 42: mappingproxy(OrderedDict([('output_dir', <Parameter \"output_dir: str\">), ('overwrite_output_dir', <Parameter \"overwrite_output_dir: bool = False\">), ('do_train', <Parameter \"do_train: bool = False\">), ('do_eval', <Parameter \"do_eval: bool = False\">), ('do_predict', <Parameter \"do_predict: bool = False\">), ('evaluation_strategy', <Parameter \"evaluation_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no'\">), ('prediction_loss_only', <Parameter \"prediction_loss_only: bool = False\">), ('per_device_train_batch_size', <Parameter \"per_device_train_batch_size: int = 8\">), ('per_device_eval_batch_size', <Parameter \"per_device_eval_batch_size: int = 8\">), ('per_gpu_train_batch_size', <Parameter \"per_gpu_train_batch_size: Optional[int] = None\">), ('per_gpu_eval_batch_size', <Parameter \"per_gpu_eval_batch_size: Optional[int] = None\">), ('gradient_accumulation_steps', <Parameter \"gradient_accumulation_steps: int = 1\">), ('eval_accumulation_steps', <Parameter \"eval_accumulation_steps: Optional[int] = None\">), ('eval_delay', <Parameter \"eval_delay: Optional[float] = 0\">), ('learning_rate', <Parameter \"learning_rate: float = 5e-05\">), ('weight_decay', <Parameter \"weight_decay: float = 0.0\">), ('adam_beta1', <Parameter \"adam_beta1: float = 0.9\">), ('adam_beta2', <Parameter \"adam_beta2: float = 0.999\">), ('adam_epsilon', <Parameter \"adam_epsilon: float = 1e-08\">), ('max_grad_norm', <Parameter \"max_grad_norm: float = 1.0\">), ('num_train_epochs', <Parameter \"num_train_epochs: float = 3.0\">), ('max_steps', <Parameter \"max_steps: int = -1\">), ('lr_scheduler_type', <Parameter \"lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear'\">), ('lr_scheduler_kwargs', <Parameter \"lr_scheduler_kwargs: Optional[Dict] = <factory>\">), ('warmup_ratio', <Parameter \"warmup_ratio: float = 0.0\">), ('warmup_steps', <Parameter \"warmup_steps: int = 0\">), ('log_level', <Parameter \"log_level: Optional[str] = 'passive'\">), ('log_level_replica', <Parameter \"log_level_replica: Optional[str] = 'warning'\">), ('log_on_each_node', <Parameter \"log_on_each_node: bool = True\">), ('logging_dir', <Parameter \"logging_dir: Optional[str] = None\">), ('logging_strategy', <Parameter \"logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'\">), ('logging_first_step', <Parameter \"logging_first_step: bool = False\">), ('logging_steps', <Parameter \"logging_steps: float = 500\">), ('logging_nan_inf_filter', <Parameter \"logging_nan_inf_filter: bool = True\">), ('save_strategy', <Parameter \"save_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'\">), ('save_steps', <Parameter \"save_steps: float = 500\">), ('save_total_limit', <Parameter \"save_total_limit: Optional[int] = None\">), ('save_safetensors', <Parameter \"save_safetensors: Optional[bool] = True\">), ('save_on_each_node', <Parameter \"save_on_each_node: bool = False\">), ('save_only_model', <Parameter \"save_only_model: bool = False\">), ('no_cuda', <Parameter \"no_cuda: bool = False\">), ('use_cpu', <Parameter \"use_cpu: bool = False\">), ('use_mps_device', <Parameter \"use_mps_device: bool = False\">), ('seed', <Parameter \"seed: int = 42\">), ('data_seed', <Parameter \"data_seed: Optional[int] = None\">), ('jit_mode_eval', <Parameter \"jit_mode_eval: bool = False\">), ('use_ipex', <Parameter \"use_ipex: bool = False\">), ('bf16', <Parameter \"bf16: bool = False\">), ('fp16', <Parameter \"fp16: bool = False\">), ('fp16_opt_level', <Parameter \"fp16_opt_level: str = 'O1'\">), ('half_precision_backend', <Parameter \"half_precision_backend: str = 'auto'\">), ('bf16_full_eval', <Parameter \"bf16_full_eval: bool = False\">), ('fp16_full_eval', <Parameter \"fp16_full_eval: bool = False\">), ('tf32', <Parameter \"tf32: Optional[bool] = None\">), ('local_rank', <Parameter \"local_rank: int = -1\">), ('ddp_backend', <Parameter \"ddp_backend: Optional[str] = None\">), ('tpu_num_cores', <Parameter \"tpu_num_cores: Optional[int] = None\">), ('tpu_metrics_debug', <Parameter \"tpu_metrics_debug: bool = False\">), ('debug', <Parameter \"debug: Union[str, List[transformers.debug_utils.DebugOption]] = ''\">), ('dataloader_drop_last', <Parameter \"dataloader_drop_last: bool = False\">), ('eval_steps', <Parameter \"eval_steps: Optional[float] = None\">), ('dataloader_num_workers', <Parameter \"dataloader_num_workers: int = 0\">), ('dataloader_prefetch_factor', <Parameter \"dataloader_prefetch_factor: Optional[int] = None\">), ('past_index', <Parameter \"past_index: int = -1\">), ('run_name', <Parameter \"run_name: Optional[str] = None\">), ('disable_tqdm', <Parameter \"disable_tqdm: Optional[bool] = None\">), ('remove_unused_columns', <Parameter \"remove_unused_columns: Optional[bool] = True\">), ('label_names', <Parameter \"label_names: Optional[List[str]] = None\">), ('load_best_model_at_end', <Parameter \"load_best_model_at_end: Optional[bool] = False\">), ('metric_for_best_model', <Parameter \"metric_for_best_model: Optional[str] = None\">), ('greater_is_better', <Parameter \"greater_is_better: Optional[bool] = None\">), ('ignore_data_skip', <Parameter \"ignore_data_skip: bool = False\">), ('fsdp', <Parameter \"fsdp: Union[List[transformers.trainer_utils.FSDPOption], str, NoneType] = ''\">), ('fsdp_min_num_params', <Parameter \"fsdp_min_num_params: int = 0\">), ('fsdp_config', <Parameter \"fsdp_config: Union[dict, str, NoneType] = None\">), ('fsdp_transformer_layer_cls_to_wrap', <Parameter \"fsdp_transformer_layer_cls_to_wrap: Optional[str] = None\">), ('accelerator_config', <Parameter \"accelerator_config: Optional[str] = None\">), ('deepspeed', <Parameter \"deepspeed: Optional[str] = None\">), ('label_smoothing_factor', <Parameter \"label_smoothing_factor: float = 0.0\">), ('optim', <Parameter \"optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch'\">), ('optim_args', <Parameter \"optim_args: Optional[str] = None\">), ('adafactor', <Parameter \"adafactor: bool = False\">), ('group_by_length', <Parameter \"group_by_length: bool = False\">), ('length_column_name', <Parameter \"length_column_name: Optional[str] = 'length'\">), ('report_to', <Parameter \"report_to: Optional[List[str]] = None\">), ('ddp_find_unused_parameters', <Parameter \"ddp_find_unused_parameters: Optional[bool] = None\">), ('ddp_bucket_cap_mb', <Parameter \"ddp_bucket_cap_mb: Optional[int] = None\">), ('ddp_broadcast_buffers', <Parameter \"ddp_broadcast_buffers: Optional[bool] = None\">), ('dataloader_pin_memory', <Parameter \"dataloader_pin_memory: bool = True\">), ('dataloader_persistent_workers', <Parameter \"dataloader_persistent_workers: bool = False\">), ('skip_memory_metrics', <Parameter \"skip_memory_metrics: bool = True\">), ('use_legacy_prediction_loop', <Parameter \"use_legacy_prediction_loop: bool = False\">), ('push_to_hub', <Parameter \"push_to_hub: bool = False\">), ('resume_from_checkpoint', <Parameter \"resume_from_checkpoint: Optional[str] = None\">), ('hub_model_id', <Parameter \"hub_model_id: Optional[str] = None\">), ('hub_strategy', <Parameter \"hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save'\">), ('hub_token', <Parameter \"hub_token: Optional[str] = None\">), ('hub_private_repo', <Parameter \"hub_private_repo: bool = False\">), ('hub_always_push', <Parameter \"hub_always_push: bool = False\">), ('gradient_checkpointing', <Parameter \"gradient_checkpointing: bool = False\">), ('gradient_checkpointing_kwargs', <Parameter \"gradient_checkpointing_kwargs: Optional[dict] = None\">), ('include_inputs_for_metrics', <Parameter \"include_inputs_for_metrics: bool = False\">), ('fp16_backend', <Parameter \"fp16_backend: str = 'auto'\">), ('push_to_hub_model_id', <Parameter \"push_to_hub_model_id: Optional[str] = None\">), ('push_to_hub_organization', <Parameter \"push_to_hub_organization: Optional[str] = None\">), ('push_to_hub_token', <Parameter \"push_to_hub_token: Optional[str] = None\">), ('mp_parameters', <Parameter \"mp_parameters: str = ''\">), ('auto_find_batch_size', <Parameter \"auto_find_batch_size: bool = False\">), ('full_determinism', <Parameter \"full_determinism: bool = False\">), ('torchdynamo', <Parameter \"torchdynamo: Optional[str] = None\">), ('ray_scope', <Parameter \"ray_scope: Optional[str] = 'last'\">), ('ddp_timeout', <Parameter \"ddp_timeout: Optional[int] = 1800\">), ('torch_compile', <Parameter \"torch_compile: bool = False\">), ('torch_compile_backend', <Parameter \"torch_compile_backend: Optional[str] = None\">), ('torch_compile_mode', <Parameter \"torch_compile_mode: Optional[str] = None\">), ('dispatch_batches', <Parameter \"dispatch_batches: Optional[bool] = None\">), ('split_batches', <Parameter \"split_batches: Optional[bool] = None\">), ('include_tokens_per_second', <Parameter \"include_tokens_per_second: Optional[bool] = False\">), ('include_num_input_tokens_seen', <Parameter \"include_num_input_tokens_seen: Optional[bool] = False\">), ('neftune_noise_alpha', <Parameter \"neftune_noise_alpha: Optional[float] = None\">)])), 66: (<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel'>, <class 'transformers.modeling_utils.PreTrainedModel'>, <class 'torch.nn.modules.module.Module'>, <class 'transformers.modeling_utils.ModuleUtilsMixin'>, <class 'transformers.generation.utils.GenerationMixin'>, <class 'transformers.utils.hub.PushToHubMixin'>, <class 'transformers.integrations.peft.PeftAdapterMixin'>, <class 'object'>), 67: <reversed object at 0x2a95639d0>, 68: [<class 'object'>, <class 'transformers.integrations.peft.PeftAdapterMixin'>, <class 'transformers.utils.hub.PushToHubMixin'>, <class 'transformers.generation.utils.GenerationMixin'>, <class 'transformers.modeling_utils.ModuleUtilsMixin'>, <class 'torch.nn.modules.module.Module'>, <class 'transformers.modeling_utils.PreTrainedModel'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>]}, '_dh': [PosixPath('/Users/gaohn/gaohn/omniverse/omniverse/playbook'), PosixPath('/Users/gaohn/gaohn/omniverse/omniverse/playbook')], 'In': ['', \"get_ipython().run_line_magic('pip', 'install -U -q langchain')\", 'from transformers import Trainer, GPT2LMHeadModel, TrainingArguments\\nfrom dataclasses import field, make_dataclass\\n\\nimport inspect\\nfrom inspect import Signature, Parameter\\nfrom typing import Any, Callable, Dict, Set, Optional, List, Type, Union, get_type_hints, overload, Tuple\\nfrom pydantic import BaseModel\\nfrom rich.pretty import pprint', 'class ParentClass:\\n    \"\"\"This is the parent class.\"\"\"\\n\\n    parent_class_attr = \\'a parent class attribute\\'\\n\\n    def __init__(self, parent_instance_attr: str) -> None:\\n        self.parent_instance_attr = parent_instance_attr\\n\\n    def parent_method(self) -> str:\\n        \"\"\"This is a method in the parent class.\"\"\"\\n        return \"Parent method called\"\\n\\nclass ChildClass(ParentClass):\\n    \"\"\"This is a subclass of ParentClass.\"\"\"\\n\\n    # Class attribute\\n    class_attr = \\'a class attribute\\'\\n\\n    # Private and protected attributes\\n    _protected_attr = \\'a protected attribute\\'\\n    __private_attr = \\'a private attribute\\'\\n\\n    def __init__(self, instance_attr: str, parent_instance_attr: str) -> None:\\n        \"\"\"Initialize the instance.\"\"\"\\n        super().__init__(parent_instance_attr)\\n        # Instance attribute\\n        self.instance_attr = instance_attr\\n\\n    @property\\n    def read_only_attr(self) -> str:\\n        \"\"\"This is a read-only attribute.\"\"\"\\n        return \\'You can read me, but you cannot change me.\\'\\n\\n    def instance_method(self, arg: str) -> str:\\n        \"\"\"This is an instance method.\"\"\"\\n        return f\\'Instance method called with argument: {arg}\\'\\n\\n    @classmethod\\n    def class_method(cls, arg: str) -> str:\\n        \"\"\"This is a class method.\"\"\"\\n        return f\\'Class method called with argument: {arg}\\'\\n\\n    @staticmethod\\n    def static_method(arg: str) -> str:\\n        \"\"\"This is a static method.\"\"\"\\n        return f\\'Static method called with argument: {arg}\\'\\n\\n    def __str__(self) -> str:\\n        \"\"\"Return a string representation of the instance.\"\"\"\\n        return f\\'MyClass(instance_attr={self.instance_attr})\\'', \"instance_child = ChildClass(instance_attr='an instance attribute', parent_instance_attr='a parent instance attribute')\\nclass_child = ChildClass\\n\\ninstance_parent = ParentClass(parent_instance_attr='a parent instance attribute')\\nclass_parent = ParentClass\", 'def func(a: int, b: str, c: List[int], d: Tuple[str, str], e: Union[int, str], **kwargs: Any) -> str:\\n    return a, b, c, d, e, kwargs', \"@overload\\ndef get_members_of_function_or_method(\\n    func_or_class: Type[object], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    ...\\n\\n\\n@overload\\ndef get_members_of_function_or_method(\\n    func_or_class: Callable[..., Any], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    ...\\n\\n\\ndef get_members_of_function_or_method(\\n    func_or_class: Union[Type[object], Callable[..., Any]], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    return inspect.getmembers(func_or_class, predicate)\\n\\ndef loop_through_members(members: List[Tuple[str, Any]], filter: Optional[str] = None) -> None:\\n    if filter is not None:\\n        members = [member for member in members if filter in member[0]]\\n    for member in members:\\n        name, value = member\\n        print(f'{name}: {value}')\", 'func_all_members = get_members_of_function_or_method(func, predicate=None)\\nloop_through_members(func_all_members)', \"loop_through_members(func_all_members, filter='__annotations__')\", 'class_child_all_members = get_members_of_function_or_method(class_child, predicate=None)\\nloop_through_members(class_child_all_members)', 'instance_child_all_members = get_members_of_function_or_method(instance_child, predicate=None)\\nloop_through_members(instance_child_all_members)', 'trainer_all_members = get_members_of_function_or_method(Trainer, predicate=None)\\nloop_through_members(trainer_all_members)', \"child_class_methods_using_dict = list(ChildClass.__dict__.keys())\\npprint(sorted(child_class_methods_using_dict))\\n\\nassert 'parent_method' not in child_class_methods_using_dict\\nassert 'read_only_attr' in child_class_methods_using_dict\\nassert 'class_method' in child_class_methods_using_dict\", 'pprint(instance_child.__class__.__dict__.keys() == ChildClass.__dict__.keys())', \"child_class_methods_using_vars = list(vars(ChildClass).keys())\\npprint(sorted(child_class_methods_using_vars))\\n\\nassert 'parent_method' not in child_class_methods_using_vars\\nassert 'read_only_attr' in child_class_methods_using_vars\\nassert 'class_method' in child_class_methods_using_vars\\n\\nassert set(child_class_methods_using_dict) == set(child_class_methods_using_vars)\", \"child_class_methods_using_dir = list(dir(ChildClass))\\npprint(sorted(child_class_methods_using_dir))\\n\\nassert 'parent_method' in child_class_methods_using_dir\\nassert 'read_only_attr' in child_class_methods_using_dir\\nassert 'class_method' in child_class_methods_using_dir\", 'predicate = inspect.isroutine\\nchild_class_methods_using_getmembers = list(get_members_of_function_or_method(ChildClass, predicate=predicate))\\n\\npprint(sorted(child_class_methods_using_getmembers))', 'predicate = inspect.isroutine\\nGPT2LMHeadModel_methods_using_getmembers = list(get_members_of_function_or_method(GPT2LMHeadModel, predicate=predicate))\\n\\npprint(sorted(GPT2LMHeadModel_methods_using_getmembers))', 'inspect.getmro(GPT2LMHeadModel)', 'func_sig: Signature = inspect.signature(func)\\npprint(func_sig.parameters)\\npprint(func_sig.return_annotation)', 'ChildClass.__bases__, GPT2LMHeadModel.__bases__[0].__bases__[0].__bases__', 'def get_base_classes(cls: Type[Any], include_self: bool = False) -> Set[Type[Any]]:\\n    \"\"\"\\n    Get the base classes of a class and all its base classes.\\n    \"\"\"\\n    return set(cls.__mro__[0:-1] if include_self else cls.__mro__[1:-1])\\n\\npprint(get_base_classes(GPT2LMHeadModel, include_self=True))', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\n\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True) -> Dict[str, Any]:\\n    namespace: Dict[str, Any] = {}\\n    # Use the helper function to get all base classes if include_bases is True\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in classes_to_inspect:\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n                namespace[name] = (get_default(param), type_hints.get(name))\\n\\n    ConfigClass = type(f\"{cls.__name__}Config\", (object,), namespace)\\n    return namespace, ConfigClass\\n\\nnamespace, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(namespace)\\npprint(config)', 'get_type_hints(get_type_hints)', 'get_type_hints(func)', 'get_type_hints(Trainer)', 'get_type_hints(Trainer.__init__)', 'get_type_hints(func)', 'get_type_hints(no_type_hints)', 'def no_type_hints(a, b, c, d, e, **kwargs):\\n    return a, b, c, d, e, kwargs', 'get_type_hints(no_type_hints)', 'get_type_hints(no_type_hints), inspect.signature(no_type_hints)', \"for name, value in inspect.getmembers(func):\\n    print(f'{name}: {value}')\", 'for name, value in inspect.signature(func).parameters.items():\\n    print(name, value.annotation)', 'for name, value in inspect.signature(func).parameters.items():\\n    print(value.default)', 'for name, value in inspect.signature(func).parameters.items():\\n    print(value.default)\\n    print(value.default is inspect.Parameter.empty)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, field(default=default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    ConfigClass = make_dataclass(f\"{cls.__name__}Config\", fields)\\n    ConfigClass.__annotations__.update(annotations)\\n    return fields, ConfigClass\\n\\nfields, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)\\npprint(config)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, field(default=default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    ConfigClass = make_dataclass(f\"{cls.__name__}Config\", fields)\\n    ConfigClass.__annotations__.update(annotations)\\n    return fields, ConfigClass\\n\\nfields, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\n# pprint(fields)\\npprint(config)', 'type(config)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, field(default=default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    ConfigClass = make_dataclass(f\"{cls.__name__}Config\", fields)\\n    # ConfigClass.__annotations__.update(annotations)\\n    return fields, ConfigClass\\n\\nfields, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\n# pprint(fields)\\npprint(config)', 'type(config)', \"from dataclasses import dataclass\\n\\n@dataclass\\nclass MyClass:\\n    a: int\\n    b: str\\n\\n# Using make_dataclass\\nAnotherClass = make_dataclass('AnotherClass', [('c', int), ('d', str)])\\n\\n# Checking their types\\nprint(type(MyClass))  # <class 'type'>\\nprint(type(AnotherClass))  # <class 'type'>\", 'inspect.signature(config).parameters', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, field(default=default_value)))\\n\\n    fields = required_fields + optional_fields\\n    namespace = {name: annotation for name, annotation in annotations.items()}\\n\\n    if as_dataclass:\\n        ConfigClass = make_dataclass(f\"{cls.__name__}Config\", fields)\\n    else:\\n        # Create a normal class, manually setting __annotations__ and defaults\\n        ConfigClass = type(f\"{cls.__name__}Config\", (object,), namespace)\\n        for name, _, default in optional_fields:\\n            setattr(ConfigClass, name, default)\\n        for name, _ in required_fields:\\n            if name not in namespace:  # Avoid overwriting defaults\\n                setattr(ConfigClass, name, None)\\n\\n    ConfigClass.__annotations__ = annotations\\n    return namespace, fields, ConfigClass\\n\\nnamespace, fields, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(namespace)\\npprint(config)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, field(default=default_value)))\\n\\n    fields = required_fields + optional_fields\\n    namespace = {name: annotation for name, annotation in annotations.items()}\\n\\n    if as_dataclass:\\n        ConfigClass = make_dataclass(f\"{cls.__name__}Config\", fields)\\n    else:\\n        # Create a normal class, manually setting __annotations__ and defaults\\n        ConfigClass = type(f\"{cls.__name__}Config\", (object,), namespace)\\n        for name, _, default in optional_fields:\\n            setattr(ConfigClass, name, default)\\n        for name, _ in required_fields:\\n            if name not in namespace:  # Avoid overwriting defaults\\n                setattr(ConfigClass, name, None)\\n\\n    ConfigClass.__annotations__ = annotations\\n\\n    if as_dataclass:\\n        return namespace, fields, ConfigClass\\n\\n\\n    fields = [(name, annotation) for name, annotation, _ in fields]\\n    return namespace, fields, ConfigClass\\n\\n\\nnamespace, fields, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(namespace)\\npprint(config)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, field(default=default_value)))\\n\\n    fields = required_fields + optional_fields\\n    namespace = {name: annotation for name, annotation in annotations.items()}\\n\\n    if as_dataclass:\\n        ConfigClass = make_dataclass(f\"{cls.__name__}Config\", fields)\\n    else:\\n        # Create a normal class, manually setting __annotations__ and defaults\\n        ConfigClass = type(f\"{cls.__name__}Config\", (object,), namespace)\\n        for name, _, default in optional_fields:\\n            setattr(ConfigClass, name, default)\\n        for name, _ in required_fields:\\n            if name not in namespace:  # Avoid overwriting defaults\\n                setattr(ConfigClass, name, None)\\n\\n    ConfigClass.__annotations__ = annotations\\n\\n    if as_dataclass:\\n        return namespace, fields, ConfigClass\\n\\n    pprint(fields)\\n    fields = [(name, annotation) for name, annotation, _ in fields]\\n    return namespace, fields, ConfigClass\\n\\n\\nnamespace, fields, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(namespace)\\npprint(config)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, (type_hint, default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations\\n\\n\\nfields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)\\npprint(annotations)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, (type_hint, default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations\\n\\n\\nfields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, (type_hint, default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations\\n\\n\\nfields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\nprint(fields)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, (type_hint, default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations\\n\\n\\nfields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)', 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    pass\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        for name, type_hint, default in fields:\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(\\n    cls: Type[Any], include_bases: bool = True\\n) -> Tuple[List[Union[Tuple[str, Tuple[Any, Any]], Tuple[str, Any, ellipsis]]], Dict[str, Any]]: # noqa: F821\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, (type_hint, default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(\\n    cls: Type[Any], include_bases: bool = True\\n) -> Tuple[List[Union[Tuple[str, Tuple[Any, Any]], Tuple[str, Any, Any]]], Dict[str, Any]]: # noqa: F821\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, (type_hint, default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations', 'fields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)', 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        for name, type_hint, default in fields:\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        for name, (type_hint, default) in fields:\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        for name, field_tuple in fields:\\n            type_hint, default = field_tuple\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        print(fields)\\n        for name, field_tuple in fields:\\n            type_hint, default = field_tuple\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(\\n    cls: Type[Any], include_bases: bool = True\\n) -> Tuple[List[Union[Tuple[str, Tuple[Any, Any]], Tuple[str, Any, Any]]], Dict[str, Any]]: # noqa: F821\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, default_value))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations', 'fields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)', 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        print(fields)\\n        for name, field_tuple in fields:\\n            type_hint, default = field_tuple\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        print(fields)\\n        for name, type_hint, default in fields:\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', 'fields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)\\nfor field in fields:\\n    print(field)', 'fields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)\\nfor field in fields:\\n    print(field)\\n    assert len(field) == 3', 'from typing import Any, List, Tuple\\nimport typing\\n\\ndef type_hint_to_str(type_hint: Any) -> str:\\n    \"\"\"\\n    Convert a type hint into its string representation.\\n    \"\"\"\\n    if hasattr(type_hint, \\'__name__\\'):\\n        return type_hint.__name__\\n    elif hasattr(type_hint, \\'_name\\') and type_hint._name is not None:\\n        return str(type_hint._name)\\n    elif type(type_hint) == typing._GenericAlias:  # For Python 3.8+\\n        # Handles complex types, e.g., List[int], Union[str, int]\\n        origin = type_hint_to_str(type_hint.__origin__)\\n        args = \\', \\'.join(type_hint_to_str(arg) for arg in type_hint.__args__)\\n        return f\"{origin}[{args}]\"\\n    else:\\n        # Fallback for unhandled types\\n        return str(type_hint)\\n\\ndef create_config_class_str(fields: List[Tuple[str, Any, Any]]) -> str:\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        for name, type_hint, default in fields:\\n            type_hint_str = type_hint_to_str(type_hint)\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint_str}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint_str} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\n# Example usage:\\n# Replace \\'fields\\' with your actual fields data\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', 'from transformers import Trainer, GPT2LMHeadModel, TrainingArguments\\nfrom dataclasses import field, make_dataclass\\n\\nimport inspect\\nfrom inspect import Signature, Parameter\\nfrom typing import Any, Callable, Dict, Set, Optional, _GenericAlias, List, Type, Union, get_type_hints, overload, Tuple\\nfrom pydantic import BaseModel\\nfrom rich.pretty import pprint', 'inspect.getmro(GPT2LMHeadModel)', 'reversed(inspect.getmro(GPT2LMHeadModel))', 'list(reversed(inspect.getmro(GPT2LMHeadModel)))', 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_field_annotations(func_or_method: Callable[..., Any]) -> Tuple[List[Tuple[str, Any, Any]], Dict[str, Any]]:\\n    if not inspect.isroutine(func_or_method):\\n        raise ValueError(\"Expected a function or method\")\\n\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    try:\\n        sig: Signature = inspect.signature(func_or_method)\\n        type_hints: Dict[str, Any] = get_type_hints(func_or_method)\\n    except ValueError:\\n        raise ValueError(\"Object does not support signature or type hints extraction.\") from None\\n\\n    for name, param in sig.parameters.items():\\n        if name == \"self\":\\n            continue\\n\\n        type_hint = type_hints.get(name, Any)\\n        annotations[name] = type_hint\\n        if param.default is param.empty:\\n            required_fields.append((name, type_hint, Ellipsis))\\n        else:\\n            default_value = param.default\\n            optional_fields.append((name, type_hint, default_value))\\n\\n    fields = required_fields + optional_fields\\n    return fields, annotations\\n\\n\\n# TODO: Tuple[str, Any, Any] should be Tuple[str, Any, ellipsis]\\ndef get_constructor_field_annotations(\\n    cls: Type[Any], include_bases: bool = True\\n) -> Tuple[List[Tuple[str, Any, Any]], Dict[str, Any]]:\\n    fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            class_fields, class_annotations = get_field_annotations(c.__init__)\\n            # Update fields and annotations with those from the current class,\\n            # avoiding duplicates.\\n            for field in class_fields:\\n                if field[0] not in annotations:\\n                    fields.append(field)  # noqa: PERF401\\n            annotations.update(class_annotations)\\n\\n    return fields, annotations', 'fields, annotations = get_constructor_field_annotations(TrainingArguments, include_bases=False)\\npprint(fields)\\nfor field in fields:\\n    print(field)\\n    assert len(field) == 3', 'fields, annotations = get_constructor_field_annotations(TrainingArguments, include_bases=False)\\npprint(fields)\\nfor field in fields:\\n    assert len(field) == 3', 'fields, annotations = get_constructor_field_annotations(TrainingArguments, include_bases=False)\\npprint(fields)\\nfor field in fields:\\n    assert len(field) == 3\\n    \\nassert get_field_annotations(TrainingArguments.__init__) == (fields, annotations)', 'def type_hint_to_str(type_hint: Any) -> str:\\n    \"\"\"\\n    Convert a type hint into its string representation.\\n    \"\"\"\\n    if hasattr(type_hint, \\'__name__\\'):\\n        return type_hint.__name__\\n    elif hasattr(type_hint, \\'_name\\') and type_hint._name is not None:\\n        return str(type_hint._name)\\n    elif type(type_hint) == _GenericAlias:  # For Python 3.8+\\n        # Handles complex types, e.g., List[int], Union[str, int]\\n        origin = type_hint_to_str(type_hint.__origin__)\\n        args = \\', \\'.join(type_hint_to_str(arg) for arg in type_hint.__args__)\\n        return f\"{origin}[{args}]\"\\n    else:\\n        # Fallback for unhandled types\\n        return str(type_hint)\\n\\ndef create_config_class_str(fields: List[Tuple[str, Any, Any]]) -> str:\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        for name, type_hint, default in fields:\\n            type_hint_str = type_hint_to_str(type_hint)\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint_str}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint_str} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\n# Example usage:\\n# Replace \\'fields\\' with your actual fields data\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', \"get_ipython().run_line_magic('pip', 'install -U -q langchain')\", 'from transformers import Trainer, GPT2LMHeadModel, TrainingArguments\\nfrom dataclasses import field, make_dataclass\\n\\nimport inspect\\nfrom inspect import Signature, Parameter\\nfrom typing import Any, Callable, Dict, Set, Optional, _GenericAlias, List, Type, Union, get_type_hints, overload, Tuple\\nfrom pydantic import BaseModel\\nfrom rich.pretty import pprint', 'class ParentClass:\\n    \"\"\"This is the parent class.\"\"\"\\n\\n    parent_class_attr = \\'a parent class attribute\\'\\n\\n    def __init__(self, parent_instance_attr: str) -> None:\\n        self.parent_instance_attr = parent_instance_attr\\n\\n    def parent_method(self) -> str:\\n        \"\"\"This is a method in the parent class.\"\"\"\\n        return \"Parent method called\"\\n\\nclass ChildClass(ParentClass):\\n    \"\"\"This is a subclass of ParentClass.\"\"\"\\n\\n    # Class attribute\\n    class_attr = \\'a class attribute\\'\\n\\n    # Private and protected attributes\\n    _protected_attr = \\'a protected attribute\\'\\n    __private_attr = \\'a private attribute\\'\\n\\n    def __init__(self, instance_attr: str, parent_instance_attr: str) -> None:\\n        \"\"\"Initialize the instance.\"\"\"\\n        super().__init__(parent_instance_attr)\\n        # Instance attribute\\n        self.instance_attr = instance_attr\\n\\n    @property\\n    def read_only_attr(self) -> str:\\n        \"\"\"This is a read-only attribute.\"\"\"\\n        return \\'You can read me, but you cannot change me.\\'\\n\\n    def instance_method(self, arg: str) -> str:\\n        \"\"\"This is an instance method.\"\"\"\\n        return f\\'Instance method called with argument: {arg}\\'\\n\\n    @classmethod\\n    def class_method(cls, arg: str) -> str:\\n        \"\"\"This is a class method.\"\"\"\\n        return f\\'Class method called with argument: {arg}\\'\\n\\n    @staticmethod\\n    def static_method(arg: str) -> str:\\n        \"\"\"This is a static method.\"\"\"\\n        return f\\'Static method called with argument: {arg}\\'\\n\\n    def __str__(self) -> str:\\n        \"\"\"Return a string representation of the instance.\"\"\"\\n        return f\\'MyClass(instance_attr={self.instance_attr})\\'', \"instance_child = ChildClass(instance_attr='an instance attribute', parent_instance_attr='a parent instance attribute')\\nclass_child = ChildClass\\n\\ninstance_parent = ParentClass(parent_instance_attr='a parent instance attribute')\\nclass_parent = ParentClass\", 'def func(a: int, b: str, c: List[int], d: Tuple[str, str], e: Union[int, str], **kwargs: Any) -> str:\\n    return a, b, c, d, e, kwargs', \"@overload\\ndef get_members_of_function_or_method(\\n    func_or_class: Type[object], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    ...\\n\\n\\n@overload\\ndef get_members_of_function_or_method(\\n    func_or_class: Callable[..., Any], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    ...\\n\\n\\ndef get_members_of_function_or_method(\\n    func_or_class: Union[Type[object], Callable[..., Any]], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    return inspect.getmembers(func_or_class, predicate)\\n\\ndef loop_through_members(members: List[Tuple[str, Any]], filter: Optional[str] = None) -> None:\\n    if filter is not None:\\n        members = [member for member in members if filter in member[0]]\\n    for member in members:\\n        name, value = member\\n        print(f'{name}: {value}')\", 'func_all_members = get_members_of_function_or_method(func, predicate=None)\\nloop_through_members(func_all_members)'], 'Out': {18: (<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel'>, <class 'transformers.modeling_utils.PreTrainedModel'>, <class 'torch.nn.modules.module.Module'>, <class 'transformers.modeling_utils.ModuleUtilsMixin'>, <class 'transformers.generation.utils.GenerationMixin'>, <class 'transformers.utils.hub.PushToHubMixin'>, <class 'transformers.integrations.peft.PeftAdapterMixin'>, <class 'object'>), 20: ((<class '__main__.ParentClass'>,), (<class 'torch.nn.modules.module.Module'>, <class 'transformers.modeling_utils.ModuleUtilsMixin'>, <class 'transformers.generation.utils.GenerationMixin'>, <class 'transformers.utils.hub.PushToHubMixin'>, <class 'transformers.integrations.peft.PeftAdapterMixin'>)), 23: {}, 24: {'a': <class 'int'>, 'b': <class 'str'>, 'c': typing.List[int], 'd': typing.Tuple[str, str], 'e': typing.Union[int, str], 'kwargs': typing.Any, 'return': <class 'str'>}, 25: {}, 26: {'model': typing.Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module, NoneType], 'args': typing.Optional[transformers.training_args.TrainingArguments], 'data_collator': typing.Optional[DataCollator], 'train_dataset': typing.Optional[torch.utils.data.dataset.Dataset], 'eval_dataset': typing.Union[torch.utils.data.dataset.Dataset, typing.Dict[str, torch.utils.data.dataset.Dataset], NoneType], 'tokenizer': typing.Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase], 'model_init': typing.Optional[typing.Callable[[], transformers.modeling_utils.PreTrainedModel]], 'compute_metrics': typing.Optional[typing.Callable[[transformers.trainer_utils.EvalPrediction], typing.Dict]], 'callbacks': typing.Optional[typing.List[transformers.trainer_callback.TrainerCallback]], 'optimizers': typing.Tuple[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR], 'preprocess_logits_for_metrics': typing.Optional[typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor]]}, 27: {'a': <class 'int'>, 'b': <class 'str'>, 'c': typing.List[int], 'd': typing.Tuple[str, str], 'e': typing.Union[int, str], 'kwargs': typing.Any, 'return': <class 'str'>}, 30: {}, 31: ({}, <Signature (a, b, c, d, e, **kwargs)>), 38: <class 'type'>, 40: <class 'type'>, 42: mappingproxy(OrderedDict([('output_dir', <Parameter \"output_dir: str\">), ('overwrite_output_dir', <Parameter \"overwrite_output_dir: bool = False\">), ('do_train', <Parameter \"do_train: bool = False\">), ('do_eval', <Parameter \"do_eval: bool = False\">), ('do_predict', <Parameter \"do_predict: bool = False\">), ('evaluation_strategy', <Parameter \"evaluation_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no'\">), ('prediction_loss_only', <Parameter \"prediction_loss_only: bool = False\">), ('per_device_train_batch_size', <Parameter \"per_device_train_batch_size: int = 8\">), ('per_device_eval_batch_size', <Parameter \"per_device_eval_batch_size: int = 8\">), ('per_gpu_train_batch_size', <Parameter \"per_gpu_train_batch_size: Optional[int] = None\">), ('per_gpu_eval_batch_size', <Parameter \"per_gpu_eval_batch_size: Optional[int] = None\">), ('gradient_accumulation_steps', <Parameter \"gradient_accumulation_steps: int = 1\">), ('eval_accumulation_steps', <Parameter \"eval_accumulation_steps: Optional[int] = None\">), ('eval_delay', <Parameter \"eval_delay: Optional[float] = 0\">), ('learning_rate', <Parameter \"learning_rate: float = 5e-05\">), ('weight_decay', <Parameter \"weight_decay: float = 0.0\">), ('adam_beta1', <Parameter \"adam_beta1: float = 0.9\">), ('adam_beta2', <Parameter \"adam_beta2: float = 0.999\">), ('adam_epsilon', <Parameter \"adam_epsilon: float = 1e-08\">), ('max_grad_norm', <Parameter \"max_grad_norm: float = 1.0\">), ('num_train_epochs', <Parameter \"num_train_epochs: float = 3.0\">), ('max_steps', <Parameter \"max_steps: int = -1\">), ('lr_scheduler_type', <Parameter \"lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear'\">), ('lr_scheduler_kwargs', <Parameter \"lr_scheduler_kwargs: Optional[Dict] = <factory>\">), ('warmup_ratio', <Parameter \"warmup_ratio: float = 0.0\">), ('warmup_steps', <Parameter \"warmup_steps: int = 0\">), ('log_level', <Parameter \"log_level: Optional[str] = 'passive'\">), ('log_level_replica', <Parameter \"log_level_replica: Optional[str] = 'warning'\">), ('log_on_each_node', <Parameter \"log_on_each_node: bool = True\">), ('logging_dir', <Parameter \"logging_dir: Optional[str] = None\">), ('logging_strategy', <Parameter \"logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'\">), ('logging_first_step', <Parameter \"logging_first_step: bool = False\">), ('logging_steps', <Parameter \"logging_steps: float = 500\">), ('logging_nan_inf_filter', <Parameter \"logging_nan_inf_filter: bool = True\">), ('save_strategy', <Parameter \"save_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'\">), ('save_steps', <Parameter \"save_steps: float = 500\">), ('save_total_limit', <Parameter \"save_total_limit: Optional[int] = None\">), ('save_safetensors', <Parameter \"save_safetensors: Optional[bool] = True\">), ('save_on_each_node', <Parameter \"save_on_each_node: bool = False\">), ('save_only_model', <Parameter \"save_only_model: bool = False\">), ('no_cuda', <Parameter \"no_cuda: bool = False\">), ('use_cpu', <Parameter \"use_cpu: bool = False\">), ('use_mps_device', <Parameter \"use_mps_device: bool = False\">), ('seed', <Parameter \"seed: int = 42\">), ('data_seed', <Parameter \"data_seed: Optional[int] = None\">), ('jit_mode_eval', <Parameter \"jit_mode_eval: bool = False\">), ('use_ipex', <Parameter \"use_ipex: bool = False\">), ('bf16', <Parameter \"bf16: bool = False\">), ('fp16', <Parameter \"fp16: bool = False\">), ('fp16_opt_level', <Parameter \"fp16_opt_level: str = 'O1'\">), ('half_precision_backend', <Parameter \"half_precision_backend: str = 'auto'\">), ('bf16_full_eval', <Parameter \"bf16_full_eval: bool = False\">), ('fp16_full_eval', <Parameter \"fp16_full_eval: bool = False\">), ('tf32', <Parameter \"tf32: Optional[bool] = None\">), ('local_rank', <Parameter \"local_rank: int = -1\">), ('ddp_backend', <Parameter \"ddp_backend: Optional[str] = None\">), ('tpu_num_cores', <Parameter \"tpu_num_cores: Optional[int] = None\">), ('tpu_metrics_debug', <Parameter \"tpu_metrics_debug: bool = False\">), ('debug', <Parameter \"debug: Union[str, List[transformers.debug_utils.DebugOption]] = ''\">), ('dataloader_drop_last', <Parameter \"dataloader_drop_last: bool = False\">), ('eval_steps', <Parameter \"eval_steps: Optional[float] = None\">), ('dataloader_num_workers', <Parameter \"dataloader_num_workers: int = 0\">), ('dataloader_prefetch_factor', <Parameter \"dataloader_prefetch_factor: Optional[int] = None\">), ('past_index', <Parameter \"past_index: int = -1\">), ('run_name', <Parameter \"run_name: Optional[str] = None\">), ('disable_tqdm', <Parameter \"disable_tqdm: Optional[bool] = None\">), ('remove_unused_columns', <Parameter \"remove_unused_columns: Optional[bool] = True\">), ('label_names', <Parameter \"label_names: Optional[List[str]] = None\">), ('load_best_model_at_end', <Parameter \"load_best_model_at_end: Optional[bool] = False\">), ('metric_for_best_model', <Parameter \"metric_for_best_model: Optional[str] = None\">), ('greater_is_better', <Parameter \"greater_is_better: Optional[bool] = None\">), ('ignore_data_skip', <Parameter \"ignore_data_skip: bool = False\">), ('fsdp', <Parameter \"fsdp: Union[List[transformers.trainer_utils.FSDPOption], str, NoneType] = ''\">), ('fsdp_min_num_params', <Parameter \"fsdp_min_num_params: int = 0\">), ('fsdp_config', <Parameter \"fsdp_config: Union[dict, str, NoneType] = None\">), ('fsdp_transformer_layer_cls_to_wrap', <Parameter \"fsdp_transformer_layer_cls_to_wrap: Optional[str] = None\">), ('accelerator_config', <Parameter \"accelerator_config: Optional[str] = None\">), ('deepspeed', <Parameter \"deepspeed: Optional[str] = None\">), ('label_smoothing_factor', <Parameter \"label_smoothing_factor: float = 0.0\">), ('optim', <Parameter \"optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch'\">), ('optim_args', <Parameter \"optim_args: Optional[str] = None\">), ('adafactor', <Parameter \"adafactor: bool = False\">), ('group_by_length', <Parameter \"group_by_length: bool = False\">), ('length_column_name', <Parameter \"length_column_name: Optional[str] = 'length'\">), ('report_to', <Parameter \"report_to: Optional[List[str]] = None\">), ('ddp_find_unused_parameters', <Parameter \"ddp_find_unused_parameters: Optional[bool] = None\">), ('ddp_bucket_cap_mb', <Parameter \"ddp_bucket_cap_mb: Optional[int] = None\">), ('ddp_broadcast_buffers', <Parameter \"ddp_broadcast_buffers: Optional[bool] = None\">), ('dataloader_pin_memory', <Parameter \"dataloader_pin_memory: bool = True\">), ('dataloader_persistent_workers', <Parameter \"dataloader_persistent_workers: bool = False\">), ('skip_memory_metrics', <Parameter \"skip_memory_metrics: bool = True\">), ('use_legacy_prediction_loop', <Parameter \"use_legacy_prediction_loop: bool = False\">), ('push_to_hub', <Parameter \"push_to_hub: bool = False\">), ('resume_from_checkpoint', <Parameter \"resume_from_checkpoint: Optional[str] = None\">), ('hub_model_id', <Parameter \"hub_model_id: Optional[str] = None\">), ('hub_strategy', <Parameter \"hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save'\">), ('hub_token', <Parameter \"hub_token: Optional[str] = None\">), ('hub_private_repo', <Parameter \"hub_private_repo: bool = False\">), ('hub_always_push', <Parameter \"hub_always_push: bool = False\">), ('gradient_checkpointing', <Parameter \"gradient_checkpointing: bool = False\">), ('gradient_checkpointing_kwargs', <Parameter \"gradient_checkpointing_kwargs: Optional[dict] = None\">), ('include_inputs_for_metrics', <Parameter \"include_inputs_for_metrics: bool = False\">), ('fp16_backend', <Parameter \"fp16_backend: str = 'auto'\">), ('push_to_hub_model_id', <Parameter \"push_to_hub_model_id: Optional[str] = None\">), ('push_to_hub_organization', <Parameter \"push_to_hub_organization: Optional[str] = None\">), ('push_to_hub_token', <Parameter \"push_to_hub_token: Optional[str] = None\">), ('mp_parameters', <Parameter \"mp_parameters: str = ''\">), ('auto_find_batch_size', <Parameter \"auto_find_batch_size: bool = False\">), ('full_determinism', <Parameter \"full_determinism: bool = False\">), ('torchdynamo', <Parameter \"torchdynamo: Optional[str] = None\">), ('ray_scope', <Parameter \"ray_scope: Optional[str] = 'last'\">), ('ddp_timeout', <Parameter \"ddp_timeout: Optional[int] = 1800\">), ('torch_compile', <Parameter \"torch_compile: bool = False\">), ('torch_compile_backend', <Parameter \"torch_compile_backend: Optional[str] = None\">), ('torch_compile_mode', <Parameter \"torch_compile_mode: Optional[str] = None\">), ('dispatch_batches', <Parameter \"dispatch_batches: Optional[bool] = None\">), ('split_batches', <Parameter \"split_batches: Optional[bool] = None\">), ('include_tokens_per_second', <Parameter \"include_tokens_per_second: Optional[bool] = False\">), ('include_num_input_tokens_seen', <Parameter \"include_num_input_tokens_seen: Optional[bool] = False\">), ('neftune_noise_alpha', <Parameter \"neftune_noise_alpha: Optional[float] = None\">)])), 66: (<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel'>, <class 'transformers.modeling_utils.PreTrainedModel'>, <class 'torch.nn.modules.module.Module'>, <class 'transformers.modeling_utils.ModuleUtilsMixin'>, <class 'transformers.generation.utils.GenerationMixin'>, <class 'transformers.utils.hub.PushToHubMixin'>, <class 'transformers.integrations.peft.PeftAdapterMixin'>, <class 'object'>), 67: <reversed object at 0x2a95639d0>, 68: [<class 'object'>, <class 'transformers.integrations.peft.PeftAdapterMixin'>, <class 'transformers.utils.hub.PushToHubMixin'>, <class 'transformers.generation.utils.GenerationMixin'>, <class 'transformers.modeling_utils.ModuleUtilsMixin'>, <class 'torch.nn.modules.module.Module'>, <class 'transformers.modeling_utils.PreTrainedModel'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>]}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x103c5d250>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x103c5de20>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x103c5de20>, 'open': <function open at 0x10298a3a0>, '_': [<class 'object'>, <class 'transformers.integrations.peft.PeftAdapterMixin'>, <class 'transformers.utils.hub.PushToHubMixin'>, <class 'transformers.generation.utils.GenerationMixin'>, <class 'transformers.modeling_utils.ModuleUtilsMixin'>, <class 'torch.nn.modules.module.Module'>, <class 'transformers.modeling_utils.PreTrainedModel'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>], '__': <reversed object at 0x2a95639d0>, '___': (<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel'>, <class 'transformers.modeling_utils.PreTrainedModel'>, <class 'torch.nn.modules.module.Module'>, <class 'transformers.modeling_utils.ModuleUtilsMixin'>, <class 'transformers.generation.utils.GenerationMixin'>, <class 'transformers.utils.hub.PushToHubMixin'>, <class 'transformers.integrations.peft.PeftAdapterMixin'>, <class 'object'>), '__vsc_ipynb_file__': '/Users/gaohn/gaohn/omniverse/omniverse/playbook/how_to_inspect_function_and_class_signatures.ipynb', '_i': \"@overload\\ndef get_members_of_function_or_method(\\n    func_or_class: Type[object], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    ...\\n\\n\\n@overload\\ndef get_members_of_function_or_method(\\n    func_or_class: Callable[..., Any], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    ...\\n\\n\\ndef get_members_of_function_or_method(\\n    func_or_class: Union[Type[object], Callable[..., Any]], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    return inspect.getmembers(func_or_class, predicate)\\n\\ndef loop_through_members(members: List[Tuple[str, Any]], filter: Optional[str] = None) -> None:\\n    if filter is not None:\\n        members = [member for member in members if filter in member[0]]\\n    for member in members:\\n        name, value = member\\n        print(f'{name}: {value}')\", '_ii': 'def func(a: int, b: str, c: List[int], d: Tuple[str, str], e: Union[int, str], **kwargs: Any) -> str:\\n    return a, b, c, d, e, kwargs', '_iii': \"instance_child = ChildClass(instance_attr='an instance attribute', parent_instance_attr='a parent instance attribute')\\nclass_child = ChildClass\\n\\ninstance_parent = ParentClass(parent_instance_attr='a parent instance attribute')\\nclass_parent = ParentClass\", '_i1': '%pip install -U -q langchain', '_exit_code': 0, '_i2': 'from transformers import Trainer, GPT2LMHeadModel, TrainingArguments\\nfrom dataclasses import field, make_dataclass\\n\\nimport inspect\\nfrom inspect import Signature, Parameter\\nfrom typing import Any, Callable, Dict, Set, Optional, List, Type, Union, get_type_hints, overload, Tuple\\nfrom pydantic import BaseModel\\nfrom rich.pretty import pprint', 'Trainer': <class 'transformers.trainer.Trainer'>, 'GPT2LMHeadModel': <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, 'TrainingArguments': <class 'transformers.training_args.TrainingArguments'>, 'field': <function field at 0x1026e39d0>, 'make_dataclass': <function make_dataclass at 0x1026fd0d0>, 'inspect': <module 'inspect' from '/opt/homebrew/Caskroom/miniconda/base/envs/omniverse/lib/python3.9/inspect.py'>, 'Signature': <class 'inspect.Signature'>, 'Parameter': <class 'inspect.Parameter'>, 'Any': typing.Any, 'Callable': typing.Callable, 'Dict': typing.Dict, 'Set': typing.Set, 'Optional': typing.Optional, 'List': typing.List, 'Type': typing.Type, 'Union': typing.Union, 'get_type_hints': <function get_type_hints at 0x100f77f70>, 'overload': <function overload at 0x100f7b3a0>, 'Tuple': typing.Tuple, 'BaseModel': <class 'pydantic.main.BaseModel'>, 'pprint': <function pprint at 0x2a09065e0>, '_i3': 'class ParentClass:\\n    \"\"\"This is the parent class.\"\"\"\\n\\n    parent_class_attr = \\'a parent class attribute\\'\\n\\n    def __init__(self, parent_instance_attr: str) -> None:\\n        self.parent_instance_attr = parent_instance_attr\\n\\n    def parent_method(self) -> str:\\n        \"\"\"This is a method in the parent class.\"\"\"\\n        return \"Parent method called\"\\n\\nclass ChildClass(ParentClass):\\n    \"\"\"This is a subclass of ParentClass.\"\"\"\\n\\n    # Class attribute\\n    class_attr = \\'a class attribute\\'\\n\\n    # Private and protected attributes\\n    _protected_attr = \\'a protected attribute\\'\\n    __private_attr = \\'a private attribute\\'\\n\\n    def __init__(self, instance_attr: str, parent_instance_attr: str) -> None:\\n        \"\"\"Initialize the instance.\"\"\"\\n        super().__init__(parent_instance_attr)\\n        # Instance attribute\\n        self.instance_attr = instance_attr\\n\\n    @property\\n    def read_only_attr(self) -> str:\\n        \"\"\"This is a read-only attribute.\"\"\"\\n        return \\'You can read me, but you cannot change me.\\'\\n\\n    def instance_method(self, arg: str) -> str:\\n        \"\"\"This is an instance method.\"\"\"\\n        return f\\'Instance method called with argument: {arg}\\'\\n\\n    @classmethod\\n    def class_method(cls, arg: str) -> str:\\n        \"\"\"This is a class method.\"\"\"\\n        return f\\'Class method called with argument: {arg}\\'\\n\\n    @staticmethod\\n    def static_method(arg: str) -> str:\\n        \"\"\"This is a static method.\"\"\"\\n        return f\\'Static method called with argument: {arg}\\'\\n\\n    def __str__(self) -> str:\\n        \"\"\"Return a string representation of the instance.\"\"\"\\n        return f\\'MyClass(instance_attr={self.instance_attr})\\'', 'ParentClass': <class '__main__.ParentClass'>, 'ChildClass': <class '__main__.ChildClass'>, '_i4': \"instance_child = ChildClass(instance_attr='an instance attribute', parent_instance_attr='a parent instance attribute')\\nclass_child = ChildClass\\n\\ninstance_parent = ParentClass(parent_instance_attr='a parent instance attribute')\\nclass_parent = ParentClass\", 'instance_child': <__main__.ChildClass object at 0x2a947ff40>, 'class_child': <class '__main__.ChildClass'>, 'instance_parent': <__main__.ParentClass object at 0x2a957ae80>, 'class_parent': <class '__main__.ParentClass'>, '_i5': 'def func(a: int, b: str, c: List[int], d: Tuple[str, str], e: Union[int, str], **kwargs: Any) -> str:\\n    return a, b, c, d, e, kwargs', 'func': <function func at 0x2a7d5b310>, '_i6': \"@overload\\ndef get_members_of_function_or_method(\\n    func_or_class: Type[object], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    ...\\n\\n\\n@overload\\ndef get_members_of_function_or_method(\\n    func_or_class: Callable[..., Any], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    ...\\n\\n\\ndef get_members_of_function_or_method(\\n    func_or_class: Union[Type[object], Callable[..., Any]], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    return inspect.getmembers(func_or_class, predicate)\\n\\ndef loop_through_members(members: List[Tuple[str, Any]], filter: Optional[str] = None) -> None:\\n    if filter is not None:\\n        members = [member for member in members if filter in member[0]]\\n    for member in members:\\n        name, value = member\\n        print(f'{name}: {value}')\", 'get_members_of_function_or_method': <function get_members_of_function_or_method at 0x2a7b941f0>, 'loop_through_members': <function loop_through_members at 0x2a7b94af0>, '_i7': 'func_all_members = get_members_of_function_or_method(func, predicate=None)\\nloop_through_members(func_all_members)', 'func_all_members': [('__annotations__', {'a': <class 'int'>, 'b': <class 'str'>, 'c': typing.List[int], 'd': typing.Tuple[str, str], 'e': typing.Union[int, str], 'kwargs': typing.Any, 'return': <class 'str'>}), ('__call__', <method-wrapper '__call__' of function object at 0x2a7d5b310>), ('__class__', <class 'function'>), ('__closure__', None), ('__code__', <code object func at 0x2a9741be0, file \"/var/folders/l2/jjqj299126j0gycr9kkkt9xm0000gn/T/ipykernel_16129/2139551385.py\", line 1>), ('__defaults__', None), ('__delattr__', <method-wrapper '__delattr__' of function object at 0x2a7d5b310>), ('__dict__', {}), ('__dir__', <built-in method __dir__ of function object at 0x2a7d5b310>), ('__doc__', None), ('__eq__', <method-wrapper '__eq__' of function object at 0x2a7d5b310>), ('__format__', <built-in method __format__ of function object at 0x2a7d5b310>), ('__ge__', <method-wrapper '__ge__' of function object at 0x2a7d5b310>), ('__get__', <method-wrapper '__get__' of function object at 0x2a7d5b310>), ('__getattribute__', <method-wrapper '__getattribute__' of function object at 0x2a7d5b310>), ('__globals__', {...}), ('__gt__', <method-wrapper '__gt__' of function object at 0x2a7d5b310>), ('__hash__', <method-wrapper '__hash__' of function object at 0x2a7d5b310>), ('__init__', <method-wrapper '__init__' of function object at 0x2a7d5b310>), ('__init_subclass__', <built-in method __init_subclass__ of type object at 0x100bb2270>), ('__kwdefaults__', None), ('__le__', <method-wrapper '__le__' of function object at 0x2a7d5b310>), ('__lt__', <method-wrapper '__lt__' of function object at 0x2a7d5b310>), ('__module__', '__main__'), ('__name__', 'func'), ('__ne__', <method-wrapper '__ne__' of function object at 0x2a7d5b310>), ('__new__', <built-in method __new__ of type object at 0x100bb2270>), ('__qualname__', 'func'), ('__reduce__', <built-in method __reduce__ of function object at 0x2a7d5b310>), ('__reduce_ex__', <built-in method __reduce_ex__ of function object at 0x2a7d5b310>), ('__repr__', <method-wrapper '__repr__' of function object at 0x2a7d5b310>), ('__setattr__', <method-wrapper '__setattr__' of function object at 0x2a7d5b310>), ('__sizeof__', <built-in method __sizeof__ of function object at 0x2a7d5b310>), ('__str__', <method-wrapper '__str__' of function object at 0x2a7d5b310>), ('__subclasshook__', <built-in method __subclasshook__ of type object at 0x100bb2270>)], '_i8': \"loop_through_members(func_all_members, filter='__annotations__')\", '_i9': 'class_child_all_members = get_members_of_function_or_method(class_child, predicate=None)\\nloop_through_members(class_child_all_members)', 'class_child_all_members': [('_ChildClass__private_attr', 'a private attribute'), ('__class__', <class 'type'>), ('__delattr__', <slot wrapper '__delattr__' of 'object' objects>), ('__dict__', mappingproxy({'__module__': '__main__', '__doc__': 'This is a subclass of ParentClass.', 'class_attr': 'a class attribute', '_protected_attr': 'a protected attribute', '_ChildClass__private_attr': 'a private attribute', '__init__': <function ChildClass.__init__ at 0x2a7b945e0>, 'read_only_attr': <property object at 0x2a7b97e00>, 'instance_method': <function ChildClass.instance_method at 0x2a7b94670>, 'class_method': <classmethod object at 0x2a7b93a90>, 'static_method': <staticmethod object at 0x2a7b93bb0>, '__str__': <function ChildClass.__str__ at 0x2a7b94820>})), ('__dir__', <method '__dir__' of 'object' objects>), ('__doc__', 'This is a subclass of ParentClass.'), ('__eq__', <slot wrapper '__eq__' of 'object' objects>), ('__format__', <method '__format__' of 'object' objects>), ('__ge__', <slot wrapper '__ge__' of 'object' objects>), ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>), ('__gt__', <slot wrapper '__gt__' of 'object' objects>), ('__hash__', <slot wrapper '__hash__' of 'object' objects>), ('__init__', <function ChildClass.__init__ at 0x2a7b945e0>), ('__init_subclass__', <built-in method __init_subclass__ of type object at 0x2a0b400d0>), ('__le__', <slot wrapper '__le__' of 'object' objects>), ('__lt__', <slot wrapper '__lt__' of 'object' objects>), ('__module__', '__main__'), ('__ne__', <slot wrapper '__ne__' of 'object' objects>), ('__new__', <built-in method __new__ of type object at 0x100bc6b00>), ('__reduce__', <method '__reduce__' of 'object' objects>), ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>), ('__repr__', <slot wrapper '__repr__' of 'object' objects>), ('__setattr__', <slot wrapper '__setattr__' of 'object' objects>), ('__sizeof__', <method '__sizeof__' of 'object' objects>), ('__str__', <function ChildClass.__str__ at 0x2a7b94820>), ('__subclasshook__', <built-in method __subclasshook__ of type object at 0x2a0b400d0>), ('__weakref__', <attribute '__weakref__' of 'ParentClass' objects>), ('_protected_attr', 'a protected attribute'), ('class_attr', 'a class attribute'), ('class_method', <bound method ChildClass.class_method of <class '__main__.ChildClass'>>), ('instance_method', <function ChildClass.instance_method at 0x2a7b94670>), ('parent_class_attr', 'a parent class attribute'), ('parent_method', <function ParentClass.parent_method at 0x2a7b94430>), ('read_only_attr', <property object at 0x2a7b97e00>), ('static_method', <function ChildClass.static_method at 0x2a7b94790>)], '_i10': 'instance_child_all_members = get_members_of_function_or_method(instance_child, predicate=None)\\nloop_through_members(instance_child_all_members)', 'instance_child_all_members': [('_ChildClass__private_attr', 'a private attribute'), ('__class__', <class '__main__.ChildClass'>), ('__delattr__', <method-wrapper '__delattr__' of ChildClass object at 0x2a7b938e0>), ('__dict__', {'parent_instance_attr': 'a parent instance attribute', 'instance_attr': 'an instance attribute'}), ('__dir__', <built-in method __dir__ of ChildClass object at 0x2a7b938e0>), ('__doc__', 'This is a subclass of ParentClass.'), ('__eq__', <method-wrapper '__eq__' of ChildClass object at 0x2a7b938e0>), ('__format__', <built-in method __format__ of ChildClass object at 0x2a7b938e0>), ('__ge__', <method-wrapper '__ge__' of ChildClass object at 0x2a7b938e0>), ('__getattribute__', <method-wrapper '__getattribute__' of ChildClass object at 0x2a7b938e0>), ('__gt__', <method-wrapper '__gt__' of ChildClass object at 0x2a7b938e0>), ('__hash__', <method-wrapper '__hash__' of ChildClass object at 0x2a7b938e0>), ('__init__', <bound method ChildClass.__init__ of <__main__.ChildClass object at 0x2a7b938e0>>), ('__init_subclass__', <built-in method __init_subclass__ of type object at 0x2a0b400d0>), ('__le__', <method-wrapper '__le__' of ChildClass object at 0x2a7b938e0>), ('__lt__', <method-wrapper '__lt__' of ChildClass object at 0x2a7b938e0>), ('__module__', '__main__'), ('__ne__', <method-wrapper '__ne__' of ChildClass object at 0x2a7b938e0>), ('__new__', <built-in method __new__ of type object at 0x100bc6b00>), ('__reduce__', <built-in method __reduce__ of ChildClass object at 0x2a7b938e0>), ('__reduce_ex__', <built-in method __reduce_ex__ of ChildClass object at 0x2a7b938e0>), ('__repr__', <method-wrapper '__repr__' of ChildClass object at 0x2a7b938e0>), ('__setattr__', <method-wrapper '__setattr__' of ChildClass object at 0x2a7b938e0>), ('__sizeof__', <built-in method __sizeof__ of ChildClass object at 0x2a7b938e0>), ('__str__', <bound method ChildClass.__str__ of <__main__.ChildClass object at 0x2a7b938e0>>), ('__subclasshook__', <built-in method __subclasshook__ of type object at 0x2a0b400d0>), ('__weakref__', None), ('_protected_attr', 'a protected attribute'), ('class_attr', 'a class attribute'), ('class_method', <bound method ChildClass.class_method of <class '__main__.ChildClass'>>), ('instance_attr', 'an instance attribute'), ('instance_method', <bound method ChildClass.instance_method of <__main__.ChildClass object at 0x2a7b938e0>>), ('parent_class_attr', 'a parent class attribute'), ('parent_instance_attr', 'a parent instance attribute'), ('parent_method', <bound method ParentClass.parent_method of <__main__.ChildClass object at 0x2a7b938e0>>), ('read_only_attr', 'You can read me, but you cannot change me.'), ('static_method', <function ChildClass.static_method at 0x2a7b94790>)], '_i11': 'trainer_all_members = get_members_of_function_or_method(Trainer, predicate=None)\\nloop_through_members(trainer_all_members)', 'trainer_all_members': [('__class__', <class 'type'>), ('__delattr__', <slot wrapper '__delattr__' of 'object' objects>), ('__dict__', mappingproxy({'__module__': 'transformers.trainer', '__doc__': \"\\n    Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for  Transformers.\\n\\n    Args:\\n        model ([`PreTrainedModel`] or `torch.nn.Module`, *optional*):\\n            The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.\\n\\n            <Tip>\\n\\n            [`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use\\n            your own models defined as `torch.nn.Module` as long as they work the same way as the  Transformers\\n            models.\\n\\n            </Tip>\\n\\n        args ([`TrainingArguments`], *optional*):\\n            The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the\\n            `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.\\n        data_collator (`DataCollator`, *optional*):\\n            The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will\\n            default to [`default_data_collator`] if no `tokenizer` is provided, an instance of\\n            [`DataCollatorWithPadding`] otherwise.\\n        train_dataset (`torch.utils.data.Dataset` or `torch.utils.data.IterableDataset`, *optional*):\\n            The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the\\n            `model.forward()` method are automatically removed.\\n\\n            Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a\\n            distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a\\n            `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will\\n            manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally\\n            sets the seed of the RNGs used.\\n        eval_dataset (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`]), *optional*):\\n             The dataset to use for evaluation. If it is a [`~datasets.Dataset`], columns not accepted by the\\n             `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each\\n             dataset prepending the dictionary key to the metric name.\\n        tokenizer ([`PreTrainedTokenizerBase`], *optional*):\\n            The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs to the\\n            maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an\\n            interrupted training or reuse the fine-tuned model.\\n        model_init (`Callable[[], PreTrainedModel]`, *optional*):\\n            A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start\\n            from a new instance of the model as given by this function.\\n\\n            The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to\\n            be able to choose different architectures according to hyper parameters (such as layer count, sizes of\\n            inner layers, dropout probabilities etc).\\n        compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):\\n            The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return\\n            a dictionary string to metric values.\\n        callbacks (List of [`TrainerCallback`], *optional*):\\n            A list of callbacks to customize the training loop. Will add those to the list of default callbacks\\n            detailed in [here](callback).\\n\\n            If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.\\n        optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*, defaults to `(None, None)`):\\n            A tuple containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your\\n            model and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.\\n        preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*):\\n            A function that preprocess the logits right before caching them at each evaluation step. Must take two\\n            tensors, the logits and the labels, and return the logits once processed as desired. The modifications made\\n            by this function will be reflected in the predictions received by `compute_metrics`.\\n\\n            Note that the labels (second parameter) will be `None` if the dataset does not have them.\\n\\n    Important attributes:\\n\\n        - **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]\\n          subclass.\\n        - **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the\\n          original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`,\\n          the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner\\n          model hasn't been wrapped, then `self.model_wrapped` is the same as `self.model`.\\n        - **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from\\n          data parallelism, this means some of the model layers are split on different GPUs).\\n        - **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set\\n          to `False` if model parallel or deepspeed is used, or if the default\\n          `TrainingArguments.place_model_on_device` is overridden to return `False` .\\n        - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while\\n          in `train`)\\n\\n    \", '_get_learning_rate': <function _get_learning_rate at 0x12db6a820>, 'log_metrics': <function log_metrics at 0x12db6b160>, 'metrics_format': <function metrics_format at 0x12db6b0d0>, 'save_metrics': <function save_metrics at 0x12db6b1f0>, 'save_state': <function save_state at 0x12db6b280>, '__init__': <function Trainer.__init__ at 0x2a7b5bdc0>, '_activate_neftune': <function Trainer._activate_neftune at 0x2a7b5be50>, '_deactivate_neftune': <function Trainer._deactivate_neftune at 0x2a7b5bee0>, 'add_callback': <function Trainer.add_callback at 0x2a7b5bf70>, 'pop_callback': <function Trainer.pop_callback at 0x2a7b5e040>, 'remove_callback': <function Trainer.remove_callback at 0x2a7b5e0d0>, '_move_model_to_device': <function Trainer._move_model_to_device at 0x2a7b5e160>, '_set_signature_columns_if_needed': <function Trainer._set_signature_columns_if_needed at 0x2a7b5e1f0>, '_remove_unused_columns': <function Trainer._remove_unused_columns at 0x2a7b5e280>, '_get_collator_with_removed_columns': <function Trainer._get_collator_with_removed_columns at 0x2a7b5e310>, '_get_train_sampler': <function Trainer._get_train_sampler at 0x2a7b5e3a0>, 'get_train_dataloader': <function Trainer.get_train_dataloader at 0x2a7b5e430>, '_get_eval_sampler': <function Trainer._get_eval_sampler at 0x2a7b5e4c0>, 'get_eval_dataloader': <function Trainer.get_eval_dataloader at 0x2a7b5e550>, 'get_test_dataloader': <function Trainer.get_test_dataloader at 0x2a7b5e5e0>, 'create_optimizer_and_scheduler': <function Trainer.create_optimizer_and_scheduler at 0x2a7b5e670>, 'get_decay_parameter_names': <function Trainer.get_decay_parameter_names at 0x2a7b5e700>, 'create_optimizer': <function Trainer.create_optimizer at 0x2a7b5e790>, 'get_optimizer_cls_and_kwargs': <staticmethod object at 0x2a799e9d0>, 'create_scheduler': <function Trainer.create_scheduler at 0x2a7b5e8b0>, 'num_examples': <function Trainer.num_examples at 0x2a7b5e940>, 'num_tokens': <function Trainer.num_tokens at 0x2a7b5e9d0>, '_hp_search_setup': <function Trainer._hp_search_setup at 0x2a7b5ea60>, '_report_to_hp_search': <function Trainer._report_to_hp_search at 0x2a7b5eaf0>, '_tune_save_checkpoint': <function Trainer._tune_save_checkpoint at 0x2a7b5eb80>, 'call_model_init': <function Trainer.call_model_init at 0x2a7b5ec10>, 'torch_jit_model_eval': <function Trainer.torch_jit_model_eval at 0x2a7b5eca0>, 'ipex_optimize_model': <function Trainer.ipex_optimize_model at 0x2a7b5ed30>, '_wrap_model': <function Trainer._wrap_model at 0x2a7b5edc0>, 'train': <function Trainer.train at 0x2a7b5ee50>, '_inner_training_loop': <function Trainer._inner_training_loop at 0x2a7b5eee0>, '_get_output_dir': <function Trainer._get_output_dir at 0x2a7b5ef70>, '_load_from_checkpoint': <function Trainer._load_from_checkpoint at 0x2a7b60040>, '_load_best_model': <function Trainer._load_best_model at 0x2a7b600d0>, '_issue_warnings_after_load': <function Trainer._issue_warnings_after_load at 0x2a7b60160>, '_maybe_log_save_evaluate': <function Trainer._maybe_log_save_evaluate at 0x2a7b601f0>, '_load_rng_state': <function Trainer._load_rng_state at 0x2a7b60280>, '_save_checkpoint': <function Trainer._save_checkpoint at 0x2a7b60310>, '_save_rng_state': <function Trainer._save_rng_state at 0x2a7b603a0>, '_save_optimizer_and_scheduler': <function Trainer._save_optimizer_and_scheduler at 0x2a7b60430>, '_load_optimizer_and_scheduler': <function Trainer._load_optimizer_and_scheduler at 0x2a7b604c0>, 'hyperparameter_search': <function Trainer.hyperparameter_search at 0x2a7b60550>, 'log': <function Trainer.log at 0x2a7b605e0>, '_prepare_input': <function Trainer._prepare_input at 0x2a7b60670>, '_prepare_inputs': <function Trainer._prepare_inputs at 0x2a7b60700>, 'compute_loss_context_manager': <function Trainer.compute_loss_context_manager at 0x2a7b60790>, 'autocast_smart_context_manager': <function Trainer.autocast_smart_context_manager at 0x2a7b60820>, 'training_step': <function Trainer.training_step at 0x2a7b608b0>, 'compute_loss': <function Trainer.compute_loss at 0x2a7b60940>, 'is_local_process_zero': <function Trainer.is_local_process_zero at 0x2a7b609d0>, 'is_world_process_zero': <function Trainer.is_world_process_zero at 0x2a7b60a60>, 'save_model': <function Trainer.save_model at 0x2a7b60af0>, '_save_tpu': <function Trainer._save_tpu at 0x2a7b60b80>, '_save': <function Trainer._save at 0x2a7b60c10>, 'store_flos': <function Trainer.store_flos at 0x2a7b60ca0>, '_sorted_checkpoints': <function Trainer._sorted_checkpoints at 0x2a7b60d30>, '_rotate_checkpoints': <function Trainer._rotate_checkpoints at 0x2a7b60dc0>, 'evaluate': <function Trainer.evaluate at 0x2a7b60e50>, 'predict': <function Trainer.predict at 0x2a7b60ee0>, 'evaluation_loop': <function Trainer.evaluation_loop at 0x2a7b60f70>, '_nested_gather': <function Trainer._nested_gather at 0x2a7b62040>, 'prediction_step': <function Trainer.prediction_step at 0x2a7b620d0>, 'floating_point_ops': <function Trainer.floating_point_ops at 0x2a7b62160>, 'init_hf_repo': <function Trainer.init_hf_repo at 0x2a7b621f0>, 'create_model_card': <function Trainer.create_model_card at 0x2a7b62280>, '_push_from_checkpoint': <function Trainer._push_from_checkpoint at 0x2a7b62310>, '_finish_current_push': <function Trainer._finish_current_push at 0x2a7b623a0>, 'push_to_hub': <function Trainer.push_to_hub at 0x2a7b62430>, 'prediction_loop': <function Trainer.prediction_loop at 0x2a7b624c0>, '_gather_and_numpify': <function Trainer._gather_and_numpify at 0x2a7b62550>, '_add_sm_patterns_to_gitignore': <function Trainer._add_sm_patterns_to_gitignore at 0x2a7b625e0>, 'create_accelerator_and_postprocess': <function Trainer.create_accelerator_and_postprocess at 0x2a7b62670>, 'propagate_args_to_deepspeed': <function Trainer.propagate_args_to_deepspeed at 0x2a7b62700>, '__dict__': <attribute '__dict__' of 'Trainer' objects>, '__weakref__': <attribute '__weakref__' of 'Trainer' objects>})), ('__dir__', <method '__dir__' of 'object' objects>), ('__doc__', \"\\n    Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for  Transformers.\\n\\n    Args:\\n        model ([`PreTrainedModel`] or `torch.nn.Module`, *optional*):\\n            The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.\\n\\n            <Tip>\\n\\n            [`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use\\n            your own models defined as `torch.nn.Module` as long as they work the same way as the  Transformers\\n            models.\\n\\n            </Tip>\\n\\n        args ([`TrainingArguments`], *optional*):\\n            The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the\\n            `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.\\n        data_collator (`DataCollator`, *optional*):\\n            The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will\\n            default to [`default_data_collator`] if no `tokenizer` is provided, an instance of\\n            [`DataCollatorWithPadding`] otherwise.\\n        train_dataset (`torch.utils.data.Dataset` or `torch.utils.data.IterableDataset`, *optional*):\\n            The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the\\n            `model.forward()` method are automatically removed.\\n\\n            Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a\\n            distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a\\n            `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will\\n            manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally\\n            sets the seed of the RNGs used.\\n        eval_dataset (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`]), *optional*):\\n             The dataset to use for evaluation. If it is a [`~datasets.Dataset`], columns not accepted by the\\n             `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each\\n             dataset prepending the dictionary key to the metric name.\\n        tokenizer ([`PreTrainedTokenizerBase`], *optional*):\\n            The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs to the\\n            maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an\\n            interrupted training or reuse the fine-tuned model.\\n        model_init (`Callable[[], PreTrainedModel]`, *optional*):\\n            A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start\\n            from a new instance of the model as given by this function.\\n\\n            The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to\\n            be able to choose different architectures according to hyper parameters (such as layer count, sizes of\\n            inner layers, dropout probabilities etc).\\n        compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):\\n            The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return\\n            a dictionary string to metric values.\\n        callbacks (List of [`TrainerCallback`], *optional*):\\n            A list of callbacks to customize the training loop. Will add those to the list of default callbacks\\n            detailed in [here](callback).\\n\\n            If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.\\n        optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*, defaults to `(None, None)`):\\n            A tuple containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your\\n            model and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.\\n        preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*):\\n            A function that preprocess the logits right before caching them at each evaluation step. Must take two\\n            tensors, the logits and the labels, and return the logits once processed as desired. The modifications made\\n            by this function will be reflected in the predictions received by `compute_metrics`.\\n\\n            Note that the labels (second parameter) will be `None` if the dataset does not have them.\\n\\n    Important attributes:\\n\\n        - **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]\\n          subclass.\\n        - **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the\\n          original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`,\\n          the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner\\n          model hasn't been wrapped, then `self.model_wrapped` is the same as `self.model`.\\n        - **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from\\n          data parallelism, this means some of the model layers are split on different GPUs).\\n        - **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set\\n          to `False` if model parallel or deepspeed is used, or if the default\\n          `TrainingArguments.place_model_on_device` is overridden to return `False` .\\n        - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while\\n          in `train`)\\n\\n    \"), ('__eq__', <slot wrapper '__eq__' of 'object' objects>), ('__format__', <method '__format__' of 'object' objects>), ('__ge__', <slot wrapper '__ge__' of 'object' objects>), ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>), ('__gt__', <slot wrapper '__gt__' of 'object' objects>), ('__hash__', <slot wrapper '__hash__' of 'object' objects>), ('__init__', <function Trainer.__init__ at 0x2a7b5bdc0>), ('__init_subclass__', <built-in method __init_subclass__ of type object at 0x2a44fa900>), ('__le__', <slot wrapper '__le__' of 'object' objects>), ('__lt__', <slot wrapper '__lt__' of 'object' objects>), ('__module__', 'transformers.trainer'), ('__ne__', <slot wrapper '__ne__' of 'object' objects>), ('__new__', <built-in method __new__ of type object at 0x100bc6b00>), ('__reduce__', <method '__reduce__' of 'object' objects>), ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>), ('__repr__', <slot wrapper '__repr__' of 'object' objects>), ('__setattr__', <slot wrapper '__setattr__' of 'object' objects>), ('__sizeof__', <method '__sizeof__' of 'object' objects>), ('__str__', <slot wrapper '__str__' of 'object' objects>), ('__subclasshook__', <built-in method __subclasshook__ of type object at 0x2a44fa900>), ('__weakref__', <attribute '__weakref__' of 'Trainer' objects>), ('_activate_neftune', <function Trainer._activate_neftune at 0x2a7b5be50>), ('_add_sm_patterns_to_gitignore', <function Trainer._add_sm_patterns_to_gitignore at 0x2a7b625e0>), ('_deactivate_neftune', <function Trainer._deactivate_neftune at 0x2a7b5bee0>), ('_finish_current_push', <function Trainer._finish_current_push at 0x2a7b623a0>), ('_gather_and_numpify', <function Trainer._gather_and_numpify at 0x2a7b62550>), ('_get_collator_with_removed_columns', <function Trainer._get_collator_with_removed_columns at 0x2a7b5e310>), ('_get_eval_sampler', <function Trainer._get_eval_sampler at 0x2a7b5e4c0>), ('_get_learning_rate', <function _get_learning_rate at 0x12db6a820>), ('_get_output_dir', <function Trainer._get_output_dir at 0x2a7b5ef70>), ('_get_train_sampler', <function Trainer._get_train_sampler at 0x2a7b5e3a0>), ('_hp_search_setup', <function Trainer._hp_search_setup at 0x2a7b5ea60>), ('_inner_training_loop', <function Trainer._inner_training_loop at 0x2a7b5eee0>), ('_issue_warnings_after_load', <function Trainer._issue_warnings_after_load at 0x2a7b60160>), ('_load_best_model', <function Trainer._load_best_model at 0x2a7b600d0>), ('_load_from_checkpoint', <function Trainer._load_from_checkpoint at 0x2a7b60040>), ('_load_optimizer_and_scheduler', <function Trainer._load_optimizer_and_scheduler at 0x2a7b604c0>), ('_load_rng_state', <function Trainer._load_rng_state at 0x2a7b60280>), ('_maybe_log_save_evaluate', <function Trainer._maybe_log_save_evaluate at 0x2a7b601f0>), ('_move_model_to_device', <function Trainer._move_model_to_device at 0x2a7b5e160>), ('_nested_gather', <function Trainer._nested_gather at 0x2a7b62040>), ('_prepare_input', <function Trainer._prepare_input at 0x2a7b60670>), ('_prepare_inputs', <function Trainer._prepare_inputs at 0x2a7b60700>), ('_push_from_checkpoint', <function Trainer._push_from_checkpoint at 0x2a7b62310>), ('_remove_unused_columns', <function Trainer._remove_unused_columns at 0x2a7b5e280>), ('_report_to_hp_search', <function Trainer._report_to_hp_search at 0x2a7b5eaf0>), ('_rotate_checkpoints', <function Trainer._rotate_checkpoints at 0x2a7b60dc0>), ('_save', <function Trainer._save at 0x2a7b60c10>), ('_save_checkpoint', <function Trainer._save_checkpoint at 0x2a7b60310>), ('_save_optimizer_and_scheduler', <function Trainer._save_optimizer_and_scheduler at 0x2a7b60430>), ('_save_rng_state', <function Trainer._save_rng_state at 0x2a7b603a0>), ('_save_tpu', <function Trainer._save_tpu at 0x2a7b60b80>), ('_set_signature_columns_if_needed', <function Trainer._set_signature_columns_if_needed at 0x2a7b5e1f0>), ('_sorted_checkpoints', <function Trainer._sorted_checkpoints at 0x2a7b60d30>), ('_tune_save_checkpoint', <function Trainer._tune_save_checkpoint at 0x2a7b5eb80>), ('_wrap_model', <function Trainer._wrap_model at 0x2a7b5edc0>), ('add_callback', <function Trainer.add_callback at 0x2a7b5bf70>), ('autocast_smart_context_manager', <function Trainer.autocast_smart_context_manager at 0x2a7b60820>), ('call_model_init', <function Trainer.call_model_init at 0x2a7b5ec10>), ('compute_loss', <function Trainer.compute_loss at 0x2a7b60940>), ('compute_loss_context_manager', <function Trainer.compute_loss_context_manager at 0x2a7b60790>), ('create_accelerator_and_postprocess', <function Trainer.create_accelerator_and_postprocess at 0x2a7b62670>), ('create_model_card', <function Trainer.create_model_card at 0x2a7b62280>), ('create_optimizer', <function Trainer.create_optimizer at 0x2a7b5e790>), ('create_optimizer_and_scheduler', <function Trainer.create_optimizer_and_scheduler at 0x2a7b5e670>), ('create_scheduler', <function Trainer.create_scheduler at 0x2a7b5e8b0>), ('evaluate', <function Trainer.evaluate at 0x2a7b60e50>), ('evaluation_loop', <function Trainer.evaluation_loop at 0x2a7b60f70>), ('floating_point_ops', <function Trainer.floating_point_ops at 0x2a7b62160>), ('get_decay_parameter_names', <function Trainer.get_decay_parameter_names at 0x2a7b5e700>), ('get_eval_dataloader', <function Trainer.get_eval_dataloader at 0x2a7b5e550>), ('get_optimizer_cls_and_kwargs', <function Trainer.get_optimizer_cls_and_kwargs at 0x2a7b5e820>), ('get_test_dataloader', <function Trainer.get_test_dataloader at 0x2a7b5e5e0>), ('get_train_dataloader', <function Trainer.get_train_dataloader at 0x2a7b5e430>), ('hyperparameter_search', <function Trainer.hyperparameter_search at 0x2a7b60550>), ('init_hf_repo', <function Trainer.init_hf_repo at 0x2a7b621f0>), ('ipex_optimize_model', <function Trainer.ipex_optimize_model at 0x2a7b5ed30>), ('is_local_process_zero', <function Trainer.is_local_process_zero at 0x2a7b609d0>), ('is_world_process_zero', <function Trainer.is_world_process_zero at 0x2a7b60a60>), ('log', <function Trainer.log at 0x2a7b605e0>), ('log_metrics', <function log_metrics at 0x12db6b160>), ('metrics_format', <function metrics_format at 0x12db6b0d0>), ('num_examples', <function Trainer.num_examples at 0x2a7b5e940>), ('num_tokens', <function Trainer.num_tokens at 0x2a7b5e9d0>), ('pop_callback', <function Trainer.pop_callback at 0x2a7b5e040>), ('predict', <function Trainer.predict at 0x2a7b60ee0>), ('prediction_loop', <function Trainer.prediction_loop at 0x2a7b624c0>), ('prediction_step', <function Trainer.prediction_step at 0x2a7b620d0>), ('propagate_args_to_deepspeed', <function Trainer.propagate_args_to_deepspeed at 0x2a7b62700>), ('push_to_hub', <function Trainer.push_to_hub at 0x2a7b62430>), ('remove_callback', <function Trainer.remove_callback at 0x2a7b5e0d0>), ('save_metrics', <function save_metrics at 0x12db6b1f0>), ('save_model', <function Trainer.save_model at 0x2a7b60af0>), ('save_state', <function save_state at 0x12db6b280>), ('store_flos', <function Trainer.store_flos at 0x2a7b60ca0>), ('torch_jit_model_eval', <function Trainer.torch_jit_model_eval at 0x2a7b5eca0>), ('train', <function Trainer.train at 0x2a7b5ee50>), ('training_step', <function Trainer.training_step at 0x2a7b608b0>)], '_i12': \"child_class_methods_using_dict = list(ChildClass.__dict__.keys())\\npprint(sorted(child_class_methods_using_dict))\\n\\nassert 'parent_method' not in child_class_methods_using_dict\\nassert 'read_only_attr' in child_class_methods_using_dict\\nassert 'class_method' in child_class_methods_using_dict\", 'child_class_methods_using_dict': ['__module__', '__doc__', 'class_attr', '_protected_attr', '_ChildClass__private_attr', '__init__', 'read_only_attr', 'instance_method', 'class_method', 'static_method', '__str__'], '_i13': 'pprint(instance_child.__class__.__dict__.keys() == ChildClass.__dict__.keys())', '_i14': \"child_class_methods_using_vars = list(vars(ChildClass).keys())\\npprint(sorted(child_class_methods_using_vars))\\n\\nassert 'parent_method' not in child_class_methods_using_vars\\nassert 'read_only_attr' in child_class_methods_using_vars\\nassert 'class_method' in child_class_methods_using_vars\\n\\nassert set(child_class_methods_using_dict) == set(child_class_methods_using_vars)\", 'child_class_methods_using_vars': ['__module__', '__doc__', 'class_attr', '_protected_attr', '_ChildClass__private_attr', '__init__', 'read_only_attr', 'instance_method', 'class_method', 'static_method', '__str__'], '_i15': \"child_class_methods_using_dir = list(dir(ChildClass))\\npprint(sorted(child_class_methods_using_dir))\\n\\nassert 'parent_method' in child_class_methods_using_dir\\nassert 'read_only_attr' in child_class_methods_using_dir\\nassert 'class_method' in child_class_methods_using_dir\", 'child_class_methods_using_dir': ['_ChildClass__private_attr', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_protected_attr', 'class_attr', 'class_method', 'instance_method', 'parent_class_attr', 'parent_method', 'read_only_attr', 'static_method'], '_i16': 'predicate = inspect.isroutine\\nchild_class_methods_using_getmembers = list(get_members_of_function_or_method(ChildClass, predicate=predicate))\\n\\npprint(sorted(child_class_methods_using_getmembers))', 'predicate': <function isroutine at 0x101903f70>, 'child_class_methods_using_getmembers': [('__delattr__', <slot wrapper '__delattr__' of 'object' objects>), ('__dir__', <method '__dir__' of 'object' objects>), ('__eq__', <slot wrapper '__eq__' of 'object' objects>), ('__format__', <method '__format__' of 'object' objects>), ('__ge__', <slot wrapper '__ge__' of 'object' objects>), ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>), ('__gt__', <slot wrapper '__gt__' of 'object' objects>), ('__hash__', <slot wrapper '__hash__' of 'object' objects>), ('__init__', <function ChildClass.__init__ at 0x2a7b945e0>), ('__init_subclass__', <built-in method __init_subclass__ of type object at 0x2a0b400d0>), ('__le__', <slot wrapper '__le__' of 'object' objects>), ('__lt__', <slot wrapper '__lt__' of 'object' objects>), ('__ne__', <slot wrapper '__ne__' of 'object' objects>), ('__new__', <built-in method __new__ of type object at 0x100bc6b00>), ('__reduce__', <method '__reduce__' of 'object' objects>), ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>), ('__repr__', <slot wrapper '__repr__' of 'object' objects>), ('__setattr__', <slot wrapper '__setattr__' of 'object' objects>), ('__sizeof__', <method '__sizeof__' of 'object' objects>), ('__str__', <function ChildClass.__str__ at 0x2a7b94820>), ('__subclasshook__', <built-in method __subclasshook__ of type object at 0x2a0b400d0>), ('class_method', <bound method ChildClass.class_method of <class '__main__.ChildClass'>>), ('instance_method', <function ChildClass.instance_method at 0x2a7b94670>), ('parent_method', <function ParentClass.parent_method at 0x2a7b94430>), ('static_method', <function ChildClass.static_method at 0x2a7b94790>)], '_i17': 'predicate = inspect.isroutine\\nGPT2LMHeadModel_methods_using_getmembers = list(get_members_of_function_or_method(GPT2LMHeadModel, predicate=predicate))\\n\\npprint(sorted(GPT2LMHeadModel_methods_using_getmembers))', 'GPT2LMHeadModel_methods_using_getmembers': [('__call__', <function Module._wrapped_call_impl at 0x11cdae1f0>), ('__delattr__', <function Module.__delattr__ at 0x11cdae550>), ('__dir__', <function Module.__dir__ at 0x11cdb2430>), ('__eq__', <slot wrapper '__eq__' of 'object' objects>), ('__format__', <method '__format__' of 'object' objects>), ('__ge__', <slot wrapper '__ge__' of 'object' objects>), ('__getattr__', <function Module.__getattr__ at 0x11cdae430>), ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>), ('__getstate__', <function Module.__getstate__ at 0x11cdae310>), ('__gt__', <slot wrapper '__gt__' of 'object' objects>), ('__hash__', <slot wrapper '__hash__' of 'object' objects>), ('__init__', <function GPT2LMHeadModel.__init__ at 0x2a7b925e0>), ('__init_subclass__', <built-in method __init_subclass__ of type object at 0x2a3e3a9d0>), ('__le__', <slot wrapper '__le__' of 'object' objects>), ('__lt__', <slot wrapper '__lt__' of 'object' objects>), ('__ne__', <slot wrapper '__ne__' of 'object' objects>), ('__new__', <built-in method __new__ of type object at 0x100bc6b00>), ('__reduce__', <method '__reduce__' of 'object' objects>), ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>), ('__repr__', <function Module.__repr__ at 0x11cdb23a0>), ('__setattr__', <function Module.__setattr__ at 0x11cdae4c0>), ('__setstate__', <function Module.__setstate__ at 0x11cdae3a0>), ('__sizeof__', <method '__sizeof__' of 'object' objects>), ('__str__', <slot wrapper '__str__' of 'object' objects>), ('__subclasshook__', <built-in method __subclasshook__ of type object at 0x2a3e3a9d0>), ('_apply', <function Module._apply at 0x11cdab550>), ('_autoset_attn_implementation', <bound method PreTrainedModel._autoset_attn_implementation of <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>>), ('_backward_compatibility_gradient_checkpointing', <function PreTrainedModel._backward_compatibility_gradient_checkpointing at 0x2a72b3670>), ('_call_impl', <function Module._call_impl at 0x11cdae280>), ('_check_and_enable_flash_attn_2', <bound method PreTrainedModel._check_and_enable_flash_attn_2 of <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>>), ('_check_and_enable_sdpa', <bound method PreTrainedModel._check_and_enable_sdpa of <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>>), ('_convert_head_mask_to_5d', <function ModuleUtilsMixin._convert_head_mask_to_5d at 0x2a72b31f0>), ('_copy_lm_head_original_to_resized', <function PreTrainedModel._copy_lm_head_original_to_resized at 0x2a72b6430>), ('_create_repo', <function PushToHubMixin._create_repo at 0x12cb2a9d0>), ('_dispatch_accelerate_model', <function PeftAdapterMixin._dispatch_accelerate_model at 0x2a7279e50>), ('_expand_inputs_for_generation', <function GenerationMixin._expand_inputs_for_generation at 0x2a727e430>), ('_extract_past_from_model_output', <function GenerationMixin._extract_past_from_model_output at 0x2a727e4c0>), ('_from_config', <bound method PreTrainedModel._from_config of <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>>), ('_get_backward_hooks', <function Module._get_backward_hooks at 0x11cdabe50>), ('_get_backward_pre_hooks', <function Module._get_backward_pre_hooks at 0x11cdabee0>), ('_get_candidate_generator', <function GenerationMixin._get_candidate_generator at 0x2a727e670>), ('_get_decoder_start_token_id', <function GenerationMixin._get_decoder_start_token_id at 0x2a727e3a0>), ('_get_files_timestamps', <function PushToHubMixin._get_files_timestamps at 0x12cb2aa60>), ('_get_generation_mode', <function GenerationMixin._get_generation_mode at 0x2a727e790>), ('_get_logits_processor', <function GenerationMixin._get_logits_processor at 0x2a727e820>), ('_get_logits_warper', <function GenerationMixin._get_logits_warper at 0x2a727e700>), ('_get_name', <function Module._get_name at 0x11cdb2280>), ('_get_no_split_modules', <function PreTrainedModel._get_no_split_modules at 0x2a72b6160>), ('_get_resized_embeddings', <function PreTrainedModel._get_resized_embeddings at 0x2a72b6310>), ('_get_resized_lm_head', <function PreTrainedModel._get_resized_lm_head at 0x2a72b63a0>), ('_get_stopping_criteria', <function GenerationMixin._get_stopping_criteria at 0x2a727e8b0>), ('_hook_rss_memory_post_forward', <function ModuleUtilsMixin._hook_rss_memory_post_forward at 0x2a72b1ca0>), ('_hook_rss_memory_pre_forward', <function ModuleUtilsMixin._hook_rss_memory_pre_forward at 0x2a72b1c10>), ('_init_weights', <function GPT2PreTrainedModel._init_weights at 0x2a7b8ee50>), ('_initialize_weights', <function PreTrainedModel._initialize_weights at 0x2a72b3ee0>), ('_load_from_state_dict', <function Module._load_from_state_dict at 0x11cdae940>), ('_load_pretrained_model', <bound method PreTrainedModel._load_pretrained_model of <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>>), ('_load_pretrained_model_low_mem', <function PreTrainedModel._load_pretrained_model_low_mem at 0x2a72b6ee0>), ('_maybe_initialize_input_ids_for_generation', <function GenerationMixin._maybe_initialize_input_ids_for_generation at 0x2a727e160>), ('_maybe_warn_non_full_backward_hook', <function Module._maybe_warn_non_full_backward_hook at 0x11cdabf70>), ('_merge_criteria_processor_list', <function GenerationMixin._merge_criteria_processor_list at 0x2a727e940>), ('_named_members', <function Module._named_members at 0x11cdaea60>), ('_prepare_attention_mask_for_generation', <function GenerationMixin._prepare_attention_mask_for_generation at 0x2a727e1f0>), ('_prepare_decoder_input_ids_for_generation', <function GenerationMixin._prepare_decoder_input_ids_for_generation at 0x2a727e310>), ('_prepare_encoder_decoder_kwargs_for_generation', <function GenerationMixin._prepare_encoder_decoder_kwargs_for_generation at 0x2a727e280>), ('_prepare_model_inputs', <function GenerationMixin._prepare_model_inputs at 0x2a727e0d0>), ('_register_load_state_dict_pre_hook', <function Module._register_load_state_dict_pre_hook at 0x11cdae820>), ('_register_state_dict_hook', <function Module._register_state_dict_hook at 0x11cdae5e0>), ('_reorder_cache', <function GPT2LMHeadModel._reorder_cache at 0x2a7b929d0>), ('_replicate_for_data_parallel', <function Module._replicate_for_data_parallel at 0x11cdb24c0>), ('_resize_token_embeddings', <function PreTrainedModel._resize_token_embeddings at 0x2a72b6280>), ('_save_to_state_dict', <function Module._save_to_state_dict at 0x11cdae700>), ('_set_default_torch_dtype', <bound method PreTrainedModel._set_default_torch_dtype of <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>>), ('_set_gradient_checkpointing', <function PreTrainedModel._set_gradient_checkpointing at 0x2a72b6790>), ('_slow_forward', <function Module._slow_forward at 0x11cdae160>), ('_temporary_reorder_cache', <function GenerationMixin._temporary_reorder_cache at 0x2a727ef70>), ('_tie_encoder_decoder_weights', <function PreTrainedModel._tie_encoder_decoder_weights at 0x2a72b6040>), ('_tie_or_clone_weights', <function PreTrainedModel._tie_or_clone_weights at 0x2a72b60d0>), ('_update_model_kwargs_for_generation', <function GenerationMixin._update_model_kwargs_for_generation at 0x2a727e550>), ('_upload_modified_files', <function PushToHubMixin._upload_modified_files at 0x12cb2aaf0>), ('_validate_generated_length', <function GenerationMixin._validate_generated_length at 0x2a727eb80>), ('_validate_model_class', <function GenerationMixin._validate_model_class at 0x2a727ea60>), ('_validate_model_kwargs', <function GenerationMixin._validate_model_kwargs at 0x2a727eaf0>), ('_wrapped_call_impl', <function Module._wrapped_call_impl at 0x11cdae1f0>), ('active_adapter', <function PeftAdapterMixin.active_adapter at 0x2a7279d30>), ('active_adapters', <function PeftAdapterMixin.active_adapters at 0x2a7279ca0>), ('add_adapter', <function PeftAdapterMixin.add_adapter at 0x2a7279a60>), ('add_memory_hooks', <function ModuleUtilsMixin.add_memory_hooks at 0x2a72b1d30>), ('add_model_tags', <function PreTrainedModel.add_model_tags at 0x2a72b3700>), ('add_module', <function Module.add_module at 0x11cdab160>), ('apply', <function Module.apply at 0x11cdab5e0>), ('assisted_decoding', <function GenerationMixin.assisted_decoding at 0x2a7279280>), ('beam_sample', <function GenerationMixin.beam_sample at 0x2a72790d0>), ('beam_search', <function GenerationMixin.beam_search at 0x2a7279040>), ('bfloat16', <function Module.bfloat16 at 0x11cdabaf0>), ('buffers', <function Module.buffers at 0x11cdaec10>), ('can_generate', <bound method PreTrainedModel.can_generate of <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>>), ('children', <function Module.children at 0x11cdaed30>), ('compile', <function Module.compile at 0x11cdb2550>), ('compute_transition_scores', <function GenerationMixin.compute_transition_scores at 0x2a727e9d0>), ('constrained_beam_search', <function GenerationMixin.constrained_beam_search at 0x2a72791f0>), ('contrastive_search', <function GenerationMixin.contrastive_search at 0x2a727edc0>), ('cpu', <function Module.cpu at 0x11cdab820>), ('create_extended_attention_mask_for_decoder', <function ModuleUtilsMixin.create_extended_attention_mask_for_decoder at 0x2a72b3040>), ('cuda', <function Module.cuda at 0x2a72b6af0>), ('deparallelize', <function GPT2LMHeadModel.deparallelize at 0x2a7b92820>), ('disable_adapters', <function PeftAdapterMixin.disable_adapters at 0x2a7279b80>), ('disable_input_require_grads', <function PreTrainedModel.disable_input_require_grads at 0x2a72b3c10>), ('double', <function Module.double at 0x11cdab9d0>), ('enable_adapters', <function PeftAdapterMixin.enable_adapters at 0x2a7279c10>), ('enable_input_require_grads', <function PreTrainedModel.enable_input_require_grads at 0x2a72b3b80>), ('estimate_tokens', <function ModuleUtilsMixin.estimate_tokens at 0x2a72b3310>), ('eval', <function Module.eval at 0x11cdb2040>), ('extra_repr', <function Module.extra_repr at 0x11cdb2310>), ('float', <function PreTrainedModel.float at 0x2a72b6ca0>), ('floating_point_ops', <function ModuleUtilsMixin.floating_point_ops at 0x2a72b33a0>), ('forward', <function GPT2LMHeadModel.forward at 0x2a7b92af0>), ('from_pretrained', <bound method PreTrainedModel.from_pretrained of <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>>), ('generate', <function GenerationMixin.generate at 0x2a727eca0>), ('get_adapter_state_dict', <function PeftAdapterMixin.get_adapter_state_dict at 0x2a7279dc0>), ('get_buffer', <function Module.get_buffer at 0x11cdab3a0>), ('get_extended_attention_mask', <function ModuleUtilsMixin.get_extended_attention_mask at 0x2a72b30d0>), ('get_extra_state', <function Module.get_extra_state at 0x11cdab430>), ('get_head_mask', <function ModuleUtilsMixin.get_head_mask at 0x2a72b3160>), ('get_input_embeddings', <function PreTrainedModel.get_input_embeddings at 0x2a72b3ca0>), ('get_memory_footprint', <function PreTrainedModel.get_memory_footprint at 0x2a72b6a60>), ('get_output_embeddings', <function GPT2LMHeadModel.get_output_embeddings at 0x2a7b92670>), ('get_parameter', <function Module.get_parameter at 0x11cdab310>), ('get_position_embeddings', <function PreTrainedModel.get_position_embeddings at 0x2a72b6550>), ('get_submodule', <function Module.get_submodule at 0x11cdab280>), ('gradient_checkpointing_disable', <function PreTrainedModel.gradient_checkpointing_disable at 0x2a72b6820>), ('gradient_checkpointing_enable', <function PreTrainedModel.gradient_checkpointing_enable at 0x2a72b6700>), ('greedy_search', <function GenerationMixin.greedy_search at 0x2a727ee50>), ('group_beam_search', <function GenerationMixin.group_beam_search at 0x2a7279160>), ('half', <function PreTrainedModel.half at 0x2a72b6c10>), ('init_weights', <function PreTrainedModel.init_weights at 0x2a72b65e0>), ('invert_attention_mask', <function ModuleUtilsMixin.invert_attention_mask at 0x2a72b1f70>), ('ipu', <function Module.ipu at 0x11cdab700>), ('load_adapter', <function PeftAdapterMixin.load_adapter at 0x2a72799d0>), ('load_state_dict', <function Module.load_state_dict at 0x11cdae9d0>), ('load_tf_weights', <function load_tf_weights_in_gpt2 at 0x2a7b62f70>), ('modules', <function Module.modules at 0x11cdaee50>), ('named_buffers', <function Module.named_buffers at 0x11cdaeca0>), ('named_children', <function Module.named_children at 0x11cdaedc0>), ('named_modules', <function Module.named_modules at 0x11cdaeee0>), ('named_parameters', <function Module.named_parameters at 0x11cdaeb80>), ('num_parameters', <function ModuleUtilsMixin.num_parameters at 0x2a72b3280>), ('parallelize', <function GPT2LMHeadModel.parallelize at 0x2a7b92790>), ('parameters', <function Module.parameters at 0x11cdaeaf0>), ('post_init', <function PreTrainedModel.post_init at 0x2a72b35e0>), ('prepare_inputs_for_generation', <function GPT2LMHeadModel.prepare_inputs_for_generation at 0x2a7b92940>), ('prune_heads', <function PreTrainedModel.prune_heads at 0x2a72b6670>), ('push_to_hub', <function PushToHubMixin.push_to_hub at 0x2a72b1b80>), ('register_backward_hook', <function Module.register_backward_hook at 0x11cdabd30>), ('register_buffer', <function Module.register_buffer at 0x11cdab040>), ('register_for_auto_class', <bound method PreTrainedModel.register_for_auto_class of <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>>), ('register_forward_hook', <function Module.register_forward_hook at 0x11cdae0d0>), ('register_forward_pre_hook', <function Module.register_forward_pre_hook at 0x11cdae040>), ('register_full_backward_hook', <function Module.register_full_backward_hook at 0x11cdabdc0>), ('register_full_backward_pre_hook', <function Module.register_full_backward_pre_hook at 0x11cdabca0>), ('register_load_state_dict_post_hook', <function Module.register_load_state_dict_post_hook at 0x11cdae8b0>), ('register_module', <function Module.register_module at 0x11cdab1f0>), ('register_parameter', <function Module.register_parameter at 0x11cdab0d0>), ('register_state_dict_pre_hook', <function Module.register_state_dict_pre_hook at 0x11cdae670>), ('requires_grad_', <function Module.requires_grad_ at 0x11cdb20d0>), ('reset_memory_hooks_state', <function ModuleUtilsMixin.reset_memory_hooks_state at 0x2a72b1dc0>), ('resize_position_embeddings', <function PreTrainedModel.resize_position_embeddings at 0x2a72b64c0>), ('resize_token_embeddings', <function PreTrainedModel.resize_token_embeddings at 0x2a72b61f0>), ('retrieve_modules_from_names', <function PreTrainedModel.retrieve_modules_from_names at 0x2a72b6e50>), ('reverse_bettertransformer', <function PreTrainedModel.reverse_bettertransformer at 0x2a72b90d0>), ('sample', <function GenerationMixin.sample at 0x2a727eee0>), ('save_pretrained', <function PreTrainedModel.save_pretrained at 0x2a72b6940>), ('set_adapter', <function PeftAdapterMixin.set_adapter at 0x2a7279af0>), ('set_extra_state', <function Module.set_extra_state at 0x11cdab4c0>), ('set_input_embeddings', <function PreTrainedModel.set_input_embeddings at 0x2a72b3d30>), ('set_output_embeddings', <function GPT2LMHeadModel.set_output_embeddings at 0x2a7b928b0>), ('share_memory', <function Module.share_memory at 0x11cdb21f0>), ('state_dict', <function Module.state_dict at 0x11cdae790>), ('tie_weights', <function PreTrainedModel.tie_weights at 0x2a72b3f70>), ('to', <function Module.to at 0x2a72b6b80>), ('to_bettertransformer', <function PreTrainedModel.to_bettertransformer at 0x2a72b9040>), ('to_empty', <function Module.to_empty at 0x11cdabb80>), ('train', <function Module.train at 0x11cdaef70>), ('type', <function Module.type at 0x11cdab8b0>), ('warn_if_padding_and_no_attention_mask', <function PreTrainedModel.warn_if_padding_and_no_attention_mask at 0x2a72b9160>), ('xpu', <function Module.xpu at 0x11cdab790>), ('zero_grad', <function Module.zero_grad at 0x11cdb2160>)], '_i18': 'inspect.getmro(GPT2LMHeadModel)', '_18': (<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel'>, <class 'transformers.modeling_utils.PreTrainedModel'>, <class 'torch.nn.modules.module.Module'>, <class 'transformers.modeling_utils.ModuleUtilsMixin'>, <class 'transformers.generation.utils.GenerationMixin'>, <class 'transformers.utils.hub.PushToHubMixin'>, <class 'transformers.integrations.peft.PeftAdapterMixin'>, <class 'object'>), '_i19': 'func_sig: Signature = inspect.signature(func)\\npprint(func_sig.parameters)\\npprint(func_sig.return_annotation)', '__annotations__': {'func_sig': <class 'inspect.Signature'>}, 'func_sig': <Signature (a: int, b: str, c: List[int], d: Tuple[str, str], e: Union[int, str], **kwargs: Any) -> str>, '_i20': 'ChildClass.__bases__, GPT2LMHeadModel.__bases__[0].__bases__[0].__bases__', '_20': ((<class '__main__.ParentClass'>,), (<class 'torch.nn.modules.module.Module'>, <class 'transformers.modeling_utils.ModuleUtilsMixin'>, <class 'transformers.generation.utils.GenerationMixin'>, <class 'transformers.utils.hub.PushToHubMixin'>, <class 'transformers.integrations.peft.PeftAdapterMixin'>)), '_i21': 'def get_base_classes(cls: Type[Any], include_self: bool = False) -> Set[Type[Any]]:\\n    \"\"\"\\n    Get the base classes of a class and all its base classes.\\n    \"\"\"\\n    return set(cls.__mro__[0:-1] if include_self else cls.__mro__[1:-1])\\n\\npprint(get_base_classes(GPT2LMHeadModel, include_self=True))', 'get_base_classes': <function get_base_classes at 0x2a7d5b820>, '_i22': 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\n\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True) -> Dict[str, Any]:\\n    namespace: Dict[str, Any] = {}\\n    # Use the helper function to get all base classes if include_bases is True\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in classes_to_inspect:\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n                namespace[name] = (get_default(param), type_hints.get(name))\\n\\n    ConfigClass = type(f\"{cls.__name__}Config\", (object,), namespace)\\n    return namespace, ConfigClass\\n\\nnamespace, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(namespace)\\npprint(config)', 'get_default': <function get_default at 0x2a927d280>, 'get_init_arg_signatures': <function get_init_arg_signatures at 0x29f1643a0>, 'namespace': {'output_dir': <class 'str'>, 'overwrite_output_dir': <class 'bool'>, 'do_train': <class 'bool'>, 'do_eval': <class 'bool'>, 'do_predict': <class 'bool'>, 'evaluation_strategy': typing.Union[transformers.trainer_utils.IntervalStrategy, str], 'prediction_loss_only': <class 'bool'>, 'per_device_train_batch_size': <class 'int'>, 'per_device_eval_batch_size': <class 'int'>, 'per_gpu_train_batch_size': typing.Optional[int], 'per_gpu_eval_batch_size': typing.Optional[int], 'gradient_accumulation_steps': <class 'int'>, 'eval_accumulation_steps': typing.Optional[int], 'eval_delay': typing.Optional[float], 'learning_rate': <class 'float'>, 'weight_decay': <class 'float'>, 'adam_beta1': <class 'float'>, 'adam_beta2': <class 'float'>, 'adam_epsilon': <class 'float'>, 'max_grad_norm': <class 'float'>, 'num_train_epochs': <class 'float'>, 'max_steps': <class 'int'>, 'lr_scheduler_type': typing.Union[transformers.trainer_utils.SchedulerType, str], 'lr_scheduler_kwargs': typing.Optional[typing.Dict], 'warmup_ratio': <class 'float'>, 'warmup_steps': <class 'int'>, 'log_level': typing.Optional[str], 'log_level_replica': typing.Optional[str], 'log_on_each_node': <class 'bool'>, 'logging_dir': typing.Optional[str], 'logging_strategy': typing.Union[transformers.trainer_utils.IntervalStrategy, str], 'logging_first_step': <class 'bool'>, 'logging_steps': <class 'float'>, 'logging_nan_inf_filter': <class 'bool'>, 'save_strategy': typing.Union[transformers.trainer_utils.IntervalStrategy, str], 'save_steps': <class 'float'>, 'save_total_limit': typing.Optional[int], 'save_safetensors': typing.Optional[bool], 'save_on_each_node': <class 'bool'>, 'save_only_model': <class 'bool'>, 'no_cuda': <class 'bool'>, 'use_cpu': <class 'bool'>, 'use_mps_device': <class 'bool'>, 'seed': <class 'int'>, 'data_seed': typing.Optional[int], 'jit_mode_eval': <class 'bool'>, 'use_ipex': <class 'bool'>, 'bf16': <class 'bool'>, 'fp16': <class 'bool'>, 'fp16_opt_level': <class 'str'>, 'half_precision_backend': <class 'str'>, 'bf16_full_eval': <class 'bool'>, 'fp16_full_eval': <class 'bool'>, 'tf32': typing.Optional[bool], 'local_rank': <class 'int'>, 'ddp_backend': typing.Optional[str], 'tpu_num_cores': typing.Optional[int], 'tpu_metrics_debug': <class 'bool'>, 'debug': typing.Union[str, typing.List[transformers.debug_utils.DebugOption]], 'dataloader_drop_last': <class 'bool'>, 'eval_steps': typing.Optional[float], 'dataloader_num_workers': <class 'int'>, 'dataloader_prefetch_factor': typing.Optional[int], 'past_index': <class 'int'>, 'run_name': typing.Optional[str], 'disable_tqdm': typing.Optional[bool], 'remove_unused_columns': typing.Optional[bool], 'label_names': typing.Optional[typing.List[str]], 'load_best_model_at_end': typing.Optional[bool], 'metric_for_best_model': typing.Optional[str], 'greater_is_better': typing.Optional[bool], 'ignore_data_skip': <class 'bool'>, 'fsdp': typing.Union[typing.List[transformers.trainer_utils.FSDPOption], str, NoneType], 'fsdp_min_num_params': <class 'int'>, 'fsdp_config': typing.Union[dict, str, NoneType], 'fsdp_transformer_layer_cls_to_wrap': typing.Optional[str], 'accelerator_config': typing.Optional[str], 'deepspeed': typing.Optional[str], 'label_smoothing_factor': <class 'float'>, 'optim': typing.Union[transformers.training_args.OptimizerNames, str], 'optim_args': typing.Optional[str], 'adafactor': <class 'bool'>, 'group_by_length': <class 'bool'>, 'length_column_name': typing.Optional[str], 'report_to': typing.Optional[typing.List[str]], 'ddp_find_unused_parameters': typing.Optional[bool], 'ddp_bucket_cap_mb': typing.Optional[int], 'ddp_broadcast_buffers': typing.Optional[bool], 'dataloader_pin_memory': <class 'bool'>, 'dataloader_persistent_workers': <class 'bool'>, 'skip_memory_metrics': <class 'bool'>, 'use_legacy_prediction_loop': <class 'bool'>, 'push_to_hub': <class 'bool'>, 'resume_from_checkpoint': typing.Optional[str], 'hub_model_id': typing.Optional[str], 'hub_strategy': typing.Union[transformers.trainer_utils.HubStrategy, str], 'hub_token': typing.Optional[str], 'hub_private_repo': <class 'bool'>, 'hub_always_push': <class 'bool'>, 'gradient_checkpointing': <class 'bool'>, 'gradient_checkpointing_kwargs': typing.Optional[dict], 'include_inputs_for_metrics': <class 'bool'>, 'fp16_backend': <class 'str'>, 'push_to_hub_model_id': typing.Optional[str], 'push_to_hub_organization': typing.Optional[str], 'push_to_hub_token': typing.Optional[str], 'mp_parameters': <class 'str'>, 'auto_find_batch_size': <class 'bool'>, 'full_determinism': <class 'bool'>, 'torchdynamo': typing.Optional[str], 'ray_scope': typing.Optional[str], 'ddp_timeout': typing.Optional[int], 'torch_compile': <class 'bool'>, 'torch_compile_backend': typing.Optional[str], 'torch_compile_mode': typing.Optional[str], 'dispatch_batches': typing.Optional[bool], 'split_batches': typing.Optional[bool], 'include_tokens_per_second': typing.Optional[bool], 'include_num_input_tokens_seen': typing.Optional[bool], 'neftune_noise_alpha': typing.Optional[float]}, 'config': <class '__main__.TrainingArgumentsConfig'>, '_i23': 'get_type_hints(get_type_hints)', '_23': {}, '_i24': 'get_type_hints(func)', '_24': {'a': <class 'int'>, 'b': <class 'str'>, 'c': typing.List[int], 'd': typing.Tuple[str, str], 'e': typing.Union[int, str], 'kwargs': typing.Any, 'return': <class 'str'>}, '_i25': 'get_type_hints(Trainer)', '_25': {}, '_i26': 'get_type_hints(Trainer.__init__)', '_26': {'model': typing.Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module, NoneType], 'args': typing.Optional[transformers.training_args.TrainingArguments], 'data_collator': typing.Optional[DataCollator], 'train_dataset': typing.Optional[torch.utils.data.dataset.Dataset], 'eval_dataset': typing.Union[torch.utils.data.dataset.Dataset, typing.Dict[str, torch.utils.data.dataset.Dataset], NoneType], 'tokenizer': typing.Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase], 'model_init': typing.Optional[typing.Callable[[], transformers.modeling_utils.PreTrainedModel]], 'compute_metrics': typing.Optional[typing.Callable[[transformers.trainer_utils.EvalPrediction], typing.Dict]], 'callbacks': typing.Optional[typing.List[transformers.trainer_callback.TrainerCallback]], 'optimizers': typing.Tuple[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR], 'preprocess_logits_for_metrics': typing.Optional[typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor]]}, '_i27': 'get_type_hints(func)', '_27': {'a': <class 'int'>, 'b': <class 'str'>, 'c': typing.List[int], 'd': typing.Tuple[str, str], 'e': typing.Union[int, str], 'kwargs': typing.Any, 'return': <class 'str'>}, '_i28': 'get_type_hints(no_type_hints)', '_i29': 'def no_type_hints(a, b, c, d, e, **kwargs):\\n    return a, b, c, d, e, kwargs', 'no_type_hints': <function no_type_hints at 0x2a927d5e0>, '_i30': 'get_type_hints(no_type_hints)', '_30': {}, '_i31': 'get_type_hints(no_type_hints), inspect.signature(no_type_hints)', '_31': ({}, <Signature (a, b, c, d, e, **kwargs)>), '_i32': \"for name, value in inspect.getmembers(func):\\n    print(f'{name}: {value}')\", 'name': 'kwargs', 'value': <Parameter \"**kwargs: Any\">, '_i33': 'for name, value in inspect.signature(func).parameters.items():\\n    print(name, value.annotation)', '_i34': 'for name, value in inspect.signature(func).parameters.items():\\n    print(value.default)', '_i35': 'for name, value in inspect.signature(func).parameters.items():\\n    print(value.default)\\n    print(value.default is inspect.Parameter.empty)', '_i36': 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, field(default=default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    ConfigClass = make_dataclass(f\"{cls.__name__}Config\", fields)\\n    ConfigClass.__annotations__.update(annotations)\\n    return fields, ConfigClass\\n\\nfields, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)\\npprint(config)', 'fields': [('output_dir', <class 'str'>, Ellipsis), ('overwrite_output_dir', <class 'bool'>, False), ('do_train', <class 'bool'>, False), ('do_eval', <class 'bool'>, False), ('do_predict', <class 'bool'>, False), ('evaluation_strategy', typing.Union[transformers.trainer_utils.IntervalStrategy, str], 'no'), ('prediction_loss_only', <class 'bool'>, False), ('per_device_train_batch_size', <class 'int'>, 8), ('per_device_eval_batch_size', <class 'int'>, 8), ('per_gpu_train_batch_size', typing.Optional[int], None), ('per_gpu_eval_batch_size', typing.Optional[int], None), ('gradient_accumulation_steps', <class 'int'>, 1), ('eval_accumulation_steps', typing.Optional[int], None), ('eval_delay', typing.Optional[float], 0), ('learning_rate', <class 'float'>, 5e-05), ('weight_decay', <class 'float'>, 0.0), ('adam_beta1', <class 'float'>, 0.9), ('adam_beta2', <class 'float'>, 0.999), ('adam_epsilon', <class 'float'>, 1e-08), ('max_grad_norm', <class 'float'>, 1.0), ('num_train_epochs', <class 'float'>, 3.0), ('max_steps', <class 'int'>, -1), ('lr_scheduler_type', typing.Union[transformers.trainer_utils.SchedulerType, str], 'linear'), ('lr_scheduler_kwargs', typing.Optional[typing.Dict], <factory>), ('warmup_ratio', <class 'float'>, 0.0), ('warmup_steps', <class 'int'>, 0), ('log_level', typing.Optional[str], 'passive'), ('log_level_replica', typing.Optional[str], 'warning'), ('log_on_each_node', <class 'bool'>, True), ('logging_dir', typing.Optional[str], None), ('logging_strategy', typing.Union[transformers.trainer_utils.IntervalStrategy, str], 'steps'), ('logging_first_step', <class 'bool'>, False), ('logging_steps', <class 'float'>, 500), ('logging_nan_inf_filter', <class 'bool'>, True), ('save_strategy', typing.Union[transformers.trainer_utils.IntervalStrategy, str], 'steps'), ('save_steps', <class 'float'>, 500), ('save_total_limit', typing.Optional[int], None), ('save_safetensors', typing.Optional[bool], True), ('save_on_each_node', <class 'bool'>, False), ('save_only_model', <class 'bool'>, False), ('no_cuda', <class 'bool'>, False), ('use_cpu', <class 'bool'>, False), ('use_mps_device', <class 'bool'>, False), ('seed', <class 'int'>, 42), ('data_seed', typing.Optional[int], None), ('jit_mode_eval', <class 'bool'>, False), ('use_ipex', <class 'bool'>, False), ('bf16', <class 'bool'>, False), ('fp16', <class 'bool'>, False), ('fp16_opt_level', <class 'str'>, 'O1'), ('half_precision_backend', <class 'str'>, 'auto'), ('bf16_full_eval', <class 'bool'>, False), ('fp16_full_eval', <class 'bool'>, False), ('tf32', typing.Optional[bool], None), ('local_rank', <class 'int'>, -1), ('ddp_backend', typing.Optional[str], None), ('tpu_num_cores', typing.Optional[int], None), ('tpu_metrics_debug', <class 'bool'>, False), ('debug', typing.Union[str, typing.List[transformers.debug_utils.DebugOption]], ''), ('dataloader_drop_last', <class 'bool'>, False), ('eval_steps', typing.Optional[float], None), ('dataloader_num_workers', <class 'int'>, 0), ('dataloader_prefetch_factor', typing.Optional[int], None), ('past_index', <class 'int'>, -1), ('run_name', typing.Optional[str], None), ('disable_tqdm', typing.Optional[bool], None), ('remove_unused_columns', typing.Optional[bool], True), ('label_names', typing.Optional[typing.List[str]], None), ('load_best_model_at_end', typing.Optional[bool], False), ('metric_for_best_model', typing.Optional[str], None), ('greater_is_better', typing.Optional[bool], None), ('ignore_data_skip', <class 'bool'>, False), ('fsdp', typing.Union[typing.List[transformers.trainer_utils.FSDPOption], str, NoneType], ''), ('fsdp_min_num_params', <class 'int'>, 0), ('fsdp_config', typing.Union[dict, str, NoneType], None), ('fsdp_transformer_layer_cls_to_wrap', typing.Optional[str], None), ('accelerator_config', typing.Optional[str], None), ('deepspeed', typing.Optional[str], None), ('label_smoothing_factor', <class 'float'>, 0.0), ('optim', typing.Union[transformers.training_args.OptimizerNames, str], 'adamw_torch'), ('optim_args', typing.Optional[str], None), ('adafactor', <class 'bool'>, False), ('group_by_length', <class 'bool'>, False), ('length_column_name', typing.Optional[str], 'length'), ('report_to', typing.Optional[typing.List[str]], None), ('ddp_find_unused_parameters', typing.Optional[bool], None), ('ddp_bucket_cap_mb', typing.Optional[int], None), ('ddp_broadcast_buffers', typing.Optional[bool], None), ('dataloader_pin_memory', <class 'bool'>, True), ('dataloader_persistent_workers', <class 'bool'>, False), ('skip_memory_metrics', <class 'bool'>, True), ('use_legacy_prediction_loop', <class 'bool'>, False), ('push_to_hub', <class 'bool'>, False), ('resume_from_checkpoint', typing.Optional[str], None), ('hub_model_id', typing.Optional[str], None), ('hub_strategy', typing.Union[transformers.trainer_utils.HubStrategy, str], 'every_save'), ('hub_token', typing.Optional[str], None), ('hub_private_repo', <class 'bool'>, False), ('hub_always_push', <class 'bool'>, False), ('gradient_checkpointing', <class 'bool'>, False), ('gradient_checkpointing_kwargs', typing.Optional[dict], None), ('include_inputs_for_metrics', <class 'bool'>, False), ('fp16_backend', <class 'str'>, 'auto'), ('push_to_hub_model_id', typing.Optional[str], None), ('push_to_hub_organization', typing.Optional[str], None), ('push_to_hub_token', typing.Optional[str], None), ('mp_parameters', <class 'str'>, ''), ('auto_find_batch_size', <class 'bool'>, False), ('full_determinism', <class 'bool'>, False), ('torchdynamo', typing.Optional[str], None), ('ray_scope', typing.Optional[str], 'last'), ('ddp_timeout', typing.Optional[int], 1800), ('torch_compile', <class 'bool'>, False), ('torch_compile_backend', typing.Optional[str], None), ('torch_compile_mode', typing.Optional[str], None), ('dispatch_batches', typing.Optional[bool], None), ('split_batches', typing.Optional[bool], None), ('include_tokens_per_second', typing.Optional[bool], False), ('include_num_input_tokens_seen', typing.Optional[bool], False), ('neftune_noise_alpha', typing.Optional[float], None)], '_i37': 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, field(default=default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    ConfigClass = make_dataclass(f\"{cls.__name__}Config\", fields)\\n    ConfigClass.__annotations__.update(annotations)\\n    return fields, ConfigClass\\n\\nfields, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\n# pprint(fields)\\npprint(config)', '_i38': 'type(config)', '_38': <class 'type'>, '_i39': 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, field(default=default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    ConfigClass = make_dataclass(f\"{cls.__name__}Config\", fields)\\n    # ConfigClass.__annotations__.update(annotations)\\n    return fields, ConfigClass\\n\\nfields, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\n# pprint(fields)\\npprint(config)', '_i40': 'type(config)', '_40': <class 'type'>, '_i41': \"from dataclasses import dataclass\\n\\n@dataclass\\nclass MyClass:\\n    a: int\\n    b: str\\n\\n# Using make_dataclass\\nAnotherClass = make_dataclass('AnotherClass', [('c', int), ('d', str)])\\n\\n# Checking their types\\nprint(type(MyClass))  # <class 'type'>\\nprint(type(AnotherClass))  # <class 'type'>\", 'dataclass': <function dataclass at 0x1026fcc10>, 'MyClass': <class '__main__.MyClass'>, 'AnotherClass': <class 'types.AnotherClass'>, '_i42': 'inspect.signature(config).parameters', '_42': mappingproxy(OrderedDict([('output_dir', <Parameter \"output_dir: str\">), ('overwrite_output_dir', <Parameter \"overwrite_output_dir: bool = False\">), ('do_train', <Parameter \"do_train: bool = False\">), ('do_eval', <Parameter \"do_eval: bool = False\">), ('do_predict', <Parameter \"do_predict: bool = False\">), ('evaluation_strategy', <Parameter \"evaluation_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no'\">), ('prediction_loss_only', <Parameter \"prediction_loss_only: bool = False\">), ('per_device_train_batch_size', <Parameter \"per_device_train_batch_size: int = 8\">), ('per_device_eval_batch_size', <Parameter \"per_device_eval_batch_size: int = 8\">), ('per_gpu_train_batch_size', <Parameter \"per_gpu_train_batch_size: Optional[int] = None\">), ('per_gpu_eval_batch_size', <Parameter \"per_gpu_eval_batch_size: Optional[int] = None\">), ('gradient_accumulation_steps', <Parameter \"gradient_accumulation_steps: int = 1\">), ('eval_accumulation_steps', <Parameter \"eval_accumulation_steps: Optional[int] = None\">), ('eval_delay', <Parameter \"eval_delay: Optional[float] = 0\">), ('learning_rate', <Parameter \"learning_rate: float = 5e-05\">), ('weight_decay', <Parameter \"weight_decay: float = 0.0\">), ('adam_beta1', <Parameter \"adam_beta1: float = 0.9\">), ('adam_beta2', <Parameter \"adam_beta2: float = 0.999\">), ('adam_epsilon', <Parameter \"adam_epsilon: float = 1e-08\">), ('max_grad_norm', <Parameter \"max_grad_norm: float = 1.0\">), ('num_train_epochs', <Parameter \"num_train_epochs: float = 3.0\">), ('max_steps', <Parameter \"max_steps: int = -1\">), ('lr_scheduler_type', <Parameter \"lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear'\">), ('lr_scheduler_kwargs', <Parameter \"lr_scheduler_kwargs: Optional[Dict] = <factory>\">), ('warmup_ratio', <Parameter \"warmup_ratio: float = 0.0\">), ('warmup_steps', <Parameter \"warmup_steps: int = 0\">), ('log_level', <Parameter \"log_level: Optional[str] = 'passive'\">), ('log_level_replica', <Parameter \"log_level_replica: Optional[str] = 'warning'\">), ('log_on_each_node', <Parameter \"log_on_each_node: bool = True\">), ('logging_dir', <Parameter \"logging_dir: Optional[str] = None\">), ('logging_strategy', <Parameter \"logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'\">), ('logging_first_step', <Parameter \"logging_first_step: bool = False\">), ('logging_steps', <Parameter \"logging_steps: float = 500\">), ('logging_nan_inf_filter', <Parameter \"logging_nan_inf_filter: bool = True\">), ('save_strategy', <Parameter \"save_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'\">), ('save_steps', <Parameter \"save_steps: float = 500\">), ('save_total_limit', <Parameter \"save_total_limit: Optional[int] = None\">), ('save_safetensors', <Parameter \"save_safetensors: Optional[bool] = True\">), ('save_on_each_node', <Parameter \"save_on_each_node: bool = False\">), ('save_only_model', <Parameter \"save_only_model: bool = False\">), ('no_cuda', <Parameter \"no_cuda: bool = False\">), ('use_cpu', <Parameter \"use_cpu: bool = False\">), ('use_mps_device', <Parameter \"use_mps_device: bool = False\">), ('seed', <Parameter \"seed: int = 42\">), ('data_seed', <Parameter \"data_seed: Optional[int] = None\">), ('jit_mode_eval', <Parameter \"jit_mode_eval: bool = False\">), ('use_ipex', <Parameter \"use_ipex: bool = False\">), ('bf16', <Parameter \"bf16: bool = False\">), ('fp16', <Parameter \"fp16: bool = False\">), ('fp16_opt_level', <Parameter \"fp16_opt_level: str = 'O1'\">), ('half_precision_backend', <Parameter \"half_precision_backend: str = 'auto'\">), ('bf16_full_eval', <Parameter \"bf16_full_eval: bool = False\">), ('fp16_full_eval', <Parameter \"fp16_full_eval: bool = False\">), ('tf32', <Parameter \"tf32: Optional[bool] = None\">), ('local_rank', <Parameter \"local_rank: int = -1\">), ('ddp_backend', <Parameter \"ddp_backend: Optional[str] = None\">), ('tpu_num_cores', <Parameter \"tpu_num_cores: Optional[int] = None\">), ('tpu_metrics_debug', <Parameter \"tpu_metrics_debug: bool = False\">), ('debug', <Parameter \"debug: Union[str, List[transformers.debug_utils.DebugOption]] = ''\">), ('dataloader_drop_last', <Parameter \"dataloader_drop_last: bool = False\">), ('eval_steps', <Parameter \"eval_steps: Optional[float] = None\">), ('dataloader_num_workers', <Parameter \"dataloader_num_workers: int = 0\">), ('dataloader_prefetch_factor', <Parameter \"dataloader_prefetch_factor: Optional[int] = None\">), ('past_index', <Parameter \"past_index: int = -1\">), ('run_name', <Parameter \"run_name: Optional[str] = None\">), ('disable_tqdm', <Parameter \"disable_tqdm: Optional[bool] = None\">), ('remove_unused_columns', <Parameter \"remove_unused_columns: Optional[bool] = True\">), ('label_names', <Parameter \"label_names: Optional[List[str]] = None\">), ('load_best_model_at_end', <Parameter \"load_best_model_at_end: Optional[bool] = False\">), ('metric_for_best_model', <Parameter \"metric_for_best_model: Optional[str] = None\">), ('greater_is_better', <Parameter \"greater_is_better: Optional[bool] = None\">), ('ignore_data_skip', <Parameter \"ignore_data_skip: bool = False\">), ('fsdp', <Parameter \"fsdp: Union[List[transformers.trainer_utils.FSDPOption], str, NoneType] = ''\">), ('fsdp_min_num_params', <Parameter \"fsdp_min_num_params: int = 0\">), ('fsdp_config', <Parameter \"fsdp_config: Union[dict, str, NoneType] = None\">), ('fsdp_transformer_layer_cls_to_wrap', <Parameter \"fsdp_transformer_layer_cls_to_wrap: Optional[str] = None\">), ('accelerator_config', <Parameter \"accelerator_config: Optional[str] = None\">), ('deepspeed', <Parameter \"deepspeed: Optional[str] = None\">), ('label_smoothing_factor', <Parameter \"label_smoothing_factor: float = 0.0\">), ('optim', <Parameter \"optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch'\">), ('optim_args', <Parameter \"optim_args: Optional[str] = None\">), ('adafactor', <Parameter \"adafactor: bool = False\">), ('group_by_length', <Parameter \"group_by_length: bool = False\">), ('length_column_name', <Parameter \"length_column_name: Optional[str] = 'length'\">), ('report_to', <Parameter \"report_to: Optional[List[str]] = None\">), ('ddp_find_unused_parameters', <Parameter \"ddp_find_unused_parameters: Optional[bool] = None\">), ('ddp_bucket_cap_mb', <Parameter \"ddp_bucket_cap_mb: Optional[int] = None\">), ('ddp_broadcast_buffers', <Parameter \"ddp_broadcast_buffers: Optional[bool] = None\">), ('dataloader_pin_memory', <Parameter \"dataloader_pin_memory: bool = True\">), ('dataloader_persistent_workers', <Parameter \"dataloader_persistent_workers: bool = False\">), ('skip_memory_metrics', <Parameter \"skip_memory_metrics: bool = True\">), ('use_legacy_prediction_loop', <Parameter \"use_legacy_prediction_loop: bool = False\">), ('push_to_hub', <Parameter \"push_to_hub: bool = False\">), ('resume_from_checkpoint', <Parameter \"resume_from_checkpoint: Optional[str] = None\">), ('hub_model_id', <Parameter \"hub_model_id: Optional[str] = None\">), ('hub_strategy', <Parameter \"hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save'\">), ('hub_token', <Parameter \"hub_token: Optional[str] = None\">), ('hub_private_repo', <Parameter \"hub_private_repo: bool = False\">), ('hub_always_push', <Parameter \"hub_always_push: bool = False\">), ('gradient_checkpointing', <Parameter \"gradient_checkpointing: bool = False\">), ('gradient_checkpointing_kwargs', <Parameter \"gradient_checkpointing_kwargs: Optional[dict] = None\">), ('include_inputs_for_metrics', <Parameter \"include_inputs_for_metrics: bool = False\">), ('fp16_backend', <Parameter \"fp16_backend: str = 'auto'\">), ('push_to_hub_model_id', <Parameter \"push_to_hub_model_id: Optional[str] = None\">), ('push_to_hub_organization', <Parameter \"push_to_hub_organization: Optional[str] = None\">), ('push_to_hub_token', <Parameter \"push_to_hub_token: Optional[str] = None\">), ('mp_parameters', <Parameter \"mp_parameters: str = ''\">), ('auto_find_batch_size', <Parameter \"auto_find_batch_size: bool = False\">), ('full_determinism', <Parameter \"full_determinism: bool = False\">), ('torchdynamo', <Parameter \"torchdynamo: Optional[str] = None\">), ('ray_scope', <Parameter \"ray_scope: Optional[str] = 'last'\">), ('ddp_timeout', <Parameter \"ddp_timeout: Optional[int] = 1800\">), ('torch_compile', <Parameter \"torch_compile: bool = False\">), ('torch_compile_backend', <Parameter \"torch_compile_backend: Optional[str] = None\">), ('torch_compile_mode', <Parameter \"torch_compile_mode: Optional[str] = None\">), ('dispatch_batches', <Parameter \"dispatch_batches: Optional[bool] = None\">), ('split_batches', <Parameter \"split_batches: Optional[bool] = None\">), ('include_tokens_per_second', <Parameter \"include_tokens_per_second: Optional[bool] = False\">), ('include_num_input_tokens_seen', <Parameter \"include_num_input_tokens_seen: Optional[bool] = False\">), ('neftune_noise_alpha', <Parameter \"neftune_noise_alpha: Optional[float] = None\">)])), '_i43': 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, field(default=default_value)))\\n\\n    fields = required_fields + optional_fields\\n    namespace = {name: annotation for name, annotation in annotations.items()}\\n\\n    if as_dataclass:\\n        ConfigClass = make_dataclass(f\"{cls.__name__}Config\", fields)\\n    else:\\n        # Create a normal class, manually setting __annotations__ and defaults\\n        ConfigClass = type(f\"{cls.__name__}Config\", (object,), namespace)\\n        for name, _, default in optional_fields:\\n            setattr(ConfigClass, name, default)\\n        for name, _ in required_fields:\\n            if name not in namespace:  # Avoid overwriting defaults\\n                setattr(ConfigClass, name, None)\\n\\n    ConfigClass.__annotations__ = annotations\\n    return namespace, fields, ConfigClass\\n\\nnamespace, fields, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(namespace)\\npprint(config)', '_i44': 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, field(default=default_value)))\\n\\n    fields = required_fields + optional_fields\\n    namespace = {name: annotation for name, annotation in annotations.items()}\\n\\n    if as_dataclass:\\n        ConfigClass = make_dataclass(f\"{cls.__name__}Config\", fields)\\n    else:\\n        # Create a normal class, manually setting __annotations__ and defaults\\n        ConfigClass = type(f\"{cls.__name__}Config\", (object,), namespace)\\n        for name, _, default in optional_fields:\\n            setattr(ConfigClass, name, default)\\n        for name, _ in required_fields:\\n            if name not in namespace:  # Avoid overwriting defaults\\n                setattr(ConfigClass, name, None)\\n\\n    ConfigClass.__annotations__ = annotations\\n\\n    if as_dataclass:\\n        return namespace, fields, ConfigClass\\n\\n\\n    fields = [(name, annotation) for name, annotation, _ in fields]\\n    return namespace, fields, ConfigClass\\n\\n\\nnamespace, fields, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(namespace)\\npprint(config)', '_i45': 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, field(default=default_value)))\\n\\n    fields = required_fields + optional_fields\\n    namespace = {name: annotation for name, annotation in annotations.items()}\\n\\n    if as_dataclass:\\n        ConfigClass = make_dataclass(f\"{cls.__name__}Config\", fields)\\n    else:\\n        # Create a normal class, manually setting __annotations__ and defaults\\n        ConfigClass = type(f\"{cls.__name__}Config\", (object,), namespace)\\n        for name, _, default in optional_fields:\\n            setattr(ConfigClass, name, default)\\n        for name, _ in required_fields:\\n            if name not in namespace:  # Avoid overwriting defaults\\n                setattr(ConfigClass, name, None)\\n\\n    ConfigClass.__annotations__ = annotations\\n\\n    if as_dataclass:\\n        return namespace, fields, ConfigClass\\n\\n    pprint(fields)\\n    fields = [(name, annotation) for name, annotation, _ in fields]\\n    return namespace, fields, ConfigClass\\n\\n\\nnamespace, fields, config = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(namespace)\\npprint(config)', '_i46': 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, (type_hint, default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations\\n\\n\\nfields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)\\npprint(annotations)', 'annotations': {'output_dir': <class 'str'>, 'overwrite_output_dir': <class 'bool'>, 'do_train': <class 'bool'>, 'do_eval': <class 'bool'>, 'do_predict': <class 'bool'>, 'evaluation_strategy': typing.Union[transformers.trainer_utils.IntervalStrategy, str], 'prediction_loss_only': <class 'bool'>, 'per_device_train_batch_size': <class 'int'>, 'per_device_eval_batch_size': <class 'int'>, 'per_gpu_train_batch_size': typing.Optional[int], 'per_gpu_eval_batch_size': typing.Optional[int], 'gradient_accumulation_steps': <class 'int'>, 'eval_accumulation_steps': typing.Optional[int], 'eval_delay': typing.Optional[float], 'learning_rate': <class 'float'>, 'weight_decay': <class 'float'>, 'adam_beta1': <class 'float'>, 'adam_beta2': <class 'float'>, 'adam_epsilon': <class 'float'>, 'max_grad_norm': <class 'float'>, 'num_train_epochs': <class 'float'>, 'max_steps': <class 'int'>, 'lr_scheduler_type': typing.Union[transformers.trainer_utils.SchedulerType, str], 'lr_scheduler_kwargs': typing.Optional[typing.Dict], 'warmup_ratio': <class 'float'>, 'warmup_steps': <class 'int'>, 'log_level': typing.Optional[str], 'log_level_replica': typing.Optional[str], 'log_on_each_node': <class 'bool'>, 'logging_dir': typing.Optional[str], 'logging_strategy': typing.Union[transformers.trainer_utils.IntervalStrategy, str], 'logging_first_step': <class 'bool'>, 'logging_steps': <class 'float'>, 'logging_nan_inf_filter': <class 'bool'>, 'save_strategy': typing.Union[transformers.trainer_utils.IntervalStrategy, str], 'save_steps': <class 'float'>, 'save_total_limit': typing.Optional[int], 'save_safetensors': typing.Optional[bool], 'save_on_each_node': <class 'bool'>, 'save_only_model': <class 'bool'>, 'no_cuda': <class 'bool'>, 'use_cpu': <class 'bool'>, 'use_mps_device': <class 'bool'>, 'seed': <class 'int'>, 'data_seed': typing.Optional[int], 'jit_mode_eval': <class 'bool'>, 'use_ipex': <class 'bool'>, 'bf16': <class 'bool'>, 'fp16': <class 'bool'>, 'fp16_opt_level': <class 'str'>, 'half_precision_backend': <class 'str'>, 'bf16_full_eval': <class 'bool'>, 'fp16_full_eval': <class 'bool'>, 'tf32': typing.Optional[bool], 'local_rank': <class 'int'>, 'ddp_backend': typing.Optional[str], 'tpu_num_cores': typing.Optional[int], 'tpu_metrics_debug': <class 'bool'>, 'debug': typing.Union[str, typing.List[transformers.debug_utils.DebugOption]], 'dataloader_drop_last': <class 'bool'>, 'eval_steps': typing.Optional[float], 'dataloader_num_workers': <class 'int'>, 'dataloader_prefetch_factor': typing.Optional[int], 'past_index': <class 'int'>, 'run_name': typing.Optional[str], 'disable_tqdm': typing.Optional[bool], 'remove_unused_columns': typing.Optional[bool], 'label_names': typing.Optional[typing.List[str]], 'load_best_model_at_end': typing.Optional[bool], 'metric_for_best_model': typing.Optional[str], 'greater_is_better': typing.Optional[bool], 'ignore_data_skip': <class 'bool'>, 'fsdp': typing.Union[typing.List[transformers.trainer_utils.FSDPOption], str, NoneType], 'fsdp_min_num_params': <class 'int'>, 'fsdp_config': typing.Union[dict, str, NoneType], 'fsdp_transformer_layer_cls_to_wrap': typing.Optional[str], 'accelerator_config': typing.Optional[str], 'deepspeed': typing.Optional[str], 'label_smoothing_factor': <class 'float'>, 'optim': typing.Union[transformers.training_args.OptimizerNames, str], 'optim_args': typing.Optional[str], 'adafactor': <class 'bool'>, 'group_by_length': <class 'bool'>, 'length_column_name': typing.Optional[str], 'report_to': typing.Optional[typing.List[str]], 'ddp_find_unused_parameters': typing.Optional[bool], 'ddp_bucket_cap_mb': typing.Optional[int], 'ddp_broadcast_buffers': typing.Optional[bool], 'dataloader_pin_memory': <class 'bool'>, 'dataloader_persistent_workers': <class 'bool'>, 'skip_memory_metrics': <class 'bool'>, 'use_legacy_prediction_loop': <class 'bool'>, 'push_to_hub': <class 'bool'>, 'resume_from_checkpoint': typing.Optional[str], 'hub_model_id': typing.Optional[str], 'hub_strategy': typing.Union[transformers.trainer_utils.HubStrategy, str], 'hub_token': typing.Optional[str], 'hub_private_repo': <class 'bool'>, 'hub_always_push': <class 'bool'>, 'gradient_checkpointing': <class 'bool'>, 'gradient_checkpointing_kwargs': typing.Optional[dict], 'include_inputs_for_metrics': <class 'bool'>, 'fp16_backend': <class 'str'>, 'push_to_hub_model_id': typing.Optional[str], 'push_to_hub_organization': typing.Optional[str], 'push_to_hub_token': typing.Optional[str], 'mp_parameters': <class 'str'>, 'auto_find_batch_size': <class 'bool'>, 'full_determinism': <class 'bool'>, 'torchdynamo': typing.Optional[str], 'ray_scope': typing.Optional[str], 'ddp_timeout': typing.Optional[int], 'torch_compile': <class 'bool'>, 'torch_compile_backend': typing.Optional[str], 'torch_compile_mode': typing.Optional[str], 'dispatch_batches': typing.Optional[bool], 'split_batches': typing.Optional[bool], 'include_tokens_per_second': typing.Optional[bool], 'include_num_input_tokens_seen': typing.Optional[bool], 'neftune_noise_alpha': typing.Optional[float]}, '_i47': 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, (type_hint, default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations\\n\\n\\nfields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)', '_i48': 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, (type_hint, default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations\\n\\n\\nfields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\nprint(fields)', '_i49': 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(cls: Type[Any], include_bases: bool = True, as_dataclass: bool = False) -> Dict[str, Any]:\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, (type_hint, default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations\\n\\n\\nfields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)', '_i50': 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    pass\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        for name, type_hint, default in fields:\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', 'create_config_class_str': <function create_config_class_str at 0x2a95d9ca0>, '_i51': 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(\\n    cls: Type[Any], include_bases: bool = True\\n) -> Tuple[List[Union[Tuple[str, Tuple[Any, Any]], Tuple[str, Any, ellipsis]]], Dict[str, Any]]: # noqa: F821\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, (type_hint, default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations', '_i52': 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(\\n    cls: Type[Any], include_bases: bool = True\\n) -> Tuple[List[Union[Tuple[str, Tuple[Any, Any]], Tuple[str, Any, Any]]], Dict[str, Any]]: # noqa: F821\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, (type_hint, default_value)))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations', '_i53': '\\nfields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)', '_i54': 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        for name, type_hint, default in fields:\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', '_i55': 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        for name, (type_hint, default) in fields:\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', '_i56': 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        for name, field_tuple in fields:\\n            type_hint, default = field_tuple\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', '_i57': 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        print(fields)\\n        for name, field_tuple in fields:\\n            type_hint, default = field_tuple\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', '_i58': 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_init_arg_signatures(\\n    cls: Type[Any], include_bases: bool = True\\n) -> Tuple[List[Union[Tuple[str, Tuple[Any, Any]], Tuple[str, Any, Any]]], Dict[str, Any]]: # noqa: F821\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            sig: Signature = inspect.signature(c.__init__)\\n            type_hints: Dict[str, Any] = get_type_hints(c.__init__)\\n            for name, param in sig.parameters.items():\\n                if name == \"self\":\\n                    continue\\n\\n                type_hint = type_hints.get(name, Any)\\n                annotations[name] = type_hint\\n                if param.default is param.empty:\\n                    required_fields.append((name, type_hint, Ellipsis))\\n                else:\\n                    default_value = param.default\\n                    optional_fields.append((name, type_hint, default_value))\\n\\n    fields = required_fields + optional_fields\\n\\n    return fields, annotations', '_i59': 'fields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)', '_i60': 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        print(fields)\\n        for name, field_tuple in fields:\\n            type_hint, default = field_tuple\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', '_i61': 'def create_config_class_str(fields):\\n    \"\"\"\\n    Creates a string representation of a config class based on the given fields.\\n\\n    Parameters:\\n    fields : List[Tuple[str, Type, Any]]\\n        A list of tuples, each containing the name, type, and default value for a field.\\n    \"\"\"\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        print(fields)\\n        for name, type_hint, default in fields:\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint.__name__}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint.__name__} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', '_i62': 'fields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)\\nfor field in fields:\\n    print(field)', '_i63': 'fields, annotations = get_init_arg_signatures(TrainingArguments, include_bases=False)\\npprint(fields)\\nfor field in fields:\\n    print(field)\\n    assert len(field) == 3', '_i64': 'from typing import Any, List, Tuple\\nimport typing\\n\\ndef type_hint_to_str(type_hint: Any) -> str:\\n    \"\"\"\\n    Convert a type hint into its string representation.\\n    \"\"\"\\n    if hasattr(type_hint, \\'__name__\\'):\\n        return type_hint.__name__\\n    elif hasattr(type_hint, \\'_name\\') and type_hint._name is not None:\\n        return str(type_hint._name)\\n    elif type(type_hint) == typing._GenericAlias:  # For Python 3.8+\\n        # Handles complex types, e.g., List[int], Union[str, int]\\n        origin = type_hint_to_str(type_hint.__origin__)\\n        args = \\', \\'.join(type_hint_to_str(arg) for arg in type_hint.__args__)\\n        return f\"{origin}[{args}]\"\\n    else:\\n        # Fallback for unhandled types\\n        return str(type_hint)\\n\\ndef create_config_class_str(fields: List[Tuple[str, Any, Any]]) -> str:\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        for name, type_hint, default in fields:\\n            type_hint_str = type_hint_to_str(type_hint)\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint_str}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint_str} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\n# Example usage:\\n# Replace \\'fields\\' with your actual fields data\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', 'typing': <module 'typing' from '/opt/homebrew/Caskroom/miniconda/base/envs/omniverse/lib/python3.9/typing.py'>, 'type_hint_to_str': <function type_hint_to_str at 0x2a95d9b80>, 'config_class_str': \"class Config:\\n    def __init__(self, output_dir: str, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, evaluation_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: typing.Optional[int] = None, per_gpu_eval_batch_size: typing.Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: typing.Optional[int] = None, eval_delay: typing.Optional[float] = 0, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: typing.Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: typing.Optional[typing.Dict] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: typing.Optional[str] = 'passive', log_level_replica: typing.Optional[str] = 'warning', log_on_each_node: bool = True, logging_dir: typing.Optional[str] = None, logging_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: typing.Optional[int] = None, save_safetensors: typing.Optional[bool] = True, save_on_each_node: bool = False, save_only_model: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: typing.Optional[int] = None, jit_mode_eval: bool = False, use_ipex: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: typing.Optional[bool] = None, local_rank: int = -1, ddp_backend: typing.Optional[str] = None, tpu_num_cores: typing.Optional[int] = None, tpu_metrics_debug: bool = False, debug: typing.Union[str, typing.List[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: typing.Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: typing.Optional[int] = None, past_index: int = -1, run_name: typing.Optional[str] = None, disable_tqdm: typing.Optional[bool] = None, remove_unused_columns: typing.Optional[bool] = True, label_names: typing.Optional[typing.List[str]] = None, load_best_model_at_end: typing.Optional[bool] = False, metric_for_best_model: typing.Optional[str] = None, greater_is_better: typing.Optional[bool] = None, ignore_data_skip: bool = False, fsdp: typing.Union[typing.List[transformers.trainer_utils.FSDPOption], str, NoneType] = '', fsdp_min_num_params: int = 0, fsdp_config: typing.Union[dict, str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: typing.Optional[str] = None, accelerator_config: typing.Optional[str] = None, deepspeed: typing.Optional[str] = None, label_smoothing_factor: float = 0.0, optim: typing.Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch', optim_args: typing.Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: typing.Optional[str] = 'length', report_to: typing.Optional[typing.List[str]] = None, ddp_find_unused_parameters: typing.Optional[bool] = None, ddp_bucket_cap_mb: typing.Optional[int] = None, ddp_broadcast_buffers: typing.Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: typing.Optional[str] = None, hub_model_id: typing.Optional[str] = None, hub_strategy: typing.Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: typing.Optional[str] = None, hub_private_repo: bool = False, hub_always_push: bool = False, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: typing.Optional[dict] = None, include_inputs_for_metrics: bool = False, fp16_backend: str = 'auto', push_to_hub_model_id: typing.Optional[str] = None, push_to_hub_organization: typing.Optional[str] = None, push_to_hub_token: typing.Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: typing.Optional[str] = None, ray_scope: typing.Optional[str] = 'last', ddp_timeout: typing.Optional[int] = 1800, torch_compile: bool = False, torch_compile_backend: typing.Optional[str] = None, torch_compile_mode: typing.Optional[str] = None, dispatch_batches: typing.Optional[bool] = None, split_batches: typing.Optional[bool] = None, include_tokens_per_second: typing.Optional[bool] = False, include_num_input_tokens_seen: typing.Optional[bool] = False, neftune_noise_alpha: typing.Optional[float] = None):\\n        self.output_dir = output_dir\\n        self.overwrite_output_dir = overwrite_output_dir\\n        self.do_train = do_train\\n        self.do_eval = do_eval\\n        self.do_predict = do_predict\\n        self.evaluation_strategy = evaluation_strategy\\n        self.prediction_loss_only = prediction_loss_only\\n        self.per_device_train_batch_size = per_device_train_batch_size\\n        self.per_device_eval_batch_size = per_device_eval_batch_size\\n        self.per_gpu_train_batch_size = per_gpu_train_batch_size\\n        self.per_gpu_eval_batch_size = per_gpu_eval_batch_size\\n        self.gradient_accumulation_steps = gradient_accumulation_steps\\n        self.eval_accumulation_steps = eval_accumulation_steps\\n        self.eval_delay = eval_delay\\n        self.learning_rate = learning_rate\\n        self.weight_decay = weight_decay\\n        self.adam_beta1 = adam_beta1\\n        self.adam_beta2 = adam_beta2\\n        self.adam_epsilon = adam_epsilon\\n        self.max_grad_norm = max_grad_norm\\n        self.num_train_epochs = num_train_epochs\\n        self.max_steps = max_steps\\n        self.lr_scheduler_type = lr_scheduler_type\\n        self.lr_scheduler_kwargs = lr_scheduler_kwargs\\n        self.warmup_ratio = warmup_ratio\\n        self.warmup_steps = warmup_steps\\n        self.log_level = log_level\\n        self.log_level_replica = log_level_replica\\n        self.log_on_each_node = log_on_each_node\\n        self.logging_dir = logging_dir\\n        self.logging_strategy = logging_strategy\\n        self.logging_first_step = logging_first_step\\n        self.logging_steps = logging_steps\\n        self.logging_nan_inf_filter = logging_nan_inf_filter\\n        self.save_strategy = save_strategy\\n        self.save_steps = save_steps\\n        self.save_total_limit = save_total_limit\\n        self.save_safetensors = save_safetensors\\n        self.save_on_each_node = save_on_each_node\\n        self.save_only_model = save_only_model\\n        self.no_cuda = no_cuda\\n        self.use_cpu = use_cpu\\n        self.use_mps_device = use_mps_device\\n        self.seed = seed\\n        self.data_seed = data_seed\\n        self.jit_mode_eval = jit_mode_eval\\n        self.use_ipex = use_ipex\\n        self.bf16 = bf16\\n        self.fp16 = fp16\\n        self.fp16_opt_level = fp16_opt_level\\n        self.half_precision_backend = half_precision_backend\\n        self.bf16_full_eval = bf16_full_eval\\n        self.fp16_full_eval = fp16_full_eval\\n        self.tf32 = tf32\\n        self.local_rank = local_rank\\n        self.ddp_backend = ddp_backend\\n        self.tpu_num_cores = tpu_num_cores\\n        self.tpu_metrics_debug = tpu_metrics_debug\\n        self.debug = debug\\n        self.dataloader_drop_last = dataloader_drop_last\\n        self.eval_steps = eval_steps\\n        self.dataloader_num_workers = dataloader_num_workers\\n        self.dataloader_prefetch_factor = dataloader_prefetch_factor\\n        self.past_index = past_index\\n        self.run_name = run_name\\n        self.disable_tqdm = disable_tqdm\\n        self.remove_unused_columns = remove_unused_columns\\n        self.label_names = label_names\\n        self.load_best_model_at_end = load_best_model_at_end\\n        self.metric_for_best_model = metric_for_best_model\\n        self.greater_is_better = greater_is_better\\n        self.ignore_data_skip = ignore_data_skip\\n        self.fsdp = fsdp\\n        self.fsdp_min_num_params = fsdp_min_num_params\\n        self.fsdp_config = fsdp_config\\n        self.fsdp_transformer_layer_cls_to_wrap = fsdp_transformer_layer_cls_to_wrap\\n        self.accelerator_config = accelerator_config\\n        self.deepspeed = deepspeed\\n        self.label_smoothing_factor = label_smoothing_factor\\n        self.optim = optim\\n        self.optim_args = optim_args\\n        self.adafactor = adafactor\\n        self.group_by_length = group_by_length\\n        self.length_column_name = length_column_name\\n        self.report_to = report_to\\n        self.ddp_find_unused_parameters = ddp_find_unused_parameters\\n        self.ddp_bucket_cap_mb = ddp_bucket_cap_mb\\n        self.ddp_broadcast_buffers = ddp_broadcast_buffers\\n        self.dataloader_pin_memory = dataloader_pin_memory\\n        self.dataloader_persistent_workers = dataloader_persistent_workers\\n        self.skip_memory_metrics = skip_memory_metrics\\n        self.use_legacy_prediction_loop = use_legacy_prediction_loop\\n        self.push_to_hub = push_to_hub\\n        self.resume_from_checkpoint = resume_from_checkpoint\\n        self.hub_model_id = hub_model_id\\n        self.hub_strategy = hub_strategy\\n        self.hub_token = hub_token\\n        self.hub_private_repo = hub_private_repo\\n        self.hub_always_push = hub_always_push\\n        self.gradient_checkpointing = gradient_checkpointing\\n        self.gradient_checkpointing_kwargs = gradient_checkpointing_kwargs\\n        self.include_inputs_for_metrics = include_inputs_for_metrics\\n        self.fp16_backend = fp16_backend\\n        self.push_to_hub_model_id = push_to_hub_model_id\\n        self.push_to_hub_organization = push_to_hub_organization\\n        self.push_to_hub_token = push_to_hub_token\\n        self.mp_parameters = mp_parameters\\n        self.auto_find_batch_size = auto_find_batch_size\\n        self.full_determinism = full_determinism\\n        self.torchdynamo = torchdynamo\\n        self.ray_scope = ray_scope\\n        self.ddp_timeout = ddp_timeout\\n        self.torch_compile = torch_compile\\n        self.torch_compile_backend = torch_compile_backend\\n        self.torch_compile_mode = torch_compile_mode\\n        self.dispatch_batches = dispatch_batches\\n        self.split_batches = split_batches\\n        self.include_tokens_per_second = include_tokens_per_second\\n        self.include_num_input_tokens_seen = include_num_input_tokens_seen\\n        self.neftune_noise_alpha = neftune_noise_alpha\", '_i65': 'from transformers import Trainer, GPT2LMHeadModel, TrainingArguments\\nfrom dataclasses import field, make_dataclass\\n\\nimport inspect\\nfrom inspect import Signature, Parameter\\nfrom typing import Any, Callable, Dict, Set, Optional, _GenericAlias, List, Type, Union, get_type_hints, overload, Tuple\\nfrom pydantic import BaseModel\\nfrom rich.pretty import pprint', '_GenericAlias': <class 'typing._GenericAlias'>, '_i66': 'inspect.getmro(GPT2LMHeadModel)', '_66': (<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel'>, <class 'transformers.modeling_utils.PreTrainedModel'>, <class 'torch.nn.modules.module.Module'>, <class 'transformers.modeling_utils.ModuleUtilsMixin'>, <class 'transformers.generation.utils.GenerationMixin'>, <class 'transformers.utils.hub.PushToHubMixin'>, <class 'transformers.integrations.peft.PeftAdapterMixin'>, <class 'object'>), '_i67': 'reversed(inspect.getmro(GPT2LMHeadModel))', '_67': <reversed object at 0x2a95639d0>, '_i68': 'list(reversed(inspect.getmro(GPT2LMHeadModel)))', '_68': [<class 'object'>, <class 'transformers.integrations.peft.PeftAdapterMixin'>, <class 'transformers.utils.hub.PushToHubMixin'>, <class 'transformers.generation.utils.GenerationMixin'>, <class 'transformers.modeling_utils.ModuleUtilsMixin'>, <class 'torch.nn.modules.module.Module'>, <class 'transformers.modeling_utils.PreTrainedModel'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel'>, <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>], '_i69': 'def get_default(param: Parameter) -> Any:\\n    \"\"\"Return the parameter\\'s default value or None if not specified.\"\"\"\\n    return param.default if param.default is not param.empty else None\\n\\ndef get_field_annotations(func_or_method: Callable[..., Any]) -> Tuple[List[Tuple[str, Any, Any]], Dict[str, Any]]:\\n    if not inspect.isroutine(func_or_method):\\n        raise ValueError(\"Expected a function or method\")\\n\\n    required_fields = []\\n    optional_fields = []\\n    annotations = {}\\n\\n    try:\\n        sig: Signature = inspect.signature(func_or_method)\\n        type_hints: Dict[str, Any] = get_type_hints(func_or_method)\\n    except ValueError:\\n        raise ValueError(\"Object does not support signature or type hints extraction.\") from None\\n\\n    for name, param in sig.parameters.items():\\n        if name == \"self\":\\n            continue\\n\\n        type_hint = type_hints.get(name, Any)\\n        annotations[name] = type_hint\\n        if param.default is param.empty:\\n            required_fields.append((name, type_hint, Ellipsis))\\n        else:\\n            default_value = param.default\\n            optional_fields.append((name, type_hint, default_value))\\n\\n    fields = required_fields + optional_fields\\n    return fields, annotations\\n\\n\\n# TODO: Tuple[str, Any, Any] should be Tuple[str, Any, ellipsis]\\ndef get_constructor_field_annotations(\\n    cls: Type[Any], include_bases: bool = True\\n) -> Tuple[List[Tuple[str, Any, Any]], Dict[str, Any]]:\\n    fields = []\\n    annotations = {}\\n\\n    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\\n\\n    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\\n        if hasattr(c, \"__init__\"):\\n            class_fields, class_annotations = get_field_annotations(c.__init__)\\n            # Update fields and annotations with those from the current class,\\n            # avoiding duplicates.\\n            for field in class_fields:\\n                if field[0] not in annotations:\\n                    fields.append(field)  # noqa: PERF401\\n            annotations.update(class_annotations)\\n\\n    return fields, annotations', 'get_field_annotations': <function get_field_annotations at 0x13fd8d160>, 'get_constructor_field_annotations': <function get_constructor_field_annotations at 0x2a92d10d0>, '_i70': 'fields, annotations = get_constructor_field_annotations(TrainingArguments, include_bases=False)\\npprint(fields)\\nfor field in fields:\\n    print(field)\\n    assert len(field) == 3', '_i71': 'fields, annotations = get_constructor_field_annotations(TrainingArguments, include_bases=False)\\npprint(fields)\\nfor field in fields:\\n    assert len(field) == 3', '_i72': 'fields, annotations = get_constructor_field_annotations(TrainingArguments, include_bases=False)\\npprint(fields)\\nfor field in fields:\\n    assert len(field) == 3\\n    \\nassert get_field_annotations(TrainingArguments.__init__) == (fields, annotations)', '_i73': 'def type_hint_to_str(type_hint: Any) -> str:\\n    \"\"\"\\n    Convert a type hint into its string representation.\\n    \"\"\"\\n    if hasattr(type_hint, \\'__name__\\'):\\n        return type_hint.__name__\\n    elif hasattr(type_hint, \\'_name\\') and type_hint._name is not None:\\n        return str(type_hint._name)\\n    elif type(type_hint) == _GenericAlias:  # For Python 3.8+\\n        # Handles complex types, e.g., List[int], Union[str, int]\\n        origin = type_hint_to_str(type_hint.__origin__)\\n        args = \\', \\'.join(type_hint_to_str(arg) for arg in type_hint.__args__)\\n        return f\"{origin}[{args}]\"\\n    else:\\n        # Fallback for unhandled types\\n        return str(type_hint)\\n\\ndef create_config_class_str(fields: List[Tuple[str, Any, Any]]) -> str:\\n    lines = [\"class Config:\"]\\n    if not fields:\\n        lines.append(\"    ...\")\\n    else:\\n        init_params = [\"self\"]\\n        init_body = []\\n        for name, type_hint, default in fields:\\n            type_hint_str = type_hint_to_str(type_hint)\\n            if default is Ellipsis:  # Required argument\\n                init_params.append(f\"{name}: {type_hint_str}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n            else:\\n                default_repr = repr(default) if default is not None else \\'None\\'\\n                init_params.append(f\"{name}: {type_hint_str} = {default_repr}\")\\n                init_body.append(f\"        self.{name} = {name}\")\\n\\n        lines.append(f\"    def __init__({\\', \\'.join(init_params)}):\")\\n        lines.extend(init_body)\\n\\n    return \\'\\\\n\\'.join(lines)\\n\\n# Example usage:\\n# Replace \\'fields\\' with your actual fields data\\nconfig_class_str = create_config_class_str(fields)\\nprint(config_class_str)', '_i74': '%pip install -U -q langchain', '_i75': 'from transformers import Trainer, GPT2LMHeadModel, TrainingArguments\\nfrom dataclasses import field, make_dataclass\\n\\nimport inspect\\nfrom inspect import Signature, Parameter\\nfrom typing import Any, Callable, Dict, Set, Optional, _GenericAlias, List, Type, Union, get_type_hints, overload, Tuple\\nfrom pydantic import BaseModel\\nfrom rich.pretty import pprint', '_i76': 'class ParentClass:\\n    \"\"\"This is the parent class.\"\"\"\\n\\n    parent_class_attr = \\'a parent class attribute\\'\\n\\n    def __init__(self, parent_instance_attr: str) -> None:\\n        self.parent_instance_attr = parent_instance_attr\\n\\n    def parent_method(self) -> str:\\n        \"\"\"This is a method in the parent class.\"\"\"\\n        return \"Parent method called\"\\n\\nclass ChildClass(ParentClass):\\n    \"\"\"This is a subclass of ParentClass.\"\"\"\\n\\n    # Class attribute\\n    class_attr = \\'a class attribute\\'\\n\\n    # Private and protected attributes\\n    _protected_attr = \\'a protected attribute\\'\\n    __private_attr = \\'a private attribute\\'\\n\\n    def __init__(self, instance_attr: str, parent_instance_attr: str) -> None:\\n        \"\"\"Initialize the instance.\"\"\"\\n        super().__init__(parent_instance_attr)\\n        # Instance attribute\\n        self.instance_attr = instance_attr\\n\\n    @property\\n    def read_only_attr(self) -> str:\\n        \"\"\"This is a read-only attribute.\"\"\"\\n        return \\'You can read me, but you cannot change me.\\'\\n\\n    def instance_method(self, arg: str) -> str:\\n        \"\"\"This is an instance method.\"\"\"\\n        return f\\'Instance method called with argument: {arg}\\'\\n\\n    @classmethod\\n    def class_method(cls, arg: str) -> str:\\n        \"\"\"This is a class method.\"\"\"\\n        return f\\'Class method called with argument: {arg}\\'\\n\\n    @staticmethod\\n    def static_method(arg: str) -> str:\\n        \"\"\"This is a static method.\"\"\"\\n        return f\\'Static method called with argument: {arg}\\'\\n\\n    def __str__(self) -> str:\\n        \"\"\"Return a string representation of the instance.\"\"\"\\n        return f\\'MyClass(instance_attr={self.instance_attr})\\'', '_i77': \"instance_child = ChildClass(instance_attr='an instance attribute', parent_instance_attr='a parent instance attribute')\\nclass_child = ChildClass\\n\\ninstance_parent = ParentClass(parent_instance_attr='a parent instance attribute')\\nclass_parent = ParentClass\", '_i78': 'def func(a: int, b: str, c: List[int], d: Tuple[str, str], e: Union[int, str], **kwargs: Any) -> str:\\n    return a, b, c, d, e, kwargs', '_i79': \"@overload\\ndef get_members_of_function_or_method(\\n    func_or_class: Type[object], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    ...\\n\\n\\n@overload\\ndef get_members_of_function_or_method(\\n    func_or_class: Callable[..., Any], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    ...\\n\\n\\ndef get_members_of_function_or_method(\\n    func_or_class: Union[Type[object], Callable[..., Any]], predicate: Optional[Callable[[Any], bool]] = None\\n) -> List[Tuple[str, Any]]:\\n    return inspect.getmembers(func_or_class, predicate)\\n\\ndef loop_through_members(members: List[Tuple[str, Any]], filter: Optional[str] = None) -> None:\\n    if filter is not None:\\n        members = [member for member in members if filter in member[0]]\\n    for member in members:\\n        name, value = member\\n        print(f'{name}: {value}')\", '_i80': 'func_all_members = get_members_of_function_or_method(func, predicate=None)\\nloop_through_members(func_all_members)'}\n",
      "__gt__: <method-wrapper '__gt__' of function object at 0x2a7d5b310>\n",
      "__hash__: <method-wrapper '__hash__' of function object at 0x2a7d5b310>\n",
      "__init__: <method-wrapper '__init__' of function object at 0x2a7d5b310>\n",
      "__init_subclass__: <built-in method __init_subclass__ of type object at 0x100bb2270>\n",
      "__kwdefaults__: None\n",
      "__le__: <method-wrapper '__le__' of function object at 0x2a7d5b310>\n",
      "__lt__: <method-wrapper '__lt__' of function object at 0x2a7d5b310>\n",
      "__module__: __main__\n",
      "__name__: func\n",
      "__ne__: <method-wrapper '__ne__' of function object at 0x2a7d5b310>\n",
      "__new__: <built-in method __new__ of type object at 0x100bb2270>\n",
      "__qualname__: func\n",
      "__reduce__: <built-in method __reduce__ of function object at 0x2a7d5b310>\n",
      "__reduce_ex__: <built-in method __reduce_ex__ of function object at 0x2a7d5b310>\n",
      "__repr__: <method-wrapper '__repr__' of function object at 0x2a7d5b310>\n",
      "__setattr__: <method-wrapper '__setattr__' of function object at 0x2a7d5b310>\n",
      "__sizeof__: <built-in method __sizeof__ of function object at 0x2a7d5b310>\n",
      "__str__: <method-wrapper '__str__' of function object at 0x2a7d5b310>\n",
      "__subclasshook__: <built-in method __subclasshook__ of type object at 0x100bb2270>\n"
     ]
    }
   ],
   "source": [
    "func_all_members = get_members_of_function_or_method(func, predicate=None)\n",
    "loop_through_members(func_all_members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to get the signature, we can just filter `'__annotations__'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__annotations__: {'a': <class 'int'>, 'b': <class 'str'>, 'c': typing.List[int], 'd': typing.Tuple[str, str], 'e': typing.Union[int, str], 'kwargs': typing.Any, 'return': <class 'str'>}\n"
     ]
    }
   ],
   "source": [
    "loop_through_members(func_all_members, filter='__annotations__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ChildClass__private_attr: a private attribute\n",
      "__class__: <class 'type'>\n",
      "__delattr__: <slot wrapper '__delattr__' of 'object' objects>\n",
      "__dict__: {'__module__': '__main__', '__doc__': 'This is a subclass of ParentClass.', 'class_attr': 'a class attribute', '_protected_attr': 'a protected attribute', '_ChildClass__private_attr': 'a private attribute', '__init__': <function ChildClass.__init__ at 0x2a95d9670>, 'read_only_attr': <property object at 0x2a9451590>, 'instance_method': <function ChildClass.instance_method at 0x2a95d9430>, 'class_method': <classmethod object at 0x2a957aa00>, 'static_method': <staticmethod object at 0x2a957abe0>, '__str__': <function ChildClass.__str__ at 0x2a95d9700>}\n",
      "__dir__: <method '__dir__' of 'object' objects>\n",
      "__doc__: This is a subclass of ParentClass.\n",
      "__eq__: <slot wrapper '__eq__' of 'object' objects>\n",
      "__format__: <method '__format__' of 'object' objects>\n",
      "__ge__: <slot wrapper '__ge__' of 'object' objects>\n",
      "__getattribute__: <slot wrapper '__getattribute__' of 'object' objects>\n",
      "__gt__: <slot wrapper '__gt__' of 'object' objects>\n",
      "__hash__: <slot wrapper '__hash__' of 'object' objects>\n",
      "__init__: <function ChildClass.__init__ at 0x2a95d9670>\n",
      "__init_subclass__: <built-in method __init_subclass__ of type object at 0x11e950d80>\n",
      "__le__: <slot wrapper '__le__' of 'object' objects>\n",
      "__lt__: <slot wrapper '__lt__' of 'object' objects>\n",
      "__module__: __main__\n",
      "__ne__: <slot wrapper '__ne__' of 'object' objects>\n",
      "__new__: <built-in method __new__ of type object at 0x100bc6b00>\n",
      "__reduce__: <method '__reduce__' of 'object' objects>\n",
      "__reduce_ex__: <method '__reduce_ex__' of 'object' objects>\n",
      "__repr__: <slot wrapper '__repr__' of 'object' objects>\n",
      "__setattr__: <slot wrapper '__setattr__' of 'object' objects>\n",
      "__sizeof__: <method '__sizeof__' of 'object' objects>\n",
      "__str__: <function ChildClass.__str__ at 0x2a95d9700>\n",
      "__subclasshook__: <built-in method __subclasshook__ of type object at 0x11e950d80>\n",
      "__weakref__: <attribute '__weakref__' of 'ParentClass' objects>\n",
      "_protected_attr: a protected attribute\n",
      "class_attr: a class attribute\n",
      "class_method: <bound method ChildClass.class_method of <class '__main__.ChildClass'>>\n",
      "instance_method: <function ChildClass.instance_method at 0x2a95d9430>\n",
      "parent_class_attr: a parent class attribute\n",
      "parent_method: <function ParentClass.parent_method at 0x2a95d93a0>\n",
      "read_only_attr: <property object at 0x2a9451590>\n",
      "static_method: <function ChildClass.static_method at 0x2a95d9a60>\n"
     ]
    }
   ],
   "source": [
    "class_child_all_members = get_members_of_function_or_method(class_child, predicate=None)\n",
    "loop_through_members(class_child_all_members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ChildClass__private_attr: a private attribute\n",
      "__class__: <class '__main__.ChildClass'>\n",
      "__delattr__: <method-wrapper '__delattr__' of ChildClass object at 0x2a947ff40>\n",
      "__dict__: {'parent_instance_attr': 'a parent instance attribute', 'instance_attr': 'an instance attribute'}\n",
      "__dir__: <built-in method __dir__ of ChildClass object at 0x2a947ff40>\n",
      "__doc__: This is a subclass of ParentClass.\n",
      "__eq__: <method-wrapper '__eq__' of ChildClass object at 0x2a947ff40>\n",
      "__format__: <built-in method __format__ of ChildClass object at 0x2a947ff40>\n",
      "__ge__: <method-wrapper '__ge__' of ChildClass object at 0x2a947ff40>\n",
      "__getattribute__: <method-wrapper '__getattribute__' of ChildClass object at 0x2a947ff40>\n",
      "__gt__: <method-wrapper '__gt__' of ChildClass object at 0x2a947ff40>\n",
      "__hash__: <method-wrapper '__hash__' of ChildClass object at 0x2a947ff40>\n",
      "__init__: <bound method ChildClass.__init__ of <__main__.ChildClass object at 0x2a947ff40>>\n",
      "__init_subclass__: <built-in method __init_subclass__ of type object at 0x11e950d80>\n",
      "__le__: <method-wrapper '__le__' of ChildClass object at 0x2a947ff40>\n",
      "__lt__: <method-wrapper '__lt__' of ChildClass object at 0x2a947ff40>\n",
      "__module__: __main__\n",
      "__ne__: <method-wrapper '__ne__' of ChildClass object at 0x2a947ff40>\n",
      "__new__: <built-in method __new__ of type object at 0x100bc6b00>\n",
      "__reduce__: <built-in method __reduce__ of ChildClass object at 0x2a947ff40>\n",
      "__reduce_ex__: <built-in method __reduce_ex__ of ChildClass object at 0x2a947ff40>\n",
      "__repr__: <method-wrapper '__repr__' of ChildClass object at 0x2a947ff40>\n",
      "__setattr__: <method-wrapper '__setattr__' of ChildClass object at 0x2a947ff40>\n",
      "__sizeof__: <built-in method __sizeof__ of ChildClass object at 0x2a947ff40>\n",
      "__str__: <bound method ChildClass.__str__ of <__main__.ChildClass object at 0x2a947ff40>>\n",
      "__subclasshook__: <built-in method __subclasshook__ of type object at 0x11e950d80>\n",
      "__weakref__: None\n",
      "_protected_attr: a protected attribute\n",
      "class_attr: a class attribute\n",
      "class_method: <bound method ChildClass.class_method of <class '__main__.ChildClass'>>\n",
      "instance_attr: an instance attribute\n",
      "instance_method: <bound method ChildClass.instance_method of <__main__.ChildClass object at 0x2a947ff40>>\n",
      "parent_class_attr: a parent class attribute\n",
      "parent_instance_attr: a parent instance attribute\n",
      "parent_method: <bound method ParentClass.parent_method of <__main__.ChildClass object at 0x2a947ff40>>\n",
      "read_only_attr: You can read me, but you cannot change me.\n",
      "static_method: <function ChildClass.static_method at 0x2a95d9a60>\n"
     ]
    }
   ],
   "source": [
    "instance_child_all_members = get_members_of_function_or_method(instance_child, predicate=None)\n",
    "loop_through_members(instance_child_all_members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__class__: <class 'type'>\n",
      "__delattr__: <slot wrapper '__delattr__' of 'object' objects>\n",
      "__dict__: {'__module__': 'transformers.trainer', '__doc__': \"\\n    Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for  Transformers.\\n\\n    Args:\\n        model ([`PreTrainedModel`] or `torch.nn.Module`, *optional*):\\n            The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.\\n\\n            <Tip>\\n\\n            [`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use\\n            your own models defined as `torch.nn.Module` as long as they work the same way as the  Transformers\\n            models.\\n\\n            </Tip>\\n\\n        args ([`TrainingArguments`], *optional*):\\n            The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the\\n            `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.\\n        data_collator (`DataCollator`, *optional*):\\n            The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will\\n            default to [`default_data_collator`] if no `tokenizer` is provided, an instance of\\n            [`DataCollatorWithPadding`] otherwise.\\n        train_dataset (`torch.utils.data.Dataset` or `torch.utils.data.IterableDataset`, *optional*):\\n            The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the\\n            `model.forward()` method are automatically removed.\\n\\n            Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a\\n            distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a\\n            `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will\\n            manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally\\n            sets the seed of the RNGs used.\\n        eval_dataset (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`]), *optional*):\\n             The dataset to use for evaluation. If it is a [`~datasets.Dataset`], columns not accepted by the\\n             `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each\\n             dataset prepending the dictionary key to the metric name.\\n        tokenizer ([`PreTrainedTokenizerBase`], *optional*):\\n            The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs to the\\n            maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an\\n            interrupted training or reuse the fine-tuned model.\\n        model_init (`Callable[[], PreTrainedModel]`, *optional*):\\n            A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start\\n            from a new instance of the model as given by this function.\\n\\n            The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to\\n            be able to choose different architectures according to hyper parameters (such as layer count, sizes of\\n            inner layers, dropout probabilities etc).\\n        compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):\\n            The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return\\n            a dictionary string to metric values.\\n        callbacks (List of [`TrainerCallback`], *optional*):\\n            A list of callbacks to customize the training loop. Will add those to the list of default callbacks\\n            detailed in [here](callback).\\n\\n            If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.\\n        optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*, defaults to `(None, None)`):\\n            A tuple containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your\\n            model and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.\\n        preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*):\\n            A function that preprocess the logits right before caching them at each evaluation step. Must take two\\n            tensors, the logits and the labels, and return the logits once processed as desired. The modifications made\\n            by this function will be reflected in the predictions received by `compute_metrics`.\\n\\n            Note that the labels (second parameter) will be `None` if the dataset does not have them.\\n\\n    Important attributes:\\n\\n        - **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]\\n          subclass.\\n        - **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the\\n          original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`,\\n          the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner\\n          model hasn't been wrapped, then `self.model_wrapped` is the same as `self.model`.\\n        - **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from\\n          data parallelism, this means some of the model layers are split on different GPUs).\\n        - **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set\\n          to `False` if model parallel or deepspeed is used, or if the default\\n          `TrainingArguments.place_model_on_device` is overridden to return `False` .\\n        - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while\\n          in `train`)\\n\\n    \", '_get_learning_rate': <function _get_learning_rate at 0x12db6a820>, 'log_metrics': <function log_metrics at 0x12db6b160>, 'metrics_format': <function metrics_format at 0x12db6b0d0>, 'save_metrics': <function save_metrics at 0x12db6b1f0>, 'save_state': <function save_state at 0x12db6b280>, '__init__': <function Trainer.__init__ at 0x2a7b5bdc0>, '_activate_neftune': <function Trainer._activate_neftune at 0x2a7b5be50>, '_deactivate_neftune': <function Trainer._deactivate_neftune at 0x2a7b5bee0>, 'add_callback': <function Trainer.add_callback at 0x2a7b5bf70>, 'pop_callback': <function Trainer.pop_callback at 0x2a7b5e040>, 'remove_callback': <function Trainer.remove_callback at 0x2a7b5e0d0>, '_move_model_to_device': <function Trainer._move_model_to_device at 0x2a7b5e160>, '_set_signature_columns_if_needed': <function Trainer._set_signature_columns_if_needed at 0x2a7b5e1f0>, '_remove_unused_columns': <function Trainer._remove_unused_columns at 0x2a7b5e280>, '_get_collator_with_removed_columns': <function Trainer._get_collator_with_removed_columns at 0x2a7b5e310>, '_get_train_sampler': <function Trainer._get_train_sampler at 0x2a7b5e3a0>, 'get_train_dataloader': <function Trainer.get_train_dataloader at 0x2a7b5e430>, '_get_eval_sampler': <function Trainer._get_eval_sampler at 0x2a7b5e4c0>, 'get_eval_dataloader': <function Trainer.get_eval_dataloader at 0x2a7b5e550>, 'get_test_dataloader': <function Trainer.get_test_dataloader at 0x2a7b5e5e0>, 'create_optimizer_and_scheduler': <function Trainer.create_optimizer_and_scheduler at 0x2a7b5e670>, 'get_decay_parameter_names': <function Trainer.get_decay_parameter_names at 0x2a7b5e700>, 'create_optimizer': <function Trainer.create_optimizer at 0x2a7b5e790>, 'get_optimizer_cls_and_kwargs': <staticmethod object at 0x2a799e9d0>, 'create_scheduler': <function Trainer.create_scheduler at 0x2a7b5e8b0>, 'num_examples': <function Trainer.num_examples at 0x2a7b5e940>, 'num_tokens': <function Trainer.num_tokens at 0x2a7b5e9d0>, '_hp_search_setup': <function Trainer._hp_search_setup at 0x2a7b5ea60>, '_report_to_hp_search': <function Trainer._report_to_hp_search at 0x2a7b5eaf0>, '_tune_save_checkpoint': <function Trainer._tune_save_checkpoint at 0x2a7b5eb80>, 'call_model_init': <function Trainer.call_model_init at 0x2a7b5ec10>, 'torch_jit_model_eval': <function Trainer.torch_jit_model_eval at 0x2a7b5eca0>, 'ipex_optimize_model': <function Trainer.ipex_optimize_model at 0x2a7b5ed30>, '_wrap_model': <function Trainer._wrap_model at 0x2a7b5edc0>, 'train': <function Trainer.train at 0x2a7b5ee50>, '_inner_training_loop': <function Trainer._inner_training_loop at 0x2a7b5eee0>, '_get_output_dir': <function Trainer._get_output_dir at 0x2a7b5ef70>, '_load_from_checkpoint': <function Trainer._load_from_checkpoint at 0x2a7b60040>, '_load_best_model': <function Trainer._load_best_model at 0x2a7b600d0>, '_issue_warnings_after_load': <function Trainer._issue_warnings_after_load at 0x2a7b60160>, '_maybe_log_save_evaluate': <function Trainer._maybe_log_save_evaluate at 0x2a7b601f0>, '_load_rng_state': <function Trainer._load_rng_state at 0x2a7b60280>, '_save_checkpoint': <function Trainer._save_checkpoint at 0x2a7b60310>, '_save_rng_state': <function Trainer._save_rng_state at 0x2a7b603a0>, '_save_optimizer_and_scheduler': <function Trainer._save_optimizer_and_scheduler at 0x2a7b60430>, '_load_optimizer_and_scheduler': <function Trainer._load_optimizer_and_scheduler at 0x2a7b604c0>, 'hyperparameter_search': <function Trainer.hyperparameter_search at 0x2a7b60550>, 'log': <function Trainer.log at 0x2a7b605e0>, '_prepare_input': <function Trainer._prepare_input at 0x2a7b60670>, '_prepare_inputs': <function Trainer._prepare_inputs at 0x2a7b60700>, 'compute_loss_context_manager': <function Trainer.compute_loss_context_manager at 0x2a7b60790>, 'autocast_smart_context_manager': <function Trainer.autocast_smart_context_manager at 0x2a7b60820>, 'training_step': <function Trainer.training_step at 0x2a7b608b0>, 'compute_loss': <function Trainer.compute_loss at 0x2a7b60940>, 'is_local_process_zero': <function Trainer.is_local_process_zero at 0x2a7b609d0>, 'is_world_process_zero': <function Trainer.is_world_process_zero at 0x2a7b60a60>, 'save_model': <function Trainer.save_model at 0x2a7b60af0>, '_save_tpu': <function Trainer._save_tpu at 0x2a7b60b80>, '_save': <function Trainer._save at 0x2a7b60c10>, 'store_flos': <function Trainer.store_flos at 0x2a7b60ca0>, '_sorted_checkpoints': <function Trainer._sorted_checkpoints at 0x2a7b60d30>, '_rotate_checkpoints': <function Trainer._rotate_checkpoints at 0x2a7b60dc0>, 'evaluate': <function Trainer.evaluate at 0x2a7b60e50>, 'predict': <function Trainer.predict at 0x2a7b60ee0>, 'evaluation_loop': <function Trainer.evaluation_loop at 0x2a7b60f70>, '_nested_gather': <function Trainer._nested_gather at 0x2a7b62040>, 'prediction_step': <function Trainer.prediction_step at 0x2a7b620d0>, 'floating_point_ops': <function Trainer.floating_point_ops at 0x2a7b62160>, 'init_hf_repo': <function Trainer.init_hf_repo at 0x2a7b621f0>, 'create_model_card': <function Trainer.create_model_card at 0x2a7b62280>, '_push_from_checkpoint': <function Trainer._push_from_checkpoint at 0x2a7b62310>, '_finish_current_push': <function Trainer._finish_current_push at 0x2a7b623a0>, 'push_to_hub': <function Trainer.push_to_hub at 0x2a7b62430>, 'prediction_loop': <function Trainer.prediction_loop at 0x2a7b624c0>, '_gather_and_numpify': <function Trainer._gather_and_numpify at 0x2a7b62550>, '_add_sm_patterns_to_gitignore': <function Trainer._add_sm_patterns_to_gitignore at 0x2a7b625e0>, 'create_accelerator_and_postprocess': <function Trainer.create_accelerator_and_postprocess at 0x2a7b62670>, 'propagate_args_to_deepspeed': <function Trainer.propagate_args_to_deepspeed at 0x2a7b62700>, '__dict__': <attribute '__dict__' of 'Trainer' objects>, '__weakref__': <attribute '__weakref__' of 'Trainer' objects>}\n",
      "__dir__: <method '__dir__' of 'object' objects>\n",
      "__doc__: \n",
      "    Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for  Transformers.\n",
      "\n",
      "    Args:\n",
      "        model ([`PreTrainedModel`] or `torch.nn.Module`, *optional*):\n",
      "            The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.\n",
      "\n",
      "            <Tip>\n",
      "\n",
      "            [`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use\n",
      "            your own models defined as `torch.nn.Module` as long as they work the same way as the  Transformers\n",
      "            models.\n",
      "\n",
      "            </Tip>\n",
      "\n",
      "        args ([`TrainingArguments`], *optional*):\n",
      "            The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the\n",
      "            `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.\n",
      "        data_collator (`DataCollator`, *optional*):\n",
      "            The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will\n",
      "            default to [`default_data_collator`] if no `tokenizer` is provided, an instance of\n",
      "            [`DataCollatorWithPadding`] otherwise.\n",
      "        train_dataset (`torch.utils.data.Dataset` or `torch.utils.data.IterableDataset`, *optional*):\n",
      "            The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the\n",
      "            `model.forward()` method are automatically removed.\n",
      "\n",
      "            Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a\n",
      "            distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a\n",
      "            `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will\n",
      "            manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally\n",
      "            sets the seed of the RNGs used.\n",
      "        eval_dataset (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`]), *optional*):\n",
      "             The dataset to use for evaluation. If it is a [`~datasets.Dataset`], columns not accepted by the\n",
      "             `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each\n",
      "             dataset prepending the dictionary key to the metric name.\n",
      "        tokenizer ([`PreTrainedTokenizerBase`], *optional*):\n",
      "            The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs to the\n",
      "            maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an\n",
      "            interrupted training or reuse the fine-tuned model.\n",
      "        model_init (`Callable[[], PreTrainedModel]`, *optional*):\n",
      "            A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start\n",
      "            from a new instance of the model as given by this function.\n",
      "\n",
      "            The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to\n",
      "            be able to choose different architectures according to hyper parameters (such as layer count, sizes of\n",
      "            inner layers, dropout probabilities etc).\n",
      "        compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):\n",
      "            The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return\n",
      "            a dictionary string to metric values.\n",
      "        callbacks (List of [`TrainerCallback`], *optional*):\n",
      "            A list of callbacks to customize the training loop. Will add those to the list of default callbacks\n",
      "            detailed in [here](callback).\n",
      "\n",
      "            If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.\n",
      "        optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*, defaults to `(None, None)`):\n",
      "            A tuple containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your\n",
      "            model and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.\n",
      "        preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*):\n",
      "            A function that preprocess the logits right before caching them at each evaluation step. Must take two\n",
      "            tensors, the logits and the labels, and return the logits once processed as desired. The modifications made\n",
      "            by this function will be reflected in the predictions received by `compute_metrics`.\n",
      "\n",
      "            Note that the labels (second parameter) will be `None` if the dataset does not have them.\n",
      "\n",
      "    Important attributes:\n",
      "\n",
      "        - **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]\n",
      "          subclass.\n",
      "        - **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the\n",
      "          original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`,\n",
      "          the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner\n",
      "          model hasn't been wrapped, then `self.model_wrapped` is the same as `self.model`.\n",
      "        - **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from\n",
      "          data parallelism, this means some of the model layers are split on different GPUs).\n",
      "        - **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set\n",
      "          to `False` if model parallel or deepspeed is used, or if the default\n",
      "          `TrainingArguments.place_model_on_device` is overridden to return `False` .\n",
      "        - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while\n",
      "          in `train`)\n",
      "\n",
      "    \n",
      "__eq__: <slot wrapper '__eq__' of 'object' objects>\n",
      "__format__: <method '__format__' of 'object' objects>\n",
      "__ge__: <slot wrapper '__ge__' of 'object' objects>\n",
      "__getattribute__: <slot wrapper '__getattribute__' of 'object' objects>\n",
      "__gt__: <slot wrapper '__gt__' of 'object' objects>\n",
      "__hash__: <slot wrapper '__hash__' of 'object' objects>\n",
      "__init__: <function Trainer.__init__ at 0x2a7b5bdc0>\n",
      "__init_subclass__: <built-in method __init_subclass__ of type object at 0x2a44fa900>\n",
      "__le__: <slot wrapper '__le__' of 'object' objects>\n",
      "__lt__: <slot wrapper '__lt__' of 'object' objects>\n",
      "__module__: transformers.trainer\n",
      "__ne__: <slot wrapper '__ne__' of 'object' objects>\n",
      "__new__: <built-in method __new__ of type object at 0x100bc6b00>\n",
      "__reduce__: <method '__reduce__' of 'object' objects>\n",
      "__reduce_ex__: <method '__reduce_ex__' of 'object' objects>\n",
      "__repr__: <slot wrapper '__repr__' of 'object' objects>\n",
      "__setattr__: <slot wrapper '__setattr__' of 'object' objects>\n",
      "__sizeof__: <method '__sizeof__' of 'object' objects>\n",
      "__str__: <slot wrapper '__str__' of 'object' objects>\n",
      "__subclasshook__: <built-in method __subclasshook__ of type object at 0x2a44fa900>\n",
      "__weakref__: <attribute '__weakref__' of 'Trainer' objects>\n",
      "_activate_neftune: <function Trainer._activate_neftune at 0x2a7b5be50>\n",
      "_add_sm_patterns_to_gitignore: <function Trainer._add_sm_patterns_to_gitignore at 0x2a7b625e0>\n",
      "_deactivate_neftune: <function Trainer._deactivate_neftune at 0x2a7b5bee0>\n",
      "_finish_current_push: <function Trainer._finish_current_push at 0x2a7b623a0>\n",
      "_gather_and_numpify: <function Trainer._gather_and_numpify at 0x2a7b62550>\n",
      "_get_collator_with_removed_columns: <function Trainer._get_collator_with_removed_columns at 0x2a7b5e310>\n",
      "_get_eval_sampler: <function Trainer._get_eval_sampler at 0x2a7b5e4c0>\n",
      "_get_learning_rate: <function _get_learning_rate at 0x12db6a820>\n",
      "_get_output_dir: <function Trainer._get_output_dir at 0x2a7b5ef70>\n",
      "_get_train_sampler: <function Trainer._get_train_sampler at 0x2a7b5e3a0>\n",
      "_hp_search_setup: <function Trainer._hp_search_setup at 0x2a7b5ea60>\n",
      "_inner_training_loop: <function Trainer._inner_training_loop at 0x2a7b5eee0>\n",
      "_issue_warnings_after_load: <function Trainer._issue_warnings_after_load at 0x2a7b60160>\n",
      "_load_best_model: <function Trainer._load_best_model at 0x2a7b600d0>\n",
      "_load_from_checkpoint: <function Trainer._load_from_checkpoint at 0x2a7b60040>\n",
      "_load_optimizer_and_scheduler: <function Trainer._load_optimizer_and_scheduler at 0x2a7b604c0>\n",
      "_load_rng_state: <function Trainer._load_rng_state at 0x2a7b60280>\n",
      "_maybe_log_save_evaluate: <function Trainer._maybe_log_save_evaluate at 0x2a7b601f0>\n",
      "_move_model_to_device: <function Trainer._move_model_to_device at 0x2a7b5e160>\n",
      "_nested_gather: <function Trainer._nested_gather at 0x2a7b62040>\n",
      "_prepare_input: <function Trainer._prepare_input at 0x2a7b60670>\n",
      "_prepare_inputs: <function Trainer._prepare_inputs at 0x2a7b60700>\n",
      "_push_from_checkpoint: <function Trainer._push_from_checkpoint at 0x2a7b62310>\n",
      "_remove_unused_columns: <function Trainer._remove_unused_columns at 0x2a7b5e280>\n",
      "_report_to_hp_search: <function Trainer._report_to_hp_search at 0x2a7b5eaf0>\n",
      "_rotate_checkpoints: <function Trainer._rotate_checkpoints at 0x2a7b60dc0>\n",
      "_save: <function Trainer._save at 0x2a7b60c10>\n",
      "_save_checkpoint: <function Trainer._save_checkpoint at 0x2a7b60310>\n",
      "_save_optimizer_and_scheduler: <function Trainer._save_optimizer_and_scheduler at 0x2a7b60430>\n",
      "_save_rng_state: <function Trainer._save_rng_state at 0x2a7b603a0>\n",
      "_save_tpu: <function Trainer._save_tpu at 0x2a7b60b80>\n",
      "_set_signature_columns_if_needed: <function Trainer._set_signature_columns_if_needed at 0x2a7b5e1f0>\n",
      "_sorted_checkpoints: <function Trainer._sorted_checkpoints at 0x2a7b60d30>\n",
      "_tune_save_checkpoint: <function Trainer._tune_save_checkpoint at 0x2a7b5eb80>\n",
      "_wrap_model: <function Trainer._wrap_model at 0x2a7b5edc0>\n",
      "add_callback: <function Trainer.add_callback at 0x2a7b5bf70>\n",
      "autocast_smart_context_manager: <function Trainer.autocast_smart_context_manager at 0x2a7b60820>\n",
      "call_model_init: <function Trainer.call_model_init at 0x2a7b5ec10>\n",
      "compute_loss: <function Trainer.compute_loss at 0x2a7b60940>\n",
      "compute_loss_context_manager: <function Trainer.compute_loss_context_manager at 0x2a7b60790>\n",
      "create_accelerator_and_postprocess: <function Trainer.create_accelerator_and_postprocess at 0x2a7b62670>\n",
      "create_model_card: <function Trainer.create_model_card at 0x2a7b62280>\n",
      "create_optimizer: <function Trainer.create_optimizer at 0x2a7b5e790>\n",
      "create_optimizer_and_scheduler: <function Trainer.create_optimizer_and_scheduler at 0x2a7b5e670>\n",
      "create_scheduler: <function Trainer.create_scheduler at 0x2a7b5e8b0>\n",
      "evaluate: <function Trainer.evaluate at 0x2a7b60e50>\n",
      "evaluation_loop: <function Trainer.evaluation_loop at 0x2a7b60f70>\n",
      "floating_point_ops: <function Trainer.floating_point_ops at 0x2a7b62160>\n",
      "get_decay_parameter_names: <function Trainer.get_decay_parameter_names at 0x2a7b5e700>\n",
      "get_eval_dataloader: <function Trainer.get_eval_dataloader at 0x2a7b5e550>\n",
      "get_optimizer_cls_and_kwargs: <function Trainer.get_optimizer_cls_and_kwargs at 0x2a7b5e820>\n",
      "get_test_dataloader: <function Trainer.get_test_dataloader at 0x2a7b5e5e0>\n",
      "get_train_dataloader: <function Trainer.get_train_dataloader at 0x2a7b5e430>\n",
      "hyperparameter_search: <function Trainer.hyperparameter_search at 0x2a7b60550>\n",
      "init_hf_repo: <function Trainer.init_hf_repo at 0x2a7b621f0>\n",
      "ipex_optimize_model: <function Trainer.ipex_optimize_model at 0x2a7b5ed30>\n",
      "is_local_process_zero: <function Trainer.is_local_process_zero at 0x2a7b609d0>\n",
      "is_world_process_zero: <function Trainer.is_world_process_zero at 0x2a7b60a60>\n",
      "log: <function Trainer.log at 0x2a7b605e0>\n",
      "log_metrics: <function log_metrics at 0x12db6b160>\n",
      "metrics_format: <function metrics_format at 0x12db6b0d0>\n",
      "num_examples: <function Trainer.num_examples at 0x2a7b5e940>\n",
      "num_tokens: <function Trainer.num_tokens at 0x2a7b5e9d0>\n",
      "pop_callback: <function Trainer.pop_callback at 0x2a7b5e040>\n",
      "predict: <function Trainer.predict at 0x2a7b60ee0>\n",
      "prediction_loop: <function Trainer.prediction_loop at 0x2a7b624c0>\n",
      "prediction_step: <function Trainer.prediction_step at 0x2a7b620d0>\n",
      "propagate_args_to_deepspeed: <function Trainer.propagate_args_to_deepspeed at 0x2a7b62700>\n",
      "push_to_hub: <function Trainer.push_to_hub at 0x2a7b62430>\n",
      "remove_callback: <function Trainer.remove_callback at 0x2a7b5e0d0>\n",
      "save_metrics: <function save_metrics at 0x12db6b1f0>\n",
      "save_model: <function Trainer.save_model at 0x2a7b60af0>\n",
      "save_state: <function save_state at 0x12db6b280>\n",
      "store_flos: <function Trainer.store_flos at 0x2a7b60ca0>\n",
      "torch_jit_model_eval: <function Trainer.torch_jit_model_eval at 0x2a7b5eca0>\n",
      "train: <function Trainer.train at 0x2a7b5ee50>\n",
      "training_step: <function Trainer.training_step at 0x2a7b608b0>\n"
     ]
    }
   ],
   "source": [
    "trainer_all_members = get_members_of_function_or_method(Trainer, predicate=None)\n",
    "loop_through_members(trainer_all_members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve All Methods of a Class\n",
    "\n",
    "There are a few ways to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `__dict__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'_ChildClass__private_attr'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__doc__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__init__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__module__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__str__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'_protected_attr'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'class_attr'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'class_method'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'instance_method'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'read_only_attr'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'static_method'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'_ChildClass__private_attr'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__doc__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__init__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__module__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__str__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'_protected_attr'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'class_attr'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'class_method'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'instance_method'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'read_only_attr'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'static_method'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "child_class_methods_using_dict = list(ChildClass.__dict__.keys())\n",
    "pprint(sorted(child_class_methods_using_dict))\n",
    "\n",
    "assert 'parent_method' not in child_class_methods_using_dict\n",
    "assert 'read_only_attr' in child_class_methods_using_dict\n",
    "assert 'class_method' in child_class_methods_using_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the parent class methods are not included!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3;92mTrue\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pprint(instance_child.__class__.__dict__.keys() == ChildClass.__dict__.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `vars`\n",
    "\n",
    "`vars` and `__dict__` are equivalent, but people are preferring the former due\n",
    "to some efficiency reasons, which can be found [in this post](https://stackoverflow.com/questions/21297203/use-dict-or-vars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'_ChildClass__private_attr'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__doc__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__init__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__module__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__str__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'_protected_attr'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'class_attr'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'class_method'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'instance_method'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'read_only_attr'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'static_method'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'_ChildClass__private_attr'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__doc__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__init__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__module__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__str__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'_protected_attr'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'class_attr'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'class_method'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'instance_method'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'read_only_attr'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'static_method'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "child_class_methods_using_vars = list(vars(ChildClass).keys())\n",
    "pprint(sorted(child_class_methods_using_vars))\n",
    "\n",
    "assert 'parent_method' not in child_class_methods_using_vars\n",
    "assert 'read_only_attr' in child_class_methods_using_vars\n",
    "assert 'class_method' in child_class_methods_using_vars\n",
    "\n",
    "assert set(child_class_methods_using_dict) == set(child_class_methods_using_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `dir`\n",
    "\n",
    "To include the base/parent class methods, we can use `dir` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'_ChildClass__private_attr'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__class__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__delattr__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__dict__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__dir__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__doc__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__eq__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__format__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__ge__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__getattribute__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__gt__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__hash__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__init__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__init_subclass__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__le__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__lt__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__module__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__ne__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__new__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__reduce__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__reduce_ex__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__repr__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__setattr__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__sizeof__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__str__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__subclasshook__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'__weakref__'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'_protected_attr'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'class_attr'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'class_method'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'instance_method'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'parent_class_attr'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'parent_method'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'read_only_attr'</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'static_method'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'_ChildClass__private_attr'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__class__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__delattr__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__dict__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__dir__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__doc__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__eq__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__format__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__ge__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__getattribute__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__gt__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__hash__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__init__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__init_subclass__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__le__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__lt__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__module__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__ne__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__new__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__reduce__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__reduce_ex__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__repr__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__setattr__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__sizeof__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__str__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__subclasshook__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'__weakref__'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'_protected_attr'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'class_attr'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'class_method'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'instance_method'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'parent_class_attr'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'parent_method'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'read_only_attr'\u001b[0m,\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'static_method'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "child_class_methods_using_dir = list(dir(ChildClass))\n",
    "pprint(sorted(child_class_methods_using_dir))\n",
    "\n",
    "assert 'parent_method' in child_class_methods_using_dir\n",
    "assert 'read_only_attr' in child_class_methods_using_dir\n",
    "assert 'class_method' in child_class_methods_using_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `inspect.getmembers`\n",
    "\n",
    "We use `inspect.getmembers` to get all members of a class, and then filter out\n",
    "via the predicate `inspect.isroutine`, a stronger filter than `inspect.isfunction`\n",
    "or `inspect.ismethod`.\n",
    "\n",
    "We attach the source code of `inspect.isroutine` here for reference.\n",
    "\n",
    "```python\n",
    "def isroutine(object):\n",
    "    \"\"\"Return true if the object is any kind of function or method.\"\"\"\n",
    "    return (isbuiltin(object)\n",
    "            or isfunction(object)\n",
    "            or ismethod(object)\n",
    "            or ismethoddescriptor(object))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__delattr__'</span>, <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">slot</span><span style=\"color: #000000; text-decoration-color: #000000\"> wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__delattr__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__dir__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;method </span><span style=\"color: #008000; text-decoration-color: #008000\">'__dir__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__eq__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__eq__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__format__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;method </span><span style=\"color: #008000; text-decoration-color: #008000\">'__format__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__ge__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__ge__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__getattribute__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__getattribute__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__gt__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__gt__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__hash__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__hash__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__init__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function ChildClass.__init__ at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a95d9670</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__init_subclass__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;built-in method __init_subclass__ of type object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11e950d80</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__le__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__le__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__lt__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__lt__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__ne__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__ne__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__new__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;built-in method __new__ of type object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x100bc6b00</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__reduce__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;method </span><span style=\"color: #008000; text-decoration-color: #008000\">'__reduce__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__reduce_ex__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;method </span><span style=\"color: #008000; text-decoration-color: #008000\">'__reduce_ex__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__repr__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__repr__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__setattr__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__setattr__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__sizeof__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;method </span><span style=\"color: #008000; text-decoration-color: #008000\">'__sizeof__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__str__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function ChildClass.__str__ at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a95d9700</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__subclasshook__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;built-in method __subclasshook__ of type object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11e950d80</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'class_method'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;bound method ChildClass.class_method of &lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'__main__.ChildClass'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'instance_method'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function ChildClass.instance_method at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a95d9430</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'parent_method'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function ParentClass.parent_method at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a95d93a0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'static_method'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function ChildClass.static_method at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a95d9a60</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'__delattr__'\u001b[0m, \u001b[1m<\u001b[0m\u001b[1;95mslot\u001b[0m\u001b[39m wrapper \u001b[0m\u001b[32m'__delattr__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__dir__'\u001b[0m\u001b[39m, <method \u001b[0m\u001b[32m'__dir__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__eq__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__eq__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__format__'\u001b[0m\u001b[39m, <method \u001b[0m\u001b[32m'__format__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__ge__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__ge__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__getattribute__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__getattribute__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__gt__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__gt__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__hash__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__hash__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__init__'\u001b[0m\u001b[39m, <function ChildClass.__init__ at \u001b[0m\u001b[1;36m0x2a95d9670\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__init_subclass__'\u001b[0m\u001b[39m, <built-in method __init_subclass__ of type object at \u001b[0m\u001b[1;36m0x11e950d80\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__le__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__le__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__lt__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__lt__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__ne__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__ne__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__new__'\u001b[0m\u001b[39m, <built-in method __new__ of type object at \u001b[0m\u001b[1;36m0x100bc6b00\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__reduce__'\u001b[0m\u001b[39m, <method \u001b[0m\u001b[32m'__reduce__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__reduce_ex__'\u001b[0m\u001b[39m, <method \u001b[0m\u001b[32m'__reduce_ex__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__repr__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__repr__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__setattr__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__setattr__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__sizeof__'\u001b[0m\u001b[39m, <method \u001b[0m\u001b[32m'__sizeof__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__str__'\u001b[0m\u001b[39m, <function ChildClass.__str__ at \u001b[0m\u001b[1;36m0x2a95d9700\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__subclasshook__'\u001b[0m\u001b[39m, <built-in method __subclasshook__ of type object at \u001b[0m\u001b[1;36m0x11e950d80\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'class_method'\u001b[0m\u001b[39m, <bound method ChildClass.class_method of <class \u001b[0m\u001b[32m'__main__.ChildClass'\u001b[0m\u001b[39m>>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'instance_method'\u001b[0m\u001b[39m, <function ChildClass.instance_method at \u001b[0m\u001b[1;36m0x2a95d9430\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'parent_method'\u001b[0m\u001b[39m, <function ParentClass.parent_method at \u001b[0m\u001b[1;36m0x2a95d93a0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'static_method'\u001b[0m\u001b[39m, <function ChildClass.static_method at \u001b[0m\u001b[1;36m0x2a95d9a60\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicate = inspect.isroutine\n",
    "child_class_methods_using_getmembers = list(get_members_of_function_or_method(ChildClass, predicate=predicate))\n",
    "\n",
    "pprint(sorted(child_class_methods_using_getmembers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the reason to retrieve all methods is a convenience if we want to\n",
    "inspect all methods at once. And if we can obtain all methods, we can then\n",
    "iteratively inspect each method's signature.\n",
    "\n",
    "### Method Resolution Order\n",
    "\n",
    "The above examples do not take into account complicated cases, such as when\n",
    "the class is a subclass of **multiple** classes, in which case if you just\n",
    "print out the methods of the class, you will have a hard time to know which\n",
    "methods are from which class. You can do so via more filtering, but this is\n",
    "beyond the scope of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__call__'</span>, <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">function</span><span style=\"color: #000000; text-decoration-color: #000000\"> Module._wrapped_call_impl at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae1f0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__delattr__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.__delattr__ at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae550</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__dir__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.__dir__ at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdb2430</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__eq__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__eq__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__format__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;method </span><span style=\"color: #008000; text-decoration-color: #008000\">'__format__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__ge__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__ge__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__getattr__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.__getattr__ at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae430</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__getattribute__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__getattribute__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__getstate__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.__getstate__ at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae310</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__gt__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__gt__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__hash__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__hash__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__init__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GPT2LMHeadModel.__init__ at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7b925e0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__init_subclass__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;built-in method __init_subclass__ of type object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a3e3a9d0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__le__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__le__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__lt__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__lt__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__ne__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__ne__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__new__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;built-in method __new__ of type object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x100bc6b00</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__reduce__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;method </span><span style=\"color: #008000; text-decoration-color: #008000\">'__reduce__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__reduce_ex__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;method </span><span style=\"color: #008000; text-decoration-color: #008000\">'__reduce_ex__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__repr__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.__repr__ at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdb23a0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__setattr__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.__setattr__ at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae4c0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__setstate__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.__setstate__ at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae3a0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__sizeof__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;method </span><span style=\"color: #008000; text-decoration-color: #008000\">'__sizeof__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__str__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;slot wrapper </span><span style=\"color: #008000; text-decoration-color: #008000\">'__str__'</span><span style=\"color: #000000; text-decoration-color: #000000\"> of </span><span style=\"color: #008000; text-decoration-color: #008000\">'object'</span><span style=\"color: #000000; text-decoration-color: #000000\"> objects&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'__subclasshook__'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;built-in method __subclasshook__ of type object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a3e3a9d0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_apply'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module._apply at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdab550</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'_autoset_attn_implementation'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;bound method PreTrainedModel._autoset_attn_implementation of &lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'_backward_compatibility_gradient_checkpointing'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;function PreTrainedModel._backward_compatibility_gradient_checkpointing at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b3670</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_call_impl'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module._call_impl at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae280</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'_check_and_enable_flash_attn_2'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;bound method PreTrainedModel._check_and_enable_flash_attn_2 of &lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'_check_and_enable_sdpa'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;bound method PreTrainedModel._check_and_enable_sdpa of &lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_convert_head_mask_to_5d'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function ModuleUtilsMixin._convert_head_mask_to_5d at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b31f0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'_copy_lm_head_original_to_resized'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;function PreTrainedModel._copy_lm_head_original_to_resized at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b6430</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_create_repo'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PushToHubMixin._create_repo at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x12cb2a9d0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_dispatch_accelerate_model'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PeftAdapterMixin._dispatch_accelerate_model at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7279e50</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_expand_inputs_for_generation'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin._expand_inputs_for_generation at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727e430</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'_extract_past_from_model_output'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;function GenerationMixin._extract_past_from_model_output at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727e4c0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'_from_config'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;bound method PreTrainedModel._from_config of &lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_get_backward_hooks'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module._get_backward_hooks at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdabe50</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_get_backward_pre_hooks'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module._get_backward_pre_hooks at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdabee0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_get_candidate_generator'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin._get_candidate_generator at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727e670</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_get_decoder_start_token_id'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin._get_decoder_start_token_id at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727e3a0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_get_files_timestamps'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PushToHubMixin._get_files_timestamps at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x12cb2aa60</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_get_generation_mode'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin._get_generation_mode at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727e790</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_get_logits_processor'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin._get_logits_processor at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727e820</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_get_logits_warper'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin._get_logits_warper at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727e700</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_get_name'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module._get_name at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdb2280</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_get_no_split_modules'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel._get_no_split_modules at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b6160</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_get_resized_embeddings'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel._get_resized_embeddings at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b6310</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_get_resized_lm_head'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel._get_resized_lm_head at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b63a0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_get_stopping_criteria'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin._get_stopping_criteria at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727e8b0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_hook_rss_memory_post_forward'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function ModuleUtilsMixin._hook_rss_memory_post_forward at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b1ca0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_hook_rss_memory_pre_forward'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function ModuleUtilsMixin._hook_rss_memory_pre_forward at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b1c10</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_init_weights'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GPT2PreTrainedModel._init_weights at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7b8ee50</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_initialize_weights'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel._initialize_weights at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b3ee0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_load_from_state_dict'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module._load_from_state_dict at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae940</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'_load_pretrained_model'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;bound method PreTrainedModel._load_pretrained_model of &lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_load_pretrained_model_low_mem'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel._load_pretrained_model_low_mem at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b6ee0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'_maybe_initialize_input_ids_for_generation'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;function GenerationMixin._maybe_initialize_input_ids_for_generation at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727e160</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_maybe_warn_non_full_backward_hook'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module._maybe_warn_non_full_backward_hook at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdabf70</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_merge_criteria_processor_list'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin._merge_criteria_processor_list at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727e940</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_named_members'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module._named_members at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdaea60</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'_prepare_attention_mask_for_generation'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;function GenerationMixin._prepare_attention_mask_for_generation at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727e1f0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'_prepare_decoder_input_ids_for_generation'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;function GenerationMixin._prepare_decoder_input_ids_for_generation at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727e310</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'_prepare_encoder_decoder_kwargs_for_generation'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;function GenerationMixin._prepare_encoder_decoder_kwargs_for_generation at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727e280</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_prepare_model_inputs'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin._prepare_model_inputs at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727e0d0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_register_load_state_dict_pre_hook'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module._register_load_state_dict_pre_hook at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae820</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_register_state_dict_hook'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module._register_state_dict_hook at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae5e0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_reorder_cache'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GPT2LMHeadModel._reorder_cache at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7b929d0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_replicate_for_data_parallel'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module._replicate_for_data_parallel at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdb24c0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_resize_token_embeddings'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel._resize_token_embeddings at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b6280</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_save_to_state_dict'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module._save_to_state_dict at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae700</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'_set_default_torch_dtype'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;bound method PreTrainedModel._set_default_torch_dtype of &lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_set_gradient_checkpointing'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel._set_gradient_checkpointing at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b6790</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_slow_forward'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module._slow_forward at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae160</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_temporary_reorder_cache'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin._temporary_reorder_cache at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727ef70</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_tie_encoder_decoder_weights'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel._tie_encoder_decoder_weights at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b6040</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_tie_or_clone_weights'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel._tie_or_clone_weights at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b60d0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'_update_model_kwargs_for_generation'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;function GenerationMixin._update_model_kwargs_for_generation at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727e550</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_upload_modified_files'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PushToHubMixin._upload_modified_files at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x12cb2aaf0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_validate_generated_length'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin._validate_generated_length at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727eb80</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_validate_model_class'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin._validate_model_class at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727ea60</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_validate_model_kwargs'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin._validate_model_kwargs at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727eaf0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'_wrapped_call_impl'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module._wrapped_call_impl at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae1f0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'active_adapter'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PeftAdapterMixin.active_adapter at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7279d30</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'active_adapters'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PeftAdapterMixin.active_adapters at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7279ca0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'add_adapter'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PeftAdapterMixin.add_adapter at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7279a60</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'add_memory_hooks'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function ModuleUtilsMixin.add_memory_hooks at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b1d30</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'add_model_tags'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.add_model_tags at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b3700</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'add_module'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.add_module at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdab160</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'apply'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.apply at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdab5e0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'assisted_decoding'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin.assisted_decoding at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7279280</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'beam_sample'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin.beam_sample at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72790d0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'beam_search'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin.beam_search at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7279040</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'bfloat16'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.bfloat16 at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdabaf0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'buffers'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.buffers at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdaec10</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'can_generate'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;bound method PreTrainedModel.can_generate of &lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'children'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.children at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdaed30</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'compile'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.compile at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdb2550</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'compute_transition_scores'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin.compute_transition_scores at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727e9d0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'constrained_beam_search'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin.constrained_beam_search at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72791f0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'contrastive_search'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin.contrastive_search at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727edc0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'cpu'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.cpu at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdab820</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'create_extended_attention_mask_for_decoder'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;function ModuleUtilsMixin.create_extended_attention_mask_for_decoder at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b3040</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'cuda'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.cuda at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b6af0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'deparallelize'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GPT2LMHeadModel.deparallelize at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7b92820</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'disable_adapters'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PeftAdapterMixin.disable_adapters at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7279b80</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'disable_input_require_grads'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.disable_input_require_grads at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b3c10</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'double'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.double at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdab9d0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'enable_adapters'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PeftAdapterMixin.enable_adapters at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7279c10</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'enable_input_require_grads'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.enable_input_require_grads at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b3b80</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'estimate_tokens'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function ModuleUtilsMixin.estimate_tokens at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b3310</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'eval'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.eval at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdb2040</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'extra_repr'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.extra_repr at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdb2310</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'float'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.float at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b6ca0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'floating_point_ops'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function ModuleUtilsMixin.floating_point_ops at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b33a0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'forward'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GPT2LMHeadModel.forward at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7b92af0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'from_pretrained'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;bound method PreTrainedModel.from_pretrained of &lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'generate'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin.generate at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727eca0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'get_adapter_state_dict'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PeftAdapterMixin.get_adapter_state_dict at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7279dc0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'get_buffer'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.get_buffer at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdab3a0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'get_extended_attention_mask'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function ModuleUtilsMixin.get_extended_attention_mask at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b30d0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'get_extra_state'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.get_extra_state at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdab430</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'get_head_mask'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function ModuleUtilsMixin.get_head_mask at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b3160</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'get_input_embeddings'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.get_input_embeddings at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b3ca0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'get_memory_footprint'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.get_memory_footprint at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b6a60</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'get_output_embeddings'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GPT2LMHeadModel.get_output_embeddings at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7b92670</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'get_parameter'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.get_parameter at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdab310</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'get_position_embeddings'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.get_position_embeddings at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b6550</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'get_submodule'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.get_submodule at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdab280</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'gradient_checkpointing_disable'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.gradient_checkpointing_disable at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b6820</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'gradient_checkpointing_enable'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.gradient_checkpointing_enable at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b6700</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'greedy_search'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin.greedy_search at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727ee50</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'group_beam_search'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin.group_beam_search at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7279160</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'half'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.half at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b6c10</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'init_weights'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.init_weights at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b65e0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'invert_attention_mask'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function ModuleUtilsMixin.invert_attention_mask at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b1f70</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'ipu'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.ipu at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdab700</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'load_adapter'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PeftAdapterMixin.load_adapter at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72799d0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'load_state_dict'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.load_state_dict at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae9d0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'load_tf_weights'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function load_tf_weights_in_gpt2 at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7b62f70</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'modules'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.modules at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdaee50</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'named_buffers'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.named_buffers at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdaeca0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'named_children'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.named_children at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdaedc0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'named_modules'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.named_modules at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdaeee0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'named_parameters'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.named_parameters at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdaeb80</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'num_parameters'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function ModuleUtilsMixin.num_parameters at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b3280</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'parallelize'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GPT2LMHeadModel.parallelize at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7b92790</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'parameters'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.parameters at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdaeaf0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'post_init'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.post_init at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b35e0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'prepare_inputs_for_generation'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GPT2LMHeadModel.prepare_inputs_for_generation at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7b92940</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'prune_heads'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.prune_heads at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b6670</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'push_to_hub'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PushToHubMixin.push_to_hub at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b1b80</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'register_backward_hook'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.register_backward_hook at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdabd30</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'register_buffer'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.register_buffer at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdab040</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'register_for_auto_class'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;bound method PreTrainedModel.register_for_auto_class of &lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'register_forward_hook'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.register_forward_hook at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae0d0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'register_forward_pre_hook'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.register_forward_pre_hook at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae040</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'register_full_backward_hook'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.register_full_backward_hook at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdabdc0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'register_full_backward_pre_hook'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.register_full_backward_pre_hook at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdabca0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'register_load_state_dict_post_hook'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.register_load_state_dict_post_hook at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae8b0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'register_module'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.register_module at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdab1f0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'register_parameter'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.register_parameter at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdab0d0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'register_state_dict_pre_hook'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.register_state_dict_pre_hook at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae670</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'requires_grad_'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.requires_grad_ at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdb20d0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'reset_memory_hooks_state'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function ModuleUtilsMixin.reset_memory_hooks_state at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b1dc0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'resize_position_embeddings'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.resize_position_embeddings at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b64c0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'resize_token_embeddings'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.resize_token_embeddings at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b61f0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'retrieve_modules_from_names'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.retrieve_modules_from_names at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b6e50</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'reverse_bettertransformer'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.reverse_bettertransformer at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b90d0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'sample'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GenerationMixin.sample at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a727eee0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'save_pretrained'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.save_pretrained at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b6940</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'set_adapter'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PeftAdapterMixin.set_adapter at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7279af0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'set_extra_state'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.set_extra_state at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdab4c0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'set_input_embeddings'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.set_input_embeddings at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b3d30</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'set_output_embeddings'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function GPT2LMHeadModel.set_output_embeddings at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a7b928b0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'share_memory'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.share_memory at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdb21f0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'state_dict'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.state_dict at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdae790</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'tie_weights'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.tie_weights at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b3f70</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'to'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.to at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b6b80</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'to_bettertransformer'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function PreTrainedModel.to_bettertransformer at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b9040</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'to_empty'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.to_empty at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdabb80</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'train'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.train at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdaef70</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.type at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdab8b0</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #008000; text-decoration-color: #008000\">'warn_if_padding_and_no_attention_mask'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">      </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;function PreTrainedModel.warn_if_padding_and_no_attention_mask at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2a72b9160</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'xpu'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.xpu at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdab790</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'zero_grad'</span><span style=\"color: #000000; text-decoration-color: #000000\">, &lt;function Module.zero_grad at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x11cdb2160</span><span style=\"font-weight: bold\">&gt;)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1m(\u001b[0m\u001b[32m'__call__'\u001b[0m, \u001b[1m<\u001b[0m\u001b[1;95mfunction\u001b[0m\u001b[39m Module._wrapped_call_impl at \u001b[0m\u001b[1;36m0x11cdae1f0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__delattr__'\u001b[0m\u001b[39m, <function Module.__delattr__ at \u001b[0m\u001b[1;36m0x11cdae550\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__dir__'\u001b[0m\u001b[39m, <function Module.__dir__ at \u001b[0m\u001b[1;36m0x11cdb2430\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__eq__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__eq__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__format__'\u001b[0m\u001b[39m, <method \u001b[0m\u001b[32m'__format__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__ge__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__ge__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__getattr__'\u001b[0m\u001b[39m, <function Module.__getattr__ at \u001b[0m\u001b[1;36m0x11cdae430\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__getattribute__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__getattribute__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__getstate__'\u001b[0m\u001b[39m, <function Module.__getstate__ at \u001b[0m\u001b[1;36m0x11cdae310\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__gt__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__gt__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__hash__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__hash__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__init__'\u001b[0m\u001b[39m, <function GPT2LMHeadModel.__init__ at \u001b[0m\u001b[1;36m0x2a7b925e0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__init_subclass__'\u001b[0m\u001b[39m, <built-in method __init_subclass__ of type object at \u001b[0m\u001b[1;36m0x2a3e3a9d0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__le__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__le__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__lt__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__lt__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__ne__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__ne__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__new__'\u001b[0m\u001b[39m, <built-in method __new__ of type object at \u001b[0m\u001b[1;36m0x100bc6b00\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__reduce__'\u001b[0m\u001b[39m, <method \u001b[0m\u001b[32m'__reduce__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__reduce_ex__'\u001b[0m\u001b[39m, <method \u001b[0m\u001b[32m'__reduce_ex__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__repr__'\u001b[0m\u001b[39m, <function Module.__repr__ at \u001b[0m\u001b[1;36m0x11cdb23a0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__setattr__'\u001b[0m\u001b[39m, <function Module.__setattr__ at \u001b[0m\u001b[1;36m0x11cdae4c0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__setstate__'\u001b[0m\u001b[39m, <function Module.__setstate__ at \u001b[0m\u001b[1;36m0x11cdae3a0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__sizeof__'\u001b[0m\u001b[39m, <method \u001b[0m\u001b[32m'__sizeof__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__str__'\u001b[0m\u001b[39m, <slot wrapper \u001b[0m\u001b[32m'__str__'\u001b[0m\u001b[39m of \u001b[0m\u001b[32m'object'\u001b[0m\u001b[39m objects>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'__subclasshook__'\u001b[0m\u001b[39m, <built-in method __subclasshook__ of type object at \u001b[0m\u001b[1;36m0x2a3e3a9d0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_apply'\u001b[0m\u001b[39m, <function Module._apply at \u001b[0m\u001b[1;36m0x11cdab550\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'_autoset_attn_implementation'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<bound method PreTrainedModel._autoset_attn_implementation of <class \u001b[0m\u001b[32m'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'\u001b[0m\u001b[39m>>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'_backward_compatibility_gradient_checkpointing'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<function PreTrainedModel._backward_compatibility_gradient_checkpointing at \u001b[0m\u001b[1;36m0x2a72b3670\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_call_impl'\u001b[0m\u001b[39m, <function Module._call_impl at \u001b[0m\u001b[1;36m0x11cdae280\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'_check_and_enable_flash_attn_2'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<bound method PreTrainedModel._check_and_enable_flash_attn_2 of <class \u001b[0m\u001b[32m'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'\u001b[0m\u001b[39m>>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'_check_and_enable_sdpa'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<bound method PreTrainedModel._check_and_enable_sdpa of <class \u001b[0m\u001b[32m'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'\u001b[0m\u001b[39m>>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_convert_head_mask_to_5d'\u001b[0m\u001b[39m, <function ModuleUtilsMixin._convert_head_mask_to_5d at \u001b[0m\u001b[1;36m0x2a72b31f0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'_copy_lm_head_original_to_resized'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<function PreTrainedModel._copy_lm_head_original_to_resized at \u001b[0m\u001b[1;36m0x2a72b6430\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_create_repo'\u001b[0m\u001b[39m, <function PushToHubMixin._create_repo at \u001b[0m\u001b[1;36m0x12cb2a9d0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_dispatch_accelerate_model'\u001b[0m\u001b[39m, <function PeftAdapterMixin._dispatch_accelerate_model at \u001b[0m\u001b[1;36m0x2a7279e50\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_expand_inputs_for_generation'\u001b[0m\u001b[39m, <function GenerationMixin._expand_inputs_for_generation at \u001b[0m\u001b[1;36m0x2a727e430\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'_extract_past_from_model_output'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<function GenerationMixin._extract_past_from_model_output at \u001b[0m\u001b[1;36m0x2a727e4c0\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'_from_config'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<bound method PreTrainedModel._from_config of <class \u001b[0m\u001b[32m'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'\u001b[0m\u001b[39m>>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_get_backward_hooks'\u001b[0m\u001b[39m, <function Module._get_backward_hooks at \u001b[0m\u001b[1;36m0x11cdabe50\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_get_backward_pre_hooks'\u001b[0m\u001b[39m, <function Module._get_backward_pre_hooks at \u001b[0m\u001b[1;36m0x11cdabee0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_get_candidate_generator'\u001b[0m\u001b[39m, <function GenerationMixin._get_candidate_generator at \u001b[0m\u001b[1;36m0x2a727e670\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_get_decoder_start_token_id'\u001b[0m\u001b[39m, <function GenerationMixin._get_decoder_start_token_id at \u001b[0m\u001b[1;36m0x2a727e3a0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_get_files_timestamps'\u001b[0m\u001b[39m, <function PushToHubMixin._get_files_timestamps at \u001b[0m\u001b[1;36m0x12cb2aa60\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_get_generation_mode'\u001b[0m\u001b[39m, <function GenerationMixin._get_generation_mode at \u001b[0m\u001b[1;36m0x2a727e790\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_get_logits_processor'\u001b[0m\u001b[39m, <function GenerationMixin._get_logits_processor at \u001b[0m\u001b[1;36m0x2a727e820\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_get_logits_warper'\u001b[0m\u001b[39m, <function GenerationMixin._get_logits_warper at \u001b[0m\u001b[1;36m0x2a727e700\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_get_name'\u001b[0m\u001b[39m, <function Module._get_name at \u001b[0m\u001b[1;36m0x11cdb2280\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_get_no_split_modules'\u001b[0m\u001b[39m, <function PreTrainedModel._get_no_split_modules at \u001b[0m\u001b[1;36m0x2a72b6160\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_get_resized_embeddings'\u001b[0m\u001b[39m, <function PreTrainedModel._get_resized_embeddings at \u001b[0m\u001b[1;36m0x2a72b6310\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_get_resized_lm_head'\u001b[0m\u001b[39m, <function PreTrainedModel._get_resized_lm_head at \u001b[0m\u001b[1;36m0x2a72b63a0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_get_stopping_criteria'\u001b[0m\u001b[39m, <function GenerationMixin._get_stopping_criteria at \u001b[0m\u001b[1;36m0x2a727e8b0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_hook_rss_memory_post_forward'\u001b[0m\u001b[39m, <function ModuleUtilsMixin._hook_rss_memory_post_forward at \u001b[0m\u001b[1;36m0x2a72b1ca0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_hook_rss_memory_pre_forward'\u001b[0m\u001b[39m, <function ModuleUtilsMixin._hook_rss_memory_pre_forward at \u001b[0m\u001b[1;36m0x2a72b1c10\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_init_weights'\u001b[0m\u001b[39m, <function GPT2PreTrainedModel._init_weights at \u001b[0m\u001b[1;36m0x2a7b8ee50\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_initialize_weights'\u001b[0m\u001b[39m, <function PreTrainedModel._initialize_weights at \u001b[0m\u001b[1;36m0x2a72b3ee0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_load_from_state_dict'\u001b[0m\u001b[39m, <function Module._load_from_state_dict at \u001b[0m\u001b[1;36m0x11cdae940\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'_load_pretrained_model'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<bound method PreTrainedModel._load_pretrained_model of <class \u001b[0m\u001b[32m'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'\u001b[0m\u001b[39m>>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_load_pretrained_model_low_mem'\u001b[0m\u001b[39m, <function PreTrainedModel._load_pretrained_model_low_mem at \u001b[0m\u001b[1;36m0x2a72b6ee0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'_maybe_initialize_input_ids_for_generation'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<function GenerationMixin._maybe_initialize_input_ids_for_generation at \u001b[0m\u001b[1;36m0x2a727e160\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_maybe_warn_non_full_backward_hook'\u001b[0m\u001b[39m, <function Module._maybe_warn_non_full_backward_hook at \u001b[0m\u001b[1;36m0x11cdabf70\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_merge_criteria_processor_list'\u001b[0m\u001b[39m, <function GenerationMixin._merge_criteria_processor_list at \u001b[0m\u001b[1;36m0x2a727e940\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_named_members'\u001b[0m\u001b[39m, <function Module._named_members at \u001b[0m\u001b[1;36m0x11cdaea60\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'_prepare_attention_mask_for_generation'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<function GenerationMixin._prepare_attention_mask_for_generation at \u001b[0m\u001b[1;36m0x2a727e1f0\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'_prepare_decoder_input_ids_for_generation'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<function GenerationMixin._prepare_decoder_input_ids_for_generation at \u001b[0m\u001b[1;36m0x2a727e310\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'_prepare_encoder_decoder_kwargs_for_generation'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<function GenerationMixin._prepare_encoder_decoder_kwargs_for_generation at \u001b[0m\u001b[1;36m0x2a727e280\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_prepare_model_inputs'\u001b[0m\u001b[39m, <function GenerationMixin._prepare_model_inputs at \u001b[0m\u001b[1;36m0x2a727e0d0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_register_load_state_dict_pre_hook'\u001b[0m\u001b[39m, <function Module._register_load_state_dict_pre_hook at \u001b[0m\u001b[1;36m0x11cdae820\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_register_state_dict_hook'\u001b[0m\u001b[39m, <function Module._register_state_dict_hook at \u001b[0m\u001b[1;36m0x11cdae5e0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_reorder_cache'\u001b[0m\u001b[39m, <function GPT2LMHeadModel._reorder_cache at \u001b[0m\u001b[1;36m0x2a7b929d0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_replicate_for_data_parallel'\u001b[0m\u001b[39m, <function Module._replicate_for_data_parallel at \u001b[0m\u001b[1;36m0x11cdb24c0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_resize_token_embeddings'\u001b[0m\u001b[39m, <function PreTrainedModel._resize_token_embeddings at \u001b[0m\u001b[1;36m0x2a72b6280\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_save_to_state_dict'\u001b[0m\u001b[39m, <function Module._save_to_state_dict at \u001b[0m\u001b[1;36m0x11cdae700\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'_set_default_torch_dtype'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<bound method PreTrainedModel._set_default_torch_dtype of <class \u001b[0m\u001b[32m'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'\u001b[0m\u001b[39m>>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_set_gradient_checkpointing'\u001b[0m\u001b[39m, <function PreTrainedModel._set_gradient_checkpointing at \u001b[0m\u001b[1;36m0x2a72b6790\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_slow_forward'\u001b[0m\u001b[39m, <function Module._slow_forward at \u001b[0m\u001b[1;36m0x11cdae160\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_temporary_reorder_cache'\u001b[0m\u001b[39m, <function GenerationMixin._temporary_reorder_cache at \u001b[0m\u001b[1;36m0x2a727ef70\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_tie_encoder_decoder_weights'\u001b[0m\u001b[39m, <function PreTrainedModel._tie_encoder_decoder_weights at \u001b[0m\u001b[1;36m0x2a72b6040\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_tie_or_clone_weights'\u001b[0m\u001b[39m, <function PreTrainedModel._tie_or_clone_weights at \u001b[0m\u001b[1;36m0x2a72b60d0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'_update_model_kwargs_for_generation'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<function GenerationMixin._update_model_kwargs_for_generation at \u001b[0m\u001b[1;36m0x2a727e550\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_upload_modified_files'\u001b[0m\u001b[39m, <function PushToHubMixin._upload_modified_files at \u001b[0m\u001b[1;36m0x12cb2aaf0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_validate_generated_length'\u001b[0m\u001b[39m, <function GenerationMixin._validate_generated_length at \u001b[0m\u001b[1;36m0x2a727eb80\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_validate_model_class'\u001b[0m\u001b[39m, <function GenerationMixin._validate_model_class at \u001b[0m\u001b[1;36m0x2a727ea60\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_validate_model_kwargs'\u001b[0m\u001b[39m, <function GenerationMixin._validate_model_kwargs at \u001b[0m\u001b[1;36m0x2a727eaf0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'_wrapped_call_impl'\u001b[0m\u001b[39m, <function Module._wrapped_call_impl at \u001b[0m\u001b[1;36m0x11cdae1f0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'active_adapter'\u001b[0m\u001b[39m, <function PeftAdapterMixin.active_adapter at \u001b[0m\u001b[1;36m0x2a7279d30\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'active_adapters'\u001b[0m\u001b[39m, <function PeftAdapterMixin.active_adapters at \u001b[0m\u001b[1;36m0x2a7279ca0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'add_adapter'\u001b[0m\u001b[39m, <function PeftAdapterMixin.add_adapter at \u001b[0m\u001b[1;36m0x2a7279a60\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'add_memory_hooks'\u001b[0m\u001b[39m, <function ModuleUtilsMixin.add_memory_hooks at \u001b[0m\u001b[1;36m0x2a72b1d30\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'add_model_tags'\u001b[0m\u001b[39m, <function PreTrainedModel.add_model_tags at \u001b[0m\u001b[1;36m0x2a72b3700\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'add_module'\u001b[0m\u001b[39m, <function Module.add_module at \u001b[0m\u001b[1;36m0x11cdab160\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'apply'\u001b[0m\u001b[39m, <function Module.apply at \u001b[0m\u001b[1;36m0x11cdab5e0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'assisted_decoding'\u001b[0m\u001b[39m, <function GenerationMixin.assisted_decoding at \u001b[0m\u001b[1;36m0x2a7279280\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'beam_sample'\u001b[0m\u001b[39m, <function GenerationMixin.beam_sample at \u001b[0m\u001b[1;36m0x2a72790d0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'beam_search'\u001b[0m\u001b[39m, <function GenerationMixin.beam_search at \u001b[0m\u001b[1;36m0x2a7279040\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'bfloat16'\u001b[0m\u001b[39m, <function Module.bfloat16 at \u001b[0m\u001b[1;36m0x11cdabaf0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'buffers'\u001b[0m\u001b[39m, <function Module.buffers at \u001b[0m\u001b[1;36m0x11cdaec10\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'can_generate'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<bound method PreTrainedModel.can_generate of <class \u001b[0m\u001b[32m'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'\u001b[0m\u001b[39m>>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'children'\u001b[0m\u001b[39m, <function Module.children at \u001b[0m\u001b[1;36m0x11cdaed30\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'compile'\u001b[0m\u001b[39m, <function Module.compile at \u001b[0m\u001b[1;36m0x11cdb2550\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'compute_transition_scores'\u001b[0m\u001b[39m, <function GenerationMixin.compute_transition_scores at \u001b[0m\u001b[1;36m0x2a727e9d0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'constrained_beam_search'\u001b[0m\u001b[39m, <function GenerationMixin.constrained_beam_search at \u001b[0m\u001b[1;36m0x2a72791f0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'contrastive_search'\u001b[0m\u001b[39m, <function GenerationMixin.contrastive_search at \u001b[0m\u001b[1;36m0x2a727edc0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'cpu'\u001b[0m\u001b[39m, <function Module.cpu at \u001b[0m\u001b[1;36m0x11cdab820\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'create_extended_attention_mask_for_decoder'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<function ModuleUtilsMixin.create_extended_attention_mask_for_decoder at \u001b[0m\u001b[1;36m0x2a72b3040\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'cuda'\u001b[0m\u001b[39m, <function Module.cuda at \u001b[0m\u001b[1;36m0x2a72b6af0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'deparallelize'\u001b[0m\u001b[39m, <function GPT2LMHeadModel.deparallelize at \u001b[0m\u001b[1;36m0x2a7b92820\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'disable_adapters'\u001b[0m\u001b[39m, <function PeftAdapterMixin.disable_adapters at \u001b[0m\u001b[1;36m0x2a7279b80\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'disable_input_require_grads'\u001b[0m\u001b[39m, <function PreTrainedModel.disable_input_require_grads at \u001b[0m\u001b[1;36m0x2a72b3c10\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'double'\u001b[0m\u001b[39m, <function Module.double at \u001b[0m\u001b[1;36m0x11cdab9d0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'enable_adapters'\u001b[0m\u001b[39m, <function PeftAdapterMixin.enable_adapters at \u001b[0m\u001b[1;36m0x2a7279c10\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'enable_input_require_grads'\u001b[0m\u001b[39m, <function PreTrainedModel.enable_input_require_grads at \u001b[0m\u001b[1;36m0x2a72b3b80\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'estimate_tokens'\u001b[0m\u001b[39m, <function ModuleUtilsMixin.estimate_tokens at \u001b[0m\u001b[1;36m0x2a72b3310\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'eval'\u001b[0m\u001b[39m, <function Module.eval at \u001b[0m\u001b[1;36m0x11cdb2040\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'extra_repr'\u001b[0m\u001b[39m, <function Module.extra_repr at \u001b[0m\u001b[1;36m0x11cdb2310\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'float'\u001b[0m\u001b[39m, <function PreTrainedModel.float at \u001b[0m\u001b[1;36m0x2a72b6ca0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'floating_point_ops'\u001b[0m\u001b[39m, <function ModuleUtilsMixin.floating_point_ops at \u001b[0m\u001b[1;36m0x2a72b33a0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'forward'\u001b[0m\u001b[39m, <function GPT2LMHeadModel.forward at \u001b[0m\u001b[1;36m0x2a7b92af0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'from_pretrained'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<bound method PreTrainedModel.from_pretrained of <class \u001b[0m\u001b[32m'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'\u001b[0m\u001b[39m>>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'generate'\u001b[0m\u001b[39m, <function GenerationMixin.generate at \u001b[0m\u001b[1;36m0x2a727eca0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'get_adapter_state_dict'\u001b[0m\u001b[39m, <function PeftAdapterMixin.get_adapter_state_dict at \u001b[0m\u001b[1;36m0x2a7279dc0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'get_buffer'\u001b[0m\u001b[39m, <function Module.get_buffer at \u001b[0m\u001b[1;36m0x11cdab3a0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'get_extended_attention_mask'\u001b[0m\u001b[39m, <function ModuleUtilsMixin.get_extended_attention_mask at \u001b[0m\u001b[1;36m0x2a72b30d0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'get_extra_state'\u001b[0m\u001b[39m, <function Module.get_extra_state at \u001b[0m\u001b[1;36m0x11cdab430\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'get_head_mask'\u001b[0m\u001b[39m, <function ModuleUtilsMixin.get_head_mask at \u001b[0m\u001b[1;36m0x2a72b3160\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'get_input_embeddings'\u001b[0m\u001b[39m, <function PreTrainedModel.get_input_embeddings at \u001b[0m\u001b[1;36m0x2a72b3ca0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'get_memory_footprint'\u001b[0m\u001b[39m, <function PreTrainedModel.get_memory_footprint at \u001b[0m\u001b[1;36m0x2a72b6a60\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'get_output_embeddings'\u001b[0m\u001b[39m, <function GPT2LMHeadModel.get_output_embeddings at \u001b[0m\u001b[1;36m0x2a7b92670\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'get_parameter'\u001b[0m\u001b[39m, <function Module.get_parameter at \u001b[0m\u001b[1;36m0x11cdab310\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'get_position_embeddings'\u001b[0m\u001b[39m, <function PreTrainedModel.get_position_embeddings at \u001b[0m\u001b[1;36m0x2a72b6550\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'get_submodule'\u001b[0m\u001b[39m, <function Module.get_submodule at \u001b[0m\u001b[1;36m0x11cdab280\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'gradient_checkpointing_disable'\u001b[0m\u001b[39m, <function PreTrainedModel.gradient_checkpointing_disable at \u001b[0m\u001b[1;36m0x2a72b6820\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'gradient_checkpointing_enable'\u001b[0m\u001b[39m, <function PreTrainedModel.gradient_checkpointing_enable at \u001b[0m\u001b[1;36m0x2a72b6700\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'greedy_search'\u001b[0m\u001b[39m, <function GenerationMixin.greedy_search at \u001b[0m\u001b[1;36m0x2a727ee50\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'group_beam_search'\u001b[0m\u001b[39m, <function GenerationMixin.group_beam_search at \u001b[0m\u001b[1;36m0x2a7279160\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'half'\u001b[0m\u001b[39m, <function PreTrainedModel.half at \u001b[0m\u001b[1;36m0x2a72b6c10\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'init_weights'\u001b[0m\u001b[39m, <function PreTrainedModel.init_weights at \u001b[0m\u001b[1;36m0x2a72b65e0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'invert_attention_mask'\u001b[0m\u001b[39m, <function ModuleUtilsMixin.invert_attention_mask at \u001b[0m\u001b[1;36m0x2a72b1f70\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'ipu'\u001b[0m\u001b[39m, <function Module.ipu at \u001b[0m\u001b[1;36m0x11cdab700\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'load_adapter'\u001b[0m\u001b[39m, <function PeftAdapterMixin.load_adapter at \u001b[0m\u001b[1;36m0x2a72799d0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'load_state_dict'\u001b[0m\u001b[39m, <function Module.load_state_dict at \u001b[0m\u001b[1;36m0x11cdae9d0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'load_tf_weights'\u001b[0m\u001b[39m, <function load_tf_weights_in_gpt2 at \u001b[0m\u001b[1;36m0x2a7b62f70\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'modules'\u001b[0m\u001b[39m, <function Module.modules at \u001b[0m\u001b[1;36m0x11cdaee50\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'named_buffers'\u001b[0m\u001b[39m, <function Module.named_buffers at \u001b[0m\u001b[1;36m0x11cdaeca0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'named_children'\u001b[0m\u001b[39m, <function Module.named_children at \u001b[0m\u001b[1;36m0x11cdaedc0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'named_modules'\u001b[0m\u001b[39m, <function Module.named_modules at \u001b[0m\u001b[1;36m0x11cdaeee0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'named_parameters'\u001b[0m\u001b[39m, <function Module.named_parameters at \u001b[0m\u001b[1;36m0x11cdaeb80\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'num_parameters'\u001b[0m\u001b[39m, <function ModuleUtilsMixin.num_parameters at \u001b[0m\u001b[1;36m0x2a72b3280\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'parallelize'\u001b[0m\u001b[39m, <function GPT2LMHeadModel.parallelize at \u001b[0m\u001b[1;36m0x2a7b92790\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'parameters'\u001b[0m\u001b[39m, <function Module.parameters at \u001b[0m\u001b[1;36m0x11cdaeaf0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'post_init'\u001b[0m\u001b[39m, <function PreTrainedModel.post_init at \u001b[0m\u001b[1;36m0x2a72b35e0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'prepare_inputs_for_generation'\u001b[0m\u001b[39m, <function GPT2LMHeadModel.prepare_inputs_for_generation at \u001b[0m\u001b[1;36m0x2a7b92940\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'prune_heads'\u001b[0m\u001b[39m, <function PreTrainedModel.prune_heads at \u001b[0m\u001b[1;36m0x2a72b6670\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'push_to_hub'\u001b[0m\u001b[39m, <function PushToHubMixin.push_to_hub at \u001b[0m\u001b[1;36m0x2a72b1b80\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'register_backward_hook'\u001b[0m\u001b[39m, <function Module.register_backward_hook at \u001b[0m\u001b[1;36m0x11cdabd30\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'register_buffer'\u001b[0m\u001b[39m, <function Module.register_buffer at \u001b[0m\u001b[1;36m0x11cdab040\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'register_for_auto_class'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<bound method PreTrainedModel.register_for_auto_class of <class \u001b[0m\u001b[32m'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'\u001b[0m\u001b[39m>>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'register_forward_hook'\u001b[0m\u001b[39m, <function Module.register_forward_hook at \u001b[0m\u001b[1;36m0x11cdae0d0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'register_forward_pre_hook'\u001b[0m\u001b[39m, <function Module.register_forward_pre_hook at \u001b[0m\u001b[1;36m0x11cdae040\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'register_full_backward_hook'\u001b[0m\u001b[39m, <function Module.register_full_backward_hook at \u001b[0m\u001b[1;36m0x11cdabdc0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'register_full_backward_pre_hook'\u001b[0m\u001b[39m, <function Module.register_full_backward_pre_hook at \u001b[0m\u001b[1;36m0x11cdabca0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'register_load_state_dict_post_hook'\u001b[0m\u001b[39m, <function Module.register_load_state_dict_post_hook at \u001b[0m\u001b[1;36m0x11cdae8b0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'register_module'\u001b[0m\u001b[39m, <function Module.register_module at \u001b[0m\u001b[1;36m0x11cdab1f0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'register_parameter'\u001b[0m\u001b[39m, <function Module.register_parameter at \u001b[0m\u001b[1;36m0x11cdab0d0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'register_state_dict_pre_hook'\u001b[0m\u001b[39m, <function Module.register_state_dict_pre_hook at \u001b[0m\u001b[1;36m0x11cdae670\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'requires_grad_'\u001b[0m\u001b[39m, <function Module.requires_grad_ at \u001b[0m\u001b[1;36m0x11cdb20d0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'reset_memory_hooks_state'\u001b[0m\u001b[39m, <function ModuleUtilsMixin.reset_memory_hooks_state at \u001b[0m\u001b[1;36m0x2a72b1dc0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'resize_position_embeddings'\u001b[0m\u001b[39m, <function PreTrainedModel.resize_position_embeddings at \u001b[0m\u001b[1;36m0x2a72b64c0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'resize_token_embeddings'\u001b[0m\u001b[39m, <function PreTrainedModel.resize_token_embeddings at \u001b[0m\u001b[1;36m0x2a72b61f0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'retrieve_modules_from_names'\u001b[0m\u001b[39m, <function PreTrainedModel.retrieve_modules_from_names at \u001b[0m\u001b[1;36m0x2a72b6e50\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'reverse_bettertransformer'\u001b[0m\u001b[39m, <function PreTrainedModel.reverse_bettertransformer at \u001b[0m\u001b[1;36m0x2a72b90d0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'sample'\u001b[0m\u001b[39m, <function GenerationMixin.sample at \u001b[0m\u001b[1;36m0x2a727eee0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'save_pretrained'\u001b[0m\u001b[39m, <function PreTrainedModel.save_pretrained at \u001b[0m\u001b[1;36m0x2a72b6940\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'set_adapter'\u001b[0m\u001b[39m, <function PeftAdapterMixin.set_adapter at \u001b[0m\u001b[1;36m0x2a7279af0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'set_extra_state'\u001b[0m\u001b[39m, <function Module.set_extra_state at \u001b[0m\u001b[1;36m0x11cdab4c0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'set_input_embeddings'\u001b[0m\u001b[39m, <function PreTrainedModel.set_input_embeddings at \u001b[0m\u001b[1;36m0x2a72b3d30\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'set_output_embeddings'\u001b[0m\u001b[39m, <function GPT2LMHeadModel.set_output_embeddings at \u001b[0m\u001b[1;36m0x2a7b928b0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'share_memory'\u001b[0m\u001b[39m, <function Module.share_memory at \u001b[0m\u001b[1;36m0x11cdb21f0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'state_dict'\u001b[0m\u001b[39m, <function Module.state_dict at \u001b[0m\u001b[1;36m0x11cdae790\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'tie_weights'\u001b[0m\u001b[39m, <function PreTrainedModel.tie_weights at \u001b[0m\u001b[1;36m0x2a72b3f70\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'to'\u001b[0m\u001b[39m, <function Module.to at \u001b[0m\u001b[1;36m0x2a72b6b80\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'to_bettertransformer'\u001b[0m\u001b[39m, <function PreTrainedModel.to_bettertransformer at \u001b[0m\u001b[1;36m0x2a72b9040\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'to_empty'\u001b[0m\u001b[39m, <function Module.to_empty at \u001b[0m\u001b[1;36m0x11cdabb80\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'train'\u001b[0m\u001b[39m, <function Module.train at \u001b[0m\u001b[1;36m0x11cdaef70\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'type'\u001b[0m\u001b[39m, <function Module.type at \u001b[0m\u001b[1;36m0x11cdab8b0\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[32m'warn_if_padding_and_no_attention_mask'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m      \u001b[0m\u001b[39m<function PreTrainedModel.warn_if_padding_and_no_attention_mask at \u001b[0m\u001b[1;36m0x2a72b9160\u001b[0m\u001b[39m>\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'xpu'\u001b[0m\u001b[39m, <function Module.xpu at \u001b[0m\u001b[1;36m0x11cdab790\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1;39m(\u001b[0m\u001b[32m'zero_grad'\u001b[0m\u001b[39m, <function Module.zero_grad at \u001b[0m\u001b[1;36m0x11cdb2160\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicate = inspect.isroutine\n",
    "GPT2LMHeadModel_methods_using_getmembers = list(get_members_of_function_or_method(GPT2LMHeadModel, predicate=predicate))\n",
    "\n",
    "pprint(sorted(GPT2LMHeadModel_methods_using_getmembers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the method resolution order (MRO) of a class via `cls.__mro__`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel,\n",
       " transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel,\n",
       " transformers.modeling_utils.PreTrainedModel,\n",
       " torch.nn.modules.module.Module,\n",
       " transformers.modeling_utils.ModuleUtilsMixin,\n",
       " transformers.generation.utils.GenerationMixin,\n",
       " transformers.utils.hub.PushToHubMixin,\n",
       " transformers.integrations.peft.PeftAdapterMixin,\n",
       " object)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getmro(GPT2LMHeadModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pseudocode to get all signatures of a class via MRO is as follows:\n",
    "\n",
    "```python\n",
    "def get_all_args(cls: Type[object]) -> Dict[str, inspect.Parameter]:\n",
    "    mro = inspect.getmro(cls)\n",
    "    all_args = {}\n",
    "    for base_class in mro[::-1]:  # reverse to start from topmost class\n",
    "        if base_class is object:  # skip the base 'object' class\n",
    "            continue\n",
    "        sig = inspect.signature(base_class.__init__)\n",
    "        all_args.update(sig.parameters)\n",
    "    return all_args\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Signature and Type Annotations of a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">mappingproxy</span><span style=\"font-weight: bold\">({</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'a'</span>: <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">Parameter</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">\"a: int\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'b'</span><span style=\"color: #000000; text-decoration-color: #000000\">: &lt;Parameter </span><span style=\"color: #008000; text-decoration-color: #008000\">\"b: str\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'c'</span><span style=\"color: #000000; text-decoration-color: #000000\">: &lt;Parameter </span><span style=\"color: #008000; text-decoration-color: #008000\">\"c: List[int]\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'d'</span><span style=\"color: #000000; text-decoration-color: #000000\">: &lt;Parameter </span><span style=\"color: #008000; text-decoration-color: #008000\">\"d: Tuple[str, str]\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'e'</span><span style=\"color: #000000; text-decoration-color: #000000\">: &lt;Parameter </span><span style=\"color: #008000; text-decoration-color: #008000\">\"e: Union[int, str]\"</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #008000; text-decoration-color: #008000\">'kwargs'</span><span style=\"color: #000000; text-decoration-color: #000000\">: &lt;Parameter </span><span style=\"color: #008000; text-decoration-color: #008000\">\"**kwargs: Any\"</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "<span style=\"font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mmappingproxy\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'a'\u001b[0m: \u001b[1m<\u001b[0m\u001b[1;95mParameter\u001b[0m\u001b[39m \u001b[0m\u001b[32m\"a: int\"\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'b'\u001b[0m\u001b[39m: <Parameter \u001b[0m\u001b[32m\"b: str\"\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'c'\u001b[0m\u001b[39m: <Parameter \u001b[0m\u001b[32m\"c: List\u001b[0m\u001b[32m[\u001b[0m\u001b[32mint\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\"\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'd'\u001b[0m\u001b[39m: <Parameter \u001b[0m\u001b[32m\"d: Tuple\u001b[0m\u001b[32m[\u001b[0m\u001b[32mstr, str\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\"\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'e'\u001b[0m\u001b[39m: <Parameter \u001b[0m\u001b[32m\"e: Union\u001b[0m\u001b[32m[\u001b[0m\u001b[32mint, str\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\"\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[32m'kwargs'\u001b[0m\u001b[39m: <Parameter \u001b[0m\u001b[32m\"**kwargs: Any\"\u001b[0m\u001b[1m>\u001b[0m\n",
       "\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'str'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'str'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "func_sig: Signature = inspect.signature(func)\n",
    "pprint(func_sig.parameters)\n",
    "pprint(func_sig.return_annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the 4 key properties of the `Parameter` object\n",
    "of the `Signature` object.\n",
    "\n",
    "```python\n",
    "@property\n",
    "def name(self):\n",
    "    return self._name\n",
    "\n",
    "@property\n",
    "def default(self):\n",
    "    return self._default\n",
    "\n",
    "@property\n",
    "def annotation(self):\n",
    "    return self._annotation\n",
    "\n",
    "@property\n",
    "def kind(self):\n",
    "    return self._kind\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use `get_type_hints` to get the type hints of a function\n",
    "instead of using the `annotations` property of `inspect.Signature`. The reason\n",
    "can be found in the docstring of `get_type_hints`:\n",
    "\n",
    "```python\n",
    "def get_type_hints(obj, globalns=None, localns=None, include_extras=False):\n",
    "    \"\"\"Return type hints for an object.\n",
    "\n",
    "    This is often the same as obj.__annotations__, but it handles\n",
    "    forward references encoded as string literals, adds Optional[t] if a\n",
    "    default value equal to None is set and recursively replaces all\n",
    "    'Annotated[T, ...]' with 'T' (unless 'include_extras=True').\n",
    "\n",
    "    ...\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_type_hints(a, b, c, d, e, **kwargs):\n",
    "    return a, b, c, d, e, kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, <Signature (a, b, c, d, e, **kwargs)>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_type_hints(no_type_hints), inspect.signature(no_type_hints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to know if a parameter is optional or not? We can use the `inspect.Parameter.empty`\n",
    "property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'inspect._empty'>\n",
      "True\n",
      "<class 'inspect._empty'>\n",
      "True\n",
      "<class 'inspect._empty'>\n",
      "True\n",
      "<class 'inspect._empty'>\n",
      "True\n",
      "<class 'inspect._empty'>\n",
      "True\n",
      "<class 'inspect._empty'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for name, value in inspect.signature(func).parameters.items():\n",
    "    print(value.default)\n",
    "    print(value.default is inspect.Parameter.empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use `__mro__` to get the method resolution order of a class\n",
    "because `__bases__` only returns the immediate parent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((__main__.ParentClass,),\n",
       " (torch.nn.modules.module.Module,\n",
       "  transformers.modeling_utils.ModuleUtilsMixin,\n",
       "  transformers.generation.utils.GenerationMixin,\n",
       "  transformers.utils.hub.PushToHubMixin,\n",
       "  transformers.integrations.peft.PeftAdapterMixin))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChildClass.__bases__, GPT2LMHeadModel.__bases__[0].__bases__[0].__bases__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[object,\n",
       " transformers.integrations.peft.PeftAdapterMixin,\n",
       " transformers.utils.hub.PushToHubMixin,\n",
       " transformers.generation.utils.GenerationMixin,\n",
       " transformers.modeling_utils.ModuleUtilsMixin,\n",
       " torch.nn.modules.module.Module,\n",
       " transformers.modeling_utils.PreTrainedModel,\n",
       " transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel,\n",
       " transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed(inspect.getmro(GPT2LMHeadModel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.modeling_utils.ModuleUtilsMixin'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.utils.hub.PushToHubMixin'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.generation.utils.GenerationMixin'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.nn.modules.module.Module'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.integrations.peft.PeftAdapterMixin'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">   </span><span style=\"color: #000000; text-decoration-color: #000000\">&lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'transformers.modeling_utils.PreTrainedModel'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'transformers.modeling_utils.ModuleUtilsMixin'\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[39m<class \u001b[0m\u001b[32m'transformers.utils.hub.PushToHubMixin'\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[39m<class \u001b[0m\u001b[32m'transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel'\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[39m<class \u001b[0m\u001b[32m'transformers.generation.utils.GenerationMixin'\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[39m<class \u001b[0m\u001b[32m'torch.nn.modules.module.Module'\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[39m<class \u001b[0m\u001b[32m'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[39m<class \u001b[0m\u001b[32m'transformers.integrations.peft.PeftAdapterMixin'\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m   \u001b[0m\u001b[39m<class \u001b[0m\u001b[32m'transformers.modeling_utils.PreTrainedModel'\u001b[0m\u001b[1m>\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_base_classes(cls: Type[Any], include_self: bool = False) -> Set[Type[Any]]:\n",
    "    \"\"\"\n",
    "    Get the base classes of a class and all its base classes.\n",
    "    \"\"\"\n",
    "    return set(cls.__mro__[0:-1] if include_self else cls.__mro__[1:-1])\n",
    "\n",
    "pprint(get_base_classes(GPT2LMHeadModel, include_self=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default(param: Parameter) -> Any:\n",
    "    \"\"\"Return the parameter's default value or None if not specified.\"\"\"\n",
    "    return param.default if param.default is not param.empty else None\n",
    "\n",
    "def get_field_annotations(func_or_method: Callable[..., Any]) -> Tuple[List[Tuple[str, Any, Any]], Dict[str, Any]]:\n",
    "    if not inspect.isroutine(func_or_method):\n",
    "        raise ValueError(\"Expected a function or method\")\n",
    "\n",
    "    required_fields = []\n",
    "    optional_fields = []\n",
    "    annotations = {}\n",
    "\n",
    "    try:\n",
    "        sig: Signature = inspect.signature(func_or_method)\n",
    "        type_hints: Dict[str, Any] = get_type_hints(func_or_method)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Object does not support signature or type hints extraction.\") from None\n",
    "\n",
    "    for name, param in sig.parameters.items():\n",
    "        if name == \"self\":\n",
    "            continue\n",
    "\n",
    "        type_hint = type_hints.get(name, Any)\n",
    "        annotations[name] = type_hint\n",
    "        if param.default is param.empty:\n",
    "            required_fields.append((name, type_hint, Ellipsis))\n",
    "        else:\n",
    "            default_value = param.default\n",
    "            optional_fields.append((name, type_hint, default_value))\n",
    "\n",
    "    fields = required_fields + optional_fields\n",
    "    return fields, annotations\n",
    "\n",
    "\n",
    "# TODO: Tuple[str, Any, Any] should be Tuple[str, Any, ellipsis]\n",
    "def get_constructor_field_annotations(\n",
    "    cls: Type[Any], include_bases: bool = True\n",
    ") -> Tuple[List[Tuple[str, Any, Any]], Dict[str, Any]]:\n",
    "    fields = []\n",
    "    annotations = {}\n",
    "\n",
    "    classes_to_inspect = [cls] + list(get_base_classes(cls, include_self=False)) if include_bases else [cls]\n",
    "\n",
    "    for c in reversed(classes_to_inspect):  # Reverse to respect MRO\n",
    "        if hasattr(c, \"__init__\"):\n",
    "            class_fields, class_annotations = get_field_annotations(c.__init__)\n",
    "            # Update fields and annotations with those from the current class,\n",
    "            # avoiding duplicates.\n",
    "            for field in class_fields:\n",
    "                if field[0] not in annotations:\n",
    "                    fields.append(field)  # noqa: PERF401\n",
    "            annotations.update(class_annotations)\n",
    "\n",
    "    return fields, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir, <class 'str'>, Ellipsis\n",
      "overwrite_output_dir, <class 'bool'>, False\n",
      "do_train, <class 'bool'>, False\n",
      "do_eval, <class 'bool'>, False\n",
      "do_predict, <class 'bool'>, False\n",
      "evaluation_strategy, typing.Union[transformers.trainer_utils.IntervalStrategy, str], no\n",
      "prediction_loss_only, <class 'bool'>, False\n",
      "per_device_train_batch_size, <class 'int'>, 8\n",
      "per_device_eval_batch_size, <class 'int'>, 8\n",
      "per_gpu_train_batch_size, typing.Optional[int], None\n",
      "per_gpu_eval_batch_size, typing.Optional[int], None\n",
      "gradient_accumulation_steps, <class 'int'>, 1\n",
      "eval_accumulation_steps, typing.Optional[int], None\n",
      "eval_delay, typing.Optional[float], 0\n",
      "learning_rate, <class 'float'>, 5e-05\n",
      "weight_decay, <class 'float'>, 0.0\n",
      "adam_beta1, <class 'float'>, 0.9\n",
      "adam_beta2, <class 'float'>, 0.999\n",
      "adam_epsilon, <class 'float'>, 1e-08\n",
      "max_grad_norm, <class 'float'>, 1.0\n",
      "num_train_epochs, <class 'float'>, 3.0\n",
      "max_steps, <class 'int'>, -1\n",
      "lr_scheduler_type, typing.Union[transformers.trainer_utils.SchedulerType, str], linear\n",
      "lr_scheduler_kwargs, typing.Optional[typing.Dict], <factory>\n",
      "warmup_ratio, <class 'float'>, 0.0\n",
      "warmup_steps, <class 'int'>, 0\n",
      "log_level, typing.Optional[str], passive\n",
      "log_level_replica, typing.Optional[str], warning\n",
      "log_on_each_node, <class 'bool'>, True\n",
      "logging_dir, typing.Optional[str], None\n",
      "logging_strategy, typing.Union[transformers.trainer_utils.IntervalStrategy, str], steps\n",
      "logging_first_step, <class 'bool'>, False\n",
      "logging_steps, <class 'float'>, 500\n",
      "logging_nan_inf_filter, <class 'bool'>, True\n",
      "save_strategy, typing.Union[transformers.trainer_utils.IntervalStrategy, str], steps\n",
      "save_steps, <class 'float'>, 500\n",
      "save_total_limit, typing.Optional[int], None\n",
      "save_safetensors, typing.Optional[bool], True\n",
      "save_on_each_node, <class 'bool'>, False\n",
      "save_only_model, <class 'bool'>, False\n",
      "no_cuda, <class 'bool'>, False\n",
      "use_cpu, <class 'bool'>, False\n",
      "use_mps_device, <class 'bool'>, False\n",
      "seed, <class 'int'>, 42\n",
      "data_seed, typing.Optional[int], None\n",
      "jit_mode_eval, <class 'bool'>, False\n",
      "use_ipex, <class 'bool'>, False\n",
      "bf16, <class 'bool'>, False\n",
      "fp16, <class 'bool'>, False\n",
      "fp16_opt_level, <class 'str'>, O1\n",
      "half_precision_backend, <class 'str'>, auto\n",
      "bf16_full_eval, <class 'bool'>, False\n",
      "fp16_full_eval, <class 'bool'>, False\n",
      "tf32, typing.Optional[bool], None\n",
      "local_rank, <class 'int'>, -1\n",
      "ddp_backend, typing.Optional[str], None\n",
      "tpu_num_cores, typing.Optional[int], None\n",
      "tpu_metrics_debug, <class 'bool'>, False\n",
      "debug, typing.Union[str, typing.List[transformers.debug_utils.DebugOption]], \n",
      "dataloader_drop_last, <class 'bool'>, False\n",
      "eval_steps, typing.Optional[float], None\n",
      "dataloader_num_workers, <class 'int'>, 0\n",
      "dataloader_prefetch_factor, typing.Optional[int], None\n",
      "past_index, <class 'int'>, -1\n",
      "run_name, typing.Optional[str], None\n",
      "disable_tqdm, typing.Optional[bool], None\n",
      "remove_unused_columns, typing.Optional[bool], True\n",
      "label_names, typing.Optional[typing.List[str]], None\n",
      "load_best_model_at_end, typing.Optional[bool], False\n",
      "metric_for_best_model, typing.Optional[str], None\n",
      "greater_is_better, typing.Optional[bool], None\n",
      "ignore_data_skip, <class 'bool'>, False\n",
      "fsdp, typing.Union[typing.List[transformers.trainer_utils.FSDPOption], str, NoneType], \n",
      "fsdp_min_num_params, <class 'int'>, 0\n",
      "fsdp_config, typing.Union[dict, str, NoneType], None\n",
      "fsdp_transformer_layer_cls_to_wrap, typing.Optional[str], None\n",
      "accelerator_config, typing.Optional[str], None\n",
      "deepspeed, typing.Optional[str], None\n",
      "label_smoothing_factor, <class 'float'>, 0.0\n",
      "optim, typing.Union[transformers.training_args.OptimizerNames, str], adamw_torch\n",
      "optim_args, typing.Optional[str], None\n",
      "adafactor, <class 'bool'>, False\n",
      "group_by_length, <class 'bool'>, False\n",
      "length_column_name, typing.Optional[str], length\n",
      "report_to, typing.Optional[typing.List[str]], None\n",
      "ddp_find_unused_parameters, typing.Optional[bool], None\n",
      "ddp_bucket_cap_mb, typing.Optional[int], None\n",
      "ddp_broadcast_buffers, typing.Optional[bool], None\n",
      "dataloader_pin_memory, <class 'bool'>, True\n",
      "dataloader_persistent_workers, <class 'bool'>, False\n",
      "skip_memory_metrics, <class 'bool'>, True\n",
      "use_legacy_prediction_loop, <class 'bool'>, False\n",
      "push_to_hub, <class 'bool'>, False\n",
      "resume_from_checkpoint, typing.Optional[str], None\n",
      "hub_model_id, typing.Optional[str], None\n",
      "hub_strategy, typing.Union[transformers.trainer_utils.HubStrategy, str], every_save\n",
      "hub_token, typing.Optional[str], None\n",
      "hub_private_repo, <class 'bool'>, False\n",
      "hub_always_push, <class 'bool'>, False\n",
      "gradient_checkpointing, <class 'bool'>, False\n",
      "gradient_checkpointing_kwargs, typing.Optional[dict], None\n",
      "include_inputs_for_metrics, <class 'bool'>, False\n",
      "fp16_backend, <class 'str'>, auto\n",
      "push_to_hub_model_id, typing.Optional[str], None\n",
      "push_to_hub_organization, typing.Optional[str], None\n",
      "push_to_hub_token, typing.Optional[str], None\n",
      "mp_parameters, <class 'str'>, \n",
      "auto_find_batch_size, <class 'bool'>, False\n",
      "full_determinism, <class 'bool'>, False\n",
      "torchdynamo, typing.Optional[str], None\n",
      "ray_scope, typing.Optional[str], last\n",
      "ddp_timeout, typing.Optional[int], 1800\n",
      "torch_compile, <class 'bool'>, False\n",
      "torch_compile_backend, typing.Optional[str], None\n",
      "torch_compile_mode, typing.Optional[str], None\n",
      "dispatch_batches, typing.Optional[bool], None\n",
      "split_batches, typing.Optional[bool], None\n",
      "include_tokens_per_second, typing.Optional[bool], False\n",
      "include_num_input_tokens_seen, typing.Optional[bool], False\n",
      "neftune_noise_alpha, typing.Optional[float], None\n"
     ]
    }
   ],
   "source": [
    "fields, annotations = get_constructor_field_annotations(TrainingArguments, include_bases=False)\n",
    "for field in fields:\n",
    "    assert len(field) == 3\n",
    "    print(f\"{field[0]}, {field[1]}, {field[2]}\")\n",
    "\n",
    "assert get_field_annotations(TrainingArguments.__init__) == (fields, annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: it does not play too well with `dataclass` and `pydantic` classes\n",
    "because they have more complex bells and whistles. However, because of the perks\n",
    "of `dataclass` and `pydantic`, we can just use\n",
    "[property](https://stackoverflow.com/questions/71183960/short-way-to-get-all-field-names-of-a-pydantic-class)\n",
    "like `model_fields` to get all fields and their types.\n",
    "\n",
    "As we can see from above, we did not handle `lr_scheduler_kwargs` well:\n",
    "\n",
    "```python\n",
    "lr_scheduler_kwargs, typing.Optional[typing.Dict], <factory>\n",
    "```\n",
    "\n",
    "where `<factory>` is the default value of the parameter. But it is actually\n",
    "referring to the `default_factory` of the `dataclass` field, which can be a default\n",
    "dict etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Config:\n",
      "    def __init__(self, output_dir: str, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, evaluation_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: typing.Optional[int] = None, per_gpu_eval_batch_size: typing.Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: typing.Optional[int] = None, eval_delay: typing.Optional[float] = 0, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: typing.Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: typing.Optional[typing.Dict] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: typing.Optional[str] = 'passive', log_level_replica: typing.Optional[str] = 'warning', log_on_each_node: bool = True, logging_dir: typing.Optional[str] = None, logging_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: typing.Optional[int] = None, save_safetensors: typing.Optional[bool] = True, save_on_each_node: bool = False, save_only_model: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: typing.Optional[int] = None, jit_mode_eval: bool = False, use_ipex: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: typing.Optional[bool] = None, local_rank: int = -1, ddp_backend: typing.Optional[str] = None, tpu_num_cores: typing.Optional[int] = None, tpu_metrics_debug: bool = False, debug: typing.Union[str, typing.List[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: typing.Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: typing.Optional[int] = None, past_index: int = -1, run_name: typing.Optional[str] = None, disable_tqdm: typing.Optional[bool] = None, remove_unused_columns: typing.Optional[bool] = True, label_names: typing.Optional[typing.List[str]] = None, load_best_model_at_end: typing.Optional[bool] = False, metric_for_best_model: typing.Optional[str] = None, greater_is_better: typing.Optional[bool] = None, ignore_data_skip: bool = False, fsdp: typing.Union[typing.List[transformers.trainer_utils.FSDPOption], str, NoneType] = '', fsdp_min_num_params: int = 0, fsdp_config: typing.Union[dict, str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: typing.Optional[str] = None, accelerator_config: typing.Optional[str] = None, deepspeed: typing.Optional[str] = None, label_smoothing_factor: float = 0.0, optim: typing.Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch', optim_args: typing.Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: typing.Optional[str] = 'length', report_to: typing.Optional[typing.List[str]] = None, ddp_find_unused_parameters: typing.Optional[bool] = None, ddp_bucket_cap_mb: typing.Optional[int] = None, ddp_broadcast_buffers: typing.Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: typing.Optional[str] = None, hub_model_id: typing.Optional[str] = None, hub_strategy: typing.Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: typing.Optional[str] = None, hub_private_repo: bool = False, hub_always_push: bool = False, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: typing.Optional[dict] = None, include_inputs_for_metrics: bool = False, fp16_backend: str = 'auto', push_to_hub_model_id: typing.Optional[str] = None, push_to_hub_organization: typing.Optional[str] = None, push_to_hub_token: typing.Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: typing.Optional[str] = None, ray_scope: typing.Optional[str] = 'last', ddp_timeout: typing.Optional[int] = 1800, torch_compile: bool = False, torch_compile_backend: typing.Optional[str] = None, torch_compile_mode: typing.Optional[str] = None, dispatch_batches: typing.Optional[bool] = None, split_batches: typing.Optional[bool] = None, include_tokens_per_second: typing.Optional[bool] = False, include_num_input_tokens_seen: typing.Optional[bool] = False, neftune_noise_alpha: typing.Optional[float] = None):\n",
      "        self.output_dir = output_dir\n",
      "        self.overwrite_output_dir = overwrite_output_dir\n",
      "        self.do_train = do_train\n",
      "        self.do_eval = do_eval\n",
      "        self.do_predict = do_predict\n",
      "        self.evaluation_strategy = evaluation_strategy\n",
      "        self.prediction_loss_only = prediction_loss_only\n",
      "        self.per_device_train_batch_size = per_device_train_batch_size\n",
      "        self.per_device_eval_batch_size = per_device_eval_batch_size\n",
      "        self.per_gpu_train_batch_size = per_gpu_train_batch_size\n",
      "        self.per_gpu_eval_batch_size = per_gpu_eval_batch_size\n",
      "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
      "        self.eval_accumulation_steps = eval_accumulation_steps\n",
      "        self.eval_delay = eval_delay\n",
      "        self.learning_rate = learning_rate\n",
      "        self.weight_decay = weight_decay\n",
      "        self.adam_beta1 = adam_beta1\n",
      "        self.adam_beta2 = adam_beta2\n",
      "        self.adam_epsilon = adam_epsilon\n",
      "        self.max_grad_norm = max_grad_norm\n",
      "        self.num_train_epochs = num_train_epochs\n",
      "        self.max_steps = max_steps\n",
      "        self.lr_scheduler_type = lr_scheduler_type\n",
      "        self.lr_scheduler_kwargs = lr_scheduler_kwargs\n",
      "        self.warmup_ratio = warmup_ratio\n",
      "        self.warmup_steps = warmup_steps\n",
      "        self.log_level = log_level\n",
      "        self.log_level_replica = log_level_replica\n",
      "        self.log_on_each_node = log_on_each_node\n",
      "        self.logging_dir = logging_dir\n",
      "        self.logging_strategy = logging_strategy\n",
      "        self.logging_first_step = logging_first_step\n",
      "        self.logging_steps = logging_steps\n",
      "        self.logging_nan_inf_filter = logging_nan_inf_filter\n",
      "        self.save_strategy = save_strategy\n",
      "        self.save_steps = save_steps\n",
      "        self.save_total_limit = save_total_limit\n",
      "        self.save_safetensors = save_safetensors\n",
      "        self.save_on_each_node = save_on_each_node\n",
      "        self.save_only_model = save_only_model\n",
      "        self.no_cuda = no_cuda\n",
      "        self.use_cpu = use_cpu\n",
      "        self.use_mps_device = use_mps_device\n",
      "        self.seed = seed\n",
      "        self.data_seed = data_seed\n",
      "        self.jit_mode_eval = jit_mode_eval\n",
      "        self.use_ipex = use_ipex\n",
      "        self.bf16 = bf16\n",
      "        self.fp16 = fp16\n",
      "        self.fp16_opt_level = fp16_opt_level\n",
      "        self.half_precision_backend = half_precision_backend\n",
      "        self.bf16_full_eval = bf16_full_eval\n",
      "        self.fp16_full_eval = fp16_full_eval\n",
      "        self.tf32 = tf32\n",
      "        self.local_rank = local_rank\n",
      "        self.ddp_backend = ddp_backend\n",
      "        self.tpu_num_cores = tpu_num_cores\n",
      "        self.tpu_metrics_debug = tpu_metrics_debug\n",
      "        self.debug = debug\n",
      "        self.dataloader_drop_last = dataloader_drop_last\n",
      "        self.eval_steps = eval_steps\n",
      "        self.dataloader_num_workers = dataloader_num_workers\n",
      "        self.dataloader_prefetch_factor = dataloader_prefetch_factor\n",
      "        self.past_index = past_index\n",
      "        self.run_name = run_name\n",
      "        self.disable_tqdm = disable_tqdm\n",
      "        self.remove_unused_columns = remove_unused_columns\n",
      "        self.label_names = label_names\n",
      "        self.load_best_model_at_end = load_best_model_at_end\n",
      "        self.metric_for_best_model = metric_for_best_model\n",
      "        self.greater_is_better = greater_is_better\n",
      "        self.ignore_data_skip = ignore_data_skip\n",
      "        self.fsdp = fsdp\n",
      "        self.fsdp_min_num_params = fsdp_min_num_params\n",
      "        self.fsdp_config = fsdp_config\n",
      "        self.fsdp_transformer_layer_cls_to_wrap = fsdp_transformer_layer_cls_to_wrap\n",
      "        self.accelerator_config = accelerator_config\n",
      "        self.deepspeed = deepspeed\n",
      "        self.label_smoothing_factor = label_smoothing_factor\n",
      "        self.optim = optim\n",
      "        self.optim_args = optim_args\n",
      "        self.adafactor = adafactor\n",
      "        self.group_by_length = group_by_length\n",
      "        self.length_column_name = length_column_name\n",
      "        self.report_to = report_to\n",
      "        self.ddp_find_unused_parameters = ddp_find_unused_parameters\n",
      "        self.ddp_bucket_cap_mb = ddp_bucket_cap_mb\n",
      "        self.ddp_broadcast_buffers = ddp_broadcast_buffers\n",
      "        self.dataloader_pin_memory = dataloader_pin_memory\n",
      "        self.dataloader_persistent_workers = dataloader_persistent_workers\n",
      "        self.skip_memory_metrics = skip_memory_metrics\n",
      "        self.use_legacy_prediction_loop = use_legacy_prediction_loop\n",
      "        self.push_to_hub = push_to_hub\n",
      "        self.resume_from_checkpoint = resume_from_checkpoint\n",
      "        self.hub_model_id = hub_model_id\n",
      "        self.hub_strategy = hub_strategy\n",
      "        self.hub_token = hub_token\n",
      "        self.hub_private_repo = hub_private_repo\n",
      "        self.hub_always_push = hub_always_push\n",
      "        self.gradient_checkpointing = gradient_checkpointing\n",
      "        self.gradient_checkpointing_kwargs = gradient_checkpointing_kwargs\n",
      "        self.include_inputs_for_metrics = include_inputs_for_metrics\n",
      "        self.fp16_backend = fp16_backend\n",
      "        self.push_to_hub_model_id = push_to_hub_model_id\n",
      "        self.push_to_hub_organization = push_to_hub_organization\n",
      "        self.push_to_hub_token = push_to_hub_token\n",
      "        self.mp_parameters = mp_parameters\n",
      "        self.auto_find_batch_size = auto_find_batch_size\n",
      "        self.full_determinism = full_determinism\n",
      "        self.torchdynamo = torchdynamo\n",
      "        self.ray_scope = ray_scope\n",
      "        self.ddp_timeout = ddp_timeout\n",
      "        self.torch_compile = torch_compile\n",
      "        self.torch_compile_backend = torch_compile_backend\n",
      "        self.torch_compile_mode = torch_compile_mode\n",
      "        self.dispatch_batches = dispatch_batches\n",
      "        self.split_batches = split_batches\n",
      "        self.include_tokens_per_second = include_tokens_per_second\n",
      "        self.include_num_input_tokens_seen = include_num_input_tokens_seen\n",
      "        self.neftune_noise_alpha = neftune_noise_alpha\n"
     ]
    }
   ],
   "source": [
    "def type_hint_to_str(type_hint: Any) -> str:\n",
    "    \"\"\"\n",
    "    Convert a type hint into its string representation.\n",
    "    \"\"\"\n",
    "    if hasattr(type_hint, '__name__'):\n",
    "        return type_hint.__name__\n",
    "    elif hasattr(type_hint, '_name') and type_hint._name is not None:\n",
    "        return str(type_hint._name)\n",
    "    elif type(type_hint) == _GenericAlias:  # For Python 3.8+\n",
    "        # Handles complex types, e.g., List[int], Union[str, int]\n",
    "        origin = type_hint_to_str(type_hint.__origin__)\n",
    "        args = ', '.join(type_hint_to_str(arg) for arg in type_hint.__args__)\n",
    "        return f\"{origin}[{args}]\"\n",
    "    else:\n",
    "        # Fallback for unhandled types\n",
    "        return str(type_hint)\n",
    "\n",
    "def create_config_class_str(fields: List[Tuple[str, Any, Any]]) -> str:\n",
    "    lines = [\"class Config:\"]\n",
    "    if not fields:\n",
    "        lines.append(\"    ...\")\n",
    "    else:\n",
    "        init_params = [\"self\"]\n",
    "        init_body = []\n",
    "        for name, type_hint, default in fields:\n",
    "            type_hint_str = type_hint_to_str(type_hint)\n",
    "            if default is Ellipsis:  # Required argument\n",
    "                param_str = f\"{name}: {type_hint_str}\"\n",
    "            elif default is field:\n",
    "                param_str = f\"{name}: {type_hint_str} = field(default_factory=dict)\"\n",
    "            else:\n",
    "                default_repr = repr(default) if default is not None else 'None'\n",
    "                param_str = f\"{name}: {type_hint_str} = {default_repr}\"\n",
    "\n",
    "            init_params.append(param_str)\n",
    "            init_body.append(f\"        self.{name} = {name}\")\n",
    "\n",
    "        lines.append(f\"    def __init__({', '.join(init_params)}):\")\n",
    "        lines.extend(init_body)\n",
    "\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "config_class_str = create_config_class_str(fields)\n",
    "print(config_class_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this as is will yield a `SyntaxError` because of the `<factory>` issue\n",
    "highlighted above. We can use on a \"normal\" class `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Config:\n",
      "    def __init__(self, model: typing.Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module, NoneType] = None, args: typing.Optional[transformers.training_args.TrainingArguments] = None, data_collator: typing.Optional[DataCollator] = None, train_dataset: typing.Optional[torch.utils.data.dataset.Dataset] = None, eval_dataset: typing.Union[torch.utils.data.dataset.Dataset, typing.Dict[str, torch.utils.data.dataset.Dataset], NoneType] = None, tokenizer: typing.Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] = None, model_init: typing.Optional[typing.Callable[[], transformers.modeling_utils.PreTrainedModel]] = None, compute_metrics: typing.Optional[typing.Callable[[transformers.trainer_utils.EvalPrediction], typing.Dict]] = None, callbacks: typing.Optional[typing.List[transformers.trainer_callback.TrainerCallback]] = None, optimizers: Tuple = (None, None), preprocess_logits_for_metrics: typing.Optional[typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None):\n",
      "        self.model = model\n",
      "        self.args = args\n",
      "        self.data_collator = data_collator\n",
      "        self.train_dataset = train_dataset\n",
      "        self.eval_dataset = eval_dataset\n",
      "        self.tokenizer = tokenizer\n",
      "        self.model_init = model_init\n",
      "        self.compute_metrics = compute_metrics\n",
      "        self.callbacks = callbacks\n",
      "        self.optimizers = optimizers\n",
      "        self.preprocess_logits_for_metrics = preprocess_logits_for_metrics\n"
     ]
    }
   ],
   "source": [
    "fields, annotations = get_constructor_field_annotations(Trainer, include_bases=False)\n",
    "config_class_str = create_config_class_str(fields)\n",
    "print(config_class_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import typing\n",
    "import torch\n",
    "from transformers import DataCollator\n",
    "\n",
    "NoneType = type(None)\n",
    "\n",
    "config_class_str = create_config_class_str(fields)\n",
    "\n",
    "# Execute the generated class definition string\n",
    "namespace = {}\n",
    "exec(config_class_str, globals(), namespace)\n",
    "\n",
    "# Extract the newly created class from the namespace\n",
    "ConfigClass = namespace['Config']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (self, model: Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module, NoneType] = None, args: Optional[transformers.training_args.TrainingArguments] = None, data_collator: Optional[DataCollator] = None, train_dataset: Optional[torch.utils.data.dataset.Dataset] = None, eval_dataset: Union[torch.utils.data.dataset.Dataset, Dict[str, torch.utils.data.dataset.Dataset], NoneType] = None, tokenizer: Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] = None, model_init: Optional[Callable[[], transformers.modeling_utils.PreTrainedModel]] = None, compute_metrics: Optional[Callable[[transformers.trainer_utils.EvalPrediction], Dict]] = None, callbacks: Optional[List[transformers.trainer_callback.TrainerCallback]] = None, optimizers: Tuple = (None, None), preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None)>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(ConfigClass.__init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Further Readings\n",
    "\n",
    "- [inspect  Inspect live objects](https://docs.python.org/3/library/inspect.html)\n",
    "- [Getting attributes of a class](https://stackoverflow.com/questions/9058305/getting-attributes-of-a-class)\n",
    "- [Use __dict__ or vars()?](https://stackoverflow.com/questions/21297203/use-dict-or-vars)\n",
    "- [How do I get list of methods in a Python class?](https://stackoverflow.com/questions/1911281/how-do-i-get-list-of-methods-in-a-python-class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omniverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
