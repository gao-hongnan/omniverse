{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registry Design Pattern\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Organized_Chaos-orange)\n",
    "[![Code](https://img.shields.io/badge/View-Code-blue?style=flat-square&logo=github)](https://github.com/gao-hongnan/omniverse/tree/196f5d8870f4a743c7fd4d8cf3ca88a6572776af/omnixamples/software_engineering/design_patterns/registry)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "\n",
    "What does the registry design pattern solve? Consider you have to implement many\n",
    "schedulers in your deep learning project. Everytime you do so you may want to\n",
    "add the choice to a global dictionary (singleton) so that you can easily\n",
    "instantiate the scheduler. This is prone to mistake and a hassle once your\n",
    "project grows as you may need to maintain it.\n",
    "\n",
    "The registry provides a central place to manage all available scheduler types.\n",
    "This makes it easier to keep track of what schedulers are available and ensures\n",
    "consistent instantiation across the application. With a simple decorator, you\n",
    "can register your scheduler and use it in your project globally.\n",
    "\n",
    "It is often combined with the factory design pattern. First, we define:\n",
    "\n",
    "1. Registry Pattern:\n",
    "\n",
    "    - The `SchedulerRegistry` maintains a collection (dictionary) of scheduler\n",
    "      classes.\n",
    "    - It provides methods to register new scheduler types (`register` method).\n",
    "    - It allows retrieval of registered scheduler classes (`get_scheduler`\n",
    "      method).\n",
    "\n",
    "2. Factory Pattern:\n",
    "    - The `create_scheduler` method acts as a factory method.\n",
    "    - It creates and returns instances of schedulers based on the provided name\n",
    "      and parameters.\n",
    "\n",
    "So, in essence, this `SchedulerRegistry` is behaving as both:\n",
    "\n",
    "1. A registry: It keeps track of available scheduler types.\n",
    "2. A factory: It creates instances of schedulers.\n",
    "\n",
    "So basically,\n",
    "\n",
    "1. Scheduler types are registered with the registry (using the\n",
    "   `@scheduler_registry.register` decorator).\n",
    "2. When you need a scheduler, you call\n",
    "   `scheduler_registry.create_scheduler(name, optimizer, **kwargs)`.\n",
    "3. The registry looks up the correct scheduler class based on the name.\n",
    "4. It then uses that class to create and return a scheduler instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Registry For PyTorch Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'StepLR'</span>: <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'__main__.StepLRConfig'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'CosineAnnealingLR'</span><span style=\"color: #000000; text-decoration-color: #000000\">: &lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'__main__.CosineAnnealingLRConfig'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'CosineAnnealingWarmRestarts'</span><span style=\"color: #000000; text-decoration-color: #000000\">: &lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'__main__.CosineAnnealingWarmRestartsConfig'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'StepLR'\u001b[0m: \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'__main__.StepLRConfig'\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'CosineAnnealingLR'\u001b[0m\u001b[39m: <class \u001b[0m\u001b[32m'__main__.CosineAnnealingLRConfig'\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'CosineAnnealingWarmRestarts'\u001b[0m\u001b[39m: <class \u001b[0m\u001b[32m'__main__.CosineAnnealingWarmRestartsConfig'\u001b[0m\u001b[1m>\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created StepLR scheduler: <torch.optim.lr_scheduler.StepLR object at 0x137b43280>\n",
      "Created CosineAnnealingLR scheduler: <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x137b94250>\n",
      "Created CosineAnnealingWarmRestarts scheduler: <torch.optim.lr_scheduler.CosineAnnealingWarmRestarts object at 0x137b94ca0>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Module for creating PyTorch scheduler instances dynamically with an enhanced Registry pattern.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Callable, Dict, Literal, Type\n",
    "\n",
    "import torch\n",
    "from pydantic import BaseModel\n",
    "from rich.pretty import pprint\n",
    "\n",
    "RegisteredSchedulers = Literal[\n",
    "    \"StepLR\",\n",
    "    \"CosineAnnealingLR\",\n",
    "    \"CosineAnnealingWarmRestarts\",\n",
    "]\n",
    "\n",
    "\n",
    "class SchedulerRegistry:\n",
    "    _schedulers: Dict[str, Type[SchedulerConfig]] = {}\n",
    "\n",
    "    @classmethod\n",
    "    def register(cls: Type[SchedulerRegistry], name: str) -> Callable[[Type[SchedulerConfig]], Type[SchedulerConfig]]:\n",
    "        def register_scheduler_cls(scheduler_cls: Type[SchedulerConfig]) -> Type[SchedulerConfig]:\n",
    "            if name in cls._schedulers:\n",
    "                raise ValueError(f\"Cannot register duplicate scheduler {name}\")\n",
    "            if not issubclass(scheduler_cls, SchedulerConfig):\n",
    "                raise ValueError(f\"Scheduler (name={name}, class={scheduler_cls.__name__}) must extend SchedulerConfig\")\n",
    "            cls._schedulers[name] = scheduler_cls\n",
    "            return scheduler_cls\n",
    "\n",
    "        return register_scheduler_cls\n",
    "\n",
    "    @classmethod\n",
    "    def get_scheduler(cls, name: str) -> Type[SchedulerConfig]:\n",
    "        scheduler_cls = cls._schedulers.get(name)\n",
    "        if not scheduler_cls:\n",
    "            raise ValueError(f\"Scheduler {name} not found in registry\")\n",
    "        return scheduler_cls\n",
    "\n",
    "    @classmethod\n",
    "    def create_scheduler(\n",
    "        cls: Type[SchedulerRegistry], name: str, optimizer: torch.optim.Optimizer, **kwargs: Any\n",
    "    ) -> torch.optim.lr_scheduler.LRScheduler:\n",
    "        scheduler_cls = cls.get_scheduler(name)\n",
    "        scheduler_config = scheduler_cls(**kwargs)\n",
    "        return scheduler_config.build(optimizer)\n",
    "\n",
    "\n",
    "class SchedulerConfig(BaseModel, ABC):\n",
    "    \"\"\"Base class for creating PyTorch scheduler instances dynamically.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def build(self, optimizer: torch.optim.Optimizer) -> torch.optim.lr_scheduler.LRScheduler:\n",
    "        \"\"\"Builder method for creating a scheduler instance.\"\"\"\n",
    "        pass\n",
    "\n",
    "    class Config:\n",
    "        extra = \"forbid\"\n",
    "\n",
    "\n",
    "@SchedulerRegistry.register(\"StepLR\")\n",
    "class StepLRConfig(SchedulerConfig):\n",
    "    step_size: int\n",
    "    gamma: float = 0.1\n",
    "    last_epoch: int = -1\n",
    "    verbose: bool = False\n",
    "\n",
    "    def build(self, optimizer: torch.optim.Optimizer) -> torch.optim.lr_scheduler.StepLR:\n",
    "        return torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=self.step_size, gamma=self.gamma, last_epoch=self.last_epoch, verbose=self.verbose\n",
    "        )\n",
    "\n",
    "\n",
    "@SchedulerRegistry.register(\"CosineAnnealingLR\")\n",
    "class CosineAnnealingLRConfig(SchedulerConfig):\n",
    "    T_max: int\n",
    "    eta_min: float = 0\n",
    "    last_epoch: int = -1\n",
    "    verbose: bool = False\n",
    "\n",
    "    def build(self, optimizer: torch.optim.Optimizer) -> torch.optim.lr_scheduler.CosineAnnealingLR:\n",
    "        return torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.T_max, eta_min=self.eta_min, last_epoch=self.last_epoch, verbose=self.verbose\n",
    "        )\n",
    "\n",
    "\n",
    "@SchedulerRegistry.register(\"CosineAnnealingWarmRestarts\")\n",
    "class CosineAnnealingWarmRestartsConfig(SchedulerConfig):\n",
    "    T_0: int\n",
    "    T_mult: int = 1\n",
    "    eta_min: float = 0\n",
    "    last_epoch: int = -1\n",
    "    verbose: bool = False\n",
    "\n",
    "    def build(self, optimizer: torch.optim.Optimizer) -> torch.optim.lr_scheduler.CosineAnnealingWarmRestarts:\n",
    "        return torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=self.T_0,\n",
    "            T_mult=self.T_mult,\n",
    "            eta_min=self.eta_min,\n",
    "            last_epoch=self.last_epoch,\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a dummy optimizer for demonstration\n",
    "    model = torch.nn.Linear(10, 2)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    pprint(SchedulerRegistry._schedulers)\n",
    "    # Create a StepLR scheduler\n",
    "    step_lr = SchedulerRegistry.create_scheduler(\"StepLR\", optimizer, step_size=30, gamma=0.1)\n",
    "    print(f\"Created StepLR scheduler: {step_lr}\")\n",
    "\n",
    "    # Create a CosineAnnealingLR scheduler\n",
    "    cosine_lr = SchedulerRegistry.create_scheduler(\"CosineAnnealingLR\", optimizer, T_max=100, eta_min=0.001)\n",
    "    print(f\"Created CosineAnnealingLR scheduler: {cosine_lr}\")\n",
    "\n",
    "    # Create a LambdaLR scheduler\n",
    "    cosine_warm_restarts = SchedulerRegistry.create_scheduler(\n",
    "        \"CosineAnnealingWarmRestarts\", optimizer, T_0=100, T_mult=2\n",
    "    )\n",
    "    print(f\"Created CosineAnnealingWarmRestarts scheduler: {cosine_warm_restarts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singleton\n",
    "\n",
    "The registry also acts globally and behaves like a singleton. So you can make\n",
    "it one if you want to, either via the below way or via metaclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Type, Callable, Any\n",
    "\n",
    "\n",
    "class SchedulerRegistry:\n",
    "    _instance = None\n",
    "    _schedulers: Dict[str, Type[SchedulerConfig]] = {}\n",
    "\n",
    "    def __new__(cls: Type[SchedulerRegistry]) -> SchedulerRegistry:\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "    def register(self, name: str) -> Callable[[Type[SchedulerConfig]], Type[SchedulerConfig]]:\n",
    "        def register_scheduler_cls(scheduler_cls: Type[SchedulerConfig]) -> Type[SchedulerConfig]:\n",
    "            if name in self._schedulers:\n",
    "                raise ValueError(f\"Cannot register duplicate scheduler {name}\")\n",
    "            if not issubclass(scheduler_cls, SchedulerConfig):\n",
    "                raise ValueError(f\"Scheduler (name={name}, class={scheduler_cls.__name__}) must extend SchedulerConfig\")\n",
    "            self._schedulers[name] = scheduler_cls\n",
    "            return scheduler_cls\n",
    "\n",
    "        return register_scheduler_cls\n",
    "\n",
    "    def get_scheduler(self, name: str) -> Type[SchedulerConfig]:\n",
    "        scheduler_cls = self._schedulers.get(name)\n",
    "        if not scheduler_cls:\n",
    "            raise ValueError(f\"Scheduler {name} not found in registry\")\n",
    "        return scheduler_cls\n",
    "\n",
    "    def create_scheduler(\n",
    "        self, name: str, optimizer: torch.optim.Optimizer, **kwargs: Any\n",
    "    ) -> torch.optim.lr_scheduler.LRScheduler:\n",
    "        scheduler_cls = self.get_scheduler(name)\n",
    "        scheduler_config = scheduler_cls(**kwargs)\n",
    "        return scheduler_config.build(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References And Further Readings\n",
    "\n",
    "-   https://charlesreid1.github.io/python-patterns-the-registry.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omniverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
