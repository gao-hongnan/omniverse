{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registry Design Pattern\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Organized_Chaos-orange)\n",
    "[![Code](https://img.shields.io/badge/View-Code-blue?style=flat-square&logo=github)](https://github.com/gao-hongnan/omniverse/tree/196f5d8870f4a743c7fd4d8cf3ca88a6572776af/omnixamples/software_engineering/design_patterns/registry\n",
    ")\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "\n",
    "What does the registry design pattern solve? Consider you have to implement many\n",
    "schedulers in your deep learning project. Everytime you do so you may want to\n",
    "add the choice to a global dictionary (singleton) so that you can easily\n",
    "instantiate the scheduler. This is prone to mistake and a hassle once your\n",
    "project grows as you may need to maintain it.\n",
    "\n",
    "The registry provides a central place to manage all available scheduler types.\n",
    "This makes it easier to keep track of what schedulers are available and ensures\n",
    "consistent instantiation across the application. With a simple decorator, you\n",
    "can register your scheduler and use it in your project globally.\n",
    "\n",
    "It is often combined with the factory design pattern. First, we define:\n",
    "\n",
    "1. Registry Pattern:\n",
    "\n",
    "    - The SchedulerRegistry maintains a collection (dictionary) of scheduler\n",
    "      classes.\n",
    "    - It provides methods to register new scheduler types (`register` method).\n",
    "    - It allows retrieval of registered scheduler classes (`get_scheduler`\n",
    "      method).\n",
    "\n",
    "2. Factory Pattern:\n",
    "    - The `create_scheduler` method acts as a factory method.\n",
    "    - It creates and returns instances of schedulers based on the provided name\n",
    "      and parameters.\n",
    "\n",
    "So, in essence, this SchedulerRegistry is behaving as both:\n",
    "\n",
    "1. A registry: It keeps track of available scheduler types.\n",
    "2. A factory: It creates instances of schedulers.\n",
    "\n",
    "So basically,\n",
    "\n",
    "1. Scheduler types are registered with the registry (using the\n",
    "   `@scheduler_registry.register` decorator).\n",
    "2. When you need a scheduler, you call\n",
    "   `scheduler_registry.create_scheduler(name, optimizer, **kwargs)`.\n",
    "3. The registry looks up the correct scheduler class based on the name.\n",
    "4. It then uses that class to create and return a scheduler instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Module for creating PyTorch scheduler instances dynamically.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Callable, Dict, Literal, Type\n",
    "\n",
    "import torch\n",
    "\n",
    "from rich.pretty import pprint\n",
    "\n",
    "from omnivault.utils.config_management.dynamic import DynamicClassFactory\n",
    "\n",
    "RegisteredSchedulers = Literal[\n",
    "    \"torch.optim.lr_scheduler.StepLR\",\n",
    "    \"torch.optim.lr_scheduler.CosineAnnealingLR\",\n",
    "    \"torch.optim.lr_scheduler.LambdaLR\",\n",
    "    \"torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\",\n",
    "    \"torch.optim.lr_scheduler.OneCycleLR\",\n",
    "]\n",
    "SCHEDULER_REGISTRY: Dict[str, Type[SchedulerConfig]] = {}\n",
    "\n",
    "\n",
    "def register_scheduler(name: str) -> Callable[[Type[SchedulerConfig]], Type[SchedulerConfig]]:\n",
    "    def register_scheduler_cls(cls: Type[SchedulerConfig]) -> Type[SchedulerConfig]:\n",
    "        if name in SCHEDULER_REGISTRY:\n",
    "            raise ValueError(f\"Cannot register duplicate scheduler {name}\")\n",
    "        if not issubclass(cls, SchedulerConfig):\n",
    "            raise ValueError(f\"Scheduler (name={name}, class={cls.__name__}) must extend SchedulerConfig\")\n",
    "        SCHEDULER_REGISTRY[name] = cls\n",
    "        return cls\n",
    "\n",
    "    return register_scheduler_cls\n",
    "\n",
    "\n",
    "class SchedulerConfig(DynamicClassFactory[torch.optim.lr_scheduler.LRScheduler]):\n",
    "    \"\"\"\n",
    "    Base class for creating PyTorch scheduler instances dynamically.\n",
    "\n",
    "    This class extends `DynamicClassFactory` to specifically handle the\n",
    "    instantiation of PyTorch scheduler classes based on provided configurations.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    build(optimizer: torch.optim.Optimizer) -> torch.optim.lr_scheduler.LRScheduler\n",
    "        Creates and returns a scheduler instance with the specified optimizer.\n",
    "    \"\"\"\n",
    "\n",
    "    name: str\n",
    "\n",
    "    def build(self, optimizer: torch.optim.Optimizer, **kwargs: Any) -> torch.optim.lr_scheduler.LRScheduler:\n",
    "        \"\"\"Builder method for creating a scheduler instance.\"\"\"\n",
    "        return self.create_instance(optimizer=optimizer, **kwargs)\n",
    "\n",
    "    class Config:\n",
    "        extra = \"forbid\"\n",
    "\n",
    "\n",
    "@register_scheduler(\"torch.optim.lr_scheduler.StepLR\")\n",
    "class StepLRConfig(SchedulerConfig):\n",
    "    step_size: int\n",
    "    gamma: float = 0.1\n",
    "    last_epoch: int = -1\n",
    "    verbose: bool = False\n",
    "\n",
    "\n",
    "@register_scheduler(\"torch.optim.lr_scheduler.CosineAnnealingLR\")\n",
    "class CosineAnnealingLRConfig(SchedulerConfig):\n",
    "    T_max: int\n",
    "    eta_min: float = 0\n",
    "    last_epoch: int = -1\n",
    "    verbose: bool = False\n",
    "\n",
    "\n",
    "@register_scheduler(\"torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\")\n",
    "class CosineAnnealingWarmRestartsConfig(SchedulerConfig):\n",
    "    \"\"\"See\n",
    "    https://wandb.ai/wandb_fc/tips/reports/How-to-Properly-Use-PyTorch-s-CosineAnnealingWarmRestarts-Scheduler--VmlldzoyMTA3MjM2\n",
    "    to see how to use this scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    T_0: int\n",
    "    T_mult: int = 1\n",
    "    eta_min: float = 0\n",
    "    last_epoch: int = -1\n",
    "    verbose: bool = False\n",
    "\n",
    "\n",
    "@register_scheduler(\"torch.optim.lr_scheduler.OneCycleLR\")\n",
    "class OneCycleLRConfig(SchedulerConfig):\n",
    "    \"\"\"You can use a utils function to retrieve the constructor args of a class\n",
    "    dynamically.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> from omnivault.utils.general.python_shenanigans import get_init_args\n",
    "    >>> get_init_args(OneCycleLRConfig)\n",
    "    \"\"\"\n",
    "\n",
    "    max_lr: float\n",
    "    total_steps: int\n",
    "    epochs: int\n",
    "    steps_per_epoch: int\n",
    "    pct_start: float = 0.3\n",
    "    anneal_strategy: Literal[\"cos\", \"linear\"] = \"cos\"\n",
    "    cycle_momentum: bool = True\n",
    "    base_momentum: float = 0.85\n",
    "    max_momentum: float = 0.95\n",
    "    div_factor: float = 25.0\n",
    "    final_div_factor: float = 10000.0\n",
    "    three_phase: bool = False\n",
    "    last_epoch: int = -1\n",
    "    verbose: bool = False\n",
    "\n",
    "\n",
    "@register_scheduler(\"torch.optim.lr_scheduler.LambdaLR\")\n",
    "class LambdaLRConfig(SchedulerConfig):\n",
    "    # The user must provide a lambda function for the scheduler\n",
    "    lr_lambda: Callable[[int], float]  # we know this lr_lambda maps int (epoch) to float (some multiplier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(SCHEDULER_REGISTRY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'StepLR'</span>: <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'__main__.StepLRConfig'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'CosineAnnealingLR'</span><span style=\"color: #000000; text-decoration-color: #000000\">: &lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'__main__.CosineAnnealingLRConfig'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'CosineAnnealingWarmRestarts'</span><span style=\"color: #000000; text-decoration-color: #000000\">: &lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'__main__.CosineAnnealingWarmRestartsConfig'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'OneCycleLR'</span><span style=\"color: #000000; text-decoration-color: #000000\">: &lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'__main__.OneCycleLRConfig'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #008000; text-decoration-color: #008000\">'LambdaLR'</span><span style=\"color: #000000; text-decoration-color: #000000\">: &lt;class </span><span style=\"color: #008000; text-decoration-color: #008000\">'__main__.LambdaLRConfig'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'StepLR'\u001b[0m: \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'__main__.StepLRConfig'\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'CosineAnnealingLR'\u001b[0m\u001b[39m: <class \u001b[0m\u001b[32m'__main__.CosineAnnealingLRConfig'\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'CosineAnnealingWarmRestarts'\u001b[0m\u001b[39m: <class \u001b[0m\u001b[32m'__main__.CosineAnnealingWarmRestartsConfig'\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'OneCycleLR'\u001b[0m\u001b[39m: <class \u001b[0m\u001b[32m'__main__.OneCycleLRConfig'\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[32m'LambdaLR'\u001b[0m\u001b[39m: <class \u001b[0m\u001b[32m'__main__.LambdaLRConfig'\u001b[0m\u001b[1m>\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created StepLR scheduler: <torch.optim.lr_scheduler.StepLR object at 0x151eb9d90>\n",
      "Created CosineAnnealingLR scheduler: <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x152432bb0>\n",
      "Created LambdaLR scheduler: <torch.optim.lr_scheduler.LambdaLR object at 0x1505218b0>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Module for creating PyTorch scheduler instances dynamically with an enhanced Registry pattern.\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Callable, Dict, Literal, Type\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import torch\n",
    "from pydantic import BaseModel\n",
    "\n",
    "RegisteredSchedulers = Literal[\n",
    "    \"StepLR\",\n",
    "    \"CosineAnnealingLR\",\n",
    "    \"LambdaLR\",\n",
    "    \"CosineAnnealingWarmRestarts\",\n",
    "    \"OneCycleLR\",\n",
    "]\n",
    "\n",
    "class SchedulerRegistry:\n",
    "    _schedulers: Dict[str, Type[SchedulerConfig]] = {}\n",
    "\n",
    "    @classmethod\n",
    "    def register(cls, name: str) -> Callable[[Type[SchedulerConfig]], Type[SchedulerConfig]]:\n",
    "        def register_scheduler_cls(scheduler_cls: Type[SchedulerConfig]) -> Type[SchedulerConfig]:\n",
    "            if name in cls._schedulers:\n",
    "                raise ValueError(f\"Cannot register duplicate scheduler {name}\")\n",
    "            if not issubclass(scheduler_cls, SchedulerConfig):\n",
    "                raise ValueError(f\"Scheduler (name={name}, class={scheduler_cls.__name__}) must extend SchedulerConfig\")\n",
    "            cls._schedulers[name] = scheduler_cls\n",
    "            return scheduler_cls\n",
    "        return register_scheduler_cls\n",
    "\n",
    "    @classmethod\n",
    "    def get_scheduler(cls, name: str) -> Type[SchedulerConfig]:\n",
    "        scheduler_cls = cls._schedulers.get(name)\n",
    "        if not scheduler_cls:\n",
    "            raise ValueError(f\"Scheduler {name} not found in registry\")\n",
    "        return scheduler_cls\n",
    "\n",
    "    @classmethod\n",
    "    def create_scheduler(cls, name: str, optimizer: torch.optim.Optimizer, **kwargs: Any) -> torch.optim.lr_scheduler.LRScheduler:\n",
    "        scheduler_cls = cls.get_scheduler(name)\n",
    "        scheduler_config = scheduler_cls(**kwargs)\n",
    "        return scheduler_config.build(optimizer)\n",
    "\n",
    "class SchedulerConfig(BaseModel, ABC):\n",
    "    \"\"\"Base class for creating PyTorch scheduler instances dynamically.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def build(self, optimizer: torch.optim.Optimizer) -> torch.optim.lr_scheduler.LRScheduler:\n",
    "        \"\"\"Builder method for creating a scheduler instance.\"\"\"\n",
    "        pass\n",
    "\n",
    "    class Config:\n",
    "        extra = \"forbid\"\n",
    "\n",
    "@SchedulerRegistry.register(\"StepLR\")\n",
    "class StepLRConfig(SchedulerConfig):\n",
    "    step_size: int\n",
    "    gamma: float = 0.1\n",
    "    last_epoch: int = -1\n",
    "    verbose: bool = False\n",
    "\n",
    "    def build(self, optimizer: torch.optim.Optimizer) -> torch.optim.lr_scheduler.StepLR:\n",
    "        return torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=self.step_size,\n",
    "            gamma=self.gamma,\n",
    "            last_epoch=self.last_epoch,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "\n",
    "@SchedulerRegistry.register(\"CosineAnnealingLR\")\n",
    "class CosineAnnealingLRConfig(SchedulerConfig):\n",
    "    T_max: int\n",
    "    eta_min: float = 0\n",
    "    last_epoch: int = -1\n",
    "    verbose: bool = False\n",
    "\n",
    "    def build(self, optimizer: torch.optim.Optimizer) -> torch.optim.lr_scheduler.CosineAnnealingLR:\n",
    "        return torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=self.T_max,\n",
    "            eta_min=self.eta_min,\n",
    "            last_epoch=self.last_epoch,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "\n",
    "@SchedulerRegistry.register(\"CosineAnnealingWarmRestarts\")\n",
    "class CosineAnnealingWarmRestartsConfig(SchedulerConfig):\n",
    "    T_0: int\n",
    "    T_mult: int = 1\n",
    "    eta_min: float = 0\n",
    "    last_epoch: int = -1\n",
    "    verbose: bool = False\n",
    "\n",
    "    def build(self, optimizer: torch.optim.Optimizer) -> torch.optim.lr_scheduler.CosineAnnealingWarmRestarts:\n",
    "        return torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=self.T_0,\n",
    "            T_mult=self.T_mult,\n",
    "            eta_min=self.eta_min,\n",
    "            last_epoch=self.last_epoch,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "\n",
    "@SchedulerRegistry.register(\"OneCycleLR\")\n",
    "class OneCycleLRConfig(SchedulerConfig):\n",
    "    max_lr: float\n",
    "    total_steps: int\n",
    "    epochs: int\n",
    "    steps_per_epoch: int\n",
    "    pct_start: float = 0.3\n",
    "    anneal_strategy: Literal[\"cos\", \"linear\"] = \"cos\"\n",
    "    cycle_momentum: bool = True\n",
    "    base_momentum: float = 0.85\n",
    "    max_momentum: float = 0.95\n",
    "    div_factor: float = 25.0\n",
    "    final_div_factor: float = 10000.0\n",
    "    three_phase: bool = False\n",
    "    last_epoch: int = -1\n",
    "    verbose: bool = False\n",
    "\n",
    "    def build(self, optimizer: torch.optim.Optimizer) -> torch.optim.lr_scheduler.OneCycleLR:\n",
    "        return torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.max_lr,\n",
    "            total_steps=self.total_steps,\n",
    "            epochs=self.epochs,\n",
    "            steps_per_epoch=self.steps_per_epoch,\n",
    "            pct_start=self.pct_start,\n",
    "            anneal_strategy=self.anneal_strategy,\n",
    "            cycle_momentum=self.cycle_momentum,\n",
    "            base_momentum=self.base_momentum,\n",
    "            max_momentum=self.max_momentum,\n",
    "            div_factor=self.div_factor,\n",
    "            final_div_factor=self.final_div_factor,\n",
    "            three_phase=self.three_phase,\n",
    "            last_epoch=self.last_epoch,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "\n",
    "@SchedulerRegistry.register(\"LambdaLR\")\n",
    "class LambdaLRConfig(SchedulerConfig):\n",
    "    lr_lambda: Callable[[int], float]\n",
    "    last_epoch: int = -1\n",
    "    verbose: bool = False\n",
    "\n",
    "    def build(self, optimizer: torch.optim.Optimizer) -> torch.optim.lr_scheduler.LambdaLR:\n",
    "        return torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lr_lambda=self.lr_lambda,\n",
    "            last_epoch=self.last_epoch,\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a dummy optimizer for demonstration\n",
    "    model = torch.nn.Linear(10, 2)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    pprint(SchedulerRegistry._schedulers)\n",
    "    # Create a StepLR scheduler\n",
    "    step_lr = SchedulerRegistry.create_scheduler(\"StepLR\", optimizer, step_size=30, gamma=0.1)\n",
    "    print(f\"Created StepLR scheduler: {step_lr}\")\n",
    "\n",
    "    # Create a CosineAnnealingLR scheduler\n",
    "    cosine_lr = SchedulerRegistry.create_scheduler(\"CosineAnnealingLR\", optimizer, T_max=100, eta_min=0.001)\n",
    "    print(f\"Created CosineAnnealingLR scheduler: {cosine_lr}\")\n",
    "\n",
    "    # Create a LambdaLR scheduler\n",
    "    lambda_lr = SchedulerRegistry.create_scheduler(\"LambdaLR\", optimizer, lr_lambda=lambda epoch: 0.95 ** epoch)\n",
    "    print(f\"Created LambdaLR scheduler: {lambda_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 3 With Singleton\n",
    "\n",
    "The registry also acts globally and behaves like a singleton. So you can make\n",
    "it one if you want to, either via the below way or via metaclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchedulerRegistry:\n",
    "    _instance = None\n",
    "    _schedulers: Dict[str, Type[SchedulerConfig]] = {}\n",
    "\n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(SchedulerRegistry, cls).__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "    def register(self, name: str) -> Callable[[Type[SchedulerConfig]], Type[SchedulerConfig]]:\n",
    "        def register_scheduler_cls(scheduler_cls: Type[SchedulerConfig]) -> Type[SchedulerConfig]:\n",
    "            if name in self._schedulers:\n",
    "                raise ValueError(f\"Cannot register duplicate scheduler {name}\")\n",
    "            if not issubclass(scheduler_cls, SchedulerConfig):\n",
    "                raise ValueError(f\"Scheduler (name={name}, class={scheduler_cls.__name__}) must extend SchedulerConfig\")\n",
    "            self._schedulers[name] = scheduler_cls\n",
    "            return scheduler_cls\n",
    "        return register_scheduler_cls\n",
    "\n",
    "    def get_scheduler(self, name: str) -> Type[SchedulerConfig]:\n",
    "        scheduler_cls = self._schedulers.get(name)\n",
    "        if not scheduler_cls:\n",
    "            raise ValueError(f\"Scheduler {name} not found in registry\")\n",
    "        return scheduler_cls\n",
    "\n",
    "    def create_scheduler(self, name: str, optimizer: torch.optim.Optimizer, **kwargs: Any) -> torch.optim.lr_scheduler.LRScheduler:\n",
    "        scheduler_cls = self.get_scheduler(name)\n",
    "        scheduler_config = scheduler_cls(**kwargs)\n",
    "        return scheduler_config.build(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References And Further Readings\n",
    "\n",
    "-   https://charlesreid1.github.io/python-patterns-the-registry.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omniverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
