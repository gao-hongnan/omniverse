{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923b7262",
   "metadata": {},
   "source": [
    "# State And Metadata Management\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Brain_Dump-red)\n",
    "![Tag](https://img.shields.io/badge/Level-Beginner-green)\n",
    "[![Code](https://img.shields.io/badge/View-Code-blue?style=flat-square&logo=github)](https://github.com/gao-hongnan/omniverse/blob/c18e2a338c2cc39a7862c329c2f4cd04d87eae40/omnivault/transformer/core/state.py)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "\n",
    "Besides the configuration that drives the whole machine learning pipeline, be it\n",
    "in training or inference, we would also have something called the metadata or\n",
    "state object which encapsulates the current state of the process. This is\n",
    "especially useful when we want to save the state of the training process and\n",
    "resume training from a checkpoint. If you are not convinced, have a look at\n",
    "[MosaicML's Composer](https://docs.mosaicml.com/projects/composer/en/latest/api_reference/generated/composer.State.html)\n",
    "for some insights.\n",
    "\n",
    "## Metadata Management\n",
    "\n",
    "In the context of a machine learning pipeline, having a mutable `Metadata` class\n",
    "can be beneficial. This is because the pipeline is a dynamic process, and\n",
    "different stages of the pipeline may need to update different parts of the\n",
    "metadata.\n",
    "\n",
    "For example:\n",
    "\n",
    "-   At the start of the pipeline, you might set the `start_time` and `run_id`.\n",
    "-   When loading the data, you might update the `dataset_version`.\n",
    "-   During training, you might update the `history` with the training loss and\n",
    "    metrics at the end of each epoch.\n",
    "-   When the pipeline finishes, you might set the `end_time` and upload the\n",
    "    final metadata as an artifact.\n",
    "\n",
    "This kind of incremental updating is easier with a mutable object. However, it's\n",
    "important to manage the mutable state carefully to avoid bugs. For example, if\n",
    "two parts of your code are both updating the metadata at the same time, they\n",
    "might interfere with each other. To avoid this, you might want to:\n",
    "\n",
    "-   Make sure that different parts of your code are responsible for updating\n",
    "    different parts of the metadata, and that they don't interfere with each\n",
    "    other.\n",
    "-   Use locks or other synchronization mechanisms if you're updating the\n",
    "    metadata from multiple threads or processes.\n",
    "-   Create a new copy of the metadata before making changes, if you need to keep\n",
    "    the old state around for some reason.\n",
    "\n",
    "An example below.\n",
    "\n",
    "```python\n",
    "@dataclass(frozen=False)\n",
    "class Metadata:\n",
    "    # pipeline run time and usage\n",
    "    start_time: str\n",
    "    end_time: str\n",
    "    time_taken: str\n",
    "    memory_usage: str\n",
    "    gpu_usage: str\n",
    "\n",
    "    # inside load.py\n",
    "    raw_file_size: int = None\n",
    "    raw_file_format: str = None\n",
    "    raw_filepath: str = None\n",
    "    raw_dvc_metadata: Dict[str, Any] = None\n",
    "\n",
    "    # inside train.py\n",
    "    model_artifacts: Dict[str, Any] = None\n",
    "    run_id: str = None\n",
    "    model_version: str = None\n",
    "    experiment_id: str = None\n",
    "    experiment_name: str = None\n",
    "    artifact_uri: str = None\n",
    "\n",
    "    # inside evaluate.py\n",
    "    holdout_performance: Dict[str, float] = None\n",
    "    avg_expected_loss: float = None\n",
    "    avg_bias: float = None\n",
    "    avg_variance: float = None\n",
    "```\n",
    "\n",
    "## State Management\n",
    "\n",
    "The `State` object represents the current state of the training process. It\n",
    "encapsulates key components such as the model, optimizer, criterion, scheduler,\n",
    "and additional metadata. The inspiration for `State` comes from\n",
    "[MosaicML's Composer](https://github.com/mosaicml/composer), a library that has\n",
    "been beneficial in the context of pretraining Language Models (LLMs) and is also\n",
    "the library that me and my team adopted for our LLM pretraining project.\n",
    "\n",
    "Here is a snippet of the `State` object when I was training a GPT model for fun.\n",
    "Note this is a very naive and bare version and does not include many features\n",
    "that Composer's `State` object has.\n",
    "\n",
    "![Adder State](./assets/state.png)\n",
    "\n",
    "Why is `State` useful? It allows me to easily access the model, optimizer,\n",
    "criterion, scheduler, and other metadata from any part of the codebase. I can\n",
    "also serialize the `State` object and save it to disk, allowing me to resume\n",
    "training from a checkpoint.\n",
    "\n",
    "Perhaps not the best design pattern, but I added a `history` attribute to the\n",
    "`State` object, which is a dictionary that stores the training history. This is\n",
    "to mimic what could be a separate `History` object, which is a common component\n",
    "in many deep learning libraries.\n",
    "\n",
    "A sample implementation of the `State` object is shown below.\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Type, Union\n",
    "\n",
    "import torch\n",
    "from pydantic import BaseModel, Field\n",
    "from rich.pretty import pprint\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def compare_models(model_a: nn.Module, model_b: nn.Module) -> bool:\n",
    "    return all(\n",
    "        torch.equal(param_a[1], param_b[1])\n",
    "        for param_a, param_b in zip(model_a.state_dict().items(), model_b.state_dict().items())\n",
    "    )\n",
    "\n",
    "\n",
    "class State(BaseModel):\n",
    "    model: nn.Module = Field(default=None, description=\"Model.\")\n",
    "\n",
    "    criterion: nn.Module = Field(default=None, description=\"Loss function.\")\n",
    "    optimizer: torch.optim.Optimizer = Field(default=None, description=\"Optimizer.\")\n",
    "    scheduler: Union[torch.optim.lr_scheduler.LRScheduler, None] = Field(default=None, description=\"Scheduler.\")\n",
    "\n",
    "    epoch_index: int = Field(default=0, description=\"Current epoch index.\")\n",
    "    train_batch_index: int = Field(\n",
    "        default=0, description=\"Current batch index and is only referring to the training batch index.\"\n",
    "    )\n",
    "    step_index: int = Field(\n",
    "        default=0,\n",
    "        description=\"We do not add prefix train because it is understood and implied that the step number is the train due to how many gradients been stepped. Current step index and is only referring to the training step index. What is the difference between step and batch? In general, they coincide for when the epoch number is 1, but after the first epoch, we usually reset the batch index to 0, while the step index keeps increasing to the next epoch.\",\n",
    "    )\n",
    "    history: Dict[str, List[float]] = Field(default={}, description=\"History of metrics.\")\n",
    "\n",
    "    # FIXME: loosen `Vocabularies` and `Tokenizers` to `Any` for now as it is too strict.\n",
    "    vocabulary: Any = Field(default=None, description=\"Vocabulary.\")\n",
    "    tokenizer: Any = Field(default=None, description=\"Tokenizer.\")\n",
    "\n",
    "    def __eq__(self, other: object) -> bool:\n",
    "        \"\"\"Check if two State instances are equal.\"\"\"\n",
    "        assert isinstance(other, State), \"Can only compare State instances.\"\n",
    "\n",
    "        models_equal = compare_models(self.model, other.model)\n",
    "        return models_equal\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Pydantic config.\"\"\"\n",
    "\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        \"\"\"Pretty print the config.\"\"\"\n",
    "        pprint(self)\n",
    "\n",
    "    def save_snapshots(self, filepath: str) -> None:\n",
    "        \"\"\"Save the state dictionaries of the components to a file.\"\"\"\n",
    "        state = {\n",
    "            \"model\": self.model.state_dict() if self.model else None,\n",
    "            \"criterion\": self.criterion.state_dict() if self.criterion else None,\n",
    "            \"optimizer\": self.optimizer.state_dict() if self.optimizer else None,\n",
    "            \"scheduler\": self.scheduler.state_dict() if self.scheduler else None,\n",
    "            \"epoch_index\": self.epoch_index,\n",
    "            \"train_batch_index\": self.train_batch_index,\n",
    "            \"step_index\": self.step_index,\n",
    "            \"history\": self.history,\n",
    "            \"vocabulary\": self.vocabulary,\n",
    "            \"tokenizer\": self.tokenizer,\n",
    "        }\n",
    "        torch.save(state, filepath)\n",
    "\n",
    "    @classmethod\n",
    "    def load_snapshots(\n",
    "        cls: Type[State],\n",
    "        filepath: str,\n",
    "        device: torch.device,\n",
    "        *,\n",
    "        model: nn.Module,\n",
    "        criterion: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "    ) -> State:\n",
    "        \"\"\"Load state dictionaries from a file and return a new State instance.\"\"\"\n",
    "        state = torch.load(filepath, map_location=device)\n",
    "\n",
    "        epoch_index = state[\"epoch_index\"]\n",
    "        train_batch_index = state[\"train_batch_index\"]\n",
    "        step_index = state[\"step_index\"]\n",
    "        history = state[\"history\"]\n",
    "        vocabulary = state[\"vocabulary\"]\n",
    "        tokenizer = state[\"tokenizer\"]\n",
    "\n",
    "        # Create a new instance of State with loaded state\n",
    "        new_state = cls(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            epoch_index=epoch_index,\n",
    "            train_batch_index=train_batch_index,\n",
    "            step_index=step_index,\n",
    "            history=history,\n",
    "            vocabulary=vocabulary,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "        # Load state dicts into the model, criterion, etc., if they exist\n",
    "        if new_state.model and \"model\" in state:\n",
    "            new_state.model.load_state_dict(state[\"model\"])\n",
    "        if new_state.criterion and \"criterion\" in state:\n",
    "            new_state.criterion.load_state_dict(state[\"criterion\"])\n",
    "        if new_state.optimizer and \"optimizer\" in state:\n",
    "            new_state.optimizer.load_state_dict(state[\"optimizer\"])\n",
    "        if new_state.scheduler and \"scheduler\" in state:\n",
    "            new_state.scheduler.load_state_dict(state[\"scheduler\"])\n",
    "\n",
    "        return new_state\n",
    "```\n",
    "\n",
    "## References and Further Readings\n",
    "\n",
    "-   [MosaicML's Composer](https://docs.mosaicml.com/projects/composer/en/latest/api_reference/generated/composer.State.html)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "mystnb": {
   "number_source_lines": true
  },
  "source_map": [
   16
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}