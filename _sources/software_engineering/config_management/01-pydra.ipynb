{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd9c603",
   "metadata": {},
   "source": [
    "# Pydantic And Hydra\n",
    "\n",
    "[![Twitter Handle](https://img.shields.io/badge/Twitter-@gaohongnan-blue?style=social&logo=twitter)](https://twitter.com/gaohongnan)\n",
    "[![LinkedIn Profile](https://img.shields.io/badge/@gaohongnan-blue?style=social&logo=linkedin)](https://linkedin.com/in/gao-hongnan)\n",
    "[![GitHub Profile](https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&logo=github)](https://github.com/gao-hongnan)\n",
    "![Tag](https://img.shields.io/badge/Tag-Brain_Dump-red)\n",
    "![Tag](https://img.shields.io/badge/Level-Beginner-green)\n",
    "[![Code](https://img.shields.io/badge/View-Code-blue?style=flat-square&logo=github)](https://github.com/gao-hongnan/omniverse/tree/4c886c6735b3164325bf9ebbd360ef42bbd71769/omnixamples/software_engineering/config_management)\n",
    "\n",
    "```{contents}\n",
    ":local:\n",
    "```\n",
    "\n",
    "This post is heavily inspired by\n",
    "[Pydra - Pydantic and Hydra for configuration management of model training experiments](https://suneeta-mall.github.io/2022/03/15/hydra-pydantic-config-management-for-training-application.html)\n",
    "which talks about combining Pydantic and Hydra for configuration management.\n",
    "\n",
    "## Hydra\n",
    "\n",
    "We won't go into the details of what [Hydra](https://hydra.cc/docs/intro/) is,\n",
    "as the documentation covers the it very well. We will show a working example on\n",
    "how to use Hydra for configuration management.\n",
    "\n",
    "### YAML Driven Configuration\n",
    "\n",
    "First we need to define the configuration files in `yaml` format.\n",
    "\n",
    "````{tab} **model.yaml**\n",
    "```yaml\n",
    "model_name: \"resnet18\"\n",
    "pretrained: True\n",
    "in_chans: 3\n",
    "num_classes: ${train.num_classes}\n",
    "global_pool: \"avg\"\n",
    "# You typically want _self_ somewhere after the schema (base_config)\n",
    "```\n",
    "````\n",
    "\n",
    "````{tab} **optimizer.yaml**\n",
    "```yaml\n",
    "optimizer_name: \"AdamW\"\n",
    "optimizer_params:\n",
    "    lr: 0.0003  # bs: 32 -> lr = 3e-4\n",
    "    betas: [0.9, 0.999]\n",
    "    amsgrad: False\n",
    "    weight_decay: 0.000001\n",
    "    eps: 0.00000001\n",
    "```\n",
    "````\n",
    "\n",
    "````{tab} **stores.yaml**\n",
    "```yaml\n",
    "project_name: ${train.project_name}\n",
    "unique_id: ${now:%Y%m%d_%H%M%S} # in sync with hydra output dir\n",
    "logs_dir: !!python/object/apply:pathlib.PosixPath [\"./logs\"]\n",
    "model_artifacts_dir: !!python/object/apply:pathlib.PosixPath [\"./artifacts\"]\n",
    "```\n",
    "````\n",
    "\n",
    "````{tab} **transform.yaml**\n",
    "```yaml\n",
    "image_size: 256\n",
    "mean: [0.485, 0.456, 0.406]\n",
    "std: [0.229, 0.224, 0.225]\n",
    "```\n",
    "````\n",
    "\n",
    "````{tab} **datamodule.yaml**\n",
    "```yaml\n",
    "data_dir: !!python/object/apply:pathlib.PosixPath [\"./data\"]\n",
    "batch_size: 32\n",
    "num_workers: 0\n",
    "shuffle: true\n",
    "```\n",
    "````\n",
    "\n",
    "````{tab} **config.yaml**\n",
    "```yaml\n",
    "defaults:\n",
    "  - _self_\n",
    "  - model: base\n",
    "  - datamodule: base\n",
    "  - transform: base\n",
    "  - stores: base\n",
    "  - optimizer: base\n",
    "\n",
    "train:\n",
    "  num_classes: 10\n",
    "  device: \"cpu\"\n",
    "  project_name: \"cifar10\"\n",
    "  debug: true\n",
    "  seed: 1992\n",
    "  num_epochs: 3\n",
    "\n",
    "hydra:\n",
    "  run:\n",
    "    dir: \"${stores.model_artifacts_dir}/${train.project_name}/${stores.unique_id}\" # in sync with stores\n",
    "```\n",
    "````\n",
    "\n",
    "Then we define an entrypoint to run, along with a simple `train` function.\n",
    "\n",
    "````{tab} **main.py**\n",
    "```python\n",
    "import logging\n",
    "\n",
    "import hydra\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "from omnixamples.software_engineering.config_management.train import train\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@hydra.main(version_base=None, config_path=\"configs\", config_name=\"config\")\n",
    "def run(config: DictConfig) -> None:\n",
    "    \"\"\"Run the main function.\"\"\"\n",
    "    LOGGER.info(\"Type of config is: %s\", type(config))\n",
    "    LOGGER.info(\"Merged Yaml:\\n%s\", OmegaConf.to_yaml(config))\n",
    "    LOGGER.info(HydraConfig.get().job.name)\n",
    "\n",
    "    train(config)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n",
    "```\n",
    "````\n",
    "\n",
    "````{tab} **train.py**\n",
    "```python\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import torchvision  # type: ignore[import-untyped]\n",
    "from rich.pretty import pprint\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms  # type: ignore[import-untyped]\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(config: Any) -> None:\n",
    "    \"\"\"Run the training pipeline, however, the code below can be further\n",
    "    modularized into functions for better readability and maintainability.\"\"\"\n",
    "    pprint(config)\n",
    "\n",
    "    torch.manual_seed(config.train.seed)\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((config.transform.image_size, config.transform.image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=config.transform.mean, std=config.transform.std),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset = datasets.CIFAR10(root=config.datamodule.data_dir, train=True, transform=transform, download=True)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.datamodule.batch_size,\n",
    "        num_workers=config.datamodule.num_workers,\n",
    "        shuffle=config.datamodule.shuffle,\n",
    "    )\n",
    "\n",
    "    model = getattr(torchvision.models, config.model.model_name)(pretrained=config.model.pretrained)\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, config.model.num_classes)\n",
    "    model = model.to(config.train.device)\n",
    "\n",
    "    optimizer = getattr(torch.optim, config.optimizer.optimizer_name)(\n",
    "        model.parameters(), **config.optimizer.optimizer_params\n",
    "    )\n",
    "\n",
    "    for epoch in range(config.train.num_epochs):\n",
    "        model.train()\n",
    "        with tqdm(dataloader, desc=f\"Epoch [{epoch+1}/{config.train.num_epochs}]\", unit=\"batch\") as tepoch:\n",
    "            for images, labels in tepoch:\n",
    "                images = images.to(config.train.device)\n",
    "                labels = labels.to(config.train.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(images)\n",
    "                loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    torch.save(model.state_dict(), config.stores.model_artifacts_dir / \"model.pth\")\n",
    "```\n",
    "````\n",
    "\n",
    "The `@hydra.main` decorator is used to initialize the hydra application. We\n",
    "specify `config_path` to tell hydra where to look for the base configuration\n",
    "files. We also specify `config_name` to tell hydra which file is the main\n",
    "controller of the hierarchy. In this case, it is `config.yaml`. Running the\n",
    "`main.py` file will create an `artifacts` folder hosting useful information from\n",
    "hydra. Note the `artifacts` folder is a configurable name, the default is\n",
    "actually `outputs`. Let's see what we have inside this folder!\n",
    "\n",
    "Running `tree -a artifacts/` will show the following:\n",
    "\n",
    "```text\n",
    "artifacts\n",
    "└── cifar10\n",
    "    └── 20240509_155331\n",
    "        ├── .hydra\n",
    "        │   ├── config.yaml\n",
    "        │   ├── hydra.yaml\n",
    "        │   └── overrides.yaml\n",
    "        └── main.log\n",
    "\n",
    "4 directories, 4 files\n",
    "```\n",
    "\n",
    "And the content is as follows:\n",
    "\n",
    "````{tab} **.hydra/config.yaml**\n",
    "```yaml\n",
    "train:\n",
    "    num_classes: 10\n",
    "    device: cpu\n",
    "    project_name: cifar10\n",
    "    debug: true\n",
    "    seed: 1992\n",
    "    num_epochs: 3\n",
    "model:\n",
    "    model_name: resnet18\n",
    "    pretrained: true\n",
    "    in_chans: 3\n",
    "    num_classes: ${train.num_classes}\n",
    "    global_pool: avg\n",
    "datamodule:\n",
    "    data_dir: !!python/object/apply:pathlib.PosixPath\n",
    "        - data\n",
    "    batch_size: 32\n",
    "    num_workers: 0\n",
    "    shuffle: true\n",
    "transform:\n",
    "    image_size: 256\n",
    "    mean:\n",
    "        - 0.485\n",
    "        - 0.456\n",
    "        - 0.406\n",
    "    std:\n",
    "        - 0.229\n",
    "        - 0.224\n",
    "        - 0.225\n",
    "stores:\n",
    "    project_name: ${train.project_name}\n",
    "    unique_id: ${now:%Y%m%d_%H%M%S}\n",
    "    logs_dir: !!python/object/apply:pathlib.PosixPath\n",
    "        - logs\n",
    "    model_artifacts_dir: !!python/object/apply:pathlib.PosixPath\n",
    "        - artifacts\n",
    "optimizer:\n",
    "    optimizer_name: AdamW\n",
    "    optimizer_params:\n",
    "        lr: 0.0003\n",
    "        betas:\n",
    "            - 0.9\n",
    "            - 0.999\n",
    "        amsgrad: false\n",
    "        weight_decay: 1.0e-06\n",
    "        eps: 1.0e-08\n",
    "```\n",
    "````\n",
    "\n",
    "````{tab} **.hydra/hydra.yaml**\n",
    "```yaml\n",
    "hydra:\n",
    "  run:\n",
    "    dir: ${stores.model_artifacts_dir}/${train.project_name}/${stores.unique_id}\n",
    "  sweep:\n",
    "    dir: multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}\n",
    "    subdir: ${hydra.job.num}\n",
    "  launcher:\n",
    "    _target_: hydra._internal.core_plugins.basic_launcher.BasicLauncher\n",
    "  sweeper:\n",
    "    _target_: hydra._internal.core_plugins.basic_sweeper.BasicSweeper\n",
    "    max_batch_size: null\n",
    "    params: null\n",
    "  help:\n",
    "    app_name: ${hydra.job.name}\n",
    "    header: '${hydra.help.app_name} is powered by Hydra.\n",
    "\n",
    "      '\n",
    "    footer: 'Powered by Hydra (https://hydra.cc)\n",
    "\n",
    "      Use --hydra-help to view Hydra specific help\n",
    "\n",
    "      '\n",
    "    template: '${hydra.help.header}\n",
    "\n",
    "      == Configuration groups ==\n",
    "\n",
    "      Compose your configuration from those groups (group=option)\n",
    "\n",
    "\n",
    "      $APP_CONFIG_GROUPS\n",
    "\n",
    "\n",
    "      == Config ==\n",
    "\n",
    "      Override anything in the config (foo.bar=value)\n",
    "\n",
    "\n",
    "      $CONFIG\n",
    "\n",
    "\n",
    "      ${hydra.help.footer}\n",
    "\n",
    "      '\n",
    "  hydra_help:\n",
    "    template: 'Hydra (${hydra.runtime.version})\n",
    "\n",
    "      See https://hydra.cc for more info.\n",
    "\n",
    "\n",
    "      == Flags ==\n",
    "\n",
    "      $FLAGS_HELP\n",
    "\n",
    "\n",
    "      == Configuration groups ==\n",
    "\n",
    "      Compose your configuration from those groups (For example, append hydra/job_logging=disabled\n",
    "      to command line)\n",
    "\n",
    "\n",
    "      $HYDRA_CONFIG_GROUPS\n",
    "\n",
    "\n",
    "      Use ''--cfg hydra'' to Show the Hydra config.\n",
    "\n",
    "      '\n",
    "    hydra_help: ???\n",
    "  hydra_logging:\n",
    "    version: 1\n",
    "    formatters:\n",
    "      simple:\n",
    "        format: '[%(asctime)s][HYDRA] %(message)s'\n",
    "    handlers:\n",
    "      console:\n",
    "        class: logging.StreamHandler\n",
    "        formatter: simple\n",
    "        stream: ext://sys.stdout\n",
    "    root:\n",
    "      level: INFO\n",
    "      handlers:\n",
    "      - console\n",
    "    loggers:\n",
    "      logging_example:\n",
    "        level: DEBUG\n",
    "    disable_existing_loggers: false\n",
    "  job_logging:\n",
    "    version: 1\n",
    "    formatters:\n",
    "      simple:\n",
    "        format: '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'\n",
    "    handlers:\n",
    "      console:\n",
    "        class: logging.StreamHandler\n",
    "        formatter: simple\n",
    "        stream: ext://sys.stdout\n",
    "      file:\n",
    "        class: logging.FileHandler\n",
    "        formatter: simple\n",
    "        filename: ${hydra.runtime.output_dir}/${hydra.job.name}.log\n",
    "    root:\n",
    "      level: INFO\n",
    "      handlers:\n",
    "      - console\n",
    "      - file\n",
    "    disable_existing_loggers: false\n",
    "  env: {}\n",
    "  mode: RUN\n",
    "  searchpath: []\n",
    "  callbacks: {}\n",
    "  output_subdir: .hydra\n",
    "  overrides:\n",
    "    hydra:\n",
    "    - hydra.mode=RUN\n",
    "    task: []\n",
    "  job:\n",
    "    name: main\n",
    "    chdir: null\n",
    "    override_dirname: ''\n",
    "    id: ???\n",
    "    num: ???\n",
    "    config_name: config\n",
    "    env_set: {}\n",
    "    env_copy: []\n",
    "    config:\n",
    "      override_dirname:\n",
    "        kv_sep: '='\n",
    "        item_sep: ','\n",
    "        exclude_keys: []\n",
    "  runtime:\n",
    "    version: 1.3.2\n",
    "    version_base: '1.3'\n",
    "    cwd: /Users/gaohn/gaohn/omniverse\n",
    "    config_sources:\n",
    "    - path: hydra.conf\n",
    "      schema: pkg\n",
    "      provider: hydra\n",
    "    - path: /Users/gaohn/gaohn/omniverse/omnixamples/software_engineering/config_management/hydra/configs\n",
    "      schema: file\n",
    "      provider: main\n",
    "    - path: ''\n",
    "      schema: structured\n",
    "      provider: schema\n",
    "    output_dir: /Users/gaohn/gaohn/omniverse/artifacts/cifar10/20240509_155331\n",
    "    choices:\n",
    "      optimizer: base\n",
    "      stores: base\n",
    "      transform: base\n",
    "      datamodule: base\n",
    "      model: base\n",
    "      hydra/env: default\n",
    "      hydra/callbacks: null\n",
    "      hydra/job_logging: default\n",
    "      hydra/hydra_logging: default\n",
    "      hydra/hydra_help: default\n",
    "      hydra/help: default\n",
    "      hydra/sweeper: basic\n",
    "      hydra/launcher: basic\n",
    "      hydra/output: default\n",
    "  verbose: false\n",
    "```\n",
    "````\n",
    "\n",
    "````{tab} **.hydra/overrides.yaml**\n",
    "```yaml\n",
    "[]\n",
    "```\n",
    "````\n",
    "\n",
    "````{tab} **main.log**\n",
    "```text\n",
    "[2024-05-09 15:53:31,713][__main__][INFO] - Type of config is: <class 'omegaconf.dictconfig.DictConfig'>\n",
    "[2024-05-09 15:53:31,715][__main__][INFO] - Merged Yaml:\n",
    "train:\n",
    "  num_classes: 10\n",
    "  device: cpu\n",
    "  project_name: cifar10\n",
    "  debug: true\n",
    "  seed: 1992\n",
    "  num_epochs: 3\n",
    "model:\n",
    "  model_name: resnet18\n",
    "  pretrained: true\n",
    "  in_chans: 3\n",
    "  num_classes: ${train.num_classes}\n",
    "  global_pool: avg\n",
    "datamodule:\n",
    "  data_dir: !!python/object/apply:pathlib.PosixPath\n",
    "  - data\n",
    "  batch_size: 32\n",
    "  num_workers: 0\n",
    "  shuffle: true\n",
    "transform:\n",
    "  image_size: 256\n",
    "  mean:\n",
    "  - 0.485\n",
    "  - 0.456\n",
    "  - 0.406\n",
    "  std:\n",
    "  - 0.229\n",
    "  - 0.224\n",
    "  - 0.225\n",
    "stores:\n",
    "  project_name: ${train.project_name}\n",
    "  unique_id: ${now:%Y%m%d_%H%M%S}\n",
    "  logs_dir: !!python/object/apply:pathlib.PosixPath\n",
    "  - logs\n",
    "  model_artifacts_dir: !!python/object/apply:pathlib.PosixPath\n",
    "  - artifacts\n",
    "optimizer:\n",
    "  optimizer_name: AdamW\n",
    "  optimizer_params:\n",
    "    lr: 0.0003\n",
    "    betas:\n",
    "    - 0.9\n",
    "    - 0.999\n",
    "    amsgrad: false\n",
    "    weight_decay: 1.0e-06\n",
    "    eps: 1.0e-08\n",
    "\n",
    "[2024-05-09 15:53:31,715][__main__][INFO] - main\n",
    "```\n",
    "````\n",
    "\n",
    "One highlight is we can override this configuration file with command line\n",
    "arguments. So if you tend to have many config folders, then this can come in\n",
    "handy.\n",
    "\n",
    "```bash\n",
    "python main.py train.num_epochs=5\n",
    "```\n",
    "\n",
    "and now in `overrides.yaml` you will see\n",
    "\n",
    "```yaml\n",
    "- train.num_epochs=5\n",
    "```\n",
    "\n",
    "### Pros\n",
    "\n",
    "1.  Once you load the configuration, you can access the configuration values\n",
    "    using the dot (chain) notation. This is because `config` is loaded as\n",
    "    [OmegaConf](https://github.com/omry/omegaconf)'s `DictConfig` object. They\n",
    "    inherit from `dict` and `MutableMapping` so you can access the values using\n",
    "    the dot notation access pattern. This is better than using\n",
    "    `config[\"train\"][\"num_classes\"]` because it is more readable and less error\n",
    "    prone, and also allow you to manipulate the object. One other reason is\n",
    "    using dictionary is a bit hard to do type checking.\n",
    "\n",
    "2.  Allow command line overrides. For example, you can override the `model_name`\n",
    "    from `model` by\n",
    "\n",
    "    ```bash\n",
    "    python main.py model.model_name=resnet50\n",
    "    ```\n",
    "\n",
    "    and the `model_name` will change during runtime. This is very useful when\n",
    "    you want to run multiple experiments with different configurations. You\n",
    "    won't need to create multiple config files for each experiment just to\n",
    "    change a single/few value(s).\n",
    "\n",
    "3.  A natural follow-up question is about persistence. If I can easily override\n",
    "    the configuration, then how do I make sure the configuration is saved\n",
    "    somewhere? Versioning the configs is just as important as versioning your\n",
    "    code in Machine Learning. Usually I would dump these config to a registry or\n",
    "    store in code.\n",
    "\n",
    "    The highlight here is Hydra also saves the final configuration to an output\n",
    "    folder. By default, it is stored in\n",
    "    `outputs/YYYY-MM-DD/HH-MM-SS/.hydra/config.yaml`. Now you can revert back to\n",
    "    this exact run by specifying the directory.\n",
    "\n",
    "    ```bash\n",
    "    python main.py --config_path outputs/2022-01-12/12-00-00/.hydra/config.yaml\n",
    "    ```\n",
    "\n",
    "    to recover your run. Here `YYYY-MM-DD/HH-MM-SS` is like your unique `run_id`\n",
    "    for each run. You can also change it as follows in `config.yaml`:\n",
    "\n",
    "    ```yaml title=\"configs/config.yaml\" linenums=\"1\"\n",
    "    hydra:\n",
    "    run:\n",
    "        dir: \"${stores.model_artifacts_dir}/${train.project_name}/${stores.unique_id}\" # in sync with stores\n",
    "    ```\n",
    "\n",
    "4.  One more good to have feature is overriding hydra's own default settings,\n",
    "    such as the `job` and `run` settings. You can either do it via `config.yaml`\n",
    "    or manually create a folder called `hydra` and put the specifics inside. See\n",
    "    [here](https://github.com/suneeta-mall/hydra_pydantic_config_management/tree/master/hydra/conf_custom_hydra/hydra)\n",
    "    for an example.\n",
    "\n",
    "5.  Multi-run. Hydra allows you to run multiple experiments in parallel. This is\n",
    "    very useful when you want to run multiple experiments with different\n",
    "    configurations. You can specify the number of runs and the configuration to\n",
    "    use. See\n",
    "    [here](https://hydra.cc/docs/tutorials/basic/running_your_app/multi-run/)\n",
    "    for more examples.\n",
    "\n",
    "    ```bash\n",
    "    python main.py model.model_name=resnet18,resnet50 --multirun\n",
    "    ```\n",
    "\n",
    "    This will run two experiments in parallel, one with `resnet18` and the other\n",
    "    with `resnet50`. We see the true power of it if you want to do\n",
    "    hyperparameter search, where you want to sweep over multiple values for a\n",
    "    single parameter.\n",
    "\n",
    "    Now in the same directory, an `multirun` folder will be created and inside\n",
    "    it, you will see two runs, one with `resnet18` and the other with\n",
    "    `resnet50`. Each process is indexed by an integer. For example, `resnet18`\n",
    "    is indexed by `0` and `resnet50` is indexed by `1`.\n",
    "\n",
    "    It is also worth noting that since the hydra's creator is from facebook, it\n",
    "    is very easy to integrate with PyTorch's\n",
    "    [distributed trainings](https://pytorch.org/tutorials/intermediate/dist_tuto.html).\n",
    "\n",
    "-   Interpolation: This is an extremely highlighted feature of hydra. For\n",
    "    example, we have `num_classes` defined under the `train` schema. However,\n",
    "    this parameter is also used in the `model` schema, defining the number of\n",
    "    output neurons. It can also be used in the `datamodule` schema (not shown\n",
    "    here), where we need to know the number of classes to create certain `class`\n",
    "    mapping. Repeatedly defining the same parameter over different config schema\n",
    "    is prone to mistake. This is like hardcoding the same value `NUM_CLASSES` in\n",
    "    different python scripts. Here we use the idea of **polymorphism** to define\n",
    "    the `num_classes` in the `train` schema and use interpolation to **inject**\n",
    "    this all around with `${train.num_classes}`.\n",
    "\n",
    "### Structured Config\n",
    "\n",
    "[Structured Config](https://hydra.cc/docs/tutorials/structured_config/schema/),\n",
    "this is what Hydra call when your config is complex enough to warrant object\n",
    "representations. For example, our `config.yaml` is currently just a Yaml\n",
    "representation. But under the hood, you can think of it as **composed** of\n",
    "`model`, `optimizer`, `store` and other schemas.\n",
    "\n",
    "For each of these objects, you can define its own schema. This is very useful\n",
    "when you want to validate the configuration.\n",
    "\n",
    "In hydra, you can define the schema using\n",
    "[dataclasses](https://docs.python.org/3/library/dataclasses.html).\n",
    "\n",
    "````{tab} **base.py**\n",
    "```python\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TransformConfig:\n",
    "    image_size: int\n",
    "    mean: List[float]\n",
    "    std: List[float]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    model_name: str\n",
    "    pretrained: bool\n",
    "    in_chans: int = field(metadata={\"ge\": 1})  # in_channels must be greater than or equal to 1\n",
    "    num_classes: int = field(metadata={\"ge\": 1})  # num_classes must be greater than or equal to 1\n",
    "    global_pool: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StoresConfig:\n",
    "    project_name: str\n",
    "    unique_id: str\n",
    "    logs_dir: Path\n",
    "    model_artifacts_dir: Path\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    device: str\n",
    "    project_name: str\n",
    "    debug: bool\n",
    "    seed: int\n",
    "    num_epochs: int\n",
    "    num_classes: int = 3\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OptimizerConfig:\n",
    "    optimizer: str\n",
    "    optimizer_params: Dict[str, Any]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    data_dir: Path\n",
    "    batch_size: int\n",
    "    num_workers: int\n",
    "    shuffle: bool = True\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model: ModelConfig\n",
    "    augmentations: TransformConfig\n",
    "    datamodule: DataConfig\n",
    "    optimizer: OptimizerConfig\n",
    "    stores: StoresConfig\n",
    "    train: TrainConfig\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, config_dict: Dict[str, Any]) -> Config:\n",
    "        return cls(\n",
    "            model=ModelConfig(**config_dict[\"model\"]),\n",
    "            augmentations=TransformConfig(**config_dict[\"augmentations\"]),\n",
    "            datamodule=DataConfig(**config_dict[\"datamodule\"]),\n",
    "            optimizer=OptimizerConfig(**config_dict[\"optimizer\"]),\n",
    "            stores=StoresConfig(**config_dict[\"stores\"]),\n",
    "            train=TrainConfig(**config_dict[\"train\"]),\n",
    "        )\n",
    "```\n",
    "````\n",
    "\n",
    "````{tab} **main_structured.py**\n",
    "```python\n",
    "import logging\n",
    "\n",
    "import hydra\n",
    "from hydra.core.config_store import ConfigStore\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from omnixamples.software_engineering.config_management.hydra.configs.base import (\n",
    "    Config,\n",
    "    DataConfig,\n",
    "    ModelConfig,\n",
    "    OptimizerConfig,\n",
    "    StoresConfig,\n",
    "    TrainConfig,\n",
    "    TransformConfig,\n",
    ")\n",
    "from omnixamples.software_engineering.config_management.train import train\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "cs = ConfigStore.instance()\n",
    "cs.store(name=\"base_config\", node=Config)\n",
    "cs.store(name=\"model\", node=ModelConfig)\n",
    "cs.store(name=\"optimizer\", node=OptimizerConfig)\n",
    "cs.store(name=\"stores\", node=StoresConfig)\n",
    "cs.store(name=\"train\", node=TrainConfig)\n",
    "cs.store(name=\"transform\", node=TransformConfig)\n",
    "cs.store(name=\"datamodule\", node=DataConfig)\n",
    "\n",
    "\n",
    "@hydra.main(version_base=None, config_path=\"configs\", config_name=\"config\")\n",
    "def run(config: Config) -> None:\n",
    "    \"\"\"Run the main function.\"\"\"\n",
    "    LOGGER.info(\"Type of config is: %s\", type(config))\n",
    "    LOGGER.info(\"Merged Yaml:\\n%s\", OmegaConf.to_yaml(config))\n",
    "    LOGGER.info(HydraConfig.get().job.name)\n",
    "\n",
    "    config_obj = OmegaConf.to_object(config)\n",
    "    LOGGER.info(\"Type of config is: %s\", type(config_obj))\n",
    "    train(config)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n",
    "```\n",
    "````\n",
    "\n",
    "More details [here](https://hydra.cc/docs/tutorials/structured_config/schema/).\n",
    "You can find how groups can be used to indicate inheritance.\n",
    "\n",
    "The dependency injection here is merely a change of the type of the `config`\n",
    "argument, from `DictConfig` to `Config`. The rest of the code remains the same.\n",
    "The command line arguments are still the same.\n",
    "\n",
    "Having dataclass representation also offers more flexibility from manipulating\n",
    "the object to type hint. But validation still remains a problem as using\n",
    "`__post_init__` method is\n",
    "[not well supported](https://github.com/facebookresearch/hydra/issues/981) when\n",
    "working with hydra. However, you can decouple the usage of dataclass from\n",
    "hydras' dependency injection. For example, you can load the config from hydra\n",
    "and instantiate through the dataclass without the use of `ConfigStore`. This\n",
    "idea is made better when used together with Pydantic since it offers validation\n",
    "and serialization. As we shall see later, Pydantic is a better version of\n",
    "dataclass where it offers more features like pre-post validation, type checking,\n",
    "constraints and better serialization. Just watch Jason Liu's\n",
    "[Pydantic Is All You Need](https://www.youtube.com/watch?v=yj-wSRJwrrc) to get a\n",
    "sense how good pydantic is. In fact, with the rise of LLM, we can see most big\n",
    "libraries built around it are leveraging pydantic.\n",
    "\n",
    "### Cons\n",
    "\n",
    "-   Serializing/Deserializing canonical or complex python objects are not well\n",
    "    supported. In earlier versions, objects like `pathlib.Path` are not\n",
    "    supported.\n",
    "\n",
    "-   Structured config is only limited to `dataclass`. This means that you cannot\n",
    "    create your own custom abstraction. For example, you cannot create a `Model`\n",
    "    class without invoking `dataclass` decorator, and still be able to interact\n",
    "    with hydra.\n",
    "\n",
    "-   No type checking. This is a big problem. You can define `num_classes` as an\n",
    "    `int` but user passes in a `str` of `\"10\"` instead of `10`, but hydra will\n",
    "    not complain. This means that you have to do type checking yourself (i.e. do\n",
    "    checks all over the application/business logic code).\n",
    "\n",
    "-   No validation support. This means that if your global pooling methods\n",
    "    support only `avg` and `max`, you have to do the validation yourself. This\n",
    "    is a big problem because you have to do the validation all over the place.\n",
    "\n",
    "-   Interpolation is really good, but the inability to simple manipulation over\n",
    "    it caused a lot of\n",
    "    [complaints](https://github.com/omry/omegaconf/issues/91). Like if you\n",
    "    defined a learning rate as `lr: 0.001` and you want to multiply it by 10 in\n",
    "    another config file, you cannot do `10 * ${lr}`.\n",
    "\n",
    "### Instantiating\n",
    "\n",
    "You can also\n",
    "[instantiate](https://hydra.cc/docs/advanced/instantiate_objects/overview/)\n",
    "objects with hydra.\n",
    "\n",
    "### Composition Order\n",
    "\n",
    "By default, Hydra 1.1 appends `_self_` to the end of the Defaults List. This\n",
    "behavior is new in Hydra 1.1 and different from previous Hydra versions. As such\n",
    "Hydra 1.1 issues a warning if `_self_` is not specified in the primary config,\n",
    "asking you to add `_self_` and thus indicate the desired composition order. To\n",
    "address the warning while maintaining the new behavior, append `_self_` to the\n",
    "end of the Defaults List. Note that in some cases it may instead be desirable to\n",
    "add `_self_` directly after the schema and before other Defaults List elements.\n",
    "\n",
    "See\n",
    "[Composition Order](https://hydra.cc/docs/advanced/defaults_list/#composition-order)\n",
    "for more information.\n",
    "\n",
    "## Pydantic\n",
    "\n",
    "Pydantic is all you need and it solves all the aforementioned problems. We still\n",
    "leverage yaml based configuration with easy command line overrides, but this\n",
    "time, we also pass the compiled configuration from hydra to pydantic for\n",
    "validation and serialization during runtime.\n",
    "\n",
    "### Pydantic Schema\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Type\n",
    "\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "class TransformConfig(BaseModel):\n",
    "    image_size: int\n",
    "    mean: List[float]\n",
    "    std: List[float]\n",
    "\n",
    "\n",
    "class ModelConfig(BaseModel):\n",
    "    model_name: str\n",
    "    pretrained: bool\n",
    "    in_chans: Annotated[int, Field(strict=True, ge=1)]  # in_channels must be greater than or equal to 1\n",
    "    num_classes: Annotated[int, Field(strict=True, ge=1)]  # num_classes must be greater than or equal to 1\n",
    "    global_pool: str\n",
    "\n",
    "    @field_validator(\"global_pool\")\n",
    "    @classmethod\n",
    "    def validate_global_pool(cls: Type[ModelConfig], global_pool: str) -> str:\n",
    "        \"\"\"Validates global_pool is in [\"avg\", \"max\"].\"\"\"\n",
    "        if global_pool not in [\"avg\", \"max\"]:\n",
    "            raise ValueError(\"global_pool must be avg or max\")\n",
    "        return global_pool\n",
    "\n",
    "    class Config:\n",
    "        protected_namespaces = ()\n",
    "\n",
    "\n",
    "class StoresConfig(BaseModel):\n",
    "    project_name: str\n",
    "    unique_id: str\n",
    "    logs_dir: Path\n",
    "    model_artifacts_dir: Path\n",
    "\n",
    "    class Config:\n",
    "        protected_namespaces = ()\n",
    "\n",
    "\n",
    "class TrainConfig(BaseModel):\n",
    "    device: str\n",
    "    project_name: str\n",
    "    debug: bool\n",
    "    seed: int\n",
    "    num_epochs: int\n",
    "    num_classes: int = 3\n",
    "\n",
    "\n",
    "class OptimizerConfig(BaseModel):\n",
    "    optimizer_name: str\n",
    "    optimizer_params: Dict[str, Any]\n",
    "\n",
    "\n",
    "class DataConfig(BaseModel):\n",
    "    data_dir: Path\n",
    "    batch_size: int\n",
    "    num_workers: int\n",
    "    shuffle: bool = True\n",
    "\n",
    "\n",
    "class Config(BaseModel):\n",
    "    model: ModelConfig\n",
    "    transform: TransformConfig\n",
    "    datamodule: DataConfig\n",
    "    optimizer: OptimizerConfig\n",
    "    stores: StoresConfig\n",
    "    train: TrainConfig\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, config_dict: Dict[str, Any]) -> Config:\n",
    "        \"\"\"Creates Config object from a dictionary.\"\"\"\n",
    "        return cls(**config_dict)\n",
    "```\n",
    "\n",
    "### Pros\n",
    "\n",
    "-   Able to serialize and deserialize objects to and from DICT, JSON, YAML, and\n",
    "    other formats. For example, the following code will serialize a `Dict`\n",
    "    object to Pydantics' `Model` object. It can also convert back to `Dict`\n",
    "    object.\n",
    "\n",
    "    ```python\n",
    "    class ModelConfig(BaseModel):\n",
    "        model_name: str\n",
    "        pretrained: bool\n",
    "        in_chans: Annotated[int, Field(strict=True, ge=1)]  # in_channels must be greater than or equal to 1\n",
    "        num_classes: Annotated[int, Field(strict=True, ge=1)]  # num_classes must be greater than or equal to 1\n",
    "        global_pool: str\n",
    "\n",
    "        @field_validator(\"global_pool\")\n",
    "        @classmethod\n",
    "        def validate_global_pool(cls: Type[ModelConfig], global_pool: str) -> str:\n",
    "            \"\"\"Validates global_pool is in [\"avg\", \"max\"].\"\"\"\n",
    "            if global_pool not in [\"avg\", \"max\"]:\n",
    "                raise ValueError(\"global_pool must be avg or max\")\n",
    "            return global_pool\n",
    "\n",
    "        class Config:\n",
    "            protected_namespaces = ()\n",
    "\n",
    "    model_config_dict = {\n",
    "        \"model_name\": \"resnet18\",\n",
    "        \"pretrained\": True,\n",
    "        \"in_chans\": 3,\n",
    "        \"num_classes\": 1000,\n",
    "        \"global_pool\": \"avg\",\n",
    "    }\n",
    "    model = Model(**model_config_dict)\n",
    "    assert model.model_dump() == model_config_dict\n",
    "    ```\n",
    "\n",
    "-   Validation of data types and values. For a large and complex configuration,\n",
    "    you either validate the sanity of config at the config level, or check at\n",
    "    the code level (i.e. sprinkled throughout your codebase). -\n",
    "    [Constrained types](https://pydantic-docs.helpmanual.io/usage/types/#constrained-types)\n",
    "\n",
    "    ```python\n",
    "    model = Model(\n",
    "        model_name=\"resnet18\",\n",
    "        pretrained=True,\n",
    "        in_chans=0,\n",
    "        num_classes=2,\n",
    "        global_pool=\"avg\",\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    This will raise an error because `in_chans` is less than 1. Pydantic offers\n",
    "    a wide range of constrained types out of the box for you to use. If that is\n",
    "    not enough, then the custom validators can be used to validate the data with\n",
    "    custom needs.\n",
    "\n",
    "-   [Custom Validators](https://pydantic-docs.helpmanual.io/usage/validators/#custom-validators)\n",
    "\n",
    "    ```python\n",
    "    model = Model(\n",
    "        model_name=\"resnet18\",\n",
    "        pretrained=True,\n",
    "        in_chans=3,\n",
    "        num_classes=2,\n",
    "        global_pool=\"average\",\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    This will raise an error because `global_pool` is not `avg` or `max`. We\n",
    "    implemented this custom checks in the `validate_global_pool` method where we\n",
    "    decorated it with `@field_validator(\"global_pool\")`.\n",
    "\n",
    "There are many other good things like in-built type checking, and coercion. In\n",
    "the next section we see how we combine Hydra and Pydantic together.\n",
    "\n",
    "## Pydra\n",
    "\n",
    "The provided code shows a way to merge Hydra and Pydantic in a machine learning\n",
    "training pipeline, using Hydra for hierarchical configuration and command-line\n",
    "interface, and Pydantic for data validation and type checking.\n",
    "\n",
    "A Hydra-based application entry point is created using the `@hydra.main()`\n",
    "decorator. This will use the Hydra library to manage configuration files and\n",
    "command-line arguments. Hydra's `config_path` and `config_name` are specified to\n",
    "tell Hydra where to find the configuration files.\n",
    "\n",
    "This `hydra_to_pydantic` will take in a `hydra`'s `DictConfig` and convert it to\n",
    "a pydantic's `Config` object.\n",
    "\n",
    "```python\n",
    "import logging\n",
    "from typing import Any, Dict\n",
    "\n",
    "import hydra\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from rich.pretty import pprint\n",
    "\n",
    "from omnixamples.software_engineering.config_management.pydantic.config import Config\n",
    "from omnixamples.software_engineering.config_management.train import train\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def hydra_to_pydantic(config: DictConfig) -> Config:\n",
    "    \"\"\"Converts Hydra config to Pydantic config.\"\"\"\n",
    "    # use to_container to resolve\n",
    "    config_dict: Dict[str, Any] = OmegaConf.to_object(config)  # type: ignore[assignment]\n",
    "    return Config(**config_dict)\n",
    "\n",
    "\n",
    "@hydra.main(version_base=None, config_path=\"../hydra/configs\", config_name=\"config\")\n",
    "def run(config: DictConfig) -> None:\n",
    "    \"\"\"Run the main function.\"\"\"\n",
    "    LOGGER.info(\"Type of config is: %s\", type(config))\n",
    "    LOGGER.info(\"Merged Yaml:\\n%s\", OmegaConf.to_yaml(config))\n",
    "    LOGGER.info(HydraConfig.get().job.name)\n",
    "\n",
    "    config_pydantic = hydra_to_pydantic(config)\n",
    "    pprint(config_pydantic)\n",
    "    train(config_pydantic)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n",
    "```\n",
    "\n",
    "## References and Further Readings\n",
    "\n",
    "-   [Hydra](https://hydra.cc/)\n",
    "-   [Pydantic](https://pydantic-docs.helpmanual.io/)\n",
    "-   [Pydra - Pydantic and Hydra for configuration management of model training experiments](https://suneeta-mall.github.io/2022/03/15/hydra-pydantic-config-management-for-training-application.html)\n",
    "-   [hydra_pydantic_config_management](https://github.com/suneeta-mall/hydra_pydantic_config_management/tree/master)\n",
    "-   [PyTorch Lightning Pipeline](https://github.com/gao-hongnan/pytorch-lightning-pipeline/tree/main)\n",
    "-   [Inversion of Control Containers and the Dependency Injection pattern](https://martinfowler.com/articles/injection.html)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "mystnb": {
   "number_source_lines": true
  },
  "source_map": [
   16
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}