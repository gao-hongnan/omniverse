<!-- _templates/html.html -->

<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Concept &#8212; Omniverse</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=bb35926c" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MYW5YKC2WF"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"defeq": "\\overset{\\text{def}}{=}", "defa": "\\overset{\\text{(a)}}{=}", "defb": "\\overset{\\text{(b)}}{=}", "defc": "\\overset{\\text{(c)}}{=}", "defd": "\\overset{\\text{(d)}}{=}", "st": "\\mid", "mod": "\\mid", "S": "\\Omega", "s": "\\omega", "e": "\\exp", "P": "\\mathbb{P}", "R": "\\mathbb{R}", "expectation": "\\mathbb{E}", "v": "\\mathbf{v}", "a": "\\mathbf{a}", "b": "\\mathbf{b}", "c": "\\mathbf{c}", "u": "\\mathbf{u}", "w": "\\mathbf{w}", "x": "\\mathbf{x}", "y": "\\mathbf{y}", "z": "\\mathbf{z}", "0": "\\mathbf{0}", "1": "\\mathbf{1}", "A": "\\mathbf{A}", "B": "\\mathbf{B}", "C": "\\mathbf{C}", "E": "\\mathcal{F}", "eventA": "\\mathcal{A}", "lset": "\\left\\{", "rset": "\\right\\}", "lsq": "\\left[", "rsq": "\\right]", "lpar": "\\left(", "rpar": "\\right)", "lcurl": "\\left\\{", "rcurl": "\\right\\}", "pmf": "p_X", "pdf": "f_X", "pdftwo": "f_{X,Y}", "pdfjoint": "f_{\\mathbf{X}}", "pmfjointxy": "p_{X, Y}", "pdfjointxy": "f_{X, Y}", "cdf": "F_X", "pspace": "(\\Omega, \\mathcal{F}, \\mathbb{P})", "var": "\\operatorname{Var}", "std": "\\operatorname{Std}", "bern": "\\operatorname{Bernoulli}", "binomial": "\\operatorname{Binomial}", "geometric": "\\operatorname{Geometric}", "poisson": "\\operatorname{Poisson}", "uniform": "\\operatorname{Uniform}", "normal": "\\operatorname{Normal}", "gaussian": "\\operatorname{Gaussian}", "gaussiansymbol": "\\mathcal{N}", "exponential": "\\operatorname{Exponential}", "iid": "\\textbf{i.i.d.}", "and": "\\text{and}", "O": "\\mathcal{O}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept';</script>
    <link rel="canonical" href="https://www.gaohongnan.com/probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Distributed Systems" href="../../../operations/distributed/intro.html" />
    <link rel="prev" title="Maximum Likelihood Estimation" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/logo.png" class="logo__image only-light" alt="Omniverse - Home"/>
    <img src="../../../_static/logo.png" class="logo__image only-dark pst-js-only" alt="Omniverse - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../intro.html">
                    Omniverse
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../notations/machine_learning.html">Machine Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Influential Ideas and Papers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../influential/generative_pretrained_transformer/01_intro.html">Generative Pre-trained Transformers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/generative_pretrained_transformer/02_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/generative_pretrained_transformer/03_concept.html">The Concept of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/generative_pretrained_transformer/04_implementation.html">The Implementation of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/generative_pretrained_transformer/05_adder.html">Training a Mini-GPT to Learn Two-Digit Addition</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../influential/low_rank_adaptation/01_intro.html">Low-Rank Adaptation Of Large Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/low_rank_adaptation/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/low_rank_adaptation/03_implementation.html">Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../influential/empirical_risk_minimization/01_intro.html">Empirical Risk Minimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/empirical_risk_minimization/02_concept.html">Concept: Empirical Risk Minimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/empirical_risk_minimization/03_bayes_optimal_classifier.html">Bayes Optimal Classifier</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../influential/learning_theory/01_intro.html">Is The Learning Problem Solvable?</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/learning_theory/02_concept.html">Concept: Learning Theory</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../influential/kmeans_clustering/01_intro.html">Lloyd’s K-Means Clustering Algorithm</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/kmeans_clustering/02_concept.html">Concept: K-Means Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/kmeans_clustering/03_implementation.html">Implementation: K-Means (Lloyd)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/kmeans_clustering/04_image_segmentation.html">Application: Image Compression and Segmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/kmeans_clustering/05_conceptual_questions.html">Conceptual Questions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../influential/naive_bayes/01_intro.html">Naive Bayes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/naive_bayes/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/naive_bayes/03_implementation.html">Naives Bayes Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/naive_bayes/04_example_penguins.html">Naive Bayes Application: Penguins</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/naive_bayes/05_application_mnist.html">Naive Bayes Application (MNIST)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../influential/gaussian_mixture_models/01_intro.html">Mixture Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/gaussian_mixture_models/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/gaussian_mixture_models/03_implementation.html">Gaussian Mixture Models Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../influential/linear_regression/01_intro.html">Linear Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/linear_regression/02_concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../influential/linear_regression/03_implementation.html">Implementation</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Playbook</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../playbook/training/intro.html">Training Dynamics And Tricks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../playbook/training/how_to_calculate_flops_in_transformer_based_models.html">How to Calculate the Number of FLOPs in Transformer Based Models?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../playbook/training/why_cosine_annealing_warmup_stabilize_training.html">Why Does Cosine Annealing With Warmup Stabilize Training?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../playbook/training/how_to_finetune_decoder_with_last_token_pooling.html">How To Fine-Tune Decoder-Only Models For Sequence Classification Using Last Token Pooling?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../playbook/training/how_to_finetune_decoder_with_cross_attention.html">How To Fine-Tune Decoder-Only Models For Sequence Classification With Cross-Attention?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../playbook/training/how_to_teacher_student_knowledge_distillation.html">How To Do Teacher-Student Knowledge Distillation?</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../playbook/why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html">Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling.</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../playbook/how_to_inspect_function_and_class_signatures.html">How to Inspect Function and Class Signatures in Python?</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Probability Theory</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../01_mathematical_preliminaries/intro.html">Chapter 1. Mathematical Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../01_mathematical_preliminaries/01_combinatorics.html">Permutations and Combinations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../01_mathematical_preliminaries/02_calculus.html">Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../01_mathematical_preliminaries/03_contours.html">Contour Maps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../01_mathematical_preliminaries/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../02_probability/intro.html">Chapter 2. Probability</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../02_probability/0202_probability_space.html">Probability Space</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../02_probability/0203_probability_axioms.html">Probability Axioms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../02_probability/0204_conditional_probability.html">Conditional Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../02_probability/0205_independence.html">Independence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../02_probability/0206_bayes_theorem.html">Baye’s Theorem and the Law of Total Probability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../02_probability/summary.html">Summary</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../03_discrete_random_variables/intro.html">Chapter 3. Discrete Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../03_discrete_random_variables/0301_random_variables.html">Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../03_discrete_random_variables/0302_discrete_random_variables.html">Discrete Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../03_discrete_random_variables/0303_probability_mass_function.html">Probability Mass Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../03_discrete_random_variables/0304_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../03_discrete_random_variables/0305_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../03_discrete_random_variables/0306_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../03_discrete_random_variables/uniform/intro.html">Discrete Uniform Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../03_discrete_random_variables/uniform/0307_discrete_uniform_distribution_application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../03_discrete_random_variables/bernoulli/intro.html">Bernoulli Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../03_discrete_random_variables/bernoulli/0308_bernoulli_distribution_application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../03_discrete_random_variables/iid.html">Independent and Identically Distributed (IID)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../03_discrete_random_variables/binomial/intro.html">Binomial Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../03_discrete_random_variables/binomial/0309_binomial_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../03_discrete_random_variables/binomial/0309_binomial_distribution_implementation.html">Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../03_discrete_random_variables/binomial/0309_binomial_distribution_application.html">Real World Examples</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../03_discrete_random_variables/geometric/intro.html">Geometric Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../03_discrete_random_variables/geometric/0310_geometric_distribution_concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../03_discrete_random_variables/poisson/intro.html">Poisson Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../03_discrete_random_variables/poisson/0311_poisson_distribution_concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../03_discrete_random_variables/poisson/0311_poisson_distribution_implementation.html">Implementation</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../03_discrete_random_variables/summary.html">Important</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../03_discrete_random_variables/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../04_continuous_random_variables/intro.html">Chapter 4. Continuous Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../04_continuous_random_variables/from_discrete_to_continuous.html">From Discrete to Continuous</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../04_continuous_random_variables/0401_continuous_random_variables.html">Continuous Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../04_continuous_random_variables/0402_probability_density_function.html">Probability Density Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../04_continuous_random_variables/0403_expectation.html">Expectation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../04_continuous_random_variables/0404_moments_and_variance.html">Moments and Variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../04_continuous_random_variables/0405_cumulative_distribution_function.html">Cumulative Distribution Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../04_continuous_random_variables/0406_mean_median_mode.html">Mean, Median and Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../04_continuous_random_variables/0407_continuous_uniform_distribution.html">Continuous Uniform Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../04_continuous_random_variables/0408_exponential_distribution.html">Exponential Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../04_continuous_random_variables/0409_gaussian_distribution.html">Gaussian Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../04_continuous_random_variables/0410_skewness_and_kurtosis.html">Skewness and Kurtosis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../04_continuous_random_variables/0411_convolve_and_sum_of_random_variables.html">Convolution and Sum of Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../04_continuous_random_variables/0412_functions_of_random_variables.html">Functions of Random Variables</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../05_joint_distributions/intro.html">Chapter 5. Joint Distributions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../05_joint_distributions/from_single_variable_to_joint_distributions.html">From Single Variable to Joint Distributions</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../05_joint_distributions/0501_joint_pmf_pdf/intro.html">Joint PMF and PDF</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../05_joint_distributions/0501_joint_pmf_pdf/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../05_joint_distributions/0502_joint_expectation_and_correlation/intro.html">Joint Expectation and Correlation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../05_joint_distributions/0502_joint_expectation_and_correlation/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../05_joint_distributions/0503_conditional_pmf_pdf/intro.html">Conditional PMF and PDF</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../05_joint_distributions/0503_conditional_pmf_pdf/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../05_joint_distributions/0503_conditional_pmf_pdf/application.html">Application</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../05_joint_distributions/0504_conditional_expectation_variance/intro.html">Conditional Expectation and Variance</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../05_joint_distributions/0504_conditional_expectation_variance/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../05_joint_distributions/0504_conditional_expectation_variance/exercises.html">Exercises</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../05_joint_distributions/0505_sum_of_random_variables/intro.html">Sum of Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../05_joint_distributions/0505_sum_of_random_variables/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../05_joint_distributions/0506_random_vectors/intro.html">Random Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../05_joint_distributions/0506_random_vectors/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../05_joint_distributions/0507_multivariate_gaussian/intro.html">Multivariate Gaussian Distribution</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../05_joint_distributions/0507_multivariate_gaussian/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../05_joint_distributions/0507_multivariate_gaussian/application_transformation.html">Application: Plots and Transformations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../05_joint_distributions/0507_multivariate_gaussian/psd.html">Covariance Matrix is Positive Semi-Definite</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../05_joint_distributions/0507_multivariate_gaussian/eigendecomposition.html">Eigendecomposition and Covariance Matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../05_joint_distributions/0507_multivariate_gaussian/geometry_of_multivariate_gaussian.html">The Geometry of Multivariate Gaussians</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../06_sample_statistics/intro.html">Chapter 6. Sample Statistics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../06_sample_statistics/0601_moment_generating_and_characteristic_functions/intro.html">Moment Generating and Characteristic Functions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function.html">Moment Generating Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../06_sample_statistics/0601_moment_generating_and_characteristic_functions/moment_generating_function_application_sum_of_rv.html">Application: Moment Generating Function and the Sum of Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../06_sample_statistics/0601_moment_generating_and_characteristic_functions/characteristic_function.html">Characteristic Function</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../06_sample_statistics/0602_probability_inequalities/intro.html">Probability Inequalities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../06_sample_statistics/0602_probability_inequalities/concept.html">Probability Inequalities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../06_sample_statistics/0602_probability_inequalities/application.html">Application: Learning Theory</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../06_sample_statistics/0603_law_of_large_numbers/intro.html">Law of Large Numbers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../06_sample_statistics/0603_law_of_large_numbers/concept.html">Concept</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../06_sample_statistics/0603_law_of_large_numbers/convergence.html">Convergence of Sample Average</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../06_sample_statistics/0603_law_of_large_numbers/application.html">Application: Learning Theory</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../intro.html">Chapter 8. Estimation Theory</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active has-children"><a class="reference internal" href="intro.html">Maximum Likelihood Estimation</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3 current active"><a class="current reference internal" href="#">Concept</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Operations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../operations/distributed/intro.html">Distributed Systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../operations/distributed/01_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../operations/distributed/02_basics.html">Basics Of Distributed Data Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../operations/distributed/03_how_to_setup_slurm_in_aws.html">How to Setup SLURM and ParallelCluster in AWS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../operations/distributed/04_ablation.html">Ablations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../operations/profiling/intro.html">Profiling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../operations/profiling/01_synchronize.html">Synchronize CUDA To Time CUDA Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../operations/profiling/02_timeit.html">Profiling Code With Timeit</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../operations/profiling/03_time_profiler.html">PyTorch’s Event And Profiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../operations/profiling/04_small_gpt_profile.html">Profile GPT Small Time And Memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../operations/profiling/05_memory_leak.html">CUDA Memory Allocations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../operations/machine_learning_lifecycle/00_intro.html">The Lifecycle of an AIOps System</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../operations/machine_learning_lifecycle/01_problem_formulation.html">Stage 1. Problem Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../operations/machine_learning_lifecycle/02_project_scoping.html">Stage 2. Project Scoping And Framing The Problem</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../operations/machine_learning_lifecycle/03_dataops_pipeline/03_dataops_pipeline.html">Stage 3. Data Pipeline (Data Engineering and DataOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../operations/machine_learning_lifecycle/03_dataops_pipeline/031_data_source_and_format.html">Stage 3.1. Data Source and Formats</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../operations/machine_learning_lifecycle/03_dataops_pipeline/032_data_model_and_storage.html">Stage 3.2. Data Model and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../operations/machine_learning_lifecycle/03_dataops_pipeline/033_etl.html">Stage 3.3. Extract, Transform, Load (ETL)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../operations/machine_learning_lifecycle/04_mlops_data_pipeline.html">Stage 4. Data Extraction (MLOps), Data Analysis (Data Science), Data Preparation (Data Science)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/05_ml_training_pipeline.html">Stage 5. Model Development and Training (MLOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/051_model_selection.html">Stage 5.1. Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/052_metric_selection.html">Stage 5.2. Metric Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/053_experiment_tracking.html">Stage 5.3. Experiment Tracking And Versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../operations/machine_learning_lifecycle/05_model_development_selection_and_training/054_model_testing.html">Stage 5.4. Model Testing</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../operations/machine_learning_lifecycle/06_model_evaluation.html">Stage 6. Model Evaluation (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../operations/machine_learning_lifecycle/07_model_validation_registry_and_pushing_model_to_production.html">Stage 7. Model Validation, Registry and Pushing Model to Production (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../operations/machine_learning_lifecycle/08_model_deployment_and_serving.html">Stage 8. Model Serving (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../operations/machine_learning_lifecycle/09_model_monitoring.html">Stage 9. Model Monitoring (MLOps)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../operations/machine_learning_lifecycle/010_continuous_integration_deployment_learning_and_training.html">Stage 10. Continuous Integration, Deployment, Learning and Training (DevOps, DataOps, MLOps)</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../software_engineering/config_management/intro.html">Config, State, Metadata Management</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/config_management/concept.html">Configuration Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/config_management/01-pydra.html">Pydantic And Hydra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/config_management/02-state.html">State And Metadata Management</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../software_engineering/design_patterns/intro.html">Design Patterns</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/design_patterns/dependency_inversion_principle.html">Dependency Inversion Principle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/design_patterns/named_constructor.html">Named Constructor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/design_patterns/strategy.html">Strategy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/design_patterns/registry.html">Registry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/design_patterns/god_object_pattern.html">Context Object Pattern (God Object)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/design_patterns/factory_method.html">Factory Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/design_patterns/singleton.html">Singleton</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../software_engineering/python/intro.html">Python</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/python/new_vs_init.html">Init vs New</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/python/gil.html">Global Interpreter Lock (GIL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/python/iterator_protocol.html">The Iterator Protocol</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/python/decorator.html">Decorator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/python/generators_over_lists.html">Generators Over Lists For Memory Efficiency</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/python/pydantic.html">Pydantic Is All You Need - Jason Liu</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/python/mutable_default.html">Do Not Use Mutable Default Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/python/set_vs_list.html">Set Over List For Frequent Membership Tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/python/late_binding_closures.html">Late Binding Closures</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/python/is_vs_equality.html">Is vs Equality</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../software_engineering/concurrency_parallelism_asynchronous/intro.html">Concurrency, Parallelism and Asynchronous Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/concurrency_parallelism_asynchronous/overview.html">Overview Of Concurrency, Parallelism, and Asynchronous Execution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/concurrency_parallelism_asynchronous/insights/locks_for_thread_safety.html">Thread Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../software_engineering/concurrency_parallelism_asynchronous/generator_yield.html">A Rudimentary Introduction to Generator and Yield in Python</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../computer_science/type_theory/intro.html">Type Theory, A Very Rudimentary Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../computer_science/type_theory/01-subtypes.html">Subtypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../computer_science/type_theory/02-type-safety.html">Type Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../computer_science/type_theory/03-subsumption.html">Subsumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../computer_science/type_theory/04-generics.html">Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../computer_science/type_theory/05-typevar-bound-constraints.html">Bound and Constraint in Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../computer_science/type_theory/06-invariance-covariance-contravariance.html">Invariance, Covariance and Contravariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../computer_science/type_theory/07-pep-3124-overloading.html">Function Overloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../computer_science/type_theory/08-pep-661-sentinel-values.html">Sentinel Types</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Structures and Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../dsa/complexity_analysis/intro.html">Complexity Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../dsa/complexity_analysis/master_theorem.html">Master Theorem </a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../dsa/array/intro.html">List/Array</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../dsa/array/concept.html">Concept</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../dsa/array/questions/intro.html">Questions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../dsa/array/questions/01-two-sum.html">Two Sum</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../dsa/hash_map/intro.html">Hash Map</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../dsa/hash_map/concept.html">Concept</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../dsa/hash_map/questions/intro.html">Questions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../dsa/hash_map/questions/01-two-sum.html">Two Sum</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../dsa/hash_map/questions/49-group-anagrams.html">Group Anagrams</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../dsa/two_pointers/intro.html">Two Pointers And Sliding Window</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../dsa/two_pointers/two_pointers.html">Two Pointers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../dsa/two_pointers/sliding_window.html">Sliding Window</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../dsa/two_pointers/questions/intro.html">Questions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../dsa/two_pointers/questions/two_pointers/intro.html">Two Pointers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../dsa/two_pointers/questions/two_pointers/26-remove-duplicates-from-sorted-array.html">Remove Duplicates from Sorted Array</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../dsa/two_pointers/questions/two_pointers/167-two-sum-ii-input-array-is-sorted.html">Two Sum II - Input Array Is Sorted</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../dsa/two_pointers/questions/sliding_window/intro.html">Sliding Window</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../dsa/two_pointers/questions/sliding_window/438-find-all-anagrams-in-a-string.html">Find All Anagrams in a String</a></li>
</ul>
</details></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../dsa/stack/intro.html">Stack</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../dsa/stack/concept.html">Concept</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../dsa/stack/questions/intro.html">Questions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../dsa/stack/questions/20-valid-parentheses.html">Valid Parentheses</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../dsa/stack/questions/155-min-stack.html">Min Stack</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../dsa/stack/questions/232-implement-queue-using-stacks.html">Implement Queue using Stacks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../dsa/stack/questions/344-reverse-string.html">Reverse String</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../dsa/queue/intro.html">Queue</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../dsa/queue/concept.html">Concept</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../dsa/queue/dequeue.html">Double Ended Queue</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../dsa/queue/questions/hot-potatoes.html">Easy - Hot Potatoes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../dsa/queue/questions/125-valid-palindrome.html">Palindrome Checker</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../dsa/searching_algorithms/linear_search/intro.html">Linear Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../dsa/searching_algorithms/linear_search/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../dsa/searching_algorithms/binary_search/intro.html">Binary Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../dsa/searching_algorithms/binary_search/concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../dsa/searching_algorithms/binary_search/problems/875-koko-eating-bananas.html">Koko Eating Bananas</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../linear_algebra/01_preliminaries/intro.html">Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../linear_algebra/01_preliminaries/01-fields.html">Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../linear_algebra/01_preliminaries/02-systems-of-linear-equations.html">Systems of Linear Equations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../linear_algebra/02_vectors/intro.html">Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../linear_algebra/02_vectors/01-vector-definition.html">Vector and Its Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../linear_algebra/02_vectors/02-vector-operation.html">Vector and Its Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../linear_algebra/02_vectors/03-vector-norm.html">Vector Norm and Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../linear_algebra/02_vectors/04-vector-products.html">A First Look at Vector Products</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../citations.html">IEEE (Style) Citations</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/gao-hongnan/omniverse/blob/main/omniverse/probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../../../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse/issues/new?title=Issue%20on%20page%20%2Fprobability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../../../_sources/probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Concept</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood">Likelihood</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#some-intuition">Some Intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-and-identically-distributed-iid">Independence and Identically Distributed (IID)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-in-the-context-of-machine-learning">Likelihood in the Context of Machine Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-in-the-context-of-supervised-learning">Likelihood in the Context of Supervised Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-likelihood-in-the-context-of-machine-learning">Conditional Likelihood in the Context of Machine Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-log-likelihood-function">The Log-Likelihood Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-likelihood-function">Visualizing the Likelihood Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation">Maximum Likelihood Estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#coin-toss-example">Coin Toss Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-likelihood-and-maximum-likelihood-estimation-as-n-increases">Visualizing Likelihood and Maximum Likelihood Estimation as <span class="math notranslate nohighlight">\(N\)</span> Increases</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-optimization-and-the-negative-log-likelihood">Numerical Optimization and the Negative Log-Likelihood</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-underflow">Numerical Underflow</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-convenience">Mathematical Convenience</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#information-theory">Information Theory</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-for-continuous-variables">Maximum Likelihood for Continuous Variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-for-common-distributions">Maximum Likelihood Estimation for Common Distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-for-univariate-gaussian">Maximum Likelihood for Univariate Gaussian</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-for-multivariate-gaussian">Maximum Likelihood Estimation for Multivariate Gaussian</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="concept">
<h1>Concept<a class="headerlink" href="#concept" title="Link to this heading">#</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#likelihood" id="id7">Likelihood</a></p>
<ul>
<li><p><a class="reference internal" href="#some-intuition" id="id8">Some Intuition</a></p></li>
<li><p><a class="reference internal" href="#definition" id="id9">Definition</a></p></li>
<li><p><a class="reference internal" href="#independence-and-identically-distributed-iid" id="id10">Independence and Identically Distributed (IID)</a></p></li>
<li><p><a class="reference internal" href="#likelihood-in-the-context-of-machine-learning" id="id11">Likelihood in the Context of Machine Learning</a></p></li>
<li><p><a class="reference internal" href="#likelihood-in-the-context-of-supervised-learning" id="id12">Likelihood in the Context of Supervised Learning</a></p></li>
<li><p><a class="reference internal" href="#conditional-likelihood-in-the-context-of-machine-learning" id="id13">Conditional Likelihood in the Context of Machine Learning</a></p></li>
<li><p><a class="reference internal" href="#the-log-likelihood-function" id="id14">The Log-Likelihood Function</a></p></li>
<li><p><a class="reference internal" href="#visualizing-the-likelihood-function" id="id15">Visualizing the Likelihood Function</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#maximum-likelihood-estimation" id="id16">Maximum Likelihood Estimation</a></p></li>
<li><p><a class="reference internal" href="#coin-toss-example" id="id17">Coin Toss Example</a></p></li>
<li><p><a class="reference internal" href="#visualizing-likelihood-and-maximum-likelihood-estimation-as-n-increases" id="id18">Visualizing Likelihood and Maximum Likelihood Estimation as <span class="math notranslate nohighlight">\(N\)</span> Increases</a></p></li>
<li><p><a class="reference internal" href="#numerical-optimization-and-the-negative-log-likelihood" id="id19">Numerical Optimization and the Negative Log-Likelihood</a></p>
<ul>
<li><p><a class="reference internal" href="#numerical-underflow" id="id20">Numerical Underflow</a></p></li>
<li><p><a class="reference internal" href="#mathematical-convenience" id="id21">Mathematical Convenience</a></p></li>
<li><p><a class="reference internal" href="#information-theory" id="id22">Information Theory</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#maximum-likelihood-for-continuous-variables" id="id23">Maximum Likelihood for Continuous Variables</a></p></li>
<li><p><a class="reference internal" href="#maximum-likelihood-estimation-for-common-distributions" id="id24">Maximum Likelihood Estimation for Common Distributions</a></p>
<ul>
<li><p><a class="reference internal" href="#maximum-likelihood-for-univariate-gaussian" id="id25">Maximum Likelihood for Univariate Gaussian</a></p></li>
<li><p><a class="reference internal" href="#maximum-likelihood-estimation-for-multivariate-gaussian" id="id26">Maximum Likelihood Estimation for Multivariate Gaussian</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#references-and-further-readings" id="id27">References and Further Readings</a></p></li>
</ul>
</nav>
<section id="likelihood">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Likelihood</a><a class="headerlink" href="#likelihood" title="Link to this heading">#</a></h2>
<section id="some-intuition">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">Some Intuition</a><a class="headerlink" href="#some-intuition" title="Link to this heading">#</a></h3>
<p><strong><em>This section is adapted from <span id="id1">[<a class="reference internal" href="../../../bibliography.html#id15" title="Stanley H. Chan. Introduction to probability for Data Science. Michigan Publishing, 2021.">Chan, 2021</a>]</span>.</em></strong></p>
<p>Consider a set of <span class="math notranslate nohighlight">\(N\)</span> data points
<span class="math notranslate nohighlight">\(\mathcal{S}=\left\{x^{(1)}, x^{(2)}, \ldots, x^{(n)}\right\}\)</span>. We want to
describe these data points using a probability distribution. What would be the
most general way of defining such a distribution?</p>
<p>Since we have <span class="math notranslate nohighlight">\(N\)</span> data points, and we do not know anything about them, the most
general way to define a distribution is as a high-dimensional probability
density function (PDF) <span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x})\)</span>. This is a PDF of a random
vector <span class="math notranslate nohighlight">\(\mathbf{X}=\left[x^{(1)}, \ldots, x^{(n)}\right]^{T}\)</span>. A particular
realization of this random vector is
<span class="math notranslate nohighlight">\(\mathbf{x}=\left[x^{(1)}, \ldots, x^{(n)}\right]^{T}\)</span>.</p>
<p><span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x})\)</span> is the most general description for the <span class="math notranslate nohighlight">\(N\)</span> data
points because <span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x})\)</span> is the <strong>joint</strong> PDF of all
variables. It provides the complete statistical description of the vector
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. For example, we can compute the mean vector
<span class="math notranslate nohighlight">\(\mathbb{E}[\mathbf{X}]\)</span>, the covariance matrix
<span class="math notranslate nohighlight">\(\operatorname{Cov}(\mathbf{X})\)</span>, the marginal distributions, the conditional
distribution, the conditional expectations, etc. In short, if we know
<span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x})\)</span>, we know everything about <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<p>The joint PDF <span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x})\)</span> is always <strong>parameterized</strong> by a
certain parameter <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. For example, if we assume that
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is drawn from a joint Gaussian distribution, then
<span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x})\)</span> is parameterized by the mean vector
<span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> and the covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>. So we say
that the parameter <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is
<span class="math notranslate nohighlight">\(\boldsymbol{\theta}=(\boldsymbol{\mu}, \boldsymbol{\Sigma})\)</span>. To state the
dependency on the parameter explicitly, we write</p>
<div class="math notranslate nohighlight">
\[
f_{\mathbf{X}}(\mathbf{x} ; \boldsymbol{\theta})=\mathrm{PDF} \text { of the random vector } \mathbf{X} \text { with a parameter } \boldsymbol{\theta} .
\]</div>
<p>When you express the joint PDF as a function of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and
<span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, you have two variables to play with. The first variable
is the <strong>observation</strong> <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, which is given by the measured data. We
usually think about the probability density function
<span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x})\)</span> in terms of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, because the PDF is
evaluated at <span class="math notranslate nohighlight">\(\mathbf{X}=\mathbf{x}\)</span>. In estimation, however, <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is
something that you cannot control. When your boss hands a dataset to you,
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is already fixed. You can consider the probability of getting this
particular <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, but you cannot change <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<p>The second variable stated in <span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x} ; \boldsymbol{\theta})\)</span>
is the <strong>parameter</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. This parameter is what we want to
find out, and it is the subject of interest in an estimation problem. Our goal
is to find the optimal <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> that can offer the “best
explanation” to data <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, in the sense that it can maximize
<span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x} ; \boldsymbol{\theta})\)</span>.</p>
<p>The likelihood function is the PDF that shifts the emphasis to
<span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, let’s define it formally.</p>
</section>
<section id="definition">
<h3><a class="toc-backref" href="#id9" role="doc-backlink">Definition</a><a class="headerlink" href="#definition" title="Link to this heading">#</a></h3>
<div class="proof definition admonition" id="def:likelihood">
<p class="admonition-title"><span class="caption-number">Definition 167 </span> (Likelihood Function)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\mathbf{X}=\left[x^{(1)}, \ldots, x^{(n)}\right]^{T}\)</span> be a random vector drawn from a joint PDF <span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x} ; \boldsymbol{\theta})\)</span>, and let <span class="math notranslate nohighlight">\(\mathbf{x}=\left[x^{(1)}, \ldots, x^{(n)}\right]^{T}\)</span> be the realizations. The likelihood function is a
function of the parameter <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> given the realizations <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> :</p>
<div class="math notranslate nohighlight" id="equation-eq-likelihood">
<span class="eqno">(221)<a class="headerlink" href="#equation-eq-likelihood" title="Link to this equation">#</a></span>\[
\mathcal{L}(\boldsymbol{\theta} \mid \mathbf{x}) \stackrel{\text { def }}{=} f_{\mathbf{X}}(\mathbf{x} ; \boldsymbol{\theta})
\]</div>
</section>
</div><div class="proof remark admonition" id="rem:likelihood">
<p class="admonition-title"><span class="caption-number">Remark 60 </span> (Likelihood is not Conditional PDF)</p>
<section class="remark-content" id="proof-content">
<p>A word of caution: <span class="math notranslate nohighlight">\(\mathcal{L}(\boldsymbol{\theta} \mid \mathbf{x})\)</span> is not a conditional PDF because <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is not a random variable. The correct way to interpret <span class="math notranslate nohighlight">\(\mathcal{L}(\boldsymbol{\theta} \mid \mathbf{x})\)</span> is to view it as a function of <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.</p>
</section>
</div></section>
<section id="independence-and-identically-distributed-iid">
<h3><a class="toc-backref" href="#id10" role="doc-backlink">Independence and Identically Distributed (IID)</a><a class="headerlink" href="#independence-and-identically-distributed-iid" title="Link to this heading">#</a></h3>
<p>While <span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x})\)</span> provides us with a complete picture of the
random vector <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, using <span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x})\)</span> is tedious. We
need to describe how each <span class="math notranslate nohighlight">\(x^{(n)}\)</span> is generated and describe how <span class="math notranslate nohighlight">\(x^{(n)}\)</span> is
related to <span class="math notranslate nohighlight">\(X_{m}\)</span> for all pairs of <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(m\)</span>. If the vector <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>
contains <span class="math notranslate nohighlight">\(N\)</span> entries, then there are <span class="math notranslate nohighlight">\(N^{2} / 2\)</span> pairs of correlations we need
to compute. When <span class="math notranslate nohighlight">\(N\)</span> is large, finding <span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x})\)</span> would be
very difficult if not impossible.</p>
<p>What does this mean? Two things.</p>
<ol class="arabic simple">
<li><p>There is no assumption of <strong>independence</strong> between the data points. This
means that describing the joint PDF <span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x})\)</span> is very
difficult.</p></li>
<li><p>Each data point <em>can</em> be drawn from a different distribution
<span class="math notranslate nohighlight">\(f_{X^{(n)}}(x^{(n)})\)</span> for each <span class="math notranslate nohighlight">\(n\)</span>.</p></li>
</ol>
<p>Hope is not lost.</p>
<p>Enter the <strong>independence and identically distributed (IID)</strong> assumption. This
assumption states that the data points <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> are independent and
identically distributed.</p>
<p>In other words, each data point <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> is drawn from <strong>identical</strong>
distribution <span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x} ; \boldsymbol{\theta})\)</span> parameterized by
<span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> and each pair of data points <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{x}^{(m)}\)</span> are <strong>independent</strong> of each other.</p>
<p>Now, we can write the problem in a much simpler way, where the joint PDF
<span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x})\)</span> is replaced by the product of the PDFs of each data
point <span class="math notranslate nohighlight">\(f_{x^{(n)}}(x^{(n)})\)</span>.</p>
<div class="math notranslate nohighlight">
\[
f_{\mathbf{X}}(\mathbf{x})=f_{x^{(1)}, \ldots, x^{(n)}}\left(x^{(1)}, \ldots, x^{(n)}\right)=\prod_{n=1}^{N} f_{x^{(n)}}\left(x^{(n)}\right) .
\]</div>
<p>or in our context, we can add the <strong>parameter</strong> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> to the
PDFs.</p>
<div class="math notranslate nohighlight">
\[
f_{\mathbf{X}}(\mathbf{x} ; \boldsymbol{\theta})=f_{x^{(1)}, \ldots, x^{(n)}}\left(x^{(1)}, \ldots, x^{(n)}\right)=\prod_{n=1}^{N} f_{x^{(n)}}\left(x^{(n)} ; \boldsymbol{\theta}\right) .
\]</div>
<p>Let’s formally redefine the likelihood function with the IID assumption. Note
this is an ubiquitous assumption in machine learning and therefore we will stick
to this unless otherwise stated.</p>
<div class="proof definition admonition" id="def:likelihood-iid">
<p class="admonition-title"><span class="caption-number">Definition 168 </span> (Likelihood Function with IID Assumption)</p>
<section class="definition-content" id="proof-content">
<p>Given <span class="math notranslate nohighlight">\(\textrm{i.i.d.}\)</span> random variables <span class="math notranslate nohighlight">\(x^{(1)}, \ldots, x^{(n)}\)</span> that all have the same PDF <span class="math notranslate nohighlight">\(f_{x^{(n)}}\left(x^{(n)}\right)\)</span>, the <strong>likelihood function</strong> is defined as:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\boldsymbol{\theta} \mid \mathbf{x}) \stackrel{\text { def }}{=} \prod_{n=1}^{N} f_{x^{(n)}}\left(x^{(n)} ; \boldsymbol{\theta}\right)
\]</div>
</section>
</div><p>Notice that in the previous sections, there was an implicit assumption that the
random vector <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is a vector of <strong>univariate*</strong> random variables.
This is not always the case. In fact, most of the time, the random vector
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is a “vector” (collection) of <strong>multivariate</strong> random variables in
the machine learning realm.</p>
<p>Let’s redefine the likelihood function for the higher dimensional case, and also
take the opportunity to introduce the definition in the context of machine
learning.</p>
</section>
<section id="likelihood-in-the-context-of-machine-learning">
<h3><a class="toc-backref" href="#id11" role="doc-backlink">Likelihood in the Context of Machine Learning</a><a class="headerlink" href="#likelihood-in-the-context-of-machine-learning" title="Link to this heading">#</a></h3>
<div class="proof definition admonition" id="def:likelihood-iid-higher-dim">
<p class="admonition-title"><span class="caption-number">Definition 169 </span> (Likelihood Function with IID Assumption (Higher Dimension))</p>
<section class="definition-content" id="proof-content">
<p>Given a dataset <span class="math notranslate nohighlight">\(\mathcal{S} = \left\{\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)}\right\}\)</span> where each <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> is a vector of <span class="math notranslate nohighlight">\(D\)</span>-dimensional drawn <span class="math notranslate nohighlight">\(\textrm{i.i.d.}\)</span> from the same underlying distribution
<span class="math notranslate nohighlight">\(\mathbb{P}_{\mathcal{D}}\left(\mathcal{X} ; \boldsymbol{\theta}\right)\)</span>, the <strong>likelihood function</strong> is defined as:</p>
<div class="math notranslate nohighlight" id="equation-eq-likelihood-machine-learning-1">
<span class="eqno">(222)<a class="headerlink" href="#equation-eq-likelihood-machine-learning-1" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathcal{L}(\boldsymbol{\theta} \mid \mathcal{S}) &amp;\stackrel{\text { def }}{=} \mathbb{P}_{\mathcal{D}}\left(\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)} ; \boldsymbol{\theta}\right) \\
&amp;= \prod_{n=1}^{N} \mathbb{P}_{\mathcal{D}}\left(\mathbf{x}^{(n)} ; \boldsymbol{\theta}\right)
\end{aligned}
\end{split}\]</div>
<p>which means what is the probability of observing a sequence of <span class="math notranslate nohighlight">\(N\)</span> data points <span class="math notranslate nohighlight">\(\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)}\)</span>?</p>
<p>Think of it as flipping <span class="math notranslate nohighlight">\(N\)</span> coins, and what is the sequence of observing the permutation of, say, <span class="math notranslate nohighlight">\(HHTTTHH\)</span>?</p>
</section>
</div></section>
<section id="likelihood-in-the-context-of-supervised-learning">
<h3><a class="toc-backref" href="#id12" role="doc-backlink">Likelihood in the Context of Supervised Learning</a><a class="headerlink" href="#likelihood-in-the-context-of-supervised-learning" title="Link to this heading">#</a></h3>
<p>In supervised learning, there is often a label <span class="math notranslate nohighlight">\(y\)</span> associated with each data
point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<div class="proof remark admonition" id="rem:where-y">
<p class="admonition-title"><span class="caption-number">Remark 61 </span> (Where’s the <span class="math notranslate nohighlight">\(y\)</span>?)</p>
<section class="remark-content" id="proof-content">
<p>Some people may ask, isn’t the setting in our classification problem a supervised one with labels?
Where are the <span class="math notranslate nohighlight">\(y\)</span> in the likelihood function? Good point, the <span class="math notranslate nohighlight">\(y\)</span> is not included in our current section
for simplicity. However, the inclusion of <span class="math notranslate nohighlight">\(y\)</span> can be merely thought as denoting “an additional”
random variable in the likelihood function above.</p>
<p>For example, let’s say <span class="math notranslate nohighlight">\(y\)</span> is the target column of a classification problem on breast cancer, denoting
whether the patient has cancer or not. Then, the likelihood function can be written as:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\boldsymbol{\theta} \mid \mathcal{S}) \stackrel{\text { def }}{=} \prod_{n=1}^{N} \mathbb{P}_{\mathcal{D}}\left(\mathbf{x}^{(n)}, y^{(n)} ; \boldsymbol{\theta}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{S} = \left\{\left(\mathbf{x}^{(1)}, y^{(1)}\right), \ldots, \left(\mathbf{x}^{(n)}, y^{(n)}\right)\right\}
\]</div>
</section>
</div><div class="proof definition admonition" id="def:likelihood-iid-supervised-learning">
<p class="admonition-title"><span class="caption-number">Definition 170 </span> (Likelihood Function with IID Assumption (Supervised Learning))</p>
<section class="definition-content" id="proof-content">
<p>More concretely, for a typical supervised learning problems, the learner <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> receives a labeled sample dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>
containing <span class="math notranslate nohighlight">\(N\)</span> i.i.d. samples <span class="math notranslate nohighlight">\(\left(\mathbf{x}^{(n)}, y^{(n)}\right)\)</span> drawn from <span class="math notranslate nohighlight">\(\mathbb{P}_{\mathcal{D}}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-eq-dataset-machine-learning">
<span class="eqno">(223)<a class="headerlink" href="#equation-eq-dataset-machine-learning" title="Link to this equation">#</a></span>\[
\mathcal{S} = \left\{\left(\mathbf{x}^{(1)}, y^{(1)}\right), \left(\mathbf{x}^{(2)}, y^{(2)}\right), \ldots, \left(\mathbf{x}^{(N)}, y^{(N)}\right)\right\} \subset \mathbb{R}^{D} \quad \overset{\small{\text{i.i.d.}}}{\sim} \quad \mathbb{P}_{\mathcal{D}}\left(\mathcal{X}, \mathcal{Y} ; \boldsymbol{\beta}\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{P}_{\mathcal{D}}\)</span> is assumed to be the underlying (joint) distribution that generates the dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>.</p>
<p>So in this setting, we generally assume that the tuple <span class="math notranslate nohighlight">\(\left(\mathbf{x}^{(n)}, y^{(n)}\right)\)</span> is drawn from the joint distribution <span class="math notranslate nohighlight">\(\mathbb{P}_{\mathcal{D}}\)</span>
and not just <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> alone.</p>
<p>Note carefully that this does not say anything about the independence of <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> and <span class="math notranslate nohighlight">\(y^{(n)}\)</span>.
It only states that each pair <span class="math notranslate nohighlight">\(\left(\mathbf{x}^{(n)}, y^{(n)}\right)\)</span> is independent, leading to this:</p>
<div class="math notranslate nohighlight" id="equation-eq-likelihood-machine-learning-2">
<span class="eqno">(224)<a class="headerlink" href="#equation-eq-likelihood-machine-learning-2" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathcal{L}(\boldsymbol{\theta} \mid \mathcal{S}) &amp;\stackrel{\text { def }}{=} \mathbb{P}_{\mathcal{D}}\left(\left(\mathbf{x}^{(1)}, y^{(1)}\right), \ldots, \left(\mathbf{x}^{(n)}, y^{(n)}\right); \boldsymbol{\theta}\right) \\
&amp;= \prod_{n=1}^{N} \mathbb{P}_{\mathcal{D}}\left(\mathbf{x}^{(n)}, y^{(n)} ; \boldsymbol{\theta}\right)
\end{aligned}
\end{split}\]</div>
<p>which answers the question of what is the probability of observing a sequence of <span class="math notranslate nohighlight">\(N\)</span>
data points <span class="math notranslate nohighlight">\(\left(\mathbf{x}^{(1)}, y^{(1)}\right), \ldots, \left(\mathbf{x}^{(n)}, y^{(n)}\right)\)</span>?</p>
</section>
</div></section>
<section id="conditional-likelihood-in-the-context-of-machine-learning">
<h3><a class="toc-backref" href="#id13" role="doc-backlink">Conditional Likelihood in the Context of Machine Learning</a><a class="headerlink" href="#conditional-likelihood-in-the-context-of-machine-learning" title="Link to this heading">#</a></h3>
<p>Now in discriminative algorithms such as
<a class="reference external" href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic Regression</a>, we are
interested in the conditional likelihood function.</p>
<div class="proof definition admonition" id="def:conditional-likelihood-machine-learning">
<p class="admonition-title"><span class="caption-number">Definition 171 </span> (Conditional Likelihood Function (Machine Learning))</p>
<section class="definition-content" id="proof-content">
<p>The conditional likelihood function is defined as:</p>
<div class="math notranslate nohighlight" id="equation-eq-conditional-likelihood-machine-learning-1">
<span class="eqno">(225)<a class="headerlink" href="#equation-eq-conditional-likelihood-machine-learning-1" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\mathcal{L}(\boldsymbol{\theta} \mid \mathcal{S}) &amp;\stackrel{\text { def }}{=} \mathbb{P}_{\mathcal{D}}\left(\mathcal{Y}\mid \mathcal{X} ; \boldsymbol{\theta}\right) \\
&amp;= \mathbb{P}_{\mathcal{D}}\left(y^{(1)}, y^{(2)}, \ldots, y^{(N)} \mid \mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(N)} ; \boldsymbol{\theta}\right) \\
&amp;= \prod_{n=1}^{N} \mathbb{P}_{\mathcal{D}}\left(y^{(n)} \mid \mathbf{x}^{(n)} ; \boldsymbol{\theta}\right)
\end{aligned}
\end{split}\]</div>
<p>where we abuse notation and define <span class="math notranslate nohighlight">\(\mathcal{X} = \left\{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \ldots, \mathbf{x}^{(N)}\right\}\)</span>
and <span class="math notranslate nohighlight">\(\mathcal{Y} = \left\{y^{(1)}, y^{(2)}, \ldots, y^{(N)}\right\}\)</span>.</p>
<p>We are instead interested in the probability of observing a sequence of <span class="math notranslate nohighlight">\(N\)</span>
conditional data points <span class="math notranslate nohighlight">\(\left(y^{(1)} \mid \mathbf{x}^{(1)}, y^{(2)} \mid \mathbf{x}^{(2)}, \ldots, y^{(N)} \mid \mathbf{x}^{(N)}\right)\)</span>.</p>
</section>
</div><p>Now why does <a class="reference internal" href="#equation-eq-conditional-likelihood-machine-learning-1">(225)</a> still hold for
the conditional likelihood function to be able to factorize into a product of
conditional probabilities?</p>
<p>Did we also assume that the conditional data points
<span class="math notranslate nohighlight">\(\left(y^{(1)} \mid \mathbf{x}^{(1)}, y^{(2)} \mid \mathbf{x}^{(2)}, \ldots, y^{(N)} \mid \mathbf{x}^{(N)}\right)\)</span>
are <span class="math notranslate nohighlight">\(\textrm{i.i.d.}\)</span> as well?</p>
<p>We can prove that this equation holds via
<a class="reference external" href="https://en.wikipedia.org/wiki/Marginal_distribution"><strong>marginilization</strong></a> and
the <a class="reference external" href="https://en.wikipedia.org/wiki/Fubini%27s_theorem"><strong>Fubini’s Theorem</strong></a>.</p>
<div class="proof admonition" id="proof">
<p>Proof. The proof is as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{P}_{\mathcal{D}}\left(\mathcal{Y} \mid \mathcal{X} ; \boldsymbol{\theta}\right) &amp;= \dfrac{\mathbb{P}_{\mathcal{D}}\left(\mathcal{Y}, \mathcal{X} ; \boldsymbol{\theta}\right)}{\mathbb{P}_{\mathcal{D}}\left(\mathcal{X} ; \boldsymbol{\theta}\right)} \\
&amp;= \dfrac{\mathbb{P}_{\mathcal{D}}\left(\mathcal{Y}, \mathcal{X} ; \boldsymbol{\theta}\right)}{\int \mathbb{P}_{\mathcal{D}}\left(\mathcal{Y}, \mathcal{X} ; \boldsymbol{\theta}\right) d\mathcal{Y}}  &amp;&amp;\text{ Marginalization} \\
&amp;= \dfrac{\mathbb{P}_{\mathcal{D}}\left(\mathcal{Y}, \mathcal{X} ; \boldsymbol{\theta}\right)}{\int \mathbb{P}_{\mathcal{D}}\left(\mathcal{Y} \mid \mathcal{X} ; \boldsymbol{\theta}\right) \mathbb{P}_{\mathcal{D}}\left(\mathcal{X} ; \boldsymbol{\theta}\right) d\mathcal{Y}}  &amp;&amp;\text{ Marginalization} \\
&amp;= \dfrac{\mathbb{P}_{\mathcal{D}}\left(\mathcal{Y}, \mathcal{X} ; \boldsymbol{\theta}\right)}{\int \int \cdots \int \prod_{n=1}^{N} \mathbb{P}_{\mathcal{D}}\left(y^{(n)} \mid \mathbf{x}^{(n)} ; \boldsymbol{\theta}\right) \mathbb{P}_{\mathcal{D}}\left(\mathcal{X} ; \boldsymbol{\theta}\right) d\mathcal{Y}}  &amp;&amp;\text{ Fubini's Theorem} \\
&amp;= \dfrac{\mathbb{P}_{\mathcal{D}}\left(\mathcal{Y}, \mathcal{X} ; \boldsymbol{\theta}\right)}{\int \int \cdots \int \prod_{n=1}^{N} \mathbb{P}_{\mathcal{D}}\left(y^{(n)} \mid \mathbf{x}^{(n)} ; \boldsymbol{\theta}\right) \mathbb{P}_{\mathcal{D}}\left(\mathcal{X} ; \boldsymbol{\theta}\right) d\mathbf{x}^{(n)}}  &amp;&amp;\text{ Fubini's Theorem} \\
&amp;= \prod_{n=1}^{N} \mathbb{P}_{\mathcal{D}}\left(y^{(n)} \mid \mathbf{x}^{(n)} ; \boldsymbol{\theta}\right)
\end{aligned}
\end{split}\]</div>
<p>See <a class="reference external" href="https://stats.stackexchange.com/questions/331215/defining-conditional-likelihood">proof here</a>.</p>
</div>
<p>Therefore, the conditional likelihood function is a product of conditional
probabilities, a consequence of the
<a class="reference external" href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables"><strong>i.i.d. assumption</strong></a>
for the data points in <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>.</p>
</section>
<section id="the-log-likelihood-function">
<h3><a class="toc-backref" href="#id14" role="doc-backlink">The Log-Likelihood Function</a><a class="headerlink" href="#the-log-likelihood-function" title="Link to this heading">#</a></h3>
<p>We will later see in an example that why the log-likelihood function is useful.
For now, let’s just say that due to numerical reasons (underflow), we will use
the log-likelihood function instead of the likelihood function. The intuition is
that the likelihood defined in <a class="reference internal" href="#equation-eq-likelihood-machine-learning-1">(222)</a> is a
product of individual PDFs. If we have 1 billion samples (i.e.
<span class="math notranslate nohighlight">\(N = 1,000,000,000\)</span>), then the likelihood function will be a product of 1
billion PDFs. This is a very small number and will cause
<a class="reference external" href="https://en.wikipedia.org/wiki/Arithmetic_underflow"><strong>arithmetic underflow</strong></a>.
The log-likelihood function is a solution to this problem.</p>
<div class="proof definition admonition" id="def:log-likelihood">
<p class="admonition-title"><span class="caption-number">Definition 172 </span> (Log-Likelihood Function)</p>
<section class="definition-content" id="proof-content">
<p>Given a dataset <span class="math notranslate nohighlight">\(\mathcal{S} = \left\{\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)}\right\}\)</span> where each <span class="math notranslate nohighlight">\(\mathbf{x}^{(n)}\)</span> is a vector of <span class="math notranslate nohighlight">\(D\)</span>-dimensional drawn <span class="math notranslate nohighlight">\(\textrm{i.i.d.}\)</span> from the same underlying distribution
<span class="math notranslate nohighlight">\(\mathbb{P}_{\mathcal{D}}\left(\mathcal{X} ; \boldsymbol{\theta}\right)\)</span>, the <strong>log-likelihood function</strong> is defined as:</p>
<div class="math notranslate nohighlight" id="equation-e-log-likelihood-machine-learning">
<span class="eqno">(226)<a class="headerlink" href="#equation-e-log-likelihood-machine-learning" title="Link to this equation">#</a></span>\[
\log \mathcal{L}(\boldsymbol{\theta} \mid \mathcal{S}) \stackrel{\text { def }}{=} \sum_{n=1}^{N} \log \mathbb{P}_{\mathcal{D}}\left(\mathbf{x}^{(n)} ; \boldsymbol{\theta}\right)
\]</div>
</section>
</div><p>One will soon see that <strong>maximization</strong> of the log-likelihood function is
equivalent to <strong>maximization</strong> of the likelihood function. They give the same
result.</p>
<p>Let’s walk through an example:</p>
<div class="proof example admonition" id="ex:log-likelihood-bernoulli">
<p class="admonition-title"><span class="caption-number">Example 41 </span> (Log-Likelihood of Bernoulli Distribution)</p>
<section class="example-content" id="proof-content">
<p>The log-likelihood of a sequence of <span class="math notranslate nohighlight">\(\textrm{i.i.d.}\)</span> Bernoulli <em>univariate</em> random variables
<span class="math notranslate nohighlight">\(x^{(1)}, \ldots, x^{(n)}\)</span> with parameter <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>If <span class="math notranslate nohighlight">\(x^{(1)}, \ldots, x^{(n)}\)</span> are i.i.d. Bernoulli random variables, we have</p>
<div class="math notranslate nohighlight">
\[
f_{\mathbf{X}}(\mathbf{x} ; \theta)=\prod_{n=1}^{N}\left\{\theta^{x^{(n)}}(1-\theta)^{1-x^{(n)}}\right\} .
\]</div>
<p>Taking the log on both sides of the equation yields the log-likelihood function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\log \mathcal{L}(\theta \mid \mathbf{x}) &amp; =\log \left\{\prod_{n=1}^{N}\left\{\theta^{x^{(n)}}(1-\theta)^{1-x^{(n)}}\right\}\right\} \\
&amp; =\sum_{n=1}^{N} \log \left\{\theta^{x^{(n)}}(1-\theta)^{1-x^{(n)}}\right\} \\
&amp; =\sum_{n=1}^{N} x^{(n)} \log \theta+\left(1-x^{(n)}\right) \log (1-\theta) \\
&amp; =\left(\sum_{n=1}^{N} x^{(n)}\right) \cdot \log \theta+\left(N-\sum_{n=1}^{N} x^{(n)}\right) \cdot \log (1-\theta)
\end{aligned}
\end{split}\]</div>
</section>
</div><p>Now there will be more examples of higher-dimensional log-likelihood functions
in the next section. Furthermore, the section Maximum Likelihood Estimation for
Priors in <a class="reference internal" href="../../../influential/naive_bayes/02_concept.html"><span class="std std-doc">Naive Bayes</span></a>,details
one example of log-likelihood function for a higher-dimensional multivariate
Bernoulli (Catagorical) distribution.</p>
</section>
<section id="visualizing-the-likelihood-function">
<h3><a class="toc-backref" href="#id15" role="doc-backlink">Visualizing the Likelihood Function</a><a class="headerlink" href="#visualizing-the-likelihood-function" title="Link to this heading">#</a></h3>
<p>This section mainly details how the likelihood function, despite being a
function of <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, also depends on the underlying dataset
<span class="math notranslate nohighlight">\(\mathcal{S}\)</span>. The presence of both should be kept in mind when we talk about
the likelihood function.</p>
<p>For a more detailed analysis, see page 471-472 of Professor Stanley Chan’s book
“Introduction to Probability for Data Science” (see references section).</p>
</section>
</section>
<section id="maximum-likelihood-estimation">
<h2><a class="toc-backref" href="#id16" role="doc-backlink">Maximum Likelihood Estimation</a><a class="headerlink" href="#maximum-likelihood-estimation" title="Link to this heading">#</a></h2>
<p>After rigorously defining the likelihood function, we can now talk about the
term <strong>maximum</strong> in maximum likelihood estimation.</p>
<p>The action of maximization is in itself under
<a class="reference external" href="https://en.wikipedia.org/wiki/Mathematical_optimization">optimization theory</a>,
a branch in mathematics. Consequently, the maximum likelihood estimation problem
is an optimization problem that seeks to find the parameter
<span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> that maximizes the likelihood function.</p>
<div class="proof definition admonition" id="def:maximum-likelihood-estimation">
<p class="admonition-title"><span class="caption-number">Definition 173 </span> (Maximum Likelihood Estimation)</p>
<section class="definition-content" id="proof-content">
<p>Given a dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> consisting of <span class="math notranslate nohighlight">\(N\)</span> samples defined as:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{S} = \left\{\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)}\right\},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> is <span class="math notranslate nohighlight">\(\textrm{i.i.d.}\)</span> generated from the distribution <span class="math notranslate nohighlight">\(\mathbb{P}_{\mathcal{D}}\left(\mathcal{X} ; \boldsymbol{\theta}\right)\)</span>, parametrized by <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, where the parameter <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> can be a vector of parameters defined as:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta} = \left\{\theta_{1}, \ldots, \theta_{k}\right\}.
\]</div>
<p>We define the likelihood function to be:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\boldsymbol{\theta}) = \mathcal{L}(\boldsymbol{\theta} \mid \mathcal{S}) \stackrel{\text { def }}{=} \mathbb{P}_{\mathcal{D}}\left(\mathcal{X} ; \boldsymbol{\theta}\right),
\]</div>
<p>then the maximum-likelihood estimate of the parameter <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is a parameter that maximizes the likelihood function:</p>
<div class="math notranslate nohighlight" id="equation-eq-maximum-likelihood-estimation">
<span class="eqno">(227)<a class="headerlink" href="#equation-eq-maximum-likelihood-estimation" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\widehat{\boldsymbol{\theta}} &amp;\stackrel{\text { def }}{=} \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\operatorname{argmax}} \mathcal{L}\left(\boldsymbol{\theta} \mid \mathcal{S}\right) \\
&amp;\stackrel{\text{ def }}{=} \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\operatorname{argmax}}\mathbb{P}_{\mathcal{D}}\left(\mathcal{X} ; \boldsymbol{\theta}\right)
\end{aligned}
\end{split}\]</div>
</section>
</div><div class="proof remark admonition" id="rmk:maximum-likelihood-estimation">
<p class="admonition-title"><span class="caption-number">Remark 62 </span> (Maximum Likelihood Estimation for <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> with Label <span class="math notranslate nohighlight">\(y\)</span>)</p>
<section class="remark-content" id="proof-content">
<p>To be more verbose, let’s also define the maximum likelihood estimate of the parameter <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> for a dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> with label <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>First, we redefine <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> to be:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{S} = \left\{\left(\mathbf{x}^{(1)}, y^{(1)}\right), \ldots, \left(\mathbf{x}^{(n)}, y^{(n)}\right)\right\}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> is generated from the distribution <span class="math notranslate nohighlight">\(\mathbb{P}_{\mathcal{D}}\left(\mathcal{X}, \mathcal{Y} ; \boldsymbol{\theta}\right)\)</span>. The likelihood function is then defined as:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\boldsymbol{\theta}) = \mathcal{L}(\boldsymbol{\theta} \mid \mathcal{S}) \stackrel{\text { def }}{=} \mathbb{P}_{\mathcal{D}}\left(\mathcal{X}, \mathcal{Y}; \boldsymbol{\theta}\right),
\]</div>
<p>then the maximum-likelihood estimate of the parameter <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is a parameter that maximizes the likelihood function:</p>
<div class="math notranslate nohighlight" id="equation-eq-maximum-likelihood-estimation-for-label-y">
<span class="eqno">(228)<a class="headerlink" href="#equation-eq-maximum-likelihood-estimation-for-label-y" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\widehat{\boldsymbol{\theta}} &amp;\stackrel{\text { def }}{=} \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\operatorname{argmax}} \mathcal{L}\left(\boldsymbol{\theta} \mid \mathcal{S}, y\right) \\
&amp;\stackrel{\text{ def }}{=} \underset{\boldsymbol{\theta} \in \boldsymbol{\Theta}}{\operatorname{argmax}} \mathbb{P}_{\mathcal{D}}\left(\mathcal{X}, \mathcal{Y}; \boldsymbol{\theta}\right)
\end{aligned}
\end{split}\]</div>
</section>
</div></section>
<section id="coin-toss-example">
<h2><a class="toc-backref" href="#id17" role="doc-backlink">Coin Toss Example</a><a class="headerlink" href="#coin-toss-example" title="Link to this heading">#</a></h2>
<figure class="align-default" id="coin-generator">
<a class="reference internal image-reference" href="../../../_images/coin_generator.jpg"><img alt="../../../_images/coin_generator.jpg" src="../../../_images/coin_generator.jpg" style="height: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 33 </span><span class="caption-text">A coin generator that generates a sequence of coin flips.</span><a class="headerlink" href="#coin-generator" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Let’s see how this works in a concrete example. Suppose we have a coin generator
as shown in <a class="reference internal" href="#coin-generator"><span class="std std-numref">Fig. 33</span></a>. We know for a fact that:</p>
<ol class="arabic simple">
<li><p>The coin generator generates coins <strong>independently</strong>.</p></li>
<li><p>The coin generator generates coins with an <strong>identical</strong> probability <span class="math notranslate nohighlight">\(\theta\)</span>
(denoted <span class="math notranslate nohighlight">\(p\)</span> in the diagram) of being heads.</p></li>
</ol>
<p>Consequently, the probability that a coin generated by the coin generator is
heads is <span class="math notranslate nohighlight">\(\theta\)</span>, and the probability that a coin generated by the coin
generator is tails is <span class="math notranslate nohighlight">\(1-\theta\)</span>.</p>
<p>Let’s say we press the button on the coin generator <span class="math notranslate nohighlight">\(N\)</span> times, and we observe
<span class="math notranslate nohighlight">\(N\)</span> coin flips. Let’s denote the observed coin flips as
<span class="math notranslate nohighlight">\(X^{(1)}, \ldots, X^{(N)}\)</span>, where the realizations of each <span class="math notranslate nohighlight">\(X^{(n)}\)</span> are
<span class="math notranslate nohighlight">\(x^{(n)} = 1\)</span> if the <span class="math notranslate nohighlight">\(n\)</span>th flip is heads and <span class="math notranslate nohighlight">\(x^{(n)} = 0\)</span> if the <span class="math notranslate nohighlight">\(n\)</span>th flip is
tails. This sequence of observations can be further collated into our familiar
dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{S} = \left\{x^{(1)}, \ldots, x^{(N)}\right\}.
\]</div>
<p>Then the probability of observing this dataset <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> is equivalent to
asking what is the probability of observing this sequence of random variables
<span class="math notranslate nohighlight">\(X^{(1)}, \ldots, X^{(N)}\)</span> and is given by:</p>
<div class="math notranslate nohighlight" id="equation-eq-coin-toss-likelihood-1">
<span class="eqno">(229)<a class="headerlink" href="#equation-eq-coin-toss-likelihood-1" title="Link to this equation">#</a></span>\[\mathbb{P}(X ; \theta) = \prod_{n=1}^N \theta^{x^{(n)}}(1-\theta)^{1-x^{(n)}}.\]</div>
<p>Our goal is to find the value of <span class="math notranslate nohighlight">\(\theta\)</span>. How? Maximum likelihood estimation!
We want to find the value of <span class="math notranslate nohighlight">\(\theta\)</span> that maximizes the <strong>joint probability</strong>
of observing the sequence of coin flips. This is equivalent to maximizing the
<strong>likelihood</strong> of observing the sequence of coin flips. The likelihood function
is given by the exact same equation as the joint probability, except that we do
a notational change:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\theta \mid \mathcal{S}) = \prod_{n=1}^N \theta^{x^{(n)}}(1-\theta)^{1-x^{(n)}}.
\]</div>
<p>Notice that this equation is none other than the product of <span class="math notranslate nohighlight">\(N\)</span> Bernoulli random
variables, each with parameter <span class="math notranslate nohighlight">\(\theta\)</span>. This is not surprising since coin toss
is usually modelled as a Bernoulli random variable.</p>
<p>If we flip <span class="math notranslate nohighlight">\(13\)</span> coins and get the sequence “HHHTHTTHHHHHT”, then the probability
of observing this sequence is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbb{P}(X ; \theta) &amp;= \theta^{x^{(1)}}(1-\theta)^{1-x^{(1)}} \times \theta^{x^{(2)}}(1-\theta)^{1-x^{(2)}} \times \theta^{x^{(3)}}(1-\theta)^{1-x^{(3)}} \times \theta^{x^{(4)}}(1-\theta)^{1-x^{(4)}} \times \theta^{x^{(5)}}(1-\theta)^{1-x^{(5)}} \times \theta^{x^{(6)}}(1-\theta)^{1-x^{(6)}} \times \theta^{x^{(7)}}(1-\theta)^{1-x^{(7)}} \times \theta^{x^{(8)}}(1-\theta)^{1-x^{(8)}} \times \theta^{x^{(9)}}(1-\theta)^{1-x^{(9)}} \times \theta^{x^{(10)}}(1-\theta)^{1-x^{(10)}} \times \theta^{x^{(11)}}(1-\theta)^{1-x^{(11)}} \times \theta^{x^{(12)}}(1-\theta)^{1-x^{(12)}} \times \theta^{x^{(13)}}(1-\theta)^{1-x^{(13)}} \\
&amp;= \theta^{9}(1-\theta)^{4}.
\end{aligned}
\end{split}\]</div>
<p>One nice thing about this example will be that we know the answer going in.
Indeed, if we said verbally, “I flipped 13 coins, and 9 came up heads, what is
our best guess for the probability that the coin comes us heads?, “ everyone
would correctly guess <span class="math notranslate nohighlight">\(9/13\)</span>. What this maximum likelihood method will give us
is a way to get that number from first principals in a way that will generalize
to vastly more complex situations <span id="id2">[<a class="reference internal" href="../../../bibliography.html#id5" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into Deep Learning. Cambridge University Press, 2023. URL: https://D2L.ai.">Zhang <em>et al.</em>, 2023</a>]</span>.</p>
<p>For our example, the plot of <span class="math notranslate nohighlight">\(P(X \mid \theta)\)</span> is as follows:</p>
<p>We know that a coin toss</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="kn">import</span> <span class="nn">sys</span>
<span class="linenos"> 2</span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="linenos"> 3</span><span class="n">parent_dir</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">Path</span><span class="p">()</span><span class="o">.</span><span class="n">resolve</span><span class="p">()</span><span class="o">.</span><span class="n">parents</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
<span class="linenos"> 4</span><span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">parent_dir</span><span class="p">)</span>
<span class="linenos"> 5</span>
<span class="linenos"> 6</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="linenos"> 7</span>
<span class="linenos"> 8</span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="kn">import</span> <span class="nn">sys</span>
<span class="linenos">11</span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="linenos">12</span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">Optional</span>
<span class="linenos">13</span><span class="kn">import</span> <span class="nn">rich</span>
<span class="linenos">14</span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span><span class="p">,</span> <span class="n">display</span>
<span class="linenos">15</span>
<span class="linenos">16</span><span class="kn">import</span> <span class="nn">math</span>
<span class="linenos">17</span>
<span class="linenos">18</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="linenos">19</span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
<span class="linenos">20</span>
<span class="linenos">21</span><span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;svg&#39;
<span class="linenos">22</span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="linenos">23</span>
<span class="linenos">24</span><span class="k">def</span> <span class="nf">find_root_dir</span><span class="p">(</span><span class="n">current_path</span><span class="p">:</span> <span class="n">Path</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">marker</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;.git&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Path</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="linenos">25</span><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="linenos">26</span><span class="sd">    Find the root directory by searching for a directory or file that serves as a</span>
<span class="linenos">27</span><span class="sd">    marker.</span>
<span class="linenos">28</span>
<span class="linenos">29</span><span class="sd">    Parameters</span>
<span class="linenos">30</span><span class="sd">    ----------</span>
<span class="linenos">31</span><span class="sd">    current_path : Path | None</span>
<span class="linenos">32</span><span class="sd">        The starting path to search from. If None, the current working directory</span>
<span class="linenos">33</span><span class="sd">        `Path.cwd()` is used.</span>
<span class="linenos">34</span><span class="sd">    marker : str</span>
<span class="linenos">35</span><span class="sd">        The name of the file or directory that signifies the root.</span>
<span class="linenos">36</span>
<span class="linenos">37</span><span class="sd">    Returns</span>
<span class="linenos">38</span><span class="sd">    -------</span>
<span class="linenos">39</span><span class="sd">    Path | None</span>
<span class="linenos">40</span><span class="sd">        The path to the root directory. Returns None if the marker is not found.</span>
<span class="linenos">41</span><span class="sd">    &quot;&quot;&quot;</span>
<span class="linenos">42</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">current_path</span><span class="p">:</span>
<span class="linenos">43</span>        <span class="n">current_path</span> <span class="o">=</span> <span class="n">Path</span><span class="o">.</span><span class="n">cwd</span><span class="p">()</span>
<span class="linenos">44</span>    <span class="n">current_path</span> <span class="o">=</span> <span class="n">current_path</span><span class="o">.</span><span class="n">resolve</span><span class="p">()</span>
<span class="linenos">45</span>    <span class="k">for</span> <span class="n">parent</span> <span class="ow">in</span> <span class="p">[</span><span class="n">current_path</span><span class="p">,</span> <span class="o">*</span><span class="n">current_path</span><span class="o">.</span><span class="n">parents</span><span class="p">]:</span>
<span class="linenos">46</span>        <span class="k">if</span> <span class="p">(</span><span class="n">parent</span> <span class="o">/</span> <span class="n">marker</span><span class="p">)</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
<span class="linenos">47</span>            <span class="k">return</span> <span class="n">parent</span>
<span class="linenos">48</span>    <span class="k">return</span> <span class="kc">None</span>
<span class="linenos">49</span>
<span class="linenos">50</span><span class="n">current_file_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;__file__&quot;</span><span class="p">)</span>
<span class="linenos">51</span><span class="n">root_dir</span>          <span class="o">=</span> <span class="n">find_root_dir</span><span class="p">(</span><span class="n">current_file_path</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;omnivault&#39;</span><span class="p">)</span>
<span class="linenos">52</span>
<span class="linenos">53</span><span class="k">if</span> <span class="n">root_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="linenos">54</span>    <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">root_dir</span><span class="p">))</span>
<span class="linenos">55</span>    <span class="kn">from</span> <span class="nn">omnivault.utils.visualization.style</span> <span class="kn">import</span> <span class="n">use_svg_display</span>
<span class="linenos">56</span>    <span class="kn">from</span> <span class="nn">omnivault.utils.reproducibility.seed</span> <span class="kn">import</span> <span class="n">seed_all</span>
<span class="linenos">57</span><span class="k">else</span><span class="p">:</span>
<span class="linenos">58</span>    <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="s2">&quot;Root directory not found.&quot;</span><span class="p">)</span>
<span class="linenos">59</span>
<span class="linenos">60</span><span class="n">use_svg_display</span><span class="p">()</span>
<span class="linenos">61</span>
<span class="linenos">62</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="linenos">63</span>
<span class="linenos">64</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">)</span>
<span class="linenos">65</span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">theta</span><span class="o">**</span><span class="mi">9</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">theta</span><span class="p">)</span><span class="o">**</span><span class="mi">4</span>
<span class="linenos">66</span>
<span class="linenos">67</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="linenos">68</span>
<span class="linenos">69</span><span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">)</span>
<span class="linenos">70</span><span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$&#39;</span><span class="p">)</span>
<span class="linenos">71</span><span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mathbb</span><span class="si">{P}</span><span class="s1">(X ; \theta)$&#39;</span><span class="p">)</span>
<span class="linenos">72</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">();</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../../_images/a9d1a99209f8a2f884740e4c5476842cfbb078fad4898bbef93b414f98d9697c.svg" src="../../../_images/a9d1a99209f8a2f884740e4c5476842cfbb078fad4898bbef93b414f98d9697c.svg" />
</div>
</div>
<p>This has its maximum value somewhere near our expected <span class="math notranslate nohighlight">\(9/13 \approx 0.7\ldots\)</span>.
To see if it is exactly there, we can turn to calculus. Notice that at the
maximum, the gradient of the function is flat. Thus, we could find the maximum
likelihood estimate by finding the values of <span class="math notranslate nohighlight">\(\theta\)</span> where the derivative is
zero, and finding the one that gives the highest probability. We compute:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
0 &amp; = \frac{d}{d\theta} \mathbb{P}(X ; \theta) \\
&amp; = \frac{d}{d\theta} \theta^9(1-\theta)^4 \\
&amp; = 9\theta^8(1-\theta)^4 - 4\theta^9(1-\theta)^3 \\
&amp; = \theta^8(1-\theta)^3(9-13\theta).
\end{aligned}
\end{split}\]</div>
<p>This has three solutions: <span class="math notranslate nohighlight">\(0\)</span>, <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(9/13\)</span>. The first two are clearly minima,
not maxima as they assign probability <span class="math notranslate nohighlight">\(0\)</span> to our sequence. The final value does
<em>not</em> assign zero probability to our sequence, and thus must be the maximum
likelihood estimate <span class="math notranslate nohighlight">\(\hat \theta = 9/13\)</span> <span id="id3">[<a class="reference internal" href="../../../bibliography.html#id5" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into Deep Learning. Cambridge University Press, 2023. URL: https://D2L.ai.">Zhang <em>et al.</em>, 2023</a>]</span>.</p>
<p>We can justify this intuition by deriving the maximum likelihood estimate for
<span class="math notranslate nohighlight">\(\theta\)</span> if we assume that the coin generator follows a Bernoulli distribution.
The more generic case of the maximum likelihood estimate for <span class="math notranslate nohighlight">\(\theta\)</span> is given
by the following.</p>
<div class="proof definition admonition" id="def:maximum-likelihood-estimation-for-bernoulli-distribution">
<p class="admonition-title"><span class="caption-number">Definition 174 </span> (Maximum Likelihood Estimation for Bernoulli Distribution)</p>
<section class="definition-content" id="proof-content">
<p>The Maximum Likelihood estimate for a set of <span class="math notranslate nohighlight">\(\textrm{i.i.d.}\)</span> Bernoulli random variables <span class="math notranslate nohighlight">\(\left\{x^{(1)}, \ldots, x^{(n)}\right\}\)</span> with <span class="math notranslate nohighlight">\(x^{(n)} \sim \operatorname{Bernoulli}(\theta)\)</span> for <span class="math notranslate nohighlight">\(n=1, \ldots, N\)</span> is derived as follows.</p>
<p>We know that the log-likelihood function of a set of i.i.d. Bernoulli random variables is given by</p>
<div class="math notranslate nohighlight">
\[
\log \mathcal{L}(\theta \mid \mathbf{x})=\left(\sum_{n=1}^{N} x^{(n)}\right) \cdot \log \theta+\left(N-\sum_{n=1}^{N} x^{(n)}\right) \cdot \log (1-\theta)
\]</div>
<p>Thus, to find the ML estimate, we need to solve the optimization problem</p>
<div class="math notranslate nohighlight">
\[
\widehat{\theta}=\underset{\theta \in \Theta}{\operatorname{argmax}}\left\{\left(\sum_{n=1}^{N} x^{(n)}\right) \cdot \log \theta+\left(N-\sum_{n=1}^{N} x^{(n)}\right) \cdot \log (1-\theta)\right\} .
\]</div>
<p>Taking the derivative with respect to <span class="math notranslate nohighlight">\(\theta\)</span> and setting it to zero, we obtain</p>
<div class="math notranslate nohighlight">
\[
\frac{d}{d \theta}\left\{\left(\sum_{n=1}^{N} x^{(n)}\right) \cdot \log \theta+\left(N-\sum_{n=1}^{N} x^{(n)}\right) \cdot \log (1-\theta)\right\}=0 .
\]</div>
<p>This gives us</p>
<div class="math notranslate nohighlight">
\[
\frac{\left(\sum_{n=1}^{N} x^{(n)}\right)}{\theta}-\frac{N-\sum_{n=1}^{N} x^{(n)}}{1-\theta}=0
\]</div>
<p>Rearranging the terms yields</p>
<div class="math notranslate nohighlight">
\[
\widehat{\theta}=\frac{1}{N} \sum_{n=1}^{N} x^{(n)}
\]</div>
</section>
</div><p>Indeed, since we have <span class="math notranslate nohighlight">\(9\)</span> heads and <span class="math notranslate nohighlight">\(4\)</span> tails, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\widehat{\theta} &amp;= \frac{1}{N} \sum_{n=1}^{N} x^{(n)} \\
&amp;= \frac{1}{13} \sum_{n=1}^{13} x^{(n)} \\
&amp;= \frac{1}{13} \cdot 9 \\
&amp;= \frac{9}{13}.
\end{aligned}
\end{split}\]</div>
<p>since there are <span class="math notranslate nohighlight">\(9\)</span> heads and <span class="math notranslate nohighlight">\(4\)</span> tails, resulting in a sum of <span class="math notranslate nohighlight">\(9\)</span> when you sum
up the <span class="math notranslate nohighlight">\(x^{(n)}\)</span>’s. Thus, the maximum likelihood estimate for <span class="math notranslate nohighlight">\(\theta\)</span> is
<span class="math notranslate nohighlight">\(\frac{9}{13}\)</span>.</p>
</section>
<section id="visualizing-likelihood-and-maximum-likelihood-estimation-as-n-increases">
<h2><a class="toc-backref" href="#id18" role="doc-backlink">Visualizing Likelihood and Maximum Likelihood Estimation as <span class="math notranslate nohighlight">\(N\)</span> Increases</a><a class="headerlink" href="#visualizing-likelihood-and-maximum-likelihood-estimation-as-n-increases" title="Link to this heading">#</a></h2>
<p>Read section 8.1.1 aznd 8.1.2 of Introduction to Probability for Data Science
written by Stanley H. Chan <span id="id4">[<a class="reference internal" href="../../../bibliography.html#id15" title="Stanley H. Chan. Introduction to probability for Data Science. Michigan Publishing, 2021.">Chan, 2021</a>]</span> for more details.</p>
</section>
<section id="numerical-optimization-and-the-negative-log-likelihood">
<h2><a class="toc-backref" href="#id19" role="doc-backlink">Numerical Optimization and the Negative Log-Likelihood</a><a class="headerlink" href="#numerical-optimization-and-the-negative-log-likelihood" title="Link to this heading">#</a></h2>
<p><strong><em>The following section is adapted from section 22.7 from Dive Into Deep
Learning, <span id="id5">[<a class="reference internal" href="../../../bibliography.html#id5" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into Deep Learning. Cambridge University Press, 2023. URL: https://D2L.ai.">Zhang <em>et al.</em>, 2023</a>]</span>.</em></strong></p>
<p>The example on coin toss is nice, but what if we have billions of parameters and
data examples?</p>
<section id="numerical-underflow">
<h3><a class="toc-backref" href="#id20" role="doc-backlink">Numerical Underflow</a><a class="headerlink" href="#numerical-underflow" title="Link to this heading">#</a></h3>
<p>First, notice that if we make the assumption that all the data examples are
independent, we can no longer practically consider the likelihood itself as it
is a product of many probabilities. Indeed, each probability is in <span class="math notranslate nohighlight">\([0,1]\)</span>, say
typically of value about <span class="math notranslate nohighlight">\(1/2\)</span>, and the product of <span class="math notranslate nohighlight">\((1/2)^{1000000000}\)</span> is far
below machine precision. We cannot work with that directly.</p>
<p>Let’s check the smallest representable positive number greater than zero for the
<code class="docutils literal notranslate"><span class="pre">float32</span></code> data type:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.1920929e-07
</pre></div>
</div>
</div>
</div>
<p>Let’s cook up a simple example to illustrate this. We will generate a random
sequence of <span class="math notranslate nohighlight">\(1000000000\)</span> coin tosses, and compute the likelihood of the sequence
given that the coin is fair. We will then compute the log-likelihood of the
sequence given that the coin is fair.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="linenos">2</span>
<span class="linenos">3</span><span class="n">seed_all</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="linenos">4</span>
<span class="linenos">5</span><span class="n">N</span> <span class="o">=</span> <span class="mi">1000000000</span>
<span class="linenos">6</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="linenos">7</span><span class="n">theta</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="linenos">8</span><span class="n">likelihood</span> <span class="o">=</span> <span class="p">(</span><span class="n">theta</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="o">*</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X</span><span class="p">)))</span>
<span class="linenos">9</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Likelihood: </span><span class="si">{</span><span class="n">likelihood</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/runner/work/omniverse/omniverse/omnivault/utils/reproducibility/seed.py:120: UserWarning: Deterministic mode is activated. This will negatively impact performance and may cause increase in CUDA memory footprint.
  configure_deterministic_mode()
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Likelihood: 0.0
</pre></div>
</div>
</div>
</div>
<p>So the likelihood is <span class="math notranslate nohighlight">\(0\)</span> because the <span class="math notranslate nohighlight">\(1000000000\)</span> when multiplied is less than
the smallest representable positive number greater than zero for the <code class="docutils literal notranslate"><span class="pre">float32</span></code>
data type.</p>
<p>However, recall that the logarithm turns products to sums, in which case</p>
<div class="math notranslate nohighlight">
\[
\log\left(\left(1/2\right)^{1000000000}\right) = 1000000000\cdot\log(1/2) \approx -301029995.6\ldots
\]</div>
<p>This number fits perfectly within even a single precision <span class="math notranslate nohighlight">\(32\)</span>-bit float. Thus,
we should consider the <em>log-likelihood</em>, which is</p>
<div class="math notranslate nohighlight">
\[
\log(\mathbb{P}(X ; \boldsymbol{\theta})).
\]</div>
<p>Since the function <span class="math notranslate nohighlight">\(x \mapsto \log(x)\)</span> is increasing, maximizing the likelihood
is the same thing as maximizing the log-likelihood.</p>
<p>We often work with loss functions, where we wish to minimize the loss. We may
turn maximum likelihood into the minimization of a loss by taking
<span class="math notranslate nohighlight">\(-\log(\mathbb{P}(X ; \boldsymbol{\theta}))\)</span>, which is the <em>negative
log-likelihood</em>.</p>
<p>To illustrate this, consider the coin flipping problem from before, and pretend
that we do not know the closed form solution. We may compute that</p>
<div class="math notranslate nohighlight">
\[
-\log(\mathbb{P}(X ; \boldsymbol{\theta})) = -\log\left(\theta^{x^{(n)}}(1-\theta)^{1-x^{(n)}}\right) = -\left(n_H\log(\theta) + n_T\log(1-\theta)\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(n_H\)</span> is the number of heads and <span class="math notranslate nohighlight">\(n_T\)</span> is the number of tails. This form
is just like in
<a class="reference internal" href="#def:maximum-likelihood-estimation-for-bernoulli-distribution">Definition 174</a>.</p>
<p>This can be written into code, and freely optimized even for billions of coin
flips.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="c1"># Set up our data</span>
<span class="linenos"> 4</span><span class="n">n_H</span> <span class="o">=</span> <span class="mi">8675309</span>
<span class="linenos"> 5</span><span class="n">n_T</span> <span class="o">=</span> <span class="mi">256245</span>
<span class="linenos"> 6</span>
<span class="linenos"> 7</span><span class="c1"># Initialize our parameters</span>
<span class="linenos"> 8</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="c1"># Perform gradient descent</span>
<span class="linenos">11</span><span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-9</span>
<span class="linenos">12</span><span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
<span class="linenos">13</span>    <span class="c1"># Compute the gradient of the loss function with respect to theta</span>
<span class="linenos">14</span>    <span class="n">grad_loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">n_H</span> <span class="o">/</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">n_T</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">))</span>
<span class="linenos">15</span>
<span class="linenos">16</span>    <span class="c1"># Update theta using the gradient</span>
<span class="linenos">17</span>    <span class="n">theta</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad_loss</span>
<span class="linenos">18</span>
<span class="linenos">19</span><span class="c1"># Check output</span>
<span class="linenos">20</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimated theta: </span><span class="si">{</span><span class="n">theta</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="linenos">21</span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Empirical theta: </span><span class="si">{</span><span class="n">n_H</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="n">n_H</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">n_T</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Estimated theta: 0.9713101437659449
Empirical theta: 0.9713101437890875
</pre></div>
</div>
</div>
</div>
</section>
<section id="mathematical-convenience">
<h3><a class="toc-backref" href="#id21" role="doc-backlink">Mathematical Convenience</a><a class="headerlink" href="#mathematical-convenience" title="Link to this heading">#</a></h3>
<p>Numerical convenience is not the only reason why people like to use negative
log-likelihoods. There are several other reasons why it is preferable.</p>
<p>The second reason we consider the log-likelihood is the simplified application
of calculus rules. As discussed above, due to independence assumptions, most
probabilities we encounter in machine learning are products of individual
probabilities.</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(X ; \boldsymbol{\theta}) = p(x_1\mid\boldsymbol{\theta})\cdot p(x_2\mid\boldsymbol{\theta})\cdots p(x_n\mid\boldsymbol{\theta}).
\]</div>
<p>This means that if we directly apply the product rule to compute a derivative we
get</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial}{\partial \boldsymbol{\theta}} \mathbb{P}(X ; \boldsymbol{\theta}) &amp; = \left(\frac{\partial}{\partial \boldsymbol{\theta}}\mathbb{P}(x_1\mid\boldsymbol{\theta})\right)\cdot \mathbb{P}(x_2\mid\boldsymbol{\theta})\cdots \mathbb{P}(x_n\mid\boldsymbol{\theta}) \\
&amp; \quad + \mathbb{P}(x_1\mid\boldsymbol{\theta})\cdot \left(\frac{\partial}{\partial \boldsymbol{\theta}}\mathbb{P}(x_2\mid\boldsymbol{\theta})\right)\cdots \mathbb{P}(x_n\mid\boldsymbol{\theta}) \\
&amp; \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \vdots \\
&amp; \quad + \mathbb{P}(x_1\mid\boldsymbol{\theta})\cdot \mathbb{P}(x_2\mid\boldsymbol{\theta}) \cdots \left(\frac{\partial}{\partial \boldsymbol{\theta}}\mathbb{P}(x_n\mid\boldsymbol{\theta})\right).
\end{aligned}
\end{split}\]</div>
<p>This requires <span class="math notranslate nohighlight">\(n(n-1)\)</span> multiplications, along with <span class="math notranslate nohighlight">\((n-1)\)</span> additions, so it is
proportional to quadratic time in the inputs! Sufficient cleverness in grouping
terms will reduce this to linear time, but it requires some thought. For the
negative log-likelihood we have instead</p>
<div class="math notranslate nohighlight">
\[
-\log\left(\mathbb{P}(X ; \boldsymbol{\theta})\right) = -\log(\mathbb{P}(x_1\mid\boldsymbol{\theta})) - \log(\mathbb{P}(x_2\mid\boldsymbol{\theta})) \cdots - \log(\mathbb{P}(x_n\mid\boldsymbol{\theta})),
\]</div>
<p>which then gives</p>
<div class="math notranslate nohighlight">
\[
- \frac{\partial}{\partial \boldsymbol{\theta}} \log\left(\mathbb{P}(X ; \boldsymbol{\theta})\right) = \frac{1}{\mathbb{P}(x_1\mid\boldsymbol{\theta})}\left(\frac{\partial}{\partial \boldsymbol{\theta}}\mathbb{P}(x_1\mid\boldsymbol{\theta})\right) + \cdots + \frac{1}{\mathbb{P}(x_n\mid\boldsymbol{\theta})}\left(\frac{\partial}{\partial \boldsymbol{\theta}}\mathbb{P}(x_n\mid\boldsymbol{\theta})\right).
\]</div>
<p>This requires only <span class="math notranslate nohighlight">\(n\)</span> divides and <span class="math notranslate nohighlight">\(n-1\)</span> sums, and thus is linear time in the
inputs.</p>
</section>
<section id="information-theory">
<h3><a class="toc-backref" href="#id22" role="doc-backlink">Information Theory</a><a class="headerlink" href="#information-theory" title="Link to this heading">#</a></h3>
<p>The third and final reason to consider the negative log-likelihood is the
relationship to information theory. We will discuss this separately in the
section on information theory. This is a rigorous mathematical theory which
gives a way to measure the degree of information or randomness in a random
variable. The key object of study in that field is the entropy which is</p>
<div class="math notranslate nohighlight">
\[
H(p) = -\sum_{i} p_i \log_2(p_i),
\]</div>
<p>which measures the randomness of a source. Notice that this is nothing more than
the average <span class="math notranslate nohighlight">\(-\log\)</span> probability, and thus if we take our negative log-likelihood
and divide by the number of data examples, we get a relative of entropy known as
cross-entropy. This theoretical interpretation alone would be sufficiently
compelling to motivate reporting the average negative log-likelihood over the
dataset as a way of measuring model performance.</p>
</section>
</section>
<section id="maximum-likelihood-for-continuous-variables">
<h2><a class="toc-backref" href="#id23" role="doc-backlink">Maximum Likelihood for Continuous Variables</a><a class="headerlink" href="#maximum-likelihood-for-continuous-variables" title="Link to this heading">#</a></h2>
<p><strong><em>The following section is adapted from section 22.7 from Dive Into Deep
Learning, <span id="id6">[<a class="reference internal" href="../../../bibliography.html#id5" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into Deep Learning. Cambridge University Press, 2023. URL: https://D2L.ai.">Zhang <em>et al.</em>, 2023</a>]</span>.</em></strong></p>
<p>Everything that we have done so far assumes we are working with discrete random
variables, but what if we want to work with continuous ones?</p>
<p>The short summary is that nothing at all changes, except we replace all the
instances of the probability with the probability density. Recalling that we
write densities with lower case <span class="math notranslate nohighlight">\(p\)</span>, this means that for example we now say</p>
<div class="math notranslate nohighlight">
\[
-\log\left(p(X ; \boldsymbol{\theta})\right) = -\log(p(x^{(1)} ; \boldsymbol{\theta})) - \log(p(x^{(2)} ; \boldsymbol{\theta})) \cdots - \log(p(x_n ; \boldsymbol{\theta})) = -\sum_i \log(p(x^{(n)} ; \theta)).
\]</div>
<p>The question becomes, “Why is this OK?” After all, the reason we introduced
densities was because probabilities of getting specific outcomes themselves was
zero, and thus is not the probability of generating our data for any set of
parameters zero?</p>
<p>Indeed, this is the case, and understanding why we can shift to densities is an
exercise in tracing what happens to the epsilons.</p>
<p>Let’s first re-define our goal. Suppose that for continuous random variables we
no longer want to compute the probability of getting exactly the right value,
but instead matching to within some range <span class="math notranslate nohighlight">\(\epsilon\)</span>. For simplicity, we assume
our data is repeated observations <span class="math notranslate nohighlight">\(x^{(1)}, \ldots, x^{(N)}\)</span> of identically
distributed random variables <span class="math notranslate nohighlight">\(X^{(1)}, \ldots, X^{(N)}\)</span>. As we have seen
previously, this can be written as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp;P(X^{(1)} \in [x^{(1)}, x^{(1)}+\epsilon], X^{(2)} \in [x^{(2)}, x^{(2)}+\epsilon], \ldots, X^{(N)} \in [x^{(N)}, x^{(N)}+\epsilon] ;\boldsymbol{\theta}) \\
\approx &amp;\epsilon^Np(x^{(1)} ; \boldsymbol{\theta})\cdot p(x^{(2)} ; \boldsymbol{\theta}) \cdots p(x_n ;\boldsymbol{\theta}).
\end{aligned}
\end{split}\]</div>
<p>Thus, if we take negative logarithms of this we obtain</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp;-\log(P(X^{(1)} \in [x^{(1)}, x^{(1)}+\epsilon], X^{(2)} \in [x^{(2)}, x^{(2)}+\epsilon], \ldots, X^{(N)} \in [x^{(N)}, x^{(N)}+\epsilon] ; \boldsymbol{\theta})) \\
\approx &amp; -N\log(\epsilon) - \sum_{i} \log(p(x^{(n)} ; \boldsymbol{\theta})).
\end{aligned}
\end{split}\]</div>
<p>If we examine this expression, the only place that the <span class="math notranslate nohighlight">\(\epsilon\)</span> occurs is in
the additive constant <span class="math notranslate nohighlight">\(-N\log(\epsilon)\)</span>. This does not depend on the parameters
<span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> at all, so the optimal choice of <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>
does not depend on our choice of <span class="math notranslate nohighlight">\(\epsilon\)</span>! If we demand four digits or
four-hundred, the best choice of <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> remains the same, thus we
may freely drop the epsilon to see that what we want to optimize is</p>
<div class="math notranslate nohighlight">
\[
- \sum_{i} \log(p(x^{(n)} ; \boldsymbol{\theta})).
\]</div>
<p>Thus, we see that the maximum likelihood point of view can operate with
continuous random variables as easily as with discrete ones by replacing the
probabilities with probability densities.</p>
</section>
<section id="maximum-likelihood-estimation-for-common-distributions">
<span id="estimation-theory-mle-common-distributions"></span><h2><a class="toc-backref" href="#id24" role="doc-backlink">Maximum Likelihood Estimation for Common Distributions</a><a class="headerlink" href="#maximum-likelihood-estimation-for-common-distributions" title="Link to this heading">#</a></h2>
<section id="maximum-likelihood-for-univariate-gaussian">
<h3><a class="toc-backref" href="#id25" role="doc-backlink">Maximum Likelihood for Univariate Gaussian</a><a class="headerlink" href="#maximum-likelihood-for-univariate-gaussian" title="Link to this heading">#</a></h3>
<p>Suppose that we are given a set of i.i.d. univariate Gaussian random variables
<span class="math notranslate nohighlight">\(X^{(1)}, \ldots, X^{(N)}\)</span>, where both the mean <span class="math notranslate nohighlight">\(\mu\)</span> and the variance
<span class="math notranslate nohighlight">\(\sigma^2\)</span> are unknown.</p>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{\theta}=\left[\mu, \sigma^2\right]^T\)</span> be the parameter. Find
the maximum likelihood estimate of <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.</p>
<p><strong>First</strong>, we define the likelihood and log-likelihood functions. Since the
random variables are i.i.d., the likelihood function is given by:</p>
<div class="math notranslate nohighlight" id="equation-eq-gaussian-likelihood-1">
<span class="eqno">(230)<a class="headerlink" href="#equation-eq-gaussian-likelihood-1" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
\overbrace{\mathcal{L}\left(\boldsymbol{\theta} \mid \mathcal{S} = \left\{X^{(1)}, \ldots, X^{(N)}\right\}\right)}^{\mathbb{P}\left(\mathcal{S} = \left\{X^{(1)}, \ldots, X^{(N)}\right\} ; \boldsymbol{\theta}\right)} &amp;= \prod_{n=1}^N \overbrace{f_{\boldsymbol{\theta}}\left(x^{(n)}\right)}^{\mathcal{N}\left(x^{(n)} ; \mu, \sigma^2\right) \\} \\
&amp;= \prod_{n=1}^N \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\left(x^{(n)} - \mu\right)^2}{2\sigma^2}\right).
\end{aligned}\end{split}\]</div>
<p>The log-likelihood function is given by:</p>
<div class="math notranslate nohighlight" id="equation-eq-gaussian-likelihood-2">
<span class="eqno">(231)<a class="headerlink" href="#equation-eq-gaussian-likelihood-2" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
\overbrace{\log\mathcal{L}\left(\boldsymbol{\theta} \mid \mathcal{S} = \left\{X^{(1)}, \ldots, X^{(N)}\right\}\right)}^{\log\mathbb{P}\left(\mathcal{S} = \left\{X^{(1)}, \ldots, X^{(N)}\right\} ; \boldsymbol{\theta}\right)} &amp;= \log\left(\prod_{n=1}^N \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\left(x^{(n)} - \mu\right)^2}{2\sigma^2}\right)\right) \\
&amp;= \sum_{n=1}^N \log\left(\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\left(x^{(n)} - \mu\right)^2}{2\sigma^2}\right)\right) &amp;&amp;(*)\\
&amp;= \sum_{n=1}^N \log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) + \sum_{n=1}^N \log\left(\exp\left(-\frac{\left(x^{(n)} - \mu\right)^2}{2\sigma^2}\right)\right) &amp;&amp;(**)\\
&amp;= \sum_{n=1}^N \log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) + \sum_{n=1}^N -\frac{\left(x^{(n)} - \mu\right)^2}{2\sigma^2} \cdot \underbrace{\log(e)}_{1} &amp;&amp;(***)\\
&amp;= \left(\sum_{n=1}^N -\frac{1}{2} \log(2\pi\sigma^2)\right) - \sum_{n=1}^N \frac{\left(x^{(n)} - \mu\right)^2}{2\sigma^2} &amp;&amp;(****)\\
&amp;= -\frac{N}{2} \log(2\pi\sigma^2) - \sum_{n=1}^N \frac{\left(x^{(n)} - \mu\right)^2}{2\sigma^2}.
\end{aligned}\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((*)\)</span> is the log-product rule, meaning that the logarithm of the product of
<span class="math notranslate nohighlight">\(N\)</span> terms is the sum of the logarithms of the <span class="math notranslate nohighlight">\(N\)</span> terms.</p></li>
<li><p><span class="math notranslate nohighlight">\((**)\)</span> is again the log-product rule.</p></li>
<li><p><span class="math notranslate nohighlight">\((***)\)</span> is the log-exponential rule, meaning that the logarithm of the
exponential of a term is the term multiplied by the logarithm of <span class="math notranslate nohighlight">\(e\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\((****)\)</span> is just writing
<span class="math notranslate nohighlight">\(\log\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right) = \log\left(\sqrt{2\pi\sigma^2}^{-\frac{1}{2}}\right)\)</span>
and therefore the log of it is just <span class="math notranslate nohighlight">\(-\frac{1}{2} \log(2\pi\sigma^2)\)</span>.</p></li>
</ul>
<p>Then, we take the derivative of the log-likelihood function with respect to
<span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> and set them to zero to find the maximum likelihood
estimates.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial}{\partial \mu} \log \mathcal{L}\left(\boldsymbol{\theta} \mid \mathcal{S}=\left\{X^{(1)}, \ldots, X^{(N)}\right\}\right) &amp; =\frac{\partial}{\partial \mu}\left(-\frac{N}{2} \log \left(2 \pi \sigma^2\right)-\sum_{n=1}^N \frac{\left(x^{(n)}-\mu\right)^2}{2 \sigma^2}\right) \\
&amp; =0-\frac{\partial}{\partial \mu}\left(\sum_{n=1}^N \frac{\left(x^{(n)}-\mu\right)^2}{2 \sigma^2}\right) \\
&amp; =-\frac{1}{2 \sigma^2} \sum_{n=1}^N \frac{\partial}{\partial \mu}\left(\left(x^{(n)}-\mu\right)^2\right) \\
&amp; =-\frac{1}{2 \sigma^2} \sum_{n=1}^N 2\left(x^{(n)}-\mu\right)(-1) \\
&amp; =\frac{1}{\sigma^2}\sum_{n=1}^N \left(x^{(n)}- \mu\right) \\
\end{aligned}
\end{split}\]</div>
<p>So the partial derivative of the log-likelihood function with respect to <span class="math notranslate nohighlight">\(\mu\)</span>
is:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial \mu} \log \mathcal{L}\left(\boldsymbol{\theta} \mid \mathcal{S}=\left\{X^{(1)}, \ldots, X^{(N)}\right\}\right)=\frac{1}{\sigma^2}\sum_{n=1}^N \left(x^{(n)}- \mu\right)
\]</div>
<p>and setting it to zero gives:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{1}{\sigma^2}\sum_{n=1}^N \left(x^{(n)}- \mu\right) = 0 &amp;\iff \sum_{n=1}^N x^{(n)}- \mu = 0 \\
&amp;\iff \sum_{n=1}^N x^{(n)} = \mu N \\
&amp;\iff \mu = \frac{1}{N}\sum_{n=1}^N x^{(n)}.
\end{aligned}
\end{split}\]</div>
<p>resulting in the maximum likelihood estimate for <span class="math notranslate nohighlight">\(\mu\)</span> to be:</p>
<div class="math notranslate nohighlight">
\[
\hat{\mu} = \frac{1}{N}\sum_{n=1}^N x^{(n)}.
\]</div>
<p>Similarly, we have for <span class="math notranslate nohighlight">\(\sigma^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial}{\partial \sigma^2} \log \mathcal{L}\left(\boldsymbol{\theta} \mid \mathcal{S}=\left\{X^{(1)}, \ldots, X^{(N)}\right\}\right) &amp; =\frac{\partial}{\partial \sigma^2}\left(-\frac{N}{2} \log \left(2 \pi \sigma^2\right)-\sum_{n=1}^N \frac{\left(x^{(n)}-\mu\right)^2}{2 \sigma^2}\right) \\
&amp; =-\frac{N}{2} \frac{\partial}{\partial \sigma^2}\left(\log \left(2 \pi \sigma^2\right)\right)-\sum_{n=1}^N \frac{\partial}{\partial \sigma^2}\left(\frac{\left(x^{(n)}-\mu\right)^2}{2 \sigma^2}\right) \\
&amp; =-\frac{N}{2} \frac{1}{\sigma^2}-\sum_{n=1}^N \frac{-1}{2\left(\sigma^2\right)^2}\left(x^{(n)}-\mu\right)^2 \\
&amp; =-\frac{N}{2 \sigma^2}+\frac{1}{2\left(\sigma^2\right)^2} \sum_{n=1}^N\left(x^{(n)}-\mu\right)^2
\end{aligned}
\end{split}\]</div>
<p>So, the partial derivative with respect to <span class="math notranslate nohighlight">\(\sigma^2\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial \sigma^2} \log \mathcal{L}\left(\boldsymbol{\theta} \mid \mathcal{S}=\left\{X^{(1)}, \ldots, X^{(N)}\right\}\right)=-\frac{N}{2 \sigma^2}+\frac{1}{2\left(\sigma^2\right)^2} \sum_{n=1}^N\left(x^{(n)}-\mu\right)^2
\]</div>
<p>and setting it to zero gives:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
-\frac{N}{2 \sigma^2}+\frac{1}{2\left(\sigma^2\right)^2} \sum_{n=1}^N\left(x^{(n)}-\mu\right)^2 = 0 &amp;\iff \frac{N}{2 \sigma^2} = \frac{1}{2\left(\sigma^2\right)^2} \sum_{n=1}^N\left(x^{(n)}-\mu\right)^2 \\
&amp;\iff \sigma^2 = \frac{1}{N}\sum_{n=1}^N\left(x^{(n)}-\mu\right)^2.
\end{aligned}
\end{split}\]</div>
<p>resulting in the maximum likelihood estimate for <span class="math notranslate nohighlight">\(\sigma^2\)</span> to be:</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^2 = \frac{1}{N}\sum_{n=1}^N\left(x^{(n)}-\hat{\mu}\right)^2.
\]</div>
<p>Note in particular that we placed <span class="math notranslate nohighlight">\(\mu\)</span> by <span class="math notranslate nohighlight">\(\hat{\mu}\)</span>.</p>
<p>Overall, the maximum likelihood estimate for the parameters of a Gaussian
distribution is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\hat{\boldsymbol{\theta}} = \begin{bmatrix} \hat{\mu} \\ \hat{\sigma}^2 \end{bmatrix} = \begin{bmatrix} \frac{1}{N}\sum_{n=1}^N x^{(n)} \\ \frac{1}{N}\sum_{n=1}^N\left(x^{(n)}-\hat{\mu}\right)^2 \end{bmatrix}
\end{split}\]</div>
</section>
<section id="maximum-likelihood-estimation-for-multivariate-gaussian">
<h3><a class="toc-backref" href="#id26" role="doc-backlink">Maximum Likelihood Estimation for Multivariate Gaussian</a><a class="headerlink" href="#maximum-likelihood-estimation-for-multivariate-gaussian" title="Link to this heading">#</a></h3>
<p>See
<a class="reference internal" href="../../../influential/linear_regression/02_concept.html"><span class="std std-doc">my proof on multiple linear regression</span></a>,
they have similar vein of logic. See
<a class="reference external" href="https://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian">here</a>
also.</p>
<p>Suppose that we are given a set of <span class="math notranslate nohighlight">\(\textrm{i.i.d.}\)</span> <span class="math notranslate nohighlight">\(D\)</span>-dimensional Gaussian
random vectors <span class="math notranslate nohighlight">\(\mathbf{X}^{(1)}, \ldots, \mathbf{X}^{(N)}\)</span> such that:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X}^{(n)} = \begin{bmatrix} X^{(n)}_1 \\ \vdots \\ X^{(n)}_D \end{bmatrix} \sim \mathcal{N}\left(\boldsymbol{\mu}, \boldsymbol{\Sigma}\right)
\end{split}\]</div>
<p>where the mean vector <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> and the covariance matrix
<span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> are given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\mu} = \begin{bmatrix} \mu_1 \\ \vdots \\ \mu_D \end{bmatrix}, \quad \boldsymbol{\Sigma} = \begin{bmatrix} \sigma_1^2 &amp; \cdots &amp; \sigma_{1D} \\ \vdots &amp; \ddots &amp; \vdots \\ \sigma_{D1} &amp; \cdots &amp; \sigma_D^2 \end{bmatrix}
\end{split}\]</div>
<p>Now the <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> are unknown, and we want to
find the maximum likelihood estimate for them.</p>
<p>As usual, we find the likelihood function for the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span>
and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>, and then find the maximum likelihood estimate for
them.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{L}\left(\boldsymbol{\mu}, \boldsymbol{\Sigma} \mid \mathcal{S}=\left\{\mathbf{X}^{(1)}, \ldots, \mathbf{X}^{(N)}\right\}\right) &amp; =\prod_{n=1}^N f_{\mathbf{X}^{(n)}}\left(\mathbf{x}^{(n)} ; \boldsymbol{\mu}, \boldsymbol{\Sigma}\right) \\
&amp; =\prod_{n=1}^N \frac{1}{\sqrt{(2 \pi)^{D}|\boldsymbol{\Sigma}|}} \exp \left\{-\frac{1}{2}\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}\right)^{T} \boldsymbol{\Sigma}^{-1}\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}\right)\right\} \\
&amp; =\left(\frac{1}{\sqrt{(2 \pi)^{D}|\boldsymbol{\Sigma}|}}\right)^N \exp \left\{-\frac{1}{2}\sum_{n=1}^N\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}\right)^{T} \boldsymbol{\Sigma}^{-1}\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}\right)\right\} \\
\end{aligned}
\end{split}\]</div>
<p>and consequently the log-likelihood function is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\log \mathcal{L}\left(\boldsymbol{\mu}, \mathbf{\Sigma} \mid \mathcal{S}=\left\{\mathbf{x}^{(1)}, \ldots, \mathbf{X}^{(N)}\right\}\right) &amp; =\log \left(\prod_{n=1}^N \frac{1}{\sqrt{(2 \pi)^D|\boldsymbol{\Sigma}|}} \exp \left\{-\frac{1}{2}\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}\right)^T \boldsymbol{\Sigma}^{-1}\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}\right)\right\}\right) \\
&amp; =\sum_{n=1}^N \log \left(\frac{1}{\sqrt{(2 \pi)^D|\mathbf{\Sigma}|}}\right)+\sum_{n=1}^N \log \left(\exp \left\{-\frac{1}{2}\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}\right)^T \boldsymbol{\Sigma}^{-1}\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}\right)\right\}\right) \\
&amp; =\sum_{n=1}^N\left(-\frac{D}{2} \log (2 \pi)-\frac{1}{2} \log (|\mathbf{\Sigma}|)-\frac{1}{2}\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}\right)^T \boldsymbol{\Sigma}^{-1}\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}\right)\right) \\
&amp; =-\frac{N D}{2} \log (2 \pi)-\frac{N}{2} \log (|\boldsymbol{\Sigma}|)-\frac{1}{2} \sum_{n=1}^N\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}\right)^T \boldsymbol{\Sigma}^{-1}\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}\right)
\end{aligned}
\end{split}\]</div>
<p>Finding the ML estimate requires taking the derivative with respect to both
<span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> :</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp; \frac{d}{d \boldsymbol{\mu}}\left\{-\frac{N D}{2} \log (2 \pi)-\frac{N}{2} \log (|\boldsymbol{\Sigma}|)-\frac{1}{2} \sum_{n=1}^N\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}\right)^T \boldsymbol{\Sigma}^{-1}\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}\right)\right\}=0 \\
&amp; \frac{d}{d \boldsymbol{\Sigma}}\left\{-\frac{N D}{2} \log (2 \pi)-\frac{N}{2} \log (|\boldsymbol{\Sigma}|)-\frac{1}{2} \sum_{n=1}^N\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}\right)^T \boldsymbol{\Sigma}^{-1}\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}\right)\right\}=0 .
\end{aligned}
\end{split}\]</div>
<p>After some tedious algebraic steps (see Duda et al., Pattern Classification,
Problem 3.14), we have that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp; \widehat{\boldsymbol{\mu}}=\frac{1}{N} \sum_{n=1}^{N} \mathbf{x}^{(n)}, \\
&amp; \widehat{\boldsymbol{\Sigma}}=\frac{1}{N} \sum_{n=1}^{N}\left(\mathbf{x}^{(n)}-\widehat{\boldsymbol{\mu}}\right)\left(\mathbf{x}^{(n)}-\widehat{\boldsymbol{\mu}}\right)^{T} .
\end{aligned}
\end{split}\]</div>
</section>
</section>
<section id="references-and-further-readings">
<h2><a class="toc-backref" href="#id27" role="doc-backlink">References and Further Readings</a><a class="headerlink" href="#references-and-further-readings" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Chan, Stanley H. “Chapter 8.1. Maximum-Likelihood Estimation.” In
Introduction to Probability for Data Science. Ann Arbor, Michigan: Michigan
Publishing Services, 2021.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./probability_theory/08_estimation_theory/maximum_likelihood_estimation"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Maximum Likelihood Estimation</p>
      </div>
    </a>
    <a class="right-next"
       href="../../../operations/distributed/intro.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Distributed Systems</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood">Likelihood</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#some-intuition">Some Intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition">Definition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-and-identically-distributed-iid">Independence and Identically Distributed (IID)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-in-the-context-of-machine-learning">Likelihood in the Context of Machine Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood-in-the-context-of-supervised-learning">Likelihood in the Context of Supervised Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-likelihood-in-the-context-of-machine-learning">Conditional Likelihood in the Context of Machine Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-log-likelihood-function">The Log-Likelihood Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-likelihood-function">Visualizing the Likelihood Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation">Maximum Likelihood Estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#coin-toss-example">Coin Toss Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-likelihood-and-maximum-likelihood-estimation-as-n-increases">Visualizing Likelihood and Maximum Likelihood Estimation as <span class="math notranslate nohighlight">\(N\)</span> Increases</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-optimization-and-the-negative-log-likelihood">Numerical Optimization and the Negative Log-Likelihood</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-underflow">Numerical Underflow</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-convenience">Mathematical Convenience</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#information-theory">Information Theory</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-for-continuous-variables">Maximum Likelihood for Continuous Variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-for-common-distributions">Maximum Likelihood Estimation for Common Distributions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-for-univariate-gaussian">Maximum Likelihood for Univariate Gaussian</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation-for-multivariate-gaussian">Maximum Likelihood Estimation for Multivariate Gaussian</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../../../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>


  <footer class="bd-footer">
  </footer>
<div class="share-buttons" style="padding: 15px; text-align: center; background-color: #f5f5f5; margin: 15px 0;">
    <div class="container" style="max-width: 500px; margin: 0 auto;">
        <h3 style="margin: 0 0 10px 0; font-size: 1.1em;">Connect with me!</h3>
        <div style="display: flex; justify-content: center; gap: 15px;">
            <!-- LinkedIn Follow Button -->
            <a href="https://www.linkedin.com/in/gao-hongnan/" target="_blank" style="text-decoration: none; flex: 1; max-width: 160px;">
                <button style="background-color: #0077b5; color: white; border: none; padding: 8px 15px; border-radius: 5px; cursor: pointer; width: 100%; font-size: 0.9em;">
                    Follow on LinkedIn
                </button>
            </a>

            <!-- Twitter/X Follow Button -->
            <a href="https://x.com/gaohongnan" target="_blank" style="text-decoration: none; flex: 1; max-width: 160px;">
                <button style="background-color: #000000; color: white; border: none; padding: 8px 15px; border-radius: 5px; cursor: pointer; width: 100%; font-size: 0.9em;">
                    Follow on X
                </button>
            </a>
        </div>

        <h3 style="margin: 12px 0 10px 0; font-size: 1.1em;">Share this page!</h3>
        <div style="display: flex; justify-content: center; gap: 15px;">
            <!-- LinkedIn Share Button -->
            <a href="javascript:void(0);" onclick="window.open('https://www.linkedin.com/sharing/share-offsite/?url=' + encodeURIComponent(window.location.href), 'linkedin-share', 'width=600,height=400')" style="text-decoration: none; flex: 1; max-width: 160px;">
                <button style="background-color: #0077b5; color: white; border: none; padding: 8px 15px; border-radius: 5px; cursor: pointer; width: 100%; font-size: 0.9em;">
                    Share on LinkedIn
                </button>
            </a>

            <!-- Twitter/X Share Button -->
            <a href="javascript:void(0);" onclick="window.open('https://twitter.com/intent/tweet?text=' + encodeURIComponent('Check out this page by @gaohongnan!') + '&url=' + encodeURIComponent(window.location.href), 'twitter-share', 'width=600,height=400')" style="text-decoration: none; flex: 1; max-width: 160px;">
                <button style="background-color: #000000; color: white; border: none; padding: 8px 15px; border-radius: 5px; cursor: pointer; width: 100%; font-size: 0.9em;">
                    Share on X
                </button>
            </a>
        </div>
    </div>
</div>

  </body>
</html>