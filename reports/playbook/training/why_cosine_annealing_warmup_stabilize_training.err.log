Traceback (most recent call last):
  File "/home/runner/work/omniverse/omniverse/.venv/lib/python3.12/site-packages/jupyter_core/utils/__init__.py", line 154, in wrapped
    asyncio.get_running_loop()
RuntimeError: no running event loop

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/runner/work/omniverse/omniverse/.venv/lib/python3.12/site-packages/jupyter_cache/executors/utils.py", line 58, in single_nb_execution
    executenb(
  File "/home/runner/work/omniverse/omniverse/.venv/lib/python3.12/site-packages/nbclient/client.py", line 1319, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/runner/work/omniverse/omniverse/.venv/lib/python3.12/site-packages/jupyter_core/utils/__init__.py", line 158, in wrapped
    return loop.run_until_complete(inner)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/asyncio/base_events.py", line 687, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/home/runner/work/omniverse/omniverse/.venv/lib/python3.12/site-packages/nbclient/client.py", line 709, in async_execute
    await self.async_execute_cell(
  File "/home/runner/work/omniverse/omniverse/.venv/lib/python3.12/site-packages/nbclient/client.py", line 1062, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/home/runner/work/omniverse/omniverse/.venv/lib/python3.12/site-packages/nbclient/client.py", line 918, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
from __future__ import annotations

import math
from functools import partial

from torch.optim.lr_scheduler import LambdaLR
from torch.optim.optimizer import Optimizer
from torch.optim import Adam
from torch import nn

def _get_cosine_schedule_with_warmup_lr_lambda(
    current_step: int, *, num_warmup_steps: int, num_training_steps: int, alpha_f: float
) -> float:
    if current_step < num_warmup_steps:
        alpha = current_step / max(1, num_warmup_steps)
    else:
        tau_w = (current_step - num_warmup_steps) / num_training_steps
        tau_w = min(1.0, tau_w)
        alpha = alpha_f + (1 - alpha_f) * (1 + math.cos(math.pi * tau_w)) / 2
    return alpha


def get_cosine_annealing_with_warmup(
    optimizer: Optimizer,
    num_warmup_steps: int,
    num_training_steps: int,
    alpha_f: float = 0.1,
    last_epoch: int = -1,
    verbose: bool = False,
) -> LambdaLR:
    lr_lambda = partial(
        _get_cosine_schedule_with_warmup_lr_lambda,
        num_warmup_steps=num_warmup_steps,
        num_training_steps=num_training_steps,
        alpha_f=alpha_f,
    )
    return LambdaLR(optimizer, lr_lambda, last_epoch, verbose)

# Experiment 1
num_warmup_steps = 5
num_training_steps = 10
alpha_f = 0.5
initial_lr = 3e-4

dummy_model = nn.Linear(1, 1)
optimizer = Adam(dummy_model.parameters(), lr=initial_lr)
scheduler = get_cosine_annealing_with_warmup(optimizer, num_warmup_steps, num_training_steps, alpha_f)
assert isinstance(scheduler, LambdaLR)
lrs1 = get_learning_rates(optimizer, scheduler, steps=num_training_steps)

# Experiment 2
num_warmup_steps = 200
num_training_steps = 1000

dummy_model = nn.Linear(1, 1)
optimizer = Adam(dummy_model.parameters(), lr=initial_lr)
scheduler = get_cosine_annealing_with_warmup(optimizer, num_warmup_steps, num_training_steps, alpha_f)
lrs2 = get_learning_rates(optimizer, scheduler, steps=num_training_steps)

fig, axes = plt.subplots(1, 2, figsize=(12, 4))
plot_learning_rates(lrs1, 'Cosine Annealing With Warmup (Short)', ax=axes[0])
plot_learning_rates(lrs2, 'Cosine Annealing With Warmup (Long)', ax=axes[1])
------------------


[31m---------------------------------------------------------------------------[39m
[31mTypeError[39m                                 Traceback (most recent call last)
[36mCell[39m[36m [39m[32mIn[3][39m[32m, line 47[39m
[32m     45[39m dummy_model = nn.Linear([32m1[39m, [32m1[39m)
[32m     46[39m optimizer = Adam(dummy_model.parameters(), lr=initial_lr)
[32m---> [39m[32m47[39m scheduler = [43mget_cosine_annealing_with_warmup[49m[43m([49m[43moptimizer[49m[43m,[49m[43m [49m[43mnum_warmup_steps[49m[43m,[49m[43m [49m[43mnum_training_steps[49m[43m,[49m[43m [49m[43malpha_f[49m[43m)[49m
[32m     48[39m [38;5;28;01massert[39;00m [38;5;28misinstance[39m(scheduler, LambdaLR)
[32m     49[39m lrs1 = get_learning_rates(optimizer, scheduler, steps=num_training_steps)

[36mCell[39m[36m [39m[32mIn[3][39m[32m, line 37[39m, in [36mget_cosine_annealing_with_warmup[39m[34m(optimizer, num_warmup_steps, num_training_steps, alpha_f, last_epoch, verbose)[39m
[32m     23[39m [38;5;28;01mdef[39;00m[38;5;250m [39m[34mget_cosine_annealing_with_warmup[39m(
[32m     24[39m     optimizer: Optimizer,
[32m     25[39m     num_warmup_steps: [38;5;28mint[39m,
[32m   (...)[39m[32m     29[39m     verbose: [38;5;28mbool[39m = [38;5;28;01mFalse[39;00m,
[32m     30[39m ) -> LambdaLR:
[32m     31[39m     lr_lambda = partial(
[32m     32[39m         _get_cosine_schedule_with_warmup_lr_lambda,
[32m     33[39m         num_warmup_steps=num_warmup_steps,
[32m     34[39m         num_training_steps=num_training_steps,
[32m     35[39m         alpha_f=alpha_f,
[32m     36[39m     )
[32m---> [39m[32m37[39m     [38;5;28;01mreturn[39;00m [43mLambdaLR[49m[43m([49m[43moptimizer[49m[43m,[49m[43m [49m[43mlr_lambda[49m[43m,[49m[43m [49m[43mlast_epoch[49m[43m,[49m[43m [49m[43mverbose[49m[43m)[49m

[31mTypeError[39m: LambdaLR.__init__() takes from 3 to 4 positional arguments but 5 were given

