Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/jupyter_cache/executors/utils.py", line 58, in single_nb_execution
    executenb(
  File "/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/nbclient/client.py", line 1314, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/jupyter_core/utils/__init__.py", line 165, in wrapped
    return loop.run_until_complete(inner)
  File "/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/nbclient/client.py", line 709, in async_execute
    await self.async_execute_cell(
  File "/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/nbclient/client.py", line 1062, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/site-packages/nbclient/client.py", line 918, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
"""Module for creating PyTorch scheduler instances dynamically."""

from __future__ import annotations

from typing import Any, Callable, Dict, Literal, Type

import torch

from rich.pretty import pprint

from omnivault.utils.config_management.dynamic import DynamicClassFactory

RegisteredSchedulers = Literal[
    "torch.optim.lr_scheduler.StepLR",
    "torch.optim.lr_scheduler.CosineAnnealingLR",
    "torch.optim.lr_scheduler.LambdaLR",
    "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts",
    "torch.optim.lr_scheduler.OneCycleLR",
]
SCHEDULER_REGISTRY: Dict[str, Type[SchedulerConfig]] = {}


def register_scheduler(name: str) -> Callable[[Type[SchedulerConfig]], Type[SchedulerConfig]]:
    def register_scheduler_cls(cls: Type[SchedulerConfig]) -> Type[SchedulerConfig]:
        if name in SCHEDULER_REGISTRY:
            raise ValueError(f"Cannot register duplicate scheduler {name}")
        if not issubclass(cls, SchedulerConfig):
            raise ValueError(f"Scheduler (name={name}, class={cls.__name__}) must extend SchedulerConfig")
        SCHEDULER_REGISTRY[name] = cls
        return cls

    return register_scheduler_cls


class SchedulerConfig(DynamicClassFactory[torch.optim.lr_scheduler.LRScheduler]):
    """
    Base class for creating PyTorch scheduler instances dynamically.

    This class extends `DynamicClassFactory` to specifically handle the
    instantiation of PyTorch scheduler classes based on provided configurations.

    Methods
    -------
    build(optimizer: torch.optim.Optimizer) -> torch.optim.lr_scheduler.LRScheduler
        Creates and returns a scheduler instance with the specified optimizer.
    """

    name: str

    def build(self, optimizer: torch.optim.Optimizer, **kwargs: Any) -> torch.optim.lr_scheduler.LRScheduler:
        """Builder method for creating a scheduler instance."""
        return self.create_instance(optimizer=optimizer, **kwargs)

    class Config:
        extra = "forbid"


@register_scheduler("torch.optim.lr_scheduler.StepLR")
class StepLRConfig(SchedulerConfig):
    step_size: int
    gamma: float = 0.1
    last_epoch: int = -1
    verbose: bool = False


@register_scheduler("torch.optim.lr_scheduler.CosineAnnealingLR")
class CosineAnnealingLRConfig(SchedulerConfig):
    T_max: int
    eta_min: float = 0
    last_epoch: int = -1
    verbose: bool = False


@register_scheduler("torch.optim.lr_scheduler.CosineAnnealingWarmRestarts")
class CosineAnnealingWarmRestartsConfig(SchedulerConfig):
    """See
    https://wandb.ai/wandb_fc/tips/reports/How-to-Properly-Use-PyTorch-s-CosineAnnealingWarmRestarts-Scheduler--VmlldzoyMTA3MjM2
    to see how to use this scheduler.
    """

    T_0: int
    T_mult: int = 1
    eta_min: float = 0
    last_epoch: int = -1
    verbose: bool = False


@register_scheduler("torch.optim.lr_scheduler.OneCycleLR")
class OneCycleLRConfig(SchedulerConfig):
    """You can use a utils function to retrieve the constructor args of a class
    dynamically.

    Example
    -------
    >>> from omnivault.utils.general.python_shenanigans import get_init_args
    >>> get_init_args(OneCycleLRConfig)
    """

    max_lr: float
    total_steps: int
    epochs: int
    steps_per_epoch: int
    pct_start: float = 0.3
    anneal_strategy: Literal["cos", "linear"] = "cos"
    cycle_momentum: bool = True
    base_momentum: float = 0.85
    max_momentum: float = 0.95
    div_factor: float = 25.0
    final_div_factor: float = 10000.0
    three_phase: bool = False
    last_epoch: int = -1
    verbose: bool = False


@register_scheduler("torch.optim.lr_scheduler.LambdaLR")
class LambdaLRConfig(SchedulerConfig):
    # The user must provide a lambda function for the scheduler
    lr_lambda: Callable[[int], float]  # we know this lr_lambda maps int (epoch) to float (some multiplier)

------------------


[0;31m---------------------------------------------------------------------------[0m
[0;31mModuleNotFoundError[0m                       Traceback (most recent call last)
Cell [0;32mIn[1], line 11[0m
[1;32m      7[0m [38;5;28;01mimport[39;00m [38;5;21;01mtorch[39;00m
[1;32m      9[0m [38;5;28;01mfrom[39;00m [38;5;21;01mrich[39;00m[38;5;21;01m.[39;00m[38;5;21;01mpretty[39;00m [38;5;28;01mimport[39;00m pprint
[0;32m---> 11[0m [38;5;28;01mfrom[39;00m [38;5;21;01momnivault[39;00m[38;5;21;01m.[39;00m[38;5;21;01mutils[39;00m[38;5;21;01m.[39;00m[38;5;21;01mconfig_management[39;00m[38;5;21;01m.[39;00m[38;5;21;01mdynamic[39;00m [38;5;28;01mimport[39;00m DynamicClassFactory
[1;32m     13[0m RegisteredSchedulers [38;5;241m=[39m Literal[
[1;32m     14[0m     [38;5;124m"[39m[38;5;124mtorch.optim.lr_scheduler.StepLR[39m[38;5;124m"[39m,
[1;32m     15[0m     [38;5;124m"[39m[38;5;124mtorch.optim.lr_scheduler.CosineAnnealingLR[39m[38;5;124m"[39m,
[0;32m   (...)[0m
[1;32m     18[0m     [38;5;124m"[39m[38;5;124mtorch.optim.lr_scheduler.OneCycleLR[39m[38;5;124m"[39m,
[1;32m     19[0m ]
[1;32m     20[0m SCHEDULER_REGISTRY: Dict[[38;5;28mstr[39m, Type[SchedulerConfig]] [38;5;241m=[39m {}

[0;31mModuleNotFoundError[0m: No module named 'omnivault'

