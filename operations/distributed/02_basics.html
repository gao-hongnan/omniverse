
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Basics Of Distributed Data Parallelism &#8212; Omniverse</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=bb35926c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MYW5YKC2WF"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'operations/distributed/02_basics';</script>
    <link rel="canonical" href="https://www.gaohongnan.com/operations/distributed/02_basics.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="How to Setup SLURM and ParallelCluster in AWS" href="03_how_to_setup_slurm_in_aws.html" />
    <link rel="prev" title="Notations" href="01_notations.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Omniverse - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Omniverse - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    ðŸŒŒ Omniverse: A Journey Through Knowledge
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notations/machine_learning.html">Machine Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Influential Ideas and Papers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../influential/generative_pretrained_transformer/01_intro.html">Generative Pre-trained Transformers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../influential/generative_pretrained_transformer/02_notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../influential/generative_pretrained_transformer/03_concept.html">The Concept of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../influential/generative_pretrained_transformer/04_implementation.html">The Implementation of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../influential/generative_pretrained_transformer/05_adder.html">Training a Mini-GPT to Learn Two-Digit Addition</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../influential/low_rank_adaptation/01_intro.html">Low-Rank Adaptation Of Large Language Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../influential/low_rank_adaptation/02_concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../influential/kmeans/01_intro.html">K-Means</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../influential/kmeans/02_concept.html">Concept: K-Means Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../influential/kmeans/03_implementation.html">Implementation: K-Means (Lloyd)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../influential/kmeans/04_image_segmentation.html">Application: Image Compression and Segmentation</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Playbook</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_calculate_flops_in_transformer_based_models.html">How to Calculate the Number of FLOPs in Transformer Based Models?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_inspect_function_and_class_signatures.html">How to Inspect Function and Class Signatures in Python?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/why_cosine_annealing_warmup_stabilize_training.html">Why Does Cosine Annealing With Warmup Stabilize Training?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html">Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling.</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_finetune_decoder_only_models.html">How To Fine-Tune Decoder-Only Models?</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Operations</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../machine_learning_lifecycle/00_intro.html">The Lifecycle of an AIOps System</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../machine_learning_lifecycle/01_problem_formulation.html">Stage 1. Problem Formulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../machine_learning_lifecycle/02_project_scoping.html">Stage 2. Project Scoping And Framing The Problem</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../machine_learning_lifecycle/03_dataops_pipeline/03_dataops_pipeline.html">Stage 3. Data Pipeline (Data Engineering and DataOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../machine_learning_lifecycle/03_dataops_pipeline/031_data_source_and_format.html">Stage 3.1. Data Source and Formats</a></li>
<li class="toctree-l3"><a class="reference internal" href="../machine_learning_lifecycle/03_dataops_pipeline/032_data_model_and_storage.html">Stage 3.2. Data Model and Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../machine_learning_lifecycle/03_dataops_pipeline/033_etl.html">Stage 3.3. Extract, Transform, Load (ETL)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../machine_learning_lifecycle/04_mlops_data_pipeline.html">Stage 4. Data Extraction (MLOps), Data Analysis (Data Science), Data Preparation (Data Science)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../machine_learning_lifecycle/05_model_development_selection_and_training/05_ml_training_pipeline.html">Stage 5. Model Development and Training (MLOps)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../machine_learning_lifecycle/05_model_development_selection_and_training/051_model_selection.html">Stage 5.1. Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../machine_learning_lifecycle/05_model_development_selection_and_training/052_metric_selection.html">Stage 5.2. Metric Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../machine_learning_lifecycle/05_model_development_selection_and_training/053_experiment_tracking.html">Stage 5.3. Experiment Tracking And Versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../machine_learning_lifecycle/05_model_development_selection_and_training/054_model_testing.html">Stage 5.4. Model Testing</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Distributed Systems</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01_notations.html">Notations</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Basics Of Distributed Data Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="03_how_to_setup_slurm_in_aws.html">How to Setup SLURM and ParallelCluster in AWS</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/config_management/concept.html">Configuration Management</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/config_management/01-pydra.html">Pydantic And Hydra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/config_management/02-state.html">State And Metadata Management</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/devops/continuous-integration/concept.html">Continuous Integration (CI) Workflow</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/devops/continuous-integration/styling.html">Styling, Formatting, and Linting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/devops/continuous-integration/testing.html">Testing</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../software_engineering/design_patterns/dependency-inversion-principle.html">Dependency Inversion Principle</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/intro.html">Concurrency, Parallelism and Asynchronous Programming</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/generator_yield.html">A Rudimentary Introduction to Generator and Yield in Python</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/serving/restful_api/intro.html">RESTful API</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/serving/restful_api/application_banking.html">Application: Designing a RESTful Banking API with FastAPI and SQLAlchemy</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../computer_science/type_theory/intro.html">Type Theory, A Very Rudimentary Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/01-subtypes.html">Subtypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/02-type-safety.html">Type Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/03-subsumption.html">Subsumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/04-generics.html">Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/05-typevar-bound-constraints.html">Bound and Constraint in Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/06-invariance-covariance-contravariance.html">Invariance, Covariance and Contravariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/07-pep-3124-overloading.html">Function Overloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/08-pep-661-sentinel-values.html">Sentinel Types</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Structures and Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/complexity_analysis/intro.html">Complexity Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/complexity_analysis/master_theorem.html">Master Theorem </a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/stack/intro.html">Stack</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/stack/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/intro.html">Linear Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/concept.html">Concept</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/intro.html">Binary Search</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/problems/875-koko-eating-bananas.html">Koko Eating Bananas</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/01_preliminaries/intro.html">Preliminaries</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/01-fields.html">Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/02-systems-of-linear-equations.html">Systems of Linear Equations</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/02_vectors/intro.html">Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/01-vector-definition.html">Vector and Its Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/02-vector-operation.html">Vector and Its Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/03-vector-norm.html">Vector Norm and Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/04-vector-products.html">A First Look at Vector Products</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../citations.html">IEEE (Style) Citations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../api/reproducibility.html">API Reference</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse/issues/new?title=Issue%20on%20page%20%2Foperations/distributed/02_basics.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/operations/distributed/02_basics.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Basics Of Distributed Data Parallelism</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up">Setting Up</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-code">The Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#command-line-arguments-cpu-and-gloo-backend">Command Line Arguments (CPU And Gloo Backend)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#process-group-initialization">Process Group Initialization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#master-port-and-master-address">Master Port and Master Address</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#backend">Backend</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#global-rank-and-world-size">Global Rank And World Size</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#set-cuda-device">Set CUDA Device</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-information">Distributed Information</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-setup-with-cuda">Multi-Node Setup With CUDA</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="basics-of-distributed-data-parallelism">
<h1>Basics Of Distributed Data Parallelism<a class="headerlink" href="#basics-of-distributed-data-parallelism" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://twitter.com/gaohongnan"><img alt="Twitter Handle" src="https://img.shields.io/badge/Twitter-&#64;gaohongnan-blue?style=social&amp;logo=twitter" /></a>
<a class="reference external" href="https://linkedin.com/in/gao-hongnan"><img alt="LinkedIn Profile" src="https://img.shields.io/badge/&#64;gaohongnan-blue?style=social&amp;logo=linkedin" /></a>
<a class="reference external" href="https://github.com/gao-hongnan"><img alt="GitHub Profile" src="https://img.shields.io/badge/GitHub-gao--hongnan-lightgrey?style=social&amp;logo=github" /></a>
<img alt="Tag" src="https://img.shields.io/badge/Tag-Brain_Dump-red" />
<img alt="Tag" src="https://img.shields.io/badge/Level-Beginner-green" /></p>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#setting-up" id="id4">Setting Up</a></p>
<ul>
<li><p><a class="reference internal" href="#the-code" id="id5">The Code</a></p></li>
<li><p><a class="reference internal" href="#command-line-arguments-cpu-and-gloo-backend" id="id6">Command Line Arguments (CPU And Gloo Backend)</a></p></li>
<li><p><a class="reference internal" href="#process-group-initialization" id="id7">Process Group Initialization</a></p>
<ul>
<li><p><a class="reference internal" href="#master-port-and-master-address" id="id8">Master Port and Master Address</a></p></li>
<li><p><a class="reference internal" href="#backend" id="id9">Backend</a></p></li>
<li><p><a class="reference internal" href="#global-rank-and-world-size" id="id10">Global Rank And World Size</a></p></li>
<li><p><a class="reference internal" href="#set-cuda-device" id="id11">Set CUDA Device</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#distributed-information" id="id12">Distributed Information</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#multi-node-setup-with-cuda" id="id13">Multi-Node Setup With CUDA</a></p></li>
<li><p><a class="reference internal" href="#references-and-further-readings" id="id14">References and Further Readings</a></p></li>
</ul>
</nav>
<p><strong>DistributedDataParallel (DDP)</strong> provides module-level data parallelism thatâ€™s
scalable across multiple machines. For effective
use<a class="footnote-reference brackets" href="#pytorch-distributed-data-parallel-tutorial" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>:</p>
<ol class="arabic simple">
<li><p>Launch multiple processes, initializing one DDP instance for each.</p></li>
<li><p>DDP leverages collective communications via the
<a class="reference external" href="https://pytorch.org/tutorials/intermediate/dist_tuto.html">torch.distributed</a>
module for gradient and buffer synchronization.</p></li>
<li><p>An autograd hook is registered for each parameter determined by
<strong><code class="docutils literal notranslate"><span class="pre">model.parameters()</span></code></strong>. This hook is activated when the gradient is
computed during the backward pass, signaling DDP to synchronize gradients
across processes. Further insights can be found in the
<a class="reference external" href="https://pytorch.org/docs/master/notes/ddp.html">DDP design note</a>.</p></li>
</ol>
<p>Traditionally if you have <code class="docutils literal notranslate"><span class="pre">train.py</span></code> script that trains a model on a single GPU
using say a <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> class, now if you were to use DDP, you can think of it as
replicating the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> class across multiple processes and note that the
<em>model</em> and <em>optimizer</em> are
<a class="reference external" href="https://engineering.fb.com/2021/07/15/open-source/fsdp/">replicated</a> across
processes as well. However, the data is not replicated across processes.
Instead, each process gets a subset of the data to work on. The gradients are
synchronized across processes using collective communications. Note that
replicating the model and optimizer across all ranks has overhead in gpu memory,
and if your model and optimizer are very large then you may need to consider
techniques like fsdp or combining DDP with model parallelism.</p>
<section id="setting-up">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Setting Up</a><a class="headerlink" href="#setting-up" title="Link to this heading">#</a></h2>
<p>In this post, we set up a simple example to demonstrate the use of DDP without
the use of the <code class="docutils literal notranslate"><span class="pre">torch.distributed.launch</span></code>, <code class="docutils literal notranslate"><span class="pre">torchrun</span></code> or SLURM.</p>
<p>We need to know a few things before we start:</p>
<ul class="simple">
<li><p>The number of nodes in the cluster, <span class="math notranslate nohighlight">\(N\)</span>.</p></li>
<li><p>The number of processes per node - the number of processes that will run on
each node. In other words, the number of GPUs per node, <span class="math notranslate nohighlight">\(G\)</span>. Please just
keep it the same for all nodes.</p></li>
<li><p>The node rank, <span class="math notranslate nohighlight">\(n\)</span>, which is the index of the node in the cluster. The node
rank is <span class="math notranslate nohighlight">\(0\)</span>-indexed.</p></li>
</ul>
<p>Knowing these three things is sufficient for one to set up DDP, even in a
distributed setting. Everything else like world size, local rank, and global
rank can be derived from these three. Additionally, you would need the
<code class="docutils literal notranslate"><span class="pre">MASTER_ADDR</span></code> and <code class="docutils literal notranslate"><span class="pre">MASTER_PORT</span></code> environment variables. These are used to set up
the rendezvous point for the processes to communicate with each other - which is
a must in a multi-node setup.</p>
<section id="the-code">
<h3><a class="toc-backref" href="#id5" role="doc-backlink">The Code</a><a class="headerlink" href="#the-code" title="Link to this heading">#</a></h3>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--0-input--1" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--1"><strong>b_demo.py</strong></label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span>
<span class="kn">import</span> <span class="nn">torch.multiprocessing</span> <span class="k">as</span> <span class="nn">mp</span>
<span class="kn">from</span> <span class="nn">rich.pretty</span> <span class="kn">import</span> <span class="n">pprint</span>
<span class="kn">from</span> <span class="nn">torch._C._distributed_c10d</span> <span class="kn">import</span> <span class="n">ReduceOp</span>

<span class="kn">from</span> <span class="nn">omnivault.distributed.core</span> <span class="kn">import</span> <span class="n">find_free_port</span><span class="p">,</span> <span class="n">is_free_port</span>
<span class="kn">from</span> <span class="nn">omnivault.utils.reproducibility.seed</span> <span class="kn">import</span> <span class="n">seed_all</span>
<span class="kn">from</span> <span class="nn">omnixamples.distributed.a_basic.a_setup</span> <span class="kn">import</span> <span class="n">init_process</span>
<span class="kn">from</span> <span class="nn">omnixamples.distributed.a_basic.config</span> <span class="kn">import</span> <span class="n">get_args_parser</span>


<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">local_rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">logger</span><span class="p">,</span> <span class="n">dist_info_per_process</span> <span class="o">=</span> <span class="n">init_process</span><span class="p">(</span><span class="n">local_rank</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dist_info_per_process</span><span class="o">.</span><span class="n">model_dump_json</span><span class="p">(</span><span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">seed_all</span><span class="p">(</span><span class="n">dist_info_per_process</span><span class="o">.</span><span class="n">global_rank</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">set_torch_deterministic</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># seed each process</span>

    <span class="n">device</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;cuda:</span><span class="si">{</span><span class="n">local_rank</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;rank </span><span class="si">{</span><span class="n">dist_info_per_process</span><span class="o">.</span><span class="n">global_rank</span><span class="si">}</span><span class="s2"> data (before all-reduce): </span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="s2"> with device </span><span class="si">{</span><span class="n">data</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">ReduceOp</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span> <span class="n">async_op</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># in-place</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;rank </span><span class="si">{</span><span class="n">dist_info_per_process</span><span class="o">.</span><span class="n">global_rank</span><span class="si">}</span><span class="s2"> data (after all-reduce): </span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="s2"> with device </span><span class="si">{</span><span class="n">data</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># MANUAL/RAW NO TORCHRUN OR SLURM OR TORCH DISTRIBUTED LAUNCHER</span>
    <span class="c1"># torchrun --nnodes=1 --nproc-per-node=4 --rdzv-backend=c10d --rdzv-endpoint=localhost:29500 sandbox.py</span>

    <span class="c1"># NOTE: if you use torchrun then a lot of env variables are auto</span>
    <span class="c1"># set when you pass in the command line arguments to torchrun.</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">get_args_parser</span><span class="p">()</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="n">pprint</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

    <span class="n">master_addr</span><span class="p">,</span> <span class="n">master_port</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">master_addr</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">master_port</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_free_port</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">master_port</span><span class="p">)):</span>
        <span class="n">master_port</span> <span class="o">=</span> <span class="n">find_free_port</span><span class="p">()</span>

    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_ADDR&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">master_addr</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_PORT&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">master_port</span><span class="p">)</span>

    <span class="n">mp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span>
        <span class="n">fn</span><span class="o">=</span><span class="n">run</span><span class="p">,</span>
        <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">args</span><span class="p">,),</span>
        <span class="n">nprocs</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">nproc_per_node</span><span class="p">,</span>
        <span class="n">join</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">daemon</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">start_method</span><span class="o">=</span><span class="s2">&quot;spawn&quot;</span><span class="p">,</span>
    <span class="p">)</span>  <span class="c1"># type: ignore[no-untyped-call]</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--0-input--2" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--2"><strong>config</strong></label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">logging</span>


<span class="k">def</span> <span class="nf">get_args_parser</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s2">&quot;Distributed Training Demo&quot;</span><span class="p">)</span>

    <span class="c1"># LOGGING</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--log_dir&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;logs&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Directory to store logs.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--log_level&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Logging level.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--log_on_master_or_all&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Whether to log only on master rank or all ranks.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># DISTRIBUTED</span>
    <span class="c1"># 1. Mandatory Environment Variables, note if use `torchrun` then not all are mandatory.</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--master_addr&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;localhost&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Master address.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--master_port&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;29500&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Master port.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--nnodes&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Number of nodes.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--nproc_per_node&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Number of processes per node.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--node_rank&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Node rank.&quot;</span><span class="p">)</span>

    <span class="c1"># 2. Optional Environment Variables, can be derived from above.</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--world_size&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Total number of processes.&quot;</span><span class="p">)</span>

    <span class="c1"># 3. Initialization of Process Group</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--backend&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;gloo&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Backend for distributed training.&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--init_method&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;env://&quot;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Initialization method for distributed training.&quot;</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">parser</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--0-input--3" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--3"><strong>a_setup.py</strong></label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span>

<span class="kn">from</span> <span class="nn">omnivault.distributed.core</span> <span class="kn">import</span> <span class="n">get_hostname</span><span class="p">,</span> <span class="n">get_process_id</span>
<span class="kn">from</span> <span class="nn">omnivault.distributed.dist_info</span> <span class="kn">import</span> <span class="n">DistInfoPerProcess</span>
<span class="kn">from</span> <span class="nn">omnivault.distributed.logger</span> <span class="kn">import</span> <span class="n">configure_logger</span>


<span class="c1"># NOTE: In DDP just imagine all your function is replicated across all processes.</span>
<span class="c1"># 1. init_process(1)</span>
<span class="c1"># 2. init_process(2)</span>
<span class="c1"># 3. init_process(3)</span>
<span class="c1"># 4. init_process(4)</span>
<span class="k">def</span> <span class="nf">init_process</span><span class="p">(</span>
    <span class="n">local_rank</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">Namespace</span><span class="p">,</span> <span class="n">logger</span><span class="p">:</span> <span class="n">logging</span><span class="o">.</span><span class="n">Logger</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">logging</span><span class="o">.</span><span class="n">Logger</span><span class="p">,</span> <span class="n">DistInfoPerProcess</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This rank should be the rank of the process spawned by `mp.spawn`.&quot;&quot;&quot;</span>
    <span class="c1"># NOTE: knowing `nnodes`, `node_rank` and `nproc_per_node` is sufficient to derive most of other env.</span>
    <span class="n">nnodes</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">nnodes</span>
    <span class="n">nproc_per_node</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">nproc_per_node</span>  <span class="c1"># local_world_size</span>

    <span class="c1"># NOTE: all these validations can be done via pydantic but for simplicity we do it here.</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="k">assert</span> <span class="n">nproc_per_node</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
    <span class="n">node_rank</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">node_rank</span>

    <span class="n">world_size</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">world_size</span>
    <span class="k">assert</span> <span class="n">world_size</span> <span class="o">==</span> <span class="n">nnodes</span> <span class="o">*</span> <span class="n">nproc_per_node</span>

    <span class="n">global_rank</span> <span class="o">=</span> <span class="n">local_rank</span> <span class="o">+</span> <span class="n">node_rank</span> <span class="o">*</span> <span class="n">nproc_per_node</span>
    <span class="k">assert</span> <span class="n">local_rank</span> <span class="o">==</span> <span class="n">global_rank</span> <span class="o">%</span> <span class="n">nproc_per_node</span>

    <span class="n">hostname</span> <span class="o">=</span> <span class="n">get_hostname</span><span class="p">()</span>
    <span class="n">process_id</span> <span class="o">=</span> <span class="n">get_process_id</span><span class="p">()</span>

    <span class="n">dist_info_per_process</span> <span class="o">=</span> <span class="n">DistInfoPerProcess</span><span class="p">(</span>
        <span class="n">master_addr</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_ADDR&quot;</span><span class="p">],</span>
        <span class="n">master_port</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;MASTER_PORT&quot;</span><span class="p">],</span>
        <span class="n">nnodes</span><span class="o">=</span><span class="n">nnodes</span><span class="p">,</span>
        <span class="n">nproc_per_node</span><span class="o">=</span><span class="n">nproc_per_node</span><span class="p">,</span>
        <span class="n">node_rank</span><span class="o">=</span><span class="n">node_rank</span><span class="p">,</span>
        <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
        <span class="n">backend</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">backend</span><span class="p">,</span>
        <span class="n">init_method</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">init_method</span><span class="p">,</span>
        <span class="n">global_rank</span><span class="o">=</span><span class="n">global_rank</span><span class="p">,</span>
        <span class="n">local_rank</span><span class="o">=</span><span class="n">local_rank</span><span class="p">,</span>
        <span class="n">local_world_size</span><span class="o">=</span><span class="n">nproc_per_node</span><span class="p">,</span>
        <span class="n">hostname</span><span class="o">=</span><span class="n">hostname</span><span class="p">,</span>
        <span class="n">process_id</span><span class="o">=</span><span class="n">process_id</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">logger</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># NOTE: this is global rank configuration for logger</span>
        <span class="n">logger</span> <span class="o">=</span> <span class="n">configure_logger</span><span class="p">(</span>
            <span class="n">rank</span><span class="o">=</span><span class="n">dist_info_per_process</span><span class="o">.</span><span class="n">global_rank</span><span class="p">,</span> <span class="n">log_dir</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">log_dir</span><span class="p">,</span> <span class="n">log_on_master_or_all</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">log_on_master_or_all</span>
        <span class="p">)</span>

    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span>
        <span class="n">backend</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">backend</span><span class="p">,</span>
        <span class="n">rank</span><span class="o">=</span><span class="n">dist_info_per_process</span><span class="o">.</span><span class="n">global_rank</span><span class="p">,</span>
        <span class="n">world_size</span><span class="o">=</span><span class="n">dist_info_per_process</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span>
        <span class="n">init_method</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">init_method</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># NOTE: safety net, sync all processes before proceeding - for example in</span>
    <span class="c1"># `configure_logger` there is an create directory operation which maybe should be</span>
    <span class="c1"># done by only master rank. Nevertheless, consider the fact that you don&#39;t</span>
    <span class="c1"># sync barrier, then you might run into problem of another rank process wanting</span>
    <span class="c1"># to write to the same directory before it is created by master rank.</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
    <span class="c1"># NOTE: set device should be for local rank, not global rank, else you run</span>
    <span class="c1"># into ordinal out of device error.</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">dist_info_per_process</span><span class="o">.</span><span class="n">local_rank</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="kc">None</span>
    <span class="k">return</span> <span class="n">logger</span><span class="p">,</span> <span class="n">dist_info_per_process</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--0-input--4" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--4"><strong>dist_info.py</strong></label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span><span class="p">,</span> <span class="n">Field</span>
<span class="kn">from</span> <span class="nn">rich.pretty</span> <span class="kn">import</span> <span class="n">pprint</span>


<span class="k">class</span> <span class="nc">DistInfoPerProcess</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Assumes one process is one worker/gpu. Immutable and should only</span>
<span class="sd">    perform data validation.&quot;&quot;&quot;</span>

    <span class="n">master_addr</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">                    This is `MASTER_ADDR` which refers to the IP address (or hostname)</span>
<span class="s2">                    of the machine or node where the rank 0 process is running.</span>
<span class="s2">                    It acts as the reference point for all other nodes and GPUs</span>
<span class="s2">                    in the distributed setup. All other processes will connect</span>
<span class="s2">                    to this address for synchronization and communication.</span>
<span class="s2">                    &quot;&quot;&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">master_port</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Denotes an available port on the `MASTER_ADDR` machine. &quot;</span>
        <span class="s2">&quot;All processes will use this port number for communication.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">nnodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Number of nodes in the distributed setup.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">nproc_per_node</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Number of processes/gpus per node in the distributed setup.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">node_rank</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Rank of the current node in the distributed setup.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">world_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Total number of processes/gpus in the distributed setup.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># 3. Initialization of Process Group</span>
    <span class="n">backend</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Backend for distributed training.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">init_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="s2">&quot;env://&quot;</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Initialization method for distributed training.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># 4. Others</span>
    <span class="n">global_rank</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Rank of the current process/gpu in the distributed setup.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">local_world_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Total number of processes/gpus in the local node.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">local_rank</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Rank of the current process/gpu in the local node.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># 5. System info</span>
    <span class="n">hostname</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Hostname of the current node.&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">process_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">Field</span><span class="p">(</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="n">description</span><span class="o">=</span><span class="s2">&quot;Process ID of the current process.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">pretty_print</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">pprint</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="command-line-arguments-cpu-and-gloo-backend">
<h3><a class="toc-backref" href="#id6" role="doc-backlink">Command Line Arguments (CPU And Gloo Backend)</a><a class="headerlink" href="#command-line-arguments-cpu-and-gloo-backend" title="Link to this heading">#</a></h3>
<p>Thereâ€™s quite a fair bit of command line arguments, but most of them are simple
and will be understood shortly. To run the script you can do (for a single node)
and with CPU and backend <code class="docutils literal notranslate"><span class="pre">gloo</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>omnixamples/distributed/a_basic/b_demo.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--master_addr<span class="o">=</span>localhost<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--master_port<span class="o">=</span><span class="m">29500</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--nnodes<span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--nproc_per_node<span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--node_rank<span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--world_size<span class="o">=</span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--backend<span class="o">=</span>gloo<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--init_method<span class="o">=</span><span class="s2">&quot;env://&quot;</span>
</pre></div>
</div>
<p>This will run the script on a single node with 4 processes. In the first tab
above, you can see the <code class="docutils literal notranslate"><span class="pre">torch.multiprocessing.spawn</span></code> function is used to spawn
the processes on the target function <code class="docutils literal notranslate"><span class="pre">run</span></code>, with <code class="docutils literal notranslate"><span class="pre">nprocs</span></code> defined as the number
of processes per node. The <code class="docutils literal notranslate"><span class="pre">run</span></code> function is where the actual work is done. A
side note is that the <code class="docutils literal notranslate"><span class="pre">torch.multiprocessing.spawn</span></code> function takes in implicitly
a <code class="docutils literal notranslate"><span class="pre">local_rank</span></code> argument which is defined as <code class="docutils literal notranslate"><span class="pre">i</span></code> in the source code below. What
this means is that the function <code class="docutils literal notranslate"><span class="pre">run</span></code> corresponds to the <code class="docutils literal notranslate"><span class="pre">fn</span></code> argument in the
<code class="docutils literal notranslate"><span class="pre">torch.multiprocessing.spawn</span></code> and since the underlying source code does
<code class="docutils literal notranslate"><span class="pre">args=(fn,</span> <span class="pre">i,</span> <span class="pre">args,</span> <span class="pre">error_queue)</span></code> then it means our <code class="docutils literal notranslate"><span class="pre">run</span></code> function should take
in an integer argument as the first argument - which is the <code class="docutils literal notranslate"><span class="pre">local_rank</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">start_processes</span><span class="p">(</span>
    <span class="n">fn</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(),</span> <span class="n">nprocs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">join</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">daemon</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">start_method</span><span class="o">=</span><span class="s2">&quot;spawn&quot;</span>
<span class="p">):</span>
    <span class="n">mp</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">get_context</span><span class="p">(</span><span class="n">start_method</span><span class="p">)</span>
    <span class="n">error_queues</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">processes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nprocs</span><span class="p">):</span>
        <span class="n">error_queue</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">SimpleQueue</span><span class="p">()</span>
        <span class="n">process</span> <span class="o">=</span> <span class="n">mp</span><span class="o">.</span><span class="n">Process</span><span class="p">(</span>
            <span class="n">target</span><span class="o">=</span><span class="n">_wrap</span><span class="p">,</span>
            <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">error_queue</span><span class="p">),</span>
            <span class="n">daemon</span><span class="o">=</span><span class="n">daemon</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">process</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
        <span class="n">error_queues</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">error_queue</span><span class="p">)</span>
        <span class="n">processes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">process</span><span class="p">)</span>

    <span class="n">context</span> <span class="o">=</span> <span class="n">ProcessContext</span><span class="p">(</span><span class="n">processes</span><span class="p">,</span> <span class="n">error_queues</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">join</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">context</span>

    <span class="c1"># Loop on join until it returns True or raises an exception.</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">join</span><span class="p">():</span>
        <span class="k">pass</span>


<span class="k">def</span> <span class="nf">spawn</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(),</span> <span class="n">nprocs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">join</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">daemon</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">start_method</span><span class="o">=</span><span class="s2">&quot;spawn&quot;</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">start_method</span> <span class="o">!=</span> <span class="s2">&quot;spawn&quot;</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;This method only supports start_method=spawn (got: </span><span class="si">%s</span><span class="s2">).</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;To use a different start_method use:</span><span class="se">\n\t\t</span><span class="s2">&quot;</span>
            <span class="s2">&quot; torch.multiprocessing.start_processes(...)&quot;</span> <span class="o">%</span> <span class="n">start_method</span>
        <span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">start_processes</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">nprocs</span><span class="p">,</span> <span class="n">join</span><span class="p">,</span> <span class="n">daemon</span><span class="p">,</span> <span class="n">start_method</span><span class="o">=</span><span class="s2">&quot;spawn&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="process-group-initialization">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">Process Group Initialization</a><a class="headerlink" href="#process-group-initialization" title="Link to this heading">#</a></h3>
<p>The idea of process group can be intuitive. Consider 1 single node with 4 gpus,
and you want to run 4 processes, one on each gpu. The process group is the
<strong>collection</strong> of these 4 processes. With the same logic, consider 2 nodes, with
2 gpus each, now you would run 2 processes on each node, and the process group
is the <strong>collection</strong> of these 4 processes. The word collection is vague, but if
you add the definition of all processes within a process group can
<strong><em>communicate</em></strong> with each other, then it makes sense. Consequently, the
process group represents multiple worker processes that coordinate and
communicate with each other via a shared
<strong><em>master</em></strong><a class="footnote-reference brackets" href="#cs336-spring2024-assignment2-systems" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>.</p>
<section id="master-port-and-master-address">
<h4><a class="toc-backref" href="#id8" role="doc-backlink">Master Port and Master Address</a><a class="headerlink" href="#master-port-and-master-address" title="Link to this heading">#</a></h4>
<p>How does the shared master get defined. It is defined by its IP address and
port, and we see in the first tab above, we need to specify the <code class="docutils literal notranslate"><span class="pre">MASTER_ADDR</span></code>
and <code class="docutils literal notranslate"><span class="pre">MASTER_PORT</span></code>. For example, if you have 2 compute nodes on SLURM, then the
<code class="docutils literal notranslate"><span class="pre">MASTER_ADDR</span></code> would be the IP address of the first node (rank 0) and the
<code class="docutils literal notranslate"><span class="pre">MASTER_PORT</span></code> would be a free port on that node. The second node would then
connect to the first node using the <code class="docutils literal notranslate"><span class="pre">MASTER_ADDR</span></code> and <code class="docutils literal notranslate"><span class="pre">MASTER_PORT</span></code>.</p>
</section>
<section id="backend">
<h4><a class="toc-backref" href="#id9" role="doc-backlink">Backend</a><a class="headerlink" href="#backend" title="Link to this heading">#</a></h4>
<p>So how should we initialize the process group? The <code class="docutils literal notranslate"><span class="pre">init_process_group</span></code> function
will be the responsible function to initialize the process group. One needs to
specify the <code class="docutils literal notranslate"><span class="pre">backend</span></code>, <code class="docutils literal notranslate"><span class="pre">rank</span></code>, <code class="docutils literal notranslate"><span class="pre">world_size</span></code>, and <code class="docutils literal notranslate"><span class="pre">init_method</span></code>, amongs others.
But of course if your <code class="docutils literal notranslate"><span class="pre">init_method</span></code> is <code class="docutils literal notranslate"><span class="pre">env://</span></code> then you donâ€™t need to specify
the <code class="docutils literal notranslate"><span class="pre">rank</span></code> and <code class="docutils literal notranslate"><span class="pre">world_size</span></code> as they are derived from the environment variables.
But we still pass in here for clarity.</p>
<p>The backend is the communication backend to use and usually is <code class="docutils literal notranslate"><span class="pre">gloo</span></code> for cpu
and <code class="docutils literal notranslate"><span class="pre">nccl</span></code> for gpu because it will use the NVIDIA Collective Communication
Library (NCCL) for communication.</p>
<p>The code snippet below from the <code class="docutils literal notranslate"><span class="pre">a_setup.py</span></code> file shows how the process group is
initialized:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span>
    <span class="n">backend</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">backend</span><span class="p">,</span>
    <span class="n">rank</span><span class="o">=</span><span class="n">dist_info_per_process</span><span class="o">.</span><span class="n">global_rank</span><span class="p">,</span>
    <span class="n">world_size</span><span class="o">=</span><span class="n">dist_info_per_process</span><span class="o">.</span><span class="n">world_size</span><span class="p">,</span>
    <span class="n">init_method</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">init_method</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="global-rank-and-world-size">
<h4><a class="toc-backref" href="#id10" role="doc-backlink">Global Rank And World Size</a><a class="headerlink" href="#global-rank-and-world-size" title="Link to this heading">#</a></h4>
<p>Now remember that we are not using any convenience cluster like SLURM. So we
need to derive the <code class="docutils literal notranslate"><span class="pre">global_rank</span></code> and <code class="docutils literal notranslate"><span class="pre">world_size</span></code> from the <code class="docutils literal notranslate"><span class="pre">local_rank</span></code>,
<code class="docutils literal notranslate"><span class="pre">node_rank</span></code>, and <code class="docutils literal notranslate"><span class="pre">nproc_per_node</span></code> in order to pass it to the
<code class="docutils literal notranslate"><span class="pre">init_process_group</span></code> function.</p>
<p>The logic is simple, if you have 2 nodes, each with 4 gpus, then world size is
just <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">4</span> <span class="pre">=</span> <span class="pre">8</span></code>. The global rank is then derived from knowing the <code class="docutils literal notranslate"><span class="pre">node_rank</span></code>,
for example, if your node rank is 0, and since <code class="docutils literal notranslate"><span class="pre">local_rank</span></code> is given by the
<code class="docutils literal notranslate"><span class="pre">mp.spawn</span></code> function from <code class="docutils literal notranslate"><span class="pre">0-3</span></code>, then the global rank is just
<code class="docutils literal notranslate"><span class="pre">local_rank</span> <span class="pre">+</span> <span class="pre">node_rank</span> <span class="pre">*</span> <span class="pre">nproc_per_node</span></code>. Similarly, if your node rank is 1,
then the global rank is also <code class="docutils literal notranslate"><span class="pre">local_rank</span> <span class="pre">+</span> <span class="pre">node_rank</span> <span class="pre">*</span> <span class="pre">nproc_per_node</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nnodes</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">nnodes</span>
<span class="n">nproc_per_node</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">nproc_per_node</span>  <span class="c1"># local_world_size</span>

<span class="c1"># NOTE: all these validations can be done via pydantic but for simplicity we do it here.</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">nproc_per_node</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
<span class="n">node_rank</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">node_rank</span>

<span class="n">world_size</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">world_size</span>
<span class="k">assert</span> <span class="n">world_size</span> <span class="o">==</span> <span class="n">nnodes</span> <span class="o">*</span> <span class="n">nproc_per_node</span>

<span class="n">global_rank</span> <span class="o">=</span> <span class="n">local_rank</span> <span class="o">+</span> <span class="n">node_rank</span> <span class="o">*</span> <span class="n">nproc_per_node</span>
</pre></div>
</div>
</section>
<section id="set-cuda-device">
<h4><a class="toc-backref" href="#id11" role="doc-backlink">Set CUDA Device</a><a class="headerlink" href="#set-cuda-device" title="Link to this heading">#</a></h4>
<p>When running multi-GPU jobs, make sure that different ranks use different GPUs.
This wonâ€™t be needed for CPU-only jobs.</p>
<p>Two methods to do that, and I used both - I mean I use both for clarity and not
worried about the overhead.</p>
<p>First method when moving tensor to device, you need to move it to local rank and
not global rank. For example, you can do <code class="docutils literal notranslate"><span class="pre">tensor.to(f&quot;cuda:{local_rank}&quot;)</span></code>
because the common <code class="docutils literal notranslate"><span class="pre">tensor.to(&quot;cuda&quot;)</span></code> will no longer work in multi-GPU setup.</p>
<p>Second method is to set the device using <code class="docutils literal notranslate"><span class="pre">torch.cuda.set_device(local_rank)</span></code>
after you init the process group. This way, when you do <code class="docutils literal notranslate"><span class="pre">tensor.to(&quot;cuda&quot;)</span></code> it
will automatically move it to the specified
device<a class="footnote-reference brackets" href="#cs336-spring2024-assignment2-systems" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>.</p>
</section>
</section>
<section id="distributed-information">
<h3><a class="toc-backref" href="#id12" role="doc-backlink">Distributed Information</a><a class="headerlink" href="#distributed-information" title="Link to this heading">#</a></h3>
<p>Letâ€™s first define a pydantic class that holds information about the distributed
setup. This class will be immutable and will only perform data validation. The
purpose of this class is after creation of distributed setup, we can inject this
object to other classes or functions to use the distributed information.</p>
<p>If you click on the tab named <code class="docutils literal notranslate"><span class="pre">dist_info.py</span></code> above, you will see the code - and
after running the above command, you will see 4 outputs, from each process,
holding the below info:</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--1-input--1" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--1">Process 0</label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DistInfoPerProcess</span><span class="p">(</span>
    <span class="n">master_addr</span><span class="o">=</span><span class="s1">&#39;localhost&#39;</span><span class="p">,</span>
    <span class="n">master_port</span><span class="o">=</span><span class="s1">&#39;29500&#39;</span><span class="p">,</span>
    <span class="n">nnodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">nproc_per_node</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">node_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;gloo&#39;</span><span class="p">,</span>
    <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;env://&#39;</span><span class="p">,</span>
    <span class="n">global_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">local_world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">local_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">hostname</span><span class="o">=</span><span class="s1">&#39;Hongnans-Mac-mini.local&#39;</span><span class="p">,</span>
    <span class="n">process_id</span><span class="o">=</span><span class="mi">72041</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--1-input--2" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--2">Process 1</label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DistInfoPerProcess</span><span class="p">(</span>
    <span class="n">master_addr</span><span class="o">=</span><span class="s1">&#39;localhost&#39;</span><span class="p">,</span>
    <span class="n">master_port</span><span class="o">=</span><span class="s1">&#39;29500&#39;</span><span class="p">,</span>
    <span class="n">nnodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">nproc_per_node</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">node_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;gloo&#39;</span><span class="p">,</span>
    <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;env://&#39;</span><span class="p">,</span>
    <span class="n">global_rank</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">local_world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">local_rank</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">hostname</span><span class="o">=</span><span class="s1">&#39;Hongnans-Mac-mini.local&#39;</span><span class="p">,</span>
    <span class="n">process_id</span><span class="o">=</span><span class="mi">72042</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--1-input--3" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--3">Process 2</label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DistInfoPerProcess</span><span class="p">(</span>
    <span class="n">master_addr</span><span class="o">=</span><span class="s1">&#39;localhost&#39;</span><span class="p">,</span>
    <span class="n">master_port</span><span class="o">=</span><span class="s1">&#39;29500&#39;</span><span class="p">,</span>
    <span class="n">nnodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">nproc_per_node</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">node_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;gloo&#39;</span><span class="p">,</span>
    <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;env://&#39;</span><span class="p">,</span>
    <span class="n">global_rank</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">local_world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">local_rank</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">hostname</span><span class="o">=</span><span class="s1">&#39;Hongnans-Mac-mini.local&#39;</span><span class="p">,</span>
    <span class="n">process_id</span><span class="o">=</span><span class="mi">72043</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--1-input--4" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--4">Process 3</label><div class="tab-content docutils">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DistInfoPerProcess</span><span class="p">(</span>
    <span class="n">master_addr</span><span class="o">=</span><span class="s1">&#39;localhost&#39;</span><span class="p">,</span>
    <span class="n">master_port</span><span class="o">=</span><span class="s1">&#39;29500&#39;</span><span class="p">,</span>
    <span class="n">nnodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">nproc_per_node</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">node_rank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;gloo&#39;</span><span class="p">,</span>
    <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;env://&#39;</span><span class="p">,</span>
    <span class="n">global_rank</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">local_world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">local_rank</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">hostname</span><span class="o">=</span><span class="s1">&#39;Hongnans-Mac-mini.local&#39;</span><span class="p">,</span>
    <span class="n">process_id</span><span class="o">=</span><span class="mi">72044</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Of course it is difficult to log pydantic objects into log file, so you can just
<code class="docutils literal notranslate"><span class="pre">model_dump_json(indent=4)</span></code> to get a json string and log that.</p>
</section>
</section>
<section id="multi-node-setup-with-cuda">
<h2><a class="toc-backref" href="#id13" role="doc-backlink">Multi-Node Setup With CUDA</a><a class="headerlink" href="#multi-node-setup-with-cuda" title="Link to this heading">#</a></h2>
<p>Now consider the case where we have 2 nodes, and 1 GPU on each node. Letâ€™s see
if our setup works!</p>
<p>We first have two nodes, so we need two scripts - one for the master and one for
the worker. The master script will write the <code class="docutils literal notranslate"><span class="pre">MASTER_ADDR</span></code> and <code class="docutils literal notranslate"><span class="pre">MASTER_PORT</span></code> to
a file, and the worker script will read from this file. The master script will
start the process group and the worker script will join the process group.</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--2-input--1" name="tab-set--2" type="radio"><label class="tab-label" for="tab-set--2-input--1"><strong>01_demo_start_master.sh</strong></label><div class="tab-content docutils">
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env sh</span>

<span class="c1"># Get master address and port</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="k">$(</span>hostname<span class="w"> </span>-i<span class="k">)</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_PORT</span><span class="o">=</span><span class="k">$(</span>comm<span class="w"> </span>-23<span class="w"> </span>&lt;<span class="o">(</span>seq<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="m">65535</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>sort<span class="k">)</span><span class="w"> </span>&lt;<span class="o">(</span>ss<span class="w"> </span>-Htan<span class="w"> </span><span class="p">|</span><span class="w"> </span>awk<span class="w"> </span><span class="s1">&#39;{print $4}&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>cut<span class="w"> </span>-d<span class="s1">&#39;:&#39;</span><span class="w"> </span>-f2<span class="w"> </span><span class="p">|</span><span class="w"> </span>sort<span class="w"> </span>-u<span class="o">)</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>shuf<span class="w"> </span><span class="p">|</span><span class="w"> </span>head<span class="w"> </span>-n<span class="w"> </span><span class="m">1</span><span class="o">)</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Master Address: </span><span class="nv">$MASTER_ADDR</span><span class="s2">&quot;</span>
<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Master Port: </span><span class="nv">$MASTER_PORT</span><span class="s2">&quot;</span>

<span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">MASTER_ADDR</span><span class="si">}</span><span class="s2">:</span><span class="si">${</span><span class="nv">MASTER_PORT</span><span class="si">}</span><span class="s2">&quot;</span><span class="w"> </span>&gt;<span class="w"> </span>master_info.txt

<span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="nv">$PYTHONPATH</span>:<span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NNODES</span><span class="o">=</span><span class="m">2</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NPROC_PER_NODE</span><span class="o">=</span><span class="m">1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NODE_RANK</span><span class="o">=</span><span class="m">0</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">WORLD_SIZE</span><span class="o">=</span><span class="m">2</span>

python<span class="w"> </span>omnixamples/distributed/a_basic/b_demo.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--master_addr<span class="o">=</span><span class="nv">$MASTER_ADDR</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--master_port<span class="o">=</span><span class="nv">$MASTER_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--nnodes<span class="o">=</span><span class="nv">$NNODES</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--nproc_per_node<span class="o">=</span><span class="nv">$NPROC_PER_NODE</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--node_rank<span class="o">=</span><span class="nv">$NODE_RANK</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--world_size<span class="o">=</span><span class="nv">$WORLD_SIZE</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--backend<span class="o">=</span>gloo<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--init_method<span class="o">=</span><span class="s2">&quot;env://&quot;</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--2-input--2" name="tab-set--2" type="radio"><label class="tab-label" for="tab-set--2-input--2"><strong>01_demo_start_worker.sh</strong></label><div class="tab-content docutils">
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env sh</span>

<span class="c1"># Read master address and port from the shared file</span>
<span class="nv">master_info</span><span class="o">=</span><span class="k">$(</span>cat<span class="w"> </span>master_info.txt<span class="k">)</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_ADDR</span><span class="o">=</span><span class="k">$(</span><span class="nb">echo</span><span class="w"> </span><span class="nv">$master_info</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>cut<span class="w"> </span>-d<span class="s1">&#39;:&#39;</span><span class="w"> </span>-f1<span class="k">)</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_PORT</span><span class="o">=</span><span class="k">$(</span><span class="nb">echo</span><span class="w"> </span><span class="nv">$master_info</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>cut<span class="w"> </span>-d<span class="s1">&#39;:&#39;</span><span class="w"> </span>-f2<span class="k">)</span>

<span class="nb">export</span><span class="w"> </span><span class="nv">PYTHONPATH</span><span class="o">=</span><span class="nv">$PYTHONPATH</span>:<span class="k">$(</span><span class="nb">pwd</span><span class="k">)</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NNODES</span><span class="o">=</span><span class="m">2</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NPROC_PER_NODE</span><span class="o">=</span><span class="m">1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NODE_RANK</span><span class="o">=</span><span class="m">1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">WORLD_SIZE</span><span class="o">=</span><span class="m">2</span>

python<span class="w"> </span>omnixamples/distributed/a_basic/b_demo.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--master_addr<span class="o">=</span><span class="nv">$MASTER_ADDR</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--master_port<span class="o">=</span><span class="nv">$MASTER_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--nnodes<span class="o">=</span><span class="nv">$NNODES</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--nproc_per_node<span class="o">=</span><span class="nv">$NPROC_PER_NODE</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--node_rank<span class="o">=</span><span class="nv">$NODE_RANK</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--world_size<span class="o">=</span><span class="nv">$WORLD_SIZE</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--backend<span class="o">=</span>gloo<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--init_method<span class="o">=</span><span class="s2">&quot;env://&quot;</span>
</pre></div>
</div>
</div>
</div>
<p>Running them shows the output logs below for <code class="docutils literal notranslate"><span class="pre">process_0</span></code> and <code class="docutils literal notranslate"><span class="pre">process_1</span></code>:</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--3-input--1" name="tab-set--3" type="radio"><label class="tab-label" for="tab-set--3-input--1">Process 0</label><div class="tab-content docutils">
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Namespace<span class="o">(</span>
<span class="w">    </span><span class="nv">log_dir</span><span class="o">=</span><span class="s2">&quot;logs&quot;</span>,
<span class="w">    </span><span class="nv">log_level</span><span class="o">=</span><span class="m">20</span>,
<span class="w">    </span><span class="nv">log_on_master_or_all</span><span class="o">=</span>False,
<span class="w">    </span><span class="nv">master_addr</span><span class="o">=</span><span class="s2">&quot;10.0.23.218&quot;</span>,
<span class="w">    </span><span class="nv">master_port</span><span class="o">=</span><span class="s2">&quot;1775&quot;</span>,
<span class="w">    </span><span class="nv">nnodes</span><span class="o">=</span><span class="m">2</span>,
<span class="w">    </span><span class="nv">nproc_per_node</span><span class="o">=</span><span class="m">1</span>,
<span class="w">    </span><span class="nv">node_rank</span><span class="o">=</span><span class="m">0</span>,
<span class="w">    </span><span class="nv">world_size</span><span class="o">=</span><span class="m">2</span>,
<span class="w">    </span><span class="nv">backend</span><span class="o">=</span><span class="s2">&quot;gloo&quot;</span>,
<span class="w">    </span><span class="nv">init_method</span><span class="o">=</span><span class="s2">&quot;env://&quot;</span>,
<span class="o">)</span>

<span class="m">2024</span>-05-18<span class="w"> </span><span class="m">10</span>:03:47<span class="w"> </span>INFO<span class="w">     </span><span class="m">2024</span>-05-18<span class="w"> </span><span class="m">10</span>:03:47<span class="w"> </span><span class="o">[</span>INFO<span class="o">]</span>:<span class="w"> </span><span class="o">{</span><span class="w">                                                           </span>b_demo.py:37
<span class="w">                                 </span><span class="s2">&quot;master_addr&quot;</span>:<span class="w"> </span><span class="s2">&quot;10.0.23.218&quot;</span>,
<span class="w">                                 </span><span class="s2">&quot;master_port&quot;</span>:<span class="w"> </span><span class="s2">&quot;1775&quot;</span>,
<span class="w">                                 </span><span class="s2">&quot;nnodes&quot;</span>:<span class="w"> </span><span class="m">2</span>,
<span class="w">                                 </span><span class="s2">&quot;nproc_per_node&quot;</span>:<span class="w"> </span><span class="m">1</span>,
<span class="w">                                 </span><span class="s2">&quot;node_rank&quot;</span>:<span class="w"> </span><span class="m">0</span>,
<span class="w">                                 </span><span class="s2">&quot;world_size&quot;</span>:<span class="w"> </span><span class="m">2</span>,
<span class="w">                                 </span><span class="s2">&quot;backend&quot;</span>:<span class="w"> </span><span class="s2">&quot;gloo&quot;</span>,
<span class="w">                                 </span><span class="s2">&quot;init_method&quot;</span>:<span class="w"> </span><span class="s2">&quot;env://&quot;</span>,
<span class="w">                                 </span><span class="s2">&quot;global_rank&quot;</span>:<span class="w"> </span><span class="m">0</span>,
<span class="w">                                 </span><span class="s2">&quot;local_world_size&quot;</span>:<span class="w"> </span><span class="m">1</span>,
<span class="w">                                 </span><span class="s2">&quot;local_rank&quot;</span>:<span class="w"> </span><span class="m">0</span>,
<span class="w">                                 </span><span class="s2">&quot;hostname&quot;</span>:<span class="w"> </span><span class="s2">&quot;distributed-queue-st-g4dn2xlarge-1&quot;</span>,
<span class="w">                                 </span><span class="s2">&quot;process_id&quot;</span>:<span class="w"> </span><span class="m">4598</span>
<span class="w">                             </span><span class="o">}</span>

<span class="m">2024</span>-05-18<span class="w"> </span><span class="m">10</span>:03:48<span class="w"> </span>INFO<span class="w"> </span><span class="m">2024</span>-05-18<span class="w"> </span><span class="m">10</span>:03:48<span class="w"> </span><span class="o">[</span>INFO<span class="o">]</span>:<span class="w"> </span>rank<span class="w"> </span><span class="m">0</span><span class="w"> </span>data<span class="w"> </span><span class="o">(</span>before<span class="w"> </span>all-reduce<span class="o">)</span>:<span class="w"> </span>tensor<span class="o">([</span><span class="m">4</span>,<span class="w"> </span><span class="m">9</span>,<span class="w"> </span><span class="m">3</span><span class="o">]</span>,<span class="w"> </span><span class="nv">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="o">)</span><span class="w"> </span>with<span class="w"> </span>device<span class="w"> </span>cuda:0.
b_demo.py:48
<span class="m">2024</span>-05-18<span class="w"> </span><span class="m">10</span>:03:48<span class="w"> </span>INFO<span class="w"> </span><span class="m">2024</span>-05-18<span class="w"> </span><span class="m">10</span>:03:48<span class="w"> </span><span class="o">[</span>INFO<span class="o">]</span>:<span class="w"> </span>rank<span class="w"> </span><span class="m">0</span><span class="w"> </span>data<span class="w"> </span><span class="o">(</span>after<span class="w"> </span>all-reduce<span class="o">)</span>:<span class="w"> </span>tensor<span class="o">([</span><span class="m">9</span>,<span class="w"> </span><span class="m">18</span>,<span class="w"> </span><span class="m">7</span><span class="o">]</span>,<span class="w"> </span><span class="nv">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="o">)</span><span class="w"> </span>with<span class="w"> </span>device<span class="w"> </span>cuda:0.
b_demo.py:50
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--3-input--2" name="tab-set--3" type="radio"><label class="tab-label" for="tab-set--3-input--2">Process 1</label><div class="tab-content docutils">
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Namespace<span class="o">(</span>
<span class="w">    </span><span class="nv">log_dir</span><span class="o">=</span><span class="s2">&quot;logs&quot;</span>,
<span class="w">    </span><span class="nv">log_level</span><span class="o">=</span><span class="m">20</span>,
<span class="w">    </span><span class="nv">log_on_master_or_all</span><span class="o">=</span>False,
<span class="w">    </span><span class="nv">master_addr</span><span class="o">=</span><span class="s2">&quot;10.0.23.218&quot;</span>,
<span class="w">    </span><span class="nv">master_port</span><span class="o">=</span><span class="s2">&quot;1775&quot;</span>,
<span class="w">    </span><span class="nv">nnodes</span><span class="o">=</span><span class="m">2</span>,
<span class="w">    </span><span class="nv">nproc_per_node</span><span class="o">=</span><span class="m">1</span>,
<span class="w">    </span><span class="nv">node_rank</span><span class="o">=</span><span class="m">1</span>,
<span class="w">    </span><span class="nv">world_size</span><span class="o">=</span><span class="m">2</span>,
<span class="w">    </span><span class="nv">backend</span><span class="o">=</span><span class="s2">&quot;gloo&quot;</span>,
<span class="w">    </span><span class="nv">init_method</span><span class="o">=</span><span class="s2">&quot;env://&quot;</span>,
<span class="o">)</span>
<span class="m">2024</span>-05-18<span class="w"> </span><span class="m">10</span>:03:47<span class="w"> </span>INFO<span class="w">     </span><span class="m">2024</span>-05-18<span class="w"> </span><span class="m">10</span>:03:47<span class="w"> </span><span class="o">[</span>INFO<span class="o">]</span>:<span class="w">  </span>b_demo.py:37
<span class="w">                             </span><span class="o">{</span>
<span class="w">                                 </span><span class="s2">&quot;master_addr&quot;</span>:
<span class="w">                             </span><span class="s2">&quot;10.0.23.218&quot;</span>,
<span class="w">                                 </span><span class="s2">&quot;master_port&quot;</span>:<span class="w"> </span><span class="s2">&quot;1775&quot;</span>,
<span class="w">                                 </span><span class="s2">&quot;nnodes&quot;</span>:<span class="w"> </span><span class="m">2</span>,
<span class="w">                                 </span><span class="s2">&quot;nproc_per_node&quot;</span>:<span class="w"> </span><span class="m">1</span>,
<span class="w">                                 </span><span class="s2">&quot;node_rank&quot;</span>:<span class="w"> </span><span class="m">1</span>,
<span class="w">                                 </span><span class="s2">&quot;world_size&quot;</span>:<span class="w"> </span><span class="m">2</span>,
<span class="w">                                 </span><span class="s2">&quot;backend&quot;</span>:<span class="w"> </span><span class="s2">&quot;gloo&quot;</span>,
<span class="w">                                 </span><span class="s2">&quot;init_method&quot;</span>:<span class="w"> </span><span class="s2">&quot;env://&quot;</span>,
<span class="w">                                 </span><span class="s2">&quot;global_rank&quot;</span>:<span class="w"> </span><span class="m">1</span>,
<span class="w">                                 </span><span class="s2">&quot;local_world_size&quot;</span>:<span class="w"> </span><span class="m">1</span>,
<span class="w">                                 </span><span class="s2">&quot;local_rank&quot;</span>:<span class="w"> </span><span class="m">0</span>,
<span class="w">                                 </span><span class="s2">&quot;hostname&quot;</span>:
<span class="w">                             </span><span class="s2">&quot;distributed-queue-st-g4dn2x</span>
<span class="s2">                             large-2&quot;</span>,
<span class="w">                                 </span><span class="s2">&quot;process_id&quot;</span>:<span class="w"> </span><span class="m">4620</span>
<span class="w">                             </span><span class="o">}</span>
<span class="m">2024</span>-05-18<span class="w"> </span><span class="m">10</span>:03:48<span class="w"> </span>INFO<span class="w"> </span><span class="m">2024</span>-05-18<span class="w"> </span><span class="m">10</span>:03:48<span class="w"> </span><span class="o">[</span>INFO<span class="o">]</span>:<span class="w"> </span>rank<span class="w"> </span><span class="m">1</span><span class="w"> </span>data<span class="w"> </span><span class="o">(</span>before<span class="w"> </span>all-reduce<span class="o">)</span>:<span class="w"> </span>tensor<span class="o">([</span><span class="m">5</span>,<span class="w"> </span><span class="m">9</span>,<span class="w"> </span><span class="m">4</span><span class="o">]</span>,<span class="w"> </span><span class="nv">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="o">)</span><span class="w"> </span>with<span class="w"> </span>device<span class="w"> </span>cuda:0.
b_demo.py:48
<span class="m">2024</span>-05-18<span class="w"> </span><span class="m">10</span>:03:48<span class="w"> </span>INFO<span class="w"> </span><span class="m">2024</span>-05-18<span class="w"> </span><span class="m">10</span>:03:48<span class="w"> </span><span class="o">[</span>INFO<span class="o">]</span>:<span class="w"> </span>rank<span class="w"> </span><span class="m">1</span><span class="w"> </span>data<span class="w"> </span><span class="o">(</span>after<span class="w"> </span>all-reduce<span class="o">)</span>:<span class="w"> </span>tensor<span class="o">([</span><span class="m">9</span>,<span class="w"> </span><span class="m">18</span>,<span class="w"> </span><span class="m">7</span><span class="o">]</span>,<span class="w"> </span><span class="nv">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="o">)</span><span class="w"> </span>with<span class="w"> </span>device<span class="w"> </span>cuda:0.
b_demo.py:50
</pre></div>
</div>
</div>
</div>
</section>
<section id="references-and-further-readings">
<h2><a class="toc-backref" href="#id14" role="doc-backlink">References and Further Readings</a><a class="headerlink" href="#references-and-further-readings" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">PyTorch: Distributed Data Parallel Tutorial</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/notes/ddp.html">How Distributed Data Parallelism is Designed in PyTorch</a></p></li>
<li><p><a class="reference external" href="https://github.com/stanford-cs336/spring2024-assignment2-systems/blob/master/cs336_spring2024_assignment2_systems.pdf">CS336: Language Modeling from Scratch</a></p></li>
</ul>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="pytorch-distributed-data-parallel-tutorial" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">PyTorch: Distributed Data Parallel Tutorial</a></p>
</aside>
<aside class="footnote brackets" id="cs336-spring2024-assignment2-systems" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id3">2</a>)</span>
<p><a class="reference external" href="https://github.com/stanford-cs336/spring2024-assignment2-systems/blob/master/cs336_spring2024_assignment2_systems.pdf">CS336: Language Modeling from Scratch</a></p>
</aside>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./operations/distributed"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="01_notations.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Notations</p>
      </div>
    </a>
    <a class="right-next"
       href="03_how_to_setup_slurm_in_aws.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">How to Setup SLURM and ParallelCluster in AWS</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up">Setting Up</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-code">The Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#command-line-arguments-cpu-and-gloo-backend">Command Line Arguments (CPU And Gloo Backend)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#process-group-initialization">Process Group Initialization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#master-port-and-master-address">Master Port and Master Address</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#backend">Backend</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#global-rank-and-world-size">Global Rank And World Size</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#set-cuda-device">Set CUDA Device</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributed-information">Distributed Information</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-setup-with-cuda">Multi-Node Setup With CUDA</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references-and-further-readings">References and Further Readings</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>