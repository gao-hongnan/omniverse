
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>The Training Phase &#8212; Omniverse</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=bb35926c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script src="../../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=36754332"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-MYW5YKC2WF"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-MYW5YKC2WF');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'transformer/decoder/train_phase';</script>
    <link rel="canonical" href="https://www.gaohongnan.com/transformer/decoder/train_phase.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="How to Calculate the Number of FLOPs in GPT-2" href="../../playbook/how_to_calculate_flops_in_gpt2.html" />
    <link rel="prev" title="Training a Mini-GPT to Learn Two-Digit Addition" href="adder.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Omniverse - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Omniverse - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    🌌 Omniverse: A Journey Through Knowledge
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Notations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notations/machine_learning.html">Machine Learning Notations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Generative Pre-trained Transformer</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Generative Pre-trained Transformers</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="notations.html">Notations</a></li>
<li class="toctree-l2"><a class="reference internal" href="concept.html">The Concept of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="implementation.html">The Implementation of Generative Pre-trained Transformers (GPT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="adder.html">Training a Mini-GPT to Learn Two-Digit Addition</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">The Training Phase</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Playbook</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_calculate_flops_in_gpt2.html">How to Calculate the Number of FLOPs in GPT-2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_inspect_function_and_class_signatures.html">How to Inspect Function and Class Signatures in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/why_cosine_annealing_warmup_stabilize_training.html">Why Does Cosine Annealing With Warmup Stabilize Training?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/why_softmax_preserves_order_translation_invariant_not_invariant_scaling.html">Softmax Preserves Order, Is Translation Invariant But Not Invariant Under Scaling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playbook/how_to_finetune_gpt2.html">How To Fine-Tune GPT-2 To Classify Text</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../deep_learning/training_chronicles/intro.html">Training Chronicles</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../deep_learning/training_chronicles/loss.html">The Loss Landscape</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Software Engineering</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/devops/continuous-integration/concept.html">Continuous Integration (CI) Workflow</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/devops/continuous-integration/styling.html">Styling, Formatting, and Linting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/devops/continuous-integration/testing.html">Testing</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../software_engineering/design_patterns/dependency-inversion-principle.html">Dependency Inversion Principle</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/intro.html">Concurrency, Parallelism and Asynchronous Programming</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/concurrency_parallelism_asynchronous/generator_yield.html">A Rudimentary Introduction to Generator and Yield in Python</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../software_engineering/serving/restful_api/intro.html">RESTful API</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../software_engineering/serving/restful_api/application_banking.html">Application: Designing a RESTful Banking API with FastAPI and SQLAlchemy</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Computer Science</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../computer_science/type_theory/intro.html">Type Theory, A Very Rudimentary Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/01-subtypes.html">Subtypes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/02-type-safety.html">Type Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/03-subsumption.html">Subsumption</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/04-generics.html">Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/05-typevar-bound-constraints.html">Bound and Constraint in Generics and Type Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/06-invariance-covariance-contravariance.html">Invariance, Covariance and Contravariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/07-pep-3124-overloading.html">Function Overloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../computer_science/type_theory/08-pep-661-sentinel-values.html">Sentinel Types</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data Structures and Algorithms</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/complexity_analysis/intro.html">Complexity Analysis</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/complexity_analysis/master_theorem.html">Master Theorem </a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/stack/intro.html">Stack</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/stack/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/intro.html">Linear Search</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/linear_search/concept.html">Concept</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/intro.html">Binary Search</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/concept.html">Concept</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dsa/searching_algorithms/binary_search/problems/875-koko-eating-bananas.html">Koko Eating Bananas</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/01_preliminaries/intro.html">Preliminaries</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/01-fields.html">Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/01_preliminaries/02-systems-of-linear-equations.html">Systems of Linear Equations</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../linear_algebra/02_vectors/intro.html">Vectors</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/01-vector-definition.html">Vector and Its Definition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/02-vector-operation.html">Vector and Its Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/03-vector-norm.html">Vector Norm and Distance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../linear_algebra/02_vectors/04-vector-products.html">A First Look at Vector Products</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References, Resources and Roadmap</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../citations.html">IEEE (Style) Citations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../api/reproducibility.html">API Reference</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/gao-hongnan/omniverse/issues/new?title=Issue%20on%20page%20%2Ftransformer/decoder/train_phase.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/transformer/decoder/train_phase.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>The Training Phase</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autoregressive-self-supervised-learning-paradigm">Autoregressive Self-Supervised Learning Paradigm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#corpus-and-tokenization">Corpus and Tokenization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#token-embedding-and-positional-encoding">Token Embedding and Positional Encoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backbone-architecture">Backbone Architecture</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-process-through-l-decoder-blocks">Iterative Process Through L Decoder Blocks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#first-decoder-block-ell-1">First Decoder Block (<span class="math notranslate nohighlight">\(\ell = 1\)</span>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#subsequent-decoder-blocks-ell-1">Subsequent Decoder Blocks (<span class="math notranslate nohighlight">\(\ell &gt; 1\)</span>)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization-before-projection">Layer Normalization Before Projection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#head">Head</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-layer">Softmax Layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-entropy-loss-function">Cross-Entropy Loss Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#table-of-notations">Table of Notations</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="the-training-phase">
<h1>The Training Phase<a class="headerlink" href="#the-training-phase" title="Link to this heading">#</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#autoregressive-self-supervised-learning-paradigm" id="id2">Autoregressive Self-Supervised Learning Paradigm</a></p></li>
<li><p><a class="reference internal" href="#corpus-and-tokenization" id="id3">Corpus and Tokenization</a></p></li>
<li><p><a class="reference internal" href="#token-embedding-and-positional-encoding" id="id4">Token Embedding and Positional Encoding</a></p></li>
<li><p><a class="reference internal" href="#backbone-architecture" id="id5">Backbone Architecture</a></p></li>
<li><p><a class="reference internal" href="#iterative-process-through-l-decoder-blocks" id="id6">Iterative Process Through L Decoder Blocks</a></p>
<ul>
<li><p><a class="reference internal" href="#first-decoder-block-ell-1" id="id7">First Decoder Block (<span class="math notranslate nohighlight">\(\ell = 1\)</span>)</a></p></li>
<li><p><a class="reference internal" href="#subsequent-decoder-blocks-ell-1" id="id8">Subsequent Decoder Blocks (<span class="math notranslate nohighlight">\(\ell &gt; 1\)</span>)</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#layer-normalization-before-projection" id="id9">Layer Normalization Before Projection</a></p></li>
<li><p><a class="reference internal" href="#head" id="id10">Head</a></p></li>
<li><p><a class="reference internal" href="#softmax-layer" id="id11">Softmax Layer</a></p></li>
<li><p><a class="reference internal" href="#cross-entropy-loss-function" id="id12">Cross-Entropy Loss Function</a></p></li>
<li><p><a class="reference internal" href="#table-of-notations" id="id13">Table of Notations</a></p></li>
</ul>
</nav>
<p>This chapter will cover the training phase of the GPT model, which involves
digging deep into the shapes and dimensions of the tensors that are passed
through the model.</p>
<section id="autoregressive-self-supervised-learning-paradigm">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Autoregressive Self-Supervised Learning Paradigm</a><a class="headerlink" href="#autoregressive-self-supervised-learning-paradigm" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> be the true but unknown distribution of the natural language
space. In the context of unsupervised learning with self-supervision, such as
language modeling, we consider both the inputs and the implicit labels derived
from the same data sequence. Thus, while traditionally we might decompose the
distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> of a supervised learning task into input space
<span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and label space <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>, in this scenario, <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and
<span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> are intrinsically linked, because <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> is a shifted
version of <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>, and so we can consider <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> as a distribution
over <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> only.</p>
<p>Since <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is a distribution, we also define it as a probability
distribution over <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>, and we can write it as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathcal{D} &amp;= \mathbb{P}(\mathcal{X} ; \boldsymbol{\Theta}) \\
            &amp;= \mathbb{P}_{\{\mathcal{X} ; \boldsymbol{\Theta}\}}(\mathbf{x})
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\Theta}\)</span> is the parameter space that defines the distribution
<span class="math notranslate nohighlight">\(\mathbb{P}(\mathcal{X} ; \boldsymbol{\Theta})\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is a sample
from <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> generated by the distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. It is common to
treat <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> as a sequence of tokens (i.e. a sentence is a sequence of
tokens), and we can write <span class="math notranslate nohighlight">\(\mathbf{x} = \left(x_1, x_2, \ldots, x_T\right)\)</span>,
where <span class="math notranslate nohighlight">\(T\)</span> is the length of the sequence.</p>
<p>Given such a sequence <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, the joint probability of the sequence can be
factorized into the product of the conditional probabilities of each token in
the sequence via the
<a class="reference external" href="https://en.wikipedia.org/wiki/Chain_rule_(probability)">chain rule of probability</a>:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(\mathbf{x} ; \boldsymbol{\Theta}) = \prod_{t=1}^T \mathbb{P}(x_t \mid x_1, x_2, \ldots, x_{t-1} ; \boldsymbol{\Theta})
\]</div>
<p>We can do this because natural language are <em>inherently ordered</em>. Such
decomposition allows for <em>tractable sampling</em> from and <em>estimation</em> of the
distribution <span class="math notranslate nohighlight">\(\mathbb{P}(\mathbf{x} ; \boldsymbol{\Theta})\)</span> as well as any
conditionals in the form of
<span class="math notranslate nohighlight">\(\mathbb{P}(x_{t-k}, x_{t-k+1}, \ldots, x_{t} \mid x_{1}, x_{2}, \ldots, x_{t-k-1} ; \boldsymbol{\Theta})\)</span>
<span id="id1">[<a class="reference internal" href="../../bibliography.html#id13" title="Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.">Radford <em>et al.</em>, 2019</a>]</span>.</p>
<p>To this end, consider a corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> with <span class="math notranslate nohighlight">\(N\)</span> sequences
<span class="math notranslate nohighlight">\(\left\{\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{N}\right\}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{S} = \left\{\mathbf{x}_{1}, \mathbf{x}_{2}, \ldots, \mathbf{x}_{N}\right\} \underset{\text{i.i.d.}}{\sim} \mathcal{D}
\]</div>
<p>where each sequence <span class="math notranslate nohighlight">\(\mathbf{x}_{n}\)</span> is a sequence of tokens that are sampled
<span class="math notranslate nohighlight">\(\text{i.i.d.}\)</span> from the distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>.</p>
<p>Then, we can frame the
<a class="reference external" href="https://gao-hongnan.github.io/gaohn-galaxy/probability_theory/08_estimation_theory/maximum_likelihood_estimation/concept.html">likelihood function</a>
<span class="math notranslate nohighlight">\(\hat{\mathcal{L}}(\cdot)\)</span> as the likelihood of observing the sequences in the
corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathcal{L}}\left(\mathcal{S} ; \hat{\boldsymbol{\Theta}}\right) = \prod_{n=1}^N \mathbb{P}(\mathbf{x}_{n} ; \hat{\boldsymbol{\Theta}})
\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Theta}}\)</span> is the estimated parameter space that
approximates the true parameter space <span class="math notranslate nohighlight">\(\boldsymbol{\Theta}\)</span>.</p>
<p>Subsequently, the objective function is now well-defined, to be the maximization
of the likelihood of the sequences in the corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\hat{\boldsymbol{\theta}}^{*} &amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmax}} \hat{\mathcal{L}}\left(\mathcal{S} ; \hat{\boldsymbol{\Theta}}\right) \\
                              &amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmax}} \prod_{n=1}^N \mathbb{P}(\mathbf{x}_{n} ; \hat{\boldsymbol{\Theta}}) \\
                              &amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmax}} \prod_{n=1}^N \prod_{t=1}^{T_n} \mathbb{P}(x_{n, t} \mid x_{n, 1}, x_{n, 2}, \ldots, x_{n, t-1} ; \hat{\boldsymbol{\Theta}}) \\
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(T_n\)</span> is the length of the sequence <span class="math notranslate nohighlight">\(\mathbf{x}_{n}\)</span>.</p>
<p>Owing to the fact that multiplying many probabilities together can lead to
<a class="reference external" href="https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html#numerical-optimization-and-the-negative-log-likelihood">numerical instability</a>
because the product of many probabilities can be very small, it is common and
necessary to use the log-likelihood as the objective function, because it can be
proven that maximizing the log-likelihood is equivalent to maximizing the
likelihood itself.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\hat{\boldsymbol{\theta}}^{*} &amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmax}} \log\left(\hat{\mathcal{L}}\left(\mathcal{S} ; \hat{\boldsymbol{\Theta}}\right)\right) \\
&amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmax}} \sum_{n=1}^N \sum_{t=1}^{T_n} \log \mathbb{P}(x_{n, t} \mid x_{n, 1}, x_{n, 2}, \ldots, x_{n, t-1} ; \hat{\boldsymbol{\Theta}}) \\
\end{aligned}
\end{split}\]</div>
<p>Furthermore, since we are treating the the loss function as a form of
minimization, we can simply negate the log-likelihood to obtain the negative
log-likelihood as the objective function to be minimized,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\hat{\boldsymbol{\theta}}^{*} &amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmin}} \left(-\log\left(\hat{\mathcal{L}}\left(\mathcal{S} ; \hat{\boldsymbol{\Theta}}\right)\right)\right) \\
&amp;= \underset{\hat{\boldsymbol{\theta}} \in \boldsymbol{\Theta}}{\text{argmin}} \left(-\sum_{n=1}^N \sum_{t=1}^{T_n} \log \mathbb{P}(x_{n, t} \mid x_{n, 1}, x_{n, 2}, \ldots, x_{n, t-1} ; \hat{\boldsymbol{\Theta}})\right) \\
\end{aligned}
\end{split}\]</div>
<p>It is worth noting that the objective function is a function of the parameter
space <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Theta}}\)</span>, and not the data <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, so all
analysis such as convergence and consistency will be with respect to the
parameter space <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\Theta}}\)</span>.</p>
<p>To this end, we denote the GPT model <span class="math notranslate nohighlight">\(\mathcal{G}\)</span> to be an <em>autoregressive</em> and
<em>self-supervised learning</em> model that is trained to maximize the likelihood of
observing all data points <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{S}\)</span> via the objective
function <span class="math notranslate nohighlight">\(\hat{\mathcal{L}}\left(\mathcal{S} ; \hat{\boldsymbol{\Theta}}\right)\)</span>
by learning the conditional probability distribution
<span class="math notranslate nohighlight">\(\mathbb{P}(x_t \mid x_{&lt;t} ; \hat{\boldsymbol{\Theta}})\)</span> over the vocabulary
<span class="math notranslate nohighlight">\(\mathcal{V}\)</span> of tokens, conditioned on the contextual preciding tokens
<span class="math notranslate nohighlight">\(x_{&lt;t} = \left(x_1, x_2, \ldots, x_{t-1}\right)\)</span>. We are clear that although
the goal is to model the joint probability distribution of the token sequences,
we can do so by estimating the joint probability distribution via the
conditional probability distributions.</p>
</section>
<section id="corpus-and-tokenization">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Corpus and Tokenization</a><a class="headerlink" href="#corpus-and-tokenization" title="Link to this heading">#</a></h2>
<div class="note admonition">
<p class="admonition-title">Step 1. Corpus</p>
<p>Consider a corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> consisting of <span class="math notranslate nohighlight">\(N\)</span> sequences, denoted as
<span class="math notranslate nohighlight">\({\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N}\)</span>, where each sequence
<span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, x_2, \ldots, x_T) \in \mathcal{S}\)</span> is a sequence of <span class="math notranslate nohighlight">\(T\)</span>
tokens. These tokens are sampled i.i.d. from a true, unknown distribution
<span class="math notranslate nohighlight">\(\mathcal{D}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{S}=\left\{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\right\} \underset{\text { i.i.d. }}{\sim} \mathcal{D}
\]</div>
<p>Each sequence <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{S}\)</span> represents a collection of tokenized
elements (e.g., words or characters), where each token <span class="math notranslate nohighlight">\(x_t\)</span> comes from a finite
vocabulary <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Step 2. Vocabulary and Tokenization</p>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{V} = \{w_1, w_2, \ldots, w_V\}\)</span> be the vocabulary set, where <span class="math notranslate nohighlight">\(w_j\)</span>
is the <span class="math notranslate nohighlight">\(j\)</span>-th token in the vocabulary and <span class="math notranslate nohighlight">\(V = |\mathcal{V}|\)</span> is the size of the
vocabulary. It is worth noting that it is common to train one’s own vocabulary
and tokenizer on the corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, but for simplicity, we assume that
the vocabulary set <span class="math notranslate nohighlight">\(\mathcal{V}\)</span> is predefined.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> be the set of all possible sequences that can be formed by
concatenating tokens from the vocabulary set <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>. Each sequence
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{X}\)</span> is a finite sequence of tokens, and the length of
each sequence is denoted by <span class="math notranslate nohighlight">\(\tau\)</span>. Formally:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{X} = \bigcup_{\tau=1}^{T} \mathcal{V}^{\tau}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{V}^\tau\)</span> represents the set of all sequences of length <span class="math notranslate nohighlight">\(\tau\)</span>
formed by concatenating tokens from <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>, and <span class="math notranslate nohighlight">\(T\)</span> is the maximum
sequence length.</p>
<p>Now, let
<span class="math notranslate nohighlight">\(\mathcal{S} = \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\} \subset \mathcal{X}\)</span>
be a corpus of <span class="math notranslate nohighlight">\(N\)</span> sequences, where each sequence <span class="math notranslate nohighlight">\(\mathbf{x}_n \in \mathcal{X}\)</span>
is a finite sequence of tokens from the vocabulary set <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>.</p>
<p>The tokenizer algorithm <span class="math notranslate nohighlight">\(\mathcal{T}\)</span> is a function that operates on individual
sequences <span class="math notranslate nohighlight">\(\mathbf{x}_n\)</span> from the corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> and maps the tokens to
their corresponding integer indices using the vocabulary set <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{T}: \mathcal{X} \rightarrow \mathbb{N}^{\leq T}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{N}^{\leq T}\)</span> represents the set of all finite sequences of
natural numbers (non-negative integers) with lengths up to <span class="math notranslate nohighlight">\(T\)</span>. The output of
<span class="math notranslate nohighlight">\(\mathcal{T}\)</span> is a tokenized sequence, which is a finite sequence of integer
indices corresponding to the tokens in the input sequence.</p>
<p>To map the tokens to their corresponding integer indices, we define a bijective
mapping function <span class="math notranslate nohighlight">\(f: \mathcal{V} \rightarrow \{1, 2, \ldots, V\}\)</span> such that:</p>
<div class="math notranslate nohighlight">
\[
f(w_j) = j, \quad \forall j \in \{1, 2, \ldots, V\}
\]</div>
<p>where <span class="math notranslate nohighlight">\(f(w_j)\)</span> represents the integer index assigned to the token
<span class="math notranslate nohighlight">\(w_j \in \mathcal{V}\)</span>.</p>
<p>Given a sequence <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, x_2, \ldots, x_\tau) \in \mathcal{X}\)</span>,
where <span class="math notranslate nohighlight">\(\tau \leq T\)</span> is the length of the sequence, the tokenizer algorithm
<span class="math notranslate nohighlight">\(\mathcal{T}\)</span> maps each token <span class="math notranslate nohighlight">\(x_t\)</span> to its corresponding integer index using the
bijective mapping function <span class="math notranslate nohighlight">\(f\)</span>. The tokenized representation of the sequence
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> can be defined as:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{T}(\mathbf{x}) = \left(f(x_1), f(x_2), \ldots, f(x_\tau)\right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(f(x_t)\)</span> is the integer index assigned to the token <span class="math notranslate nohighlight">\(x_t\)</span> based on <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p>In the case where a token <span class="math notranslate nohighlight">\(x_t\)</span> is not present in the vocabulary set
<span class="math notranslate nohighlight">\(\mathcal{V}\)</span>, a special token index, such as <span class="math notranslate nohighlight">\(f(\text{&lt;UNK&gt;})\)</span>, can be assigned
to represent an unknown token.</p>
<p>The tokenizer algorithm <span class="math notranslate nohighlight">\(\mathcal{T}\)</span> can be applied to each sequence
<span class="math notranslate nohighlight">\(\mathbf{x}_n\)</span> in the corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> to obtain the tokenized corpus
<span class="math notranslate nohighlight">\(\mathcal{S}^{\mathcal{T}}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{S}^{\mathcal{T}} = \left\{\mathcal{T}(\mathbf{x}_1), \mathcal{T}(\mathbf{x}_2), \ldots, \mathcal{T}(\mathbf{x}_N)\right\} \subset \mathbb{N}^{\leq T}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{T}(\mathbf{x}_n)\)</span> is the tokenized representation of the
sequence <span class="math notranslate nohighlight">\(\mathbf{x}_n \in \mathcal{S}\)</span>.</p>
<p>The tokenized corpus <span class="math notranslate nohighlight">\(\mathcal{S}^{\mathcal{T}}\)</span> is a set of sequences, where
each sequence is a finite sequence of integer indices representing the tokens in
the original sequences from the corpus <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>.</p>
</div>
</section>
<section id="token-embedding-and-positional-encoding">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Token Embedding and Positional Encoding</a><a class="headerlink" href="#token-embedding-and-positional-encoding" title="Link to this heading">#</a></h2>
<div class="note admonition">
<p class="admonition-title">Step 3. One Hot Encoding</p>
<p>For each sequence <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{S}^{\mathcal{T}}\)</span> in the corpus, we
would apply one hot encoding so that each sample/sequence is transformed to
<span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}} \in \{0, 1\}^{T \times V}\)</span> where <span class="math notranslate nohighlight">\(V\)</span> is the vocabulary
size and <span class="math notranslate nohighlight">\(T\)</span> the pre-defined context window size.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X}^{\text{ohe}} = \begin{bmatrix}
o_{1,1} &amp; o_{1,2} &amp; \cdots &amp; o_{1,V} \\
o_{2,1} &amp; o_{2,2} &amp; \cdots &amp; o_{2,V} \\
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
o_{T,1} &amp; o_{T,2} &amp; \cdots &amp; o_{T,V}
\end{bmatrix} \in \{0, 1\}^{T \times V}
\end{split}\]</div>
<p>Each row <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}_{t} \in \mathbb{R}^{1 \times V}\)</span> represents
the one-hot encoded representation of the token at position <span class="math notranslate nohighlight">\(t\)</span> in the sequence.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Step 4. Token Embedding</p>
<p>Given the one-hot encoded input
<span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}} \in \{0, 1\}^{T \times |\mathcal{V}|}\)</span>, where <span class="math notranslate nohighlight">\(T\)</span> is
the sequence length and <span class="math notranslate nohighlight">\(V = |\mathcal{V}|\)</span> is the vocabulary size, we obtain
the token embedding matrix <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{T \times D}\)</span> by matrix
multiplying <span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span> with the token embedding weight matrix
<span class="math notranslate nohighlight">\(\mathbf{W}_e \in \mathbb{R}^{V \times D}\)</span>, where <span class="math notranslate nohighlight">\(D\)</span> is the embedding
dimension:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{X} &amp;= \mathbf{X}^{\text{ohe}} \operatorname{&#64;} \mathbf{W}_{e} \\
T \times D                      &amp;\leftarrow T \times V \operatorname{&#64;} V \times D  \\
\mathcal{B} \times T \times D   &amp;\leftarrow\mathcal{B} \times T \times V \operatorname{&#64;} V \times D
\end{aligned}
\end{split}\]</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Weight Sharing</p>
<p>Note carefully that with the addition of batch dimension <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> the
matrix multiplication is still well-defined for such tensor in PyTorch because
we are essentially just performing matrix multiplication in <span class="math notranslate nohighlight">\(T \times D\)</span> for
each sequence <span class="math notranslate nohighlight">\(\mathbf{X}_b \in \mathbf{X}^{\mathcal{B}}\)</span> with the same weight
matrix <span class="math notranslate nohighlight">\(\mathbf{W}_{e}\)</span>.</p>
<p>The token embedding weight matrix <span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span> with dimensions <span class="math notranslate nohighlight">\(V \times D\)</span> is
shared across all sequences in the batch. Each sequence <span class="math notranslate nohighlight">\(\mathbf{X}^{(b)}\)</span> in the
batched input tensor <span class="math notranslate nohighlight">\(\mathbf{X}^{\mathcal{B}}\)</span> undergoes the same matrix
multiplication with <span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span> to obtain the corresponding embedded sequence
representation.</p>
<p>The idea of weight sharing is that the same set of parameters (in this case, the
embedding weights) is used for processing multiple instances of the input
(sequences in the batch). Instead of having separate embedding weights for each
sequence, the same embedding matrix is applied to all sequences. This parameter
sharing allows the model to learn a common representation for the tokens across
different sequences.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Step 5. Positional Embedding</p>
<p>In addition to the token embeddings, we incorporate positional information into
the input representation to capture the sequential nature of the input
sequences. Let <span class="math notranslate nohighlight">\(\operatorname{PE}(\cdot)\)</span> denote the positional encoding
function that maps the token positions to their corresponding positional
embeddings.</p>
<p>Given the token embedding matrix <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{T \times D}\)</span>, where
<span class="math notranslate nohighlight">\(T\)</span> is the sequence length and <span class="math notranslate nohighlight">\(D\)</span> is the embedding dimension, we add the
positional embeddings to obtain the position-aware input representation
<span class="math notranslate nohighlight">\(\tilde{\mathbf{X}} \in \mathbb{R}^{T \times D}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\tilde{\mathbf{X}} &amp;= \operatorname{PE}(\mathbf{X}) + \mathbf{X} \\
T \times D                      &amp;\leftarrow T \times D \operatorname{+} T \times D  \\
\mathcal{B} \times T \times D   &amp;\leftarrow \mathcal{B} \times T \times D \operatorname{+} \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>The positional encoding function <span class="math notranslate nohighlight">\(\operatorname{PE}(\cdot)\)</span> can be implemented
in various ways, such as using fixed sinusoidal functions or learned positional
embeddings. For the latter, we can easily replace <span class="math notranslate nohighlight">\(\operatorname{PE}(\cdot)\)</span>
with a learnable positional embedding layer in the model architecture
(<span class="math notranslate nohighlight">\(\mathbf{W}_{p}\)</span>).</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Dropout And Elementwise Operation</p>
<p>At this stage, it is common practice to apply a dropout layer
<span class="math notranslate nohighlight">\(\operatorname{Dropout}(\cdot)\)</span> to the position-aware input representation
<span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span> (or <span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}_{\text{batch}}\)</span> in the case of a
batch). Dropout is a regularization technique that randomly sets a fraction of
the elements in the input tensor to zero during training and is an
<strong><em>element-wise</em></strong> operation that acts <strong><em>independently</em></strong> on each element in
the tensor. This means that each element has a fixed probability (usually
denoted as <span class="math notranslate nohighlight">\(p\)</span>) of being set to zero, regardless of its position or the values
of other elements in the tensor.</p>
<p>Mathematically, for an input tensor <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{T \times D}\)</span>,
elementwise dropout can be expressed as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{X}^{\text{dropout}} &amp;= \mathbf{X} \odot \mathbf{M} \\
T \times D &amp;= T \times D \odot T \times D
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\odot\)</span> denotes the elementwise (Hadamard) product, and
<span class="math notranslate nohighlight">\(\mathbf{M} \in \{0, 1\}^{T \times D}\)</span> is a binary mask tensor of the same shape
as <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. Each element in <span class="math notranslate nohighlight">\(\mathbf{M}\)</span> is independently sampled from a
Bernoulli distribution with probability <span class="math notranslate nohighlight">\(p\)</span> of being 0 (i.e., dropped) and
probability <span class="math notranslate nohighlight">\(1-p\)</span> of being 1 (i.e., retained).</p>
</div>
</section>
<section id="backbone-architecture">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Backbone Architecture</a><a class="headerlink" href="#backbone-architecture" title="Link to this heading">#</a></h2>
<div class="note admonition">
<p class="admonition-title">Step 6. Pre-Layer Normalization For Masked Multi-Head Attention</p>
<p>Before passing the input through the Multi-Head Attention (MHA) layer, we apply
Layer Normalization to the positionally encoded embeddings <span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span>.
This is known as pre-layer Normalization in the more modern GPT architecture (as
opposed to post-layer Normalization, which is applied after the MHA layer).</p>
<p>The Layer Normalization function <span class="math notranslate nohighlight">\(\operatorname{LayerNorm}(\cdot)\)</span> is a
<strong><em>vectorwise</em></strong> operation that operates on the feature dimension <span class="math notranslate nohighlight">\(D\)</span> of the
input tensor. It normalizes the activations to have zero mean and unit variance
across the features for each token independently. The vectorwise nature of Layer
Normalization arises from the fact that it computes the mean and standard
deviation along the feature dimension, requiring <strong>aggregation</strong> of information
across the entire feature vector for each token.</p>
<p>Mathematically, for an input tensor <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{T \times D}\)</span>,
Layer Normalization is applied independently to each row
<span class="math notranslate nohighlight">\(\mathbf{x}_t \in \mathbb{R}^{1 \times D}\)</span>, where <span class="math notranslate nohighlight">\(t \in \{1, 2, \ldots, T\}\)</span>.
The normalization is performed using the following formula:</p>
<div class="math notranslate nohighlight">
\[
\operatorname{LayerNorm}(\mathbf{x}_t) = \frac{\mathbf{x}_t - \mu_t}{\sqrt{\sigma_t^2 + \epsilon}} \odot \gamma + \beta
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu_t \in \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(\sigma_t^2 \in \mathbb{R}\)</span> are the mean and
variance of the features in <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> (broadcasted), respectively,
<span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant for numerical stability,
<span class="math notranslate nohighlight">\(\gamma \in \mathbb{R}^D\)</span> and <span class="math notranslate nohighlight">\(\beta \in \mathbb{R}^D\)</span> are learnable affine
parameters (scale and shift), and <span class="math notranslate nohighlight">\(\odot\)</span> denotes the elementwise product.</p>
<p>Applying Layer Normalization to the positionally encoded embeddings
<span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span> at layer <span class="math notranslate nohighlight">\(\ell\)</span> results in the normalized embeddings
<span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(\ell)}_1 &amp;= \operatorname{LayerNorm}\left(\tilde{\mathbf{X}}\right) \\
T \times D &amp;\leftarrow T \times D \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span> represents the normalized embeddings at layer
<span class="math notranslate nohighlight">\(\ell\)</span>, and the index <span class="math notranslate nohighlight">\(1\)</span> refers to the first sub-layer/sub-step in the decoder
block.</p>
<p>For the first layer (<span class="math notranslate nohighlight">\(\ell = 1\)</span>), <span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span> is the output from Step 4
(Positional Embedding). So we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(1)}_1 &amp;= \operatorname{LayerNorm}\left(\tilde{\mathbf{X}}\right) \\
T \times D &amp;\leftarrow T \times D \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>In code we have:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># [1]</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># [2]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">elementwise_affine</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span>  <span class="c1"># [3]</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>  <span class="c1"># [4]</span>
</pre></div>
</div>
</div>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Line</strong></p></th>
<th class="head"><p><strong>Code</strong></p></th>
<th class="head"><p><strong>Operation Description</strong></p></th>
<th class="head"><p><strong>Input Shape</strong></p></th>
<th class="head"><p><strong>Output Shape</strong></p></th>
<th class="head"><p><strong>Notes</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>[1]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">mean</span> <span class="pre">=</span> <span class="pre">x.mean(dim=-1,</span> <span class="pre">keepdim=True)</span></code></p></td>
<td><p>Computes the mean of <code class="docutils literal notranslate"><span class="pre">x</span></code> along the last dimension.</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times 1\)</span></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">keepdim=True</span></code> ensures the number of dimensions is preserved, facilitating broadcasting in subsequent operations. Mean is computed for each feature vector.</p></td>
</tr>
<tr class="row-odd"><td><p>[2]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">std</span> <span class="pre">=</span> <span class="pre">x.std(dim=-1,</span> <span class="pre">keepdim=True,</span> <span class="pre">unbiased=False)</span></code></p></td>
<td><p>Computes the standard deviation along the last dimension.</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times 1\)</span></p></td>
<td><p>Similar to the mean, <code class="docutils literal notranslate"><span class="pre">std</span></code> is computed per feature vector with unbiased variance estimation disabled (appropriate for normalization purposes).</p></td>
</tr>
<tr class="row-even"><td><p>[3]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">return</span> <span class="pre">self.gamma</span> <span class="pre">*</span> <span class="pre">(x</span> <span class="pre">-</span> <span class="pre">mean)</span> <span class="pre">/</span> <span class="pre">(std</span> <span class="pre">+</span> <span class="pre">self.eps)</span> <span class="pre">+</span> <span class="pre">self.beta</span></code></p></td>
<td><p>Applies the normalization formula with learnable parameters gamma (<span class="math notranslate nohighlight">\(\gamma\)</span>) and beta (<span class="math notranslate nohighlight">\(\beta\)</span>).</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>Element-wise operations are used. <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> are of shape <span class="math notranslate nohighlight">\(D\)</span>, and are broadcasted to match the input shape. This line only executes if <code class="docutils literal notranslate"><span class="pre">elementwise_affine</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p></td>
</tr>
<tr class="row-odd"><td><p>[4]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">return</span> <span class="pre">(x</span> <span class="pre">-</span> <span class="pre">mean)</span> <span class="pre">/</span> <span class="pre">(std</span> <span class="pre">+</span> <span class="pre">self.eps)</span></code></p></td>
<td><p>Applies the normalization formula without learnable parameters.</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>Simple normalization where each element in the feature vector <span class="math notranslate nohighlight">\(x\)</span> is normalized by the corresponding mean and standard deviation.</p></td>
</tr>
</tbody>
</table>
<div class="note admonition">
<p class="admonition-title">Step 7. Masked Multi-Head Self-Attention</p>
<p>Given the normalized input embeddings
<span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1 \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span> from Step
6 (Pre-Layer Normalization), we apply the masked multi-head self-attention
mechanism to compute the output embeddings <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span>, where the
index <span class="math notranslate nohighlight">\(2\)</span> denotes the second sub-layer within the <span class="math notranslate nohighlight">\(\ell\)</span>-th decoder layer
(multi-head attention).</p>
<p>Let <span class="math notranslate nohighlight">\(\operatorname{MaskedMultiHead}^{(\ell)}(\cdot)\)</span> denote the masked
multi-head self-attention function at layer <span class="math notranslate nohighlight">\(\ell\)</span>. The masked multi-head
self-attention operation takes the normalized input embeddings
<span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span> as the query, key, and value matrices, and produces the
output embeddings <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span>.</p>
<p>For the first layer (<span class="math notranslate nohighlight">\(\ell = 1\)</span>), the masked multi-head self-attention operation
can be expressed as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(1)}_2 &amp;= \operatorname{MaskedMultiHead}^{(1)}\left(\mathbf{Z}^{(1)}_1, \mathbf{Z}^{(1)}_1, \mathbf{Z}^{(1)}_1\right) \\
\mathcal{B} \times T \times D &amp;\leftarrow \operatorname{MaskedMultiHead}^{(1)}\left(\mathcal{B} \times T \times D, \mathcal{B} \times T \times D, \mathcal{B} \times T \times D\right) \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\mathbf{Z}^{(1)}_2 \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span>
represents the output embeddings of the masked multi-head self-attention
operation at layer <span class="math notranslate nohighlight">\(1\)</span>, and
<span class="math notranslate nohighlight">\(\mathbf{Z}^{(1)}_1 \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span> represents
the normalized input embeddings from Step 6.</p>
<p>The <span class="math notranslate nohighlight">\(\operatorname{MaskedMultiHead}^{(\ell)}(\cdot)\)</span> function internally
performs the following steps:</p>
<ol class="arabic simple">
<li><p>Linearly projects the input embeddings <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span> into query,
key, and value matrices for each attention head.</p></li>
<li><p>Computes the scaled dot-product attention scores between the query and key
matrices, and applies the attention mask to prevent attending to future
tokens.</p></li>
<li><p>Applies the softmax function to the masked attention scores to obtain the
attention weights.</p></li>
<li><p>Multiplies the attention weights with the value matrices to produce the
output embeddings for each attention head.</p></li>
<li><p>Concatenates the output embeddings from all attention heads and linearly
projects them to obtain the final output embeddings <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span>.</p></li>
</ol>
<p>The specifics of the scaled dot-product attention mechanism and the multi-head
attention operation will be discussed in the next few steps.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Step 7.1. Linear Projections, Query, Key, and Value Matrices</p>
<p>In the masked multi-head self-attention mechanism, the first step is to linearly
project the normalized input embeddings <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span> into query, key,
and value matrices for each attention head. This step is performed using
learnable weight matrices <span class="math notranslate nohighlight">\(\mathbf{W}^{Q, (\ell)}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{W}^{K, (\ell)}\)</span>,
and <span class="math notranslate nohighlight">\(\mathbf{W}^{V, (\ell)}\)</span>.</p>
<p>Mathematically, the linear projections can be expressed as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Q}^{(\ell)} &amp;= \mathbf{Z}^{(\ell)}_1 \mathbf{W}^{Q, (\ell)} ,\quad \mathcal{B} \times T \times D \leftarrow \mathcal{B} \times T \times D \times D \\
\mathbf{K}^{(\ell)} &amp;= \mathbf{Z}^{(\ell)}_1 \mathbf{W}^{K, (\ell)} ,\quad \mathcal{B} \times T \times D \leftarrow \mathcal{B} \times T \times D \times D \\
\mathbf{V}^{(\ell)} &amp;= \mathbf{Z}^{(\ell)}_1 \mathbf{W}^{V, (\ell)} ,\quad \mathcal{B} \times T \times D \leftarrow \mathcal{B} \times T \times D \times D
\end{aligned}
\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{Q}^{(\ell)} \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span> is the
query matrix for the <span class="math notranslate nohighlight">\(\ell\)</span>-th decoder layer.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{K}^{(\ell)} \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span> is the
key matrix for the <span class="math notranslate nohighlight">\(\ell\)</span>-th decoder layer.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{V}^{(\ell)} \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span> is the
value matrix for the <span class="math notranslate nohighlight">\(\ell\)</span>-th decoder layer.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{Q, (\ell)} \in \mathbb{R}^{D \times D}\)</span>,
<span class="math notranslate nohighlight">\(\mathbf{W}^{K, (\ell)} \in \mathbb{R}^{D \times D}\)</span>, and
<span class="math notranslate nohighlight">\(\mathbf{W}^{V, (\ell)} \in \mathbb{R}^{D \times D}\)</span> are the learnable
weight matrices that transform the normalized embeddings into queries, keys,
and values, respectively.</p></li>
<li><p>Again notice that we are using the same weight matrices for all heads,
weight/parameters sharing.</p></li>
</ul>
<p>The linear projections are performed using matrix multiplication between the
normalized input embeddings <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span> and the corresponding weight
matrices. The resulting query, key, and value matrices have the same shape as
the input embeddings: <span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span>.</p>
<p>In the provided code snippet, the linear projections are implemented using the
<code class="docutils literal notranslate"><span class="pre">torch.nn.Linear</span></code> modules <code class="docutils literal notranslate"><span class="pre">self.W_Q</span></code>, <code class="docutils literal notranslate"><span class="pre">self.W_K</span></code>, and <code class="docutils literal notranslate"><span class="pre">self.W_V</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_Q</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>  <span class="c1"># Z @ W_Q = [B, T, D] @ [D, D] = [B, T, D]</span>
<span class="n">K</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_K</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>  <span class="c1"># Z @ W_K = [B, T, D] @ [D, D] = [B, T, D]</span>
<span class="n">V</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_V</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>  <span class="c1"># Z @ W_V = [B, T, D] @ [D, D] = [B, T, D]</span>
</pre></div>
</div>
</div>
<div class="note admonition">
<p class="admonition-title">Step 7.2. Reshaping and Transposing Query, Key, and Value Matrices</p>
<p>Subsequently, we have already known that instead of for loop to compute each
head, we can compute all heads in parallel using matrix operations. The query,
key, and value matrices are split into <span class="math notranslate nohighlight">\(H\)</span> heads, and the attention scores are
computed in parallel. So our aim is simple, we want to reshape the query, key,
and value matrices to include the head dimension, basically splitting the <span class="math notranslate nohighlight">\(D\)</span>
dimension into <span class="math notranslate nohighlight">\(H\)</span> heads. We can denote the reshaping and transposition
operation using tensor index notation which makes it explicit how indices are
permuted and combined:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Q}_{b,t,d} &amp; \rightarrow \mathbf{Q}_{b,t,h,d_q} \quad \text{where } d = h \cdot (D // H) + d_q, \text{ for } h \in [0, H-1] \text{ and } d_q \in [0, D//H-1] \\
\mathbf{Q}_{b,t,h,d_q} &amp; \rightarrow \mathbf{Q}_{b,h,t,d_q} \quad \text{(transpose dimensions)}
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{K}_{b,t,d} &amp; \rightarrow \mathbf{K}_{b,t,h,d_k} \quad \text{where } d = h \cdot (D // H) + d_k, \text{ for } h \in [0, H-1] \text{ and } d_k \in [0, D//H-1] \\
\mathbf{K}_{b,t,h,d_k} &amp; \rightarrow \mathbf{K}_{b,h,t,d_k} \quad \text{(transpose dimensions)}
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{V}_{b,t,d} &amp; \rightarrow \mathbf{V}_{b,t,h,d_v} \quad \text{where } d = h \cdot (D // H) + d_v, \text{ for } h \in [0, H-1] \text{ and } d_v \in [0, D//H-1] \\
\mathbf{V}_{b,t,h,d_v} &amp; \rightarrow \mathbf{V}_{b,h,t,d_v} \quad \text{(transpose dimensions)}
\end{aligned}
\end{split}\]</div>
<p>To this end, we have reshaped and transposed the query, key, and value matrices
as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Q}^{(\ell)} &amp;\in \mathbb{R}^{\mathcal{B} \times T \times D} \rightarrow \mathbf{Q}^{(\ell)}  \in \mathbb{R}^{\mathcal{B} \times T \times H \times D // H} \rightarrow \mathbf{Q}^{(\ell)}  \in \mathbb{R}^{\mathcal{B} \times H \times T \times D // H} \\
\mathbf{K}^{(\ell)}  &amp;\in \mathbb{R}^{\mathcal{B} \times T \times D} \rightarrow \mathbf{K}^{(\ell)}  \in \mathbb{R}^{\mathcal{B} \times T \times H \times D // H} \rightarrow \mathbf{K}^{(\ell)}  \in \mathbb{R}^{\mathcal{B} \times H \times T \times D // H} \\
\mathbf{V}^{(\ell)}  &amp;\in \mathbb{R}^{\mathcal{B} \times T \times D} \rightarrow \mathbf{V}^{(\ell)}  \in \mathbb{R}^{\mathcal{B} \times T \times H \times D // H} \rightarrow \mathbf{V}^{(\ell)}  \in \mathbb{R}^{\mathcal{B} \times H \times T \times D // H}
\end{aligned}
\end{split}\]</div>
<p>In code, the reshaping and transposition operations are performed as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">,</span> <span class="n">D</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">dim0</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># [B, T, D] -&gt; [B, T, H, D // H] -&gt; [B, H, T, D//H]</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">,</span> <span class="n">D</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">dim0</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">V</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">,</span> <span class="n">D</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">H</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">dim0</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">view</span></code> operation reshapes the matrices to include the head dimension, and
the <code class="docutils literal notranslate"><span class="pre">transpose</span></code> operation swaps the sequence and head dimensions to obtain the
desired ordering of dimensions.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Step 7.3. Scaled Dot-Product Attention and Masking</p>
<p>The attention mask matrix <span class="math notranslate nohighlight">\(\mathbf{M} \in \{0, -\infty\}^{T \times T}\)</span> is
initially constructed as a lower triangular matrix of ones and zeros:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{M} = \begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
1 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\
1 &amp; 1 &amp; 1 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; 1 &amp; 1 &amp; \cdots &amp; 1
\end{bmatrix}
\end{split}\]</div>
<p>In this matrix, “1” (conceptually) allows attention, and “0” blocks it. This
mask is broadcastable to the attention scores tensor
<span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span> since this is a typical shape for
multi-head self-attention.</p>
<p>However, before applying the mask, the ones in <span class="math notranslate nohighlight">\(\mathbf{M}\)</span> are typically
replaced with zeros, and the zeros are replaced with a large negative value
(e.g., <span class="math notranslate nohighlight">\(-\infty\)</span>) to effectively set the attention weights of the masked
positions to zero after the softmax operation (which is to mimic the
<code class="docutils literal notranslate"><span class="pre">masked_fill</span></code> operation in the code). We define
<span class="math notranslate nohighlight">\(\mathbf{M}^{-\infty} \in \{0, -\infty\}^{T \times T} = 1 - \mathbf{M}\)</span>, where:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{M}^{-\infty}_{ij} = \left\{\begin{array}{ll}
0 &amp; \text{if } i \geq j \\
-\infty &amp; \text{if } i &lt; j
\end{array} \right\} \quad \text{forms a triangular matrix:} \quad \begin{bmatrix}
0 &amp; -\infty &amp; -\infty &amp; \cdots &amp; -\infty \\
0 &amp; 0 &amp; -\infty &amp; \cdots &amp; -\infty \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; -\infty \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0
\end{bmatrix}
\end{split}\]</div>
<div class="warning admonition">
<p class="admonition-title">Notation Abuse</p>
<p>For the ease of notation, we abuse notation by using <span class="math notranslate nohighlight">\(\mathbf{M}\)</span> to denote the
final mask matrix <span class="math notranslate nohighlight">\(\mathbf{M}^{-\infty}\)</span>.</p>
</div>
<p>The masking operation will then be applied elementwise to the attention scores
tensor <span class="math notranslate nohighlight">\(\mathbf{A}_{s}^{(\ell)}\)</span>, which is first obtained by computing the
scaled dot product between the query matrix <span class="math notranslate nohighlight">\(\mathbf{Q}^{(\ell)}\)</span> and the key
matrix <span class="math notranslate nohighlight">\(\mathbf{K}^{(\ell)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{A}_{s}^{(\ell)} &amp;= \frac{\mathbf{Q}^{(\ell)} (\mathbf{K}^{(\ell)})^T}{\sqrt{D//H}} \\
\mathcal{B} \times H \times T \times T &amp;\leftarrow \frac{\mathcal{B} \times H \times T \times D//H \operatorname{&#64;} \mathcal{B} \times H \times D//H \times T}{\sqrt{D//H}}
\end{aligned}
\end{split}\]</div>
<p>The masking operation is performed using the elementwise sum (<span class="math notranslate nohighlight">\(\oplus\)</span>) between
the attention scores tensor
<span class="math notranslate nohighlight">\(\mathbf{A}_{s}^{(\ell)} \in \mathbb{R}^{\mathcal{B} \times H \times T \times T}\)</span>
and the <strong><em>broadcasted</em></strong> mask matrix
<span class="math notranslate nohighlight">\(\mathbf{M} \in \{0, -\infty\}^{\mathcal{B} \times H \times T \times T}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{A}_{s}^{M, (\ell)} &amp;= \mathbf{A}_{s}^{(\ell)} \oplus \mathbf{M} \\
\mathcal{B} \times H \times T \times T &amp;\leftarrow \mathcal{B} \times H \times T \times T \oplus \mathcal{B} \times H \times T \times T
\end{aligned}
\end{split}\]</div>
<p>The elementwise sum ensures that the attention scores corresponding to future
tokens (positions above the diagonal) are added by <span class="math notranslate nohighlight">\(-\infty\)</span> to effectively
block attention to those positions and added by <span class="math notranslate nohighlight">\(0\)</span> for the rest of the
positions that are allowed to attend to.</p>
<div class="dropdown admonition">
<p class="admonition-title">Alternative Masking</p>
<p>I have some quirks with the above because it “feels weird” to use <span class="math notranslate nohighlight">\(0\)</span> as
“allowed” instead of <span class="math notranslate nohighlight">\(1\)</span>. After all the above formulation neatly fits the
notation. However, an alternative way is to use <span class="math notranslate nohighlight">\(1\)</span> as “allowed” - where we
define <span class="math notranslate nohighlight">\(\mathbf{M}^{-\infty}\)</span> again but this time we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{A}_{s}^{M, (\ell)} &amp;= \mathbf{A}_{s}^{(\ell)} \odot \mathbf{M} + \mathbf{M}^{-\infty} \\
\mathcal{B} \times H \times T \times T &amp;\leftarrow (\mathcal{B} \times H \times T \times T \odot \mathcal{B} \times H \times T \times T) + \mathcal{B} \times H \times T \times T
\end{aligned}
\end{split}\]</div>
<p>The elementwise multiplication <span class="math notranslate nohighlight">\(\mathbf{A}_{s}^{(\ell)} \odot \mathbf{M}\)</span>
preserves the attention scores for the allowed positions (where <span class="math notranslate nohighlight">\(\mathbf{M}\)</span>
is 1) and sets the scores to zero for the masked positions (where <span class="math notranslate nohighlight">\(\mathbf{M}\)</span>
is 0). Then, the elementwise addition with <span class="math notranslate nohighlight">\(\mathbf{M}_{-\infty}\)</span> effectively
pushes the attention scores of the masked positions towards negative infinity,
while leaving the scores of the allowed positions unchanged.</p>
</div>
<p>Finally, the masked attention scores <span class="math notranslate nohighlight">\(\mathbf{A}_{s}^{M, (\ell)}\)</span> are passed
through the softmax function to obtain the attention weights
<span class="math notranslate nohighlight">\(\mathbf{A}_{w}^{(\ell)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{A}_{w}^{(\ell)} &amp;= \operatorname{softmax}\left(\mathbf{A}_{s}^{M, (\ell)}\right) \\
\mathcal{B} \times H \times T \times T &amp;\leftarrow \operatorname{softmax}\left(\mathcal{B} \times H \times T \times T\right) \\
\mathcal{B} \times H \times T \times T &amp;\leftarrow \mathcal{B} \times H \times T \times T
\end{aligned}
\end{split}\]</div>
<p>The softmax function is then applied to the masked attention scores tensor in a
<strong>vectorwise</strong> manner. Specifically, the softmax operation is applied
<strong>independently</strong> to each row of the last two dimensions (<span class="math notranslate nohighlight">\(T \times T\)</span>) for each
batch and head. This ensures that the attention weights for each token position
across the sequence length sum up to 1. The large negative values in the masked
positions ensure that the corresponding attention weights become close to zero
after the softmax operation.</p>
<p>Finally, the context matrix <span class="math notranslate nohighlight">\(\mathbf{C}^{(\ell)}\)</span> is obtained by multiplying the
attention weights <span class="math notranslate nohighlight">\(\mathbf{A}_{w}^{(\ell)}\)</span> with the value matrix
<span class="math notranslate nohighlight">\(\mathbf{V}^{(\ell)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{C}^{(\ell)} &amp;= \mathbf{A}_{w}^{(\ell)} \mathbf{V}^{(\ell)} \\
\mathcal{B} \times H \times T \times D//H &amp;\leftarrow \mathcal{B} \times H \times T \times T \times \mathcal{B} \times H \times T \times D//H
\end{aligned}
\end{split}\]</div>
<p>The resulting context matrix <span class="math notranslate nohighlight">\(\mathbf{C}^{(\ell)}\)</span> contains the attended values
for each head in layer <span class="math notranslate nohighlight">\(\ell\)</span>.</p>
<p>Note <span class="math notranslate nohighlight">\(\mathbf{C}^{(\ell)}\)</span> is the context matrix which is the output of the
self-attention mechanism and it contains <span class="math notranslate nohighlight">\(\operatorname{head}_{\ell, h}^{M}\)</span> for
each head <span class="math notranslate nohighlight">\(h\)</span> in the layer <span class="math notranslate nohighlight">\(\ell\)</span>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">d_q</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">dim0</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">d_q</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span>         <span class="c1"># [B, H, T, d_q] @ [B, H, d_q, T] = [B, H, T, T]</span>
<span class="n">attention_scores</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span> <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">attention_scores</span>     <span class="c1"># [B, H, T, T]</span>

<span class="n">attention_weights</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>        <span class="c1"># [B, H, T, T]</span>
<span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">)</span>         <span class="c1"># [B, H, T, T]</span>

<span class="n">context_vector</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>     <span class="c1"># [B, H, T, T] @ [B, H, T, d_v] = [B, H, T, d_v]</span>
</pre></div>
</div>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Line</strong></p></th>
<th class="head"><p><strong>Code</strong></p></th>
<th class="head"><p><strong>Operation Description</strong></p></th>
<th class="head"><p><strong>Input Shape</strong></p></th>
<th class="head"><p><strong>Output Shape</strong></p></th>
<th class="head"><p><strong>Notes</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>[1]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">d_q</span> <span class="pre">=</span> <span class="pre">query.size(dim=-1)</span></code></p></td>
<td><p>Retrieves the dimension of the query vectors.</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times d_q\)</span></p></td>
<td><p>Scalar value</p></td>
<td><p>The last dimension of the query tensor represents the query vector dimension.</p></td>
</tr>
<tr class="row-odd"><td><p>[2]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">attention_scores</span> <span class="pre">=</span> <span class="pre">torch.matmul(query,</span> <span class="pre">key.transpose(dim0=-2,</span> <span class="pre">dim1=-1))</span> <span class="pre">/</span> <span class="pre">torch.sqrt(torch.tensor(d_q).float())</span></code></p></td>
<td><p>Computes the scaled dot-product attention scores by matrix multiplying the query and key matrices and scaling by the square root of the query dimension.</p></td>
<td><p>Query: <span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times d_q\)</span><br>Key: <span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times d_k\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p>The key matrix is transposed to align the dimensions for matrix multiplication. The scaling factor helps stabilize gradients during training.</p></td>
</tr>
<tr class="row-even"><td><p>[3]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">attention_scores</span> <span class="pre">=</span> <span class="pre">attention_scores.masked_fill(mask</span> <span class="pre">==</span> <span class="pre">0,</span> <span class="pre">float(&quot;-inf&quot;))</span> <span class="pre">if</span> <span class="pre">mask</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">None</span> <span class="pre">else</span> <span class="pre">attention_scores</span></code></p></td>
<td><p>Applies the attention mask to the attention scores. Positions where the mask is 0 are filled with <span class="math notranslate nohighlight">\(-\infty\)</span> to effectively block attention to those positions.</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p>The <code class="docutils literal notranslate"><span class="pre">masked_fill</span></code> operation is an elementwise operation that replaces the attention scores at masked positions with <span class="math notranslate nohighlight">\(-\infty\)</span>. This step is skipped if no mask is provided.</p></td>
</tr>
<tr class="row-odd"><td><p>[4]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">attention_weights</span> <span class="pre">=</span> <span class="pre">attention_scores.softmax(dim=-1)</span></code></p></td>
<td><p>Applies the softmax function to the masked attention scores along the last dimension to obtain the attention weights.</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p>The softmax operation is applied in a vectorwise manner, independently for each row of the last two dimensions (<span class="math notranslate nohighlight">\(T \times T\)</span>) for each batch and head. This ensures that the attention weights for each token position across the sequence length sum up to 1.</p></td>
</tr>
<tr class="row-even"><td><p>[5]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">attention_weights</span> <span class="pre">=</span> <span class="pre">self.dropout(attention_weights)</span></code></p></td>
<td><p>Applies dropout regularization to the attention weights to prevent overfitting.</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p>Dropout randomly sets a fraction of the attention weights to zero during training, which helps improve generalization. This is element-wise operation.</p></td>
</tr>
<tr class="row-odd"><td><p>[6]</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">context_vector</span> <span class="pre">=</span> <span class="pre">torch.matmul(attention_weights,</span> <span class="pre">value)</span></code></p></td>
<td><p>Computes the context vector by matrix multiplying the attention weights with the value matrix.</p></td>
<td><p>Attention Weights: <span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span><br>Value: <span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times d_v\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times d_v\)</span></p></td>
<td><p>The attention weights are used to weight the importance of each token’s value vector. The resulting context vector captures the attended information from the input sequence.</p></td>
</tr>
</tbody>
</table>
<div class="note admonition">
<p class="admonition-title">Step 7.4. Concatenation and Projection</p>
<p>Recall that the output from the masked multi-head self-attention operation,
denoted as <span class="math notranslate nohighlight">\(\mathbf{C}^{(\ell)}\)</span>, has a shape of
<span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times D//H\)</span>, where <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> is the batch
size, <span class="math notranslate nohighlight">\(H\)</span> is the number of attention heads, <span class="math notranslate nohighlight">\(T\)</span> is the sequence length, and
<span class="math notranslate nohighlight">\(D//H\)</span> is the dimension of each head.</p>
<p>To concatenate the heads and obtain a tensor of shape
<span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span>, we first need to transpose the dimensions of
<span class="math notranslate nohighlight">\(\mathbf{C}^{(\ell)}\)</span> from <span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times D//H\)</span> to
<span class="math notranslate nohighlight">\(\mathcal{B} \times T \times H \times D//H\)</span> - necessary to concatenate the heads
along the last dimension (feature dimension).</p>
<p>Using tensor index notation or semi einsum notation, we can denote the
transposition operation as follows:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbf{C}^{(\ell)}_{b,h,t,d} &amp; \rightarrow \mathbf{C}^{(\ell)}_{b,t,h,d} \quad \text{(transpose dimensions)}
\end{aligned}
\]</div>
<p>After transposition, the heads are concatenated along the last dimension
(feature dimension) to obtain a tensor of shape <span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span>.
The concatenation operation can be expressed using the direct sum notation as
follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{C}^{(\ell)}_{\text{concat}} &amp;= \bigoplus_{h=0}^{H-1} \mathbf{C}^{(\ell)}_{b,t,h,:} \\
&amp;= \mathbf{C}^{(\ell)}_{b,t,0,:} \oplus \mathbf{C}^{(\ell)}_{b,t,1,:} \oplus \cdots \oplus \mathbf{C}^{(\ell)}_{b,t,H-1,:} \\
&amp;= \operatorname{head}^{\ell}_1 \oplus \operatorname{head}^{\ell}_2 \oplus \cdots \oplus \operatorname{head}^{\ell}_H \\
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{C}^{(\ell)}_{b,t,h,:}\)</span> represents the tensor slice corresponding
to the <span class="math notranslate nohighlight">\(h\)</span>-th head at batch index <span class="math notranslate nohighlight">\(b\)</span> and time step <span class="math notranslate nohighlight">\(t\)</span>, and <span class="math notranslate nohighlight">\(\oplus\)</span> denotes
the concatenation operation along the feature dimension.</p>
<div class="dropdown admonition">
<p class="admonition-title">Direct Sum Notation Is Concatenation</p>
<p>In the concatenation of attention heads, the direct sum notation (<span class="math notranslate nohighlight">\(\oplus\)</span>) is
used to represent the concatenation operation, not elementwise addition. The
direct sum combines the output tensors from each attention head along a specific
dimension (usually the feature dimension). It is a vectorwise operation that
stacks the tensors along the specified dimension, creating a new tensor with an
increased dimension size.</p>
</div>
<p>The concatenation operation can be summarized as:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbf{C}^{(\ell)} &amp;\in \mathbb{R}^{\mathcal{B} \times T \times H \times D//H} \xrightarrow{\text{concatenate}} \mathbf{C}^{(\ell)}_{\text{concat}} \in \mathbb{R}^{\mathcal{B} \times T \times D}
\end{aligned}
\]</div>
<p>To summarize, the transposition and concatenation operations can be represented
as:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
\mathbf{C}^{(\ell)} &amp;\in \mathbb{R}^{\mathcal{B} \times H \times T \times D//H} \rightarrow \mathbf{C}^{(\ell)}  \in \mathbb{R}^{\mathcal{B} \times T \times H \times D//H} \rightarrow \mathbf{C}^{(\ell)}  \in \mathbb{R}^{\mathcal{B} \times T \times D}
\end{aligned}
\]</div>
<p>Finally, the concatenated tensor is linearly transformed using the projection
matrix <span class="math notranslate nohighlight">\(\mathbf{W}^{O, (\ell)}\)</span> to obtain the output <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(\ell)}_2 &amp;= \mathbf{C}^{(\ell)}_{\text{concat}} \mathbf{W}^{O, (\ell)} \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D \operatorname{&#64;} D \times D \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>In code, these operations can be implemented as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">context_vector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">context_vector</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">dim0</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim1</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>  <span class="c1"># merge all heads together</span>

<span class="n">projected_context_vector</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">resid_dropout</span><span class="p">(</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">context_projection</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">context_vector</span><span class="p">)</span>  <span class="c1"># [B, T, D] @ [D, D] = [B, T, D]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>To this end, for layer <span class="math notranslate nohighlight">\(\ell=1\)</span>, we would have the output tensor
<span class="math notranslate nohighlight">\(\mathbf{Z}^{(1)}_2\)</span> - which becomes the input to the next sub-layer within the
same block <span class="math notranslate nohighlight">\(\ell\)</span>. Optionally, we can apply a projection dropout to the output
tensor <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span> before passing it to the next sub-layer.</p>
</div>
<div class="note admonition">
<p class="admonition-title">Step 8. Residual Connection</p>
<p>In the Transformer/GPT architecture, residual connections are used to facilitate
the flow of information and gradients throughout the network. The residual
connection in the decoder block is added between the input to the block and the
output of the Masked Multi-Head Attention layer.</p>
<p>For the first decoder block (<span class="math notranslate nohighlight">\(\ell = 1\)</span>), the residual connection is added
between the positionally encoded embeddings <span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span> and the output
of the Masked Multi-Head Attention layer <span class="math notranslate nohighlight">\(\mathbf{Z}^{(1)}_2\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(1)}_{3} &amp;= \tilde{\mathbf{X}} + \text{MaskedMultiHead}^{(1)}\left(\text{LayerNorm}\left(\tilde{\mathbf{X}}\right), \text{LayerNorm}\left(\tilde{\mathbf{X}}\right), \text{LayerNorm}\left(\tilde{\mathbf{X}}\right)\right) \\
&amp;= \tilde{\mathbf{X}} + \text{MaskedMultiHead}^{(1)}\left(\mathbf{Z}^{(1)}_{1}, \mathbf{Z}^{(1)}_{1}, \mathbf{Z}^{(1)}_{1}\right) \\
&amp;= \tilde{\mathbf{X}} + \mathbf{Z}^{(1)}_{2} \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D + \mathcal{B} \times T \times D \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{Z}^{(1)}_{3}\)</span> represents the output of the residual connection
for the first decoder block, <span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span> is the positionally encoded
embeddings, and <span class="math notranslate nohighlight">\(\mathbf{Z}^{(1)}_{2}\)</span> is the output of the Masked Multi-Head
Attention layer.</p>
<p>For subsequent decoder blocks (<span class="math notranslate nohighlight">\(\ell &gt; 1\)</span>), the residual connection is added
between the output of the previous decoder block
<span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell-1)}_{\text{out}}\)</span> and the output of the Masked Multi-Head
Attention layer <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span> of the current block:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z}^{(\ell)}_3 = \mathbf{Z}^{(\ell-1)}_{\text{out}} + \mathbf{Z}^{(\ell)}_2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_3\)</span> represents the output of the residual connection
for the <span class="math notranslate nohighlight">\(\ell\)</span>-th decoder block, <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell-1)}_{\text{out}}\)</span> is the
output of the previous decoder block after the Position-wise Feed-Forward
Network and the second residual connection, and <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span> is the
output of the Masked Multi-Head Attention layer of the current block.</p>
<p>The complete set of equations for the <span class="math notranslate nohighlight">\(\ell\)</span>-th decoder block (<span class="math notranslate nohighlight">\(\ell &gt; 1\)</span>) can
be summarized as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(\ell)}_1 &amp;= \text{LayerNorm}\left(\mathbf{Z}^{(\ell-1)}_{\text{out}}\right) \\
\mathbf{Z}^{(\ell)}_2 &amp;= \text{MaskedMultiHead}^{(\ell)}\left(\mathbf{Z}^{(\ell)}_1, \mathbf{Z}^{(\ell)}_1, \mathbf{Z}^{(\ell)}_1\right) \\
\mathbf{Z}^{(\ell)}_3 &amp;= \mathbf{Z}^{(\ell-1)}_{\text{out}} + \mathbf{Z}^{(\ell)}_2
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span> represents the output of the Layer Normalization
step, <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span> represents the output of the Masked Multi-Head
Attention layer, and <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_3\)</span> represents the output of the
residual connection.</p>
<p>The residual connection allows the model to learn the identity function more
easily, enabling the flow of information and gradients across multiple layers.
By adding the input of the block to the output of the Masked Multi-Head
Attention layer, the model can choose to either learn new information from the
attention mechanism or retain the original input information if it is already
sufficient.</p>
<p>In code, the whole series of operation up till now is simply:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="n">z</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ln_1</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="note admonition">
<p class="admonition-title">Step 9. Pre-Layer Normalization For Position-wise Feed-Forward Network</p>
<p>After the masked multi-head attention block and the residual connection, the
next step is to apply the position-wise feed-forward network (FFN) to the output
of the self-attention mechanism and the residual block <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_3\)</span>.
However, before applying the FFN, we perform pre-layer normalization on the
input to the FFN.</p>
<p>Mathematically, the pre-layer normalization step for the FFN can be expressed
as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(\ell)}_4 &amp;= \operatorname{LayerNorm}\left(\mathbf{Z}^{(\ell)}_3\right) \\
\mathcal{B} \times T \times D &amp;\leftarrow \operatorname{LayerNorm}\left(\mathcal{B} \times T \times D\right) \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_4\)</span> represents the normalized input to the FFN at
layer <span class="math notranslate nohighlight">\(\ell\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_3\)</span> is the output of the residual
connection from the previous step.</p>
<p>As discussed in Step 6, the Layer Normalization function
<span class="math notranslate nohighlight">\(\operatorname{LayerNorm}(\cdot)\)</span> is a vectorwise operation that operates on the
feature dimension <span class="math notranslate nohighlight">\(D\)</span> of the input tensor. It normalizes the activations to have
zero mean and unit variance across the features for each token independently.
It’s important to note that the pre-layer normalization step is applied
independently to each token
<span class="math notranslate nohighlight">\(\mathbf{Z}_{3, t}^{(\ell)} \in \mathbf{Z}_3^{(\ell)}\)</span> in the sequence with
shape <span class="math notranslate nohighlight">\(T \times D\)</span>, where <span class="math notranslate nohighlight">\(T\)</span> is the sequence length and <span class="math notranslate nohighlight">\(D\)</span> is the feature
dimension, normalizing the features across the feature dimension <span class="math notranslate nohighlight">\(D\)</span>.</p>
<p>For the first layer (<span class="math notranslate nohighlight">\(\ell = 1\)</span>), the pre-layer normalization step can be
written as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z}^{(1)}_4 = \operatorname{LayerNorm}\left(\mathbf{Z}^{(1)}_3\right)
\]</div>
</div>
<div class="note admonition">
<p class="admonition-title">Step 10. Position-wise Feed-Forward Network</p>
<p>Given the normalized input <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_4\)</span> to the Position-wise
Feed-Forward Network (FFN) in layer <span class="math notranslate nohighlight">\(\ell\)</span>, the FFN applies two linear
transformations with a GELU activation function in between. The operations
within the FFN can be mathematically represented as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{FF, (\ell)}_1 &amp;= \text{GELU}\left(\mathbf{Z}^{(\ell)}_4 \mathbf{W}^{FF, (\ell)}_1 + \mathbf{b}^{FF, (\ell)}_1\right) \\
\mathcal{B} \times T \times d_{\text{ff}} &amp;\leftarrow \text{GELU}\left(\mathcal{B} \times T \times D \operatorname{&#64;} D \times d_{\text{ff}} + d_{\text{ff}}\right) \\
\mathcal{B} \times T \times d_{\text{ff}} &amp;\leftarrow \mathcal{B} \times T \times d_{\text{ff}} \\
\mathbf{Z}^{(\ell)}_5 &amp;= \mathbf{Z}^{FF, (\ell)}_1 \mathbf{W}^{FF, (\ell)}_2 + \mathbf{b}^{FF, (\ell)}_2 \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times d_{\text{ff}} \operatorname{&#64;} d_{\text{ff}} \times D + D \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{FF, (\ell)}_1 \in \mathbb{R}^{D \times d_{\text{ff}}}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{b}^{FF, (\ell)}_1 \in \mathbb{R}^{d_{\text{ff}}}\)</span> are the weights
and biases of the first linear transformation, respectively.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{FF, (\ell)}_2 \in \mathbb{R}^{d_{\text{ff}} \times D}\)</span> and
<span class="math notranslate nohighlight">\(\mathbf{b}^{FF, (\ell)}_2 \in \mathbb{R}^{D}\)</span> are the weights and biases of
the second linear transformation, respectively.</p></li>
<li><p><span class="math notranslate nohighlight">\(d_{\text{ff}}\)</span> is the dimensionality of the hidden layer in the FFN, which
is typically larger than the input dimensionality <span class="math notranslate nohighlight">\(D\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{GELU}(\cdot)\)</span> denotes the Gaussian Error Linear Unit
activation function.</p></li>
</ul>
<p>Note the slight abuse of notation where <span class="math notranslate nohighlight">\(\mathbf{Z}^{FF, (\ell)}_1\)</span> is used to
denote the intermediate output of the first linear transformation in the FFN.
This should not be confused with the earlier notation <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span>.</p>
<p>For the first layer (<span class="math notranslate nohighlight">\(\ell = 1\)</span>), the FFN operations can be written as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{FF, (1)}_1 &amp;= \operatorname{GELU}\left(\mathbf{Z}^{(1)}_4 \mathbf{W}^{FF, (1)}_1 + \mathbf{b}^{FF, (1)}_1\right) \\
\mathbf{Z}^{(1)}_5 &amp;= \mathbf{Z}^{FF, (1)}_1 \mathbf{W}^{FF, (1)}_2 + \mathbf{b}^{FF, (1)}_2
\end{aligned}
\end{split}\]</div>
</div>
<div class="note admonition">
<p class="admonition-title">Step 11. Residual Connection</p>
<p>After obtaining the output <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_5\)</span> from the Position-wise
Feed-Forward Network (FFN) in layer <span class="math notranslate nohighlight">\(\ell\)</span>, the final step in the decoder block
is to apply a residual connection.</p>
<p>Mathematically, this step can be represented as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(\ell)}_{\text{out}} &amp;= \mathbf{Z}^{(\ell)}_3 + \mathbf{Z}^{(\ell)}_5 \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D + \mathcal{B} \times T \times D \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_3 \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span> is
the output of the residual connection from the Masked Multi-Head Attention
block (Step 8).</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_5 \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span> is
the output from the FFN (Step 10).</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_{\text{out}} \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span>
is the output of the decoder block at layer <span class="math notranslate nohighlight">\(\ell\)</span> and serves as the input
to the next decoder block (<span class="math notranslate nohighlight">\(\ell + 1\)</span>).</p></li>
</ul>
<p>The residual connection is performed by adding the output of the Masked
Multi-Head Attention block <span class="math notranslate nohighlight">\(\left(\mathbf{Z}^{(\ell)}_3\right)\)</span> and the output
of the FFN <span class="math notranslate nohighlight">\(\left(\mathbf{Z}^{(\ell)}_5\right)\)</span>. This addition operation is
element-wise, where corresponding elements of the two tensors are added
together.</p>
<p>For the first layer (<span class="math notranslate nohighlight">\(\ell = 1\)</span>), the residual connection step can be written
as:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z}^{(1)}_{\text{out}} = \mathbf{Z}^{(1)}_3 + \mathbf{Z}^{(1)}_5
\]</div>
<p>The output of this step, <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_{\text{out}}\)</span>, becomes the input
to the next decoder block.</p>
</div>
</section>
<section id="iterative-process-through-l-decoder-blocks">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Iterative Process Through L Decoder Blocks</a><a class="headerlink" href="#iterative-process-through-l-decoder-blocks" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{Z}^{(1)}_{\text{out}}\)</span> be the output of the first decoder block.
This output becomes the input to the next decoder block, and the process
continues iteratively. Each decoder block builds upon the output of the previous
block through a series of mathematical transformations. The subscript notation
<span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_i\)</span> indicates the i-th step output of the <span class="math notranslate nohighlight">\(\ell\)</span>-th decoder
block.</p>
<section id="first-decoder-block-ell-1">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">First Decoder Block (<span class="math notranslate nohighlight">\(\ell = 1\)</span>)</a><a class="headerlink" href="#first-decoder-block-ell-1" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(1)}_1 &amp;= \operatorname{LayerNorm}\left(\tilde{\mathbf{X}}\right) &amp; \text{(Initial normalization of inputs)} \\
\mathbf{Z}^{(1)}_2 &amp;= \operatorname{MaskedMultiHead}\left(\mathbf{Z}^{(1)}_1, \mathbf{Z}^{(1)}_1, \mathbf{Z}^{(1)}_1\right) &amp; \text{(Self-attention mechanism)} \\
\mathbf{Z}^{(1)}_3 &amp;= \tilde{\mathbf{X}} + \mathbf{Z}^{(1)}_2 &amp; \text{(Addition of the first residual connection)} \\
\mathbf{Z}^{(1)}_4 &amp;= \operatorname{LayerNorm}\left(\mathbf{Z}^{(1)}_3\right) &amp; \text{(Normalization before FFN)}\\
\mathbf{Z}^{(1)}_5 &amp;= \operatorname{FFN}\left(\mathbf{Z}^{(1)}_4\right) &amp; \text{(Feed-forward network)}\\
\mathbf{Z}^{(1)}_{\text{out}} &amp;= \mathbf{Z}^{(1)}_3 + \mathbf{Z}^{(1)}_5 &amp; \text{(Second residual connection)}
\end{aligned}
\end{split}\]</div>
</section>
<section id="subsequent-decoder-blocks-ell-1">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">Subsequent Decoder Blocks (<span class="math notranslate nohighlight">\(\ell &gt; 1\)</span>)</a><a class="headerlink" href="#subsequent-decoder-blocks-ell-1" title="Link to this heading">#</a></h3>
<p>For each subsequent decoder block <span class="math notranslate nohighlight">\(\ell\)</span>, the output of the previous block’s
final step <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell-1)}_{\text{out}}\)</span> serves as the input for the
current block’s operations.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{(\ell)}_1 &amp;= \operatorname{LayerNorm}\left(\mathbf{Z}^{(\ell-1)}_{\text{out}}\right) &amp; \text{(Normalization of previous block's output)} \\
\mathbf{Z}^{(\ell)}_2 &amp;= \operatorname{MaskedMultiHead}\left(\mathbf{Z}^{(\ell)}_1, \mathbf{Z}^{(\ell)}_1, \mathbf{Z}^{(\ell)}_1\right) &amp; \text{(Self-attention mechanism)} \\
\mathbf{Z}^{(\ell)}_3 &amp;= \mathbf{Z}^{(\ell-1)}_{\text{out}} + \mathbf{Z}^{(\ell)}_2 &amp; \text{(First residual connection post self-attention)} \\
\mathbf{Z}^{(\ell)}_4 &amp;= \operatorname{LayerNorm}\left(\mathbf{Z}^{(\ell)}_3\right) &amp; \text{(Normalization before FFN)}\\
\mathbf{Z}^{(\ell)}_5 &amp;= \operatorname{FFN}\left(\mathbf{Z}^{(\ell)}_4\right) &amp; \text{(Feed-forward network)}\\
\mathbf{Z}^{(\ell)}_{\text{out}} &amp;= \mathbf{Z}^{(\ell)}_3 + \mathbf{Z}^{(\ell)}_5 &amp; \text{(Second residual connection post FFN)}
\end{aligned}
\end{split}\]</div>
<p>After processing the input through a total of <span class="math notranslate nohighlight">\(L\)</span> decoder blocks, the final
output is denoted as <span class="math notranslate nohighlight">\(\mathbf{Z}^{(L)}_{\text{out}}\)</span>, which is the output of the
last decoder block. The shape of <span class="math notranslate nohighlight">\(\mathbf{Z}^{(L)}_{\text{out}}\)</span> is
<span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span>, where <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> is the batch size, <span class="math notranslate nohighlight">\(T\)</span> is
the sequence length, and <span class="math notranslate nohighlight">\(D\)</span> is the hidden dimension.</p>
</section>
</section>
<section id="layer-normalization-before-projection">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Layer Normalization Before Projection</a><a class="headerlink" href="#layer-normalization-before-projection" title="Link to this heading">#</a></h2>
<div class="note admonition">
<p class="admonition-title">Step 10. Layer Normalization Before Projection</p>
<p>The final output of the decoder block <span class="math notranslate nohighlight">\(\mathbf{Z}^{(L)}_{\text{out}}\)</span> undergoes
a layer normalization step before being projected to the vocabulary space. This
step is commonly referred to as the “pre-projection layer normalization” or
“final layer normalization” in the context of the Transformer/GPT architecture.</p>
<p>The pre-projection/head layer normalization can be represented as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}_{\text{pre-head}} &amp;= \operatorname{LayerNorm}\left(\mathbf{Z}^{(L)}_{\text{out}}\right) \\
\mathcal{B} \times T \times D &amp;\leftarrow \operatorname{LayerNorm}\left(\mathcal{B} \times T \times D\right) \\
\mathcal{B} \times T \times D &amp;\leftarrow \mathcal{B} \times T \times D
\end{aligned}
\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(L)}_{\text{out}} \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span>
is the output of the last decoder block (<span class="math notranslate nohighlight">\(\ell = L\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(\operatorname{LayerNorm}(\cdot)\)</span> denotes the Layer Normalization operation,
which normalizes the activations across the feature dimension <span class="math notranslate nohighlight">\(D\)</span> for each
token independently.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Z}_{\text{pre-proj}} \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span>
is the normalized output, which is ready to be projected to the vocabulary
space.</p></li>
</ul>
<p>The usual purpose of applying layer normalization before the projection step is
to stabilize the activations and improve training stability.</p>
</div>
</section>
<section id="head">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">Head</a><a class="headerlink" href="#head" title="Link to this heading">#</a></h2>
<p>The final step in the GPT architecture is to project the normalized output of
the last decoder block, <span class="math notranslate nohighlight">\(\mathbf{Z}_{\text{pre-head}}\)</span>, to the vocabulary space.
This projection is performed using a linear transformation, where the weights of
the projection layer are denoted as
<span class="math notranslate nohighlight">\(\mathbf{W}_{s} \in \mathbb{R}^{D \times V}\)</span>. The subscript <span class="math notranslate nohighlight">\(s\)</span> indicates that
this is the projection layer before the softmax operation, and <span class="math notranslate nohighlight">\(V\)</span> represents
the size of the vocabulary.</p>
<p>Mathematically, the projection operation can be expressed as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z} &amp;= \mathbf{Z}_{\text{pre-head}} \mathbf{W}_{s} \\
\mathcal{B} \times T \times V &amp;\leftarrow \mathcal{B} \times T \times D \times \operatorname{&#64;} D \times V \\
\mathcal{B} \times T \times V &amp;\leftarrow \mathcal{B} \times T \times V
\end{aligned}
\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{Z}_{\text{pre-head}} \in \mathbb{R}^{\mathcal{B} \times T \times D}\)</span>
is the normalized output from the pre-projection layer normalization step
(Step 12).</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}_{s} \in \mathbb{R}^{D \times V}\)</span> is the weight matrix of the
projection layer, which maps the hidden dimension <span class="math notranslate nohighlight">\(D\)</span> to the vocabulary size
<span class="math notranslate nohighlight">\(V\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Z} \in \mathbb{R}^{\mathcal{B} \times T \times V}\)</span> is the resulting
logits tensor, representing the unnormalized scores for each token in the
vocabulary at each position in the sequence.</p></li>
</ul>
<p>The purpose of the projection layer is to map the hidden representations from
the decoder to the vocabulary space, allowing the model to generate probability
distributions over the vocabulary for each token position. The logits tensor
<span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> can be further processed by applying a softmax function to obtain
the final probability distribution for token prediction.</p>
</section>
<section id="softmax-layer">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Softmax Layer</a><a class="headerlink" href="#softmax-layer" title="Link to this heading">#</a></h2>
<p>The softmax function is applied to the logits tensor
<span class="math notranslate nohighlight">\(\mathbf{Z} \in \mathbb{R}^{\mathcal{B} \times T \times V}\)</span> to obtain the
predicted probability distribution over the vocabulary for each token position
in the sequence. The softmax operation is performed vector-wise along the
vocabulary dimension (i.e., the last dimension) of <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> independently
for each token position and each instance in the batch.</p>
<p>The softmax operation can be defined as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{P} &amp;= \operatorname{softmax}(\mathbf{Z}) \\
\mathbf{P}_{b,t,v} &amp;= \frac{\exp(\mathbf{Z}_{b,t,v})}{\sum_{v'=1}^{V} \exp(\mathbf{Z}_{b,t,v'})} \\
\mathcal{B} \times T \times V &amp;\leftarrow \mathcal{B} \times T \times V
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{P} \in \mathbb{R}^{\mathcal{B} \times T \times V}\)</span> is the
resulting probability tensor, and <span class="math notranslate nohighlight">\(\mathbf{P}_{b,t,v}\)</span> represents the predicted
probability of token <span class="math notranslate nohighlight">\(v\)</span> at position <span class="math notranslate nohighlight">\(t\)</span> in the sequence for batch instance <span class="math notranslate nohighlight">\(b\)</span>.
The softmax function ensures that the probabilities sum to 1 along the
vocabulary dimension for each token position and each batch instance, i.e.,
<span class="math notranslate nohighlight">\(\sum_{v=1}^{V} \mathbf{P}_{b,t,v} = 1\)</span> for all <span class="math notranslate nohighlight">\(b \in {1, \ldots, \mathcal{B}}\)</span>
and <span class="math notranslate nohighlight">\(t \in {1, \ldots, T}\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_close</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
<p>The softmax operation is applied independently to each row of the last two
dimensions <span class="math notranslate nohighlight">\((T \times V)\)</span> of the logits tensor <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>, while the batch
dimension <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> remains unchanged. This means that for each batch
instance <span class="math notranslate nohighlight">\(b\)</span> and each token position <span class="math notranslate nohighlight">\(t\)</span>, the softmax function takes the
corresponding row vector <span class="math notranslate nohighlight">\(\mathbf{Z}_{b,t,:} \in \mathbb{R}^{1 \times V}\)</span> and
computes the predicted probability distribution
<span class="math notranslate nohighlight">\(\mathbf{P}_{b,t,:} \in \mathbb{R}^{1 \times V}\)</span> over the vocabulary.</p>
</section>
<section id="cross-entropy-loss-function">
<h2><a class="toc-backref" href="#id12" role="doc-backlink">Cross-Entropy Loss Function</a><a class="headerlink" href="#cross-entropy-loss-function" title="Link to this heading">#</a></h2>
<p>The cross-entropy loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is used to measure the
dissimilarity between the predicted probability distribution <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> and
the true token distribution <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>. The true token distribution is
typically represented as a one-hot encoded tensor
<span class="math notranslate nohighlight">\(\mathbf{Y} \in \{0, 1\}^{\mathcal{B} \times T \times V}\)</span> (though in practice we
just pass in a non one-hot encoded version).</p>
<p>The one-hot encoded tensor <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> has the same shape as the predicted
probability tensor <span class="math notranslate nohighlight">\(\mathbf{P}\)</span>, i.e., <span class="math notranslate nohighlight">\(\mathcal{B} \times T \times V\)</span>. For each
batch instance <span class="math notranslate nohighlight">\(b\)</span> and each token position <span class="math notranslate nohighlight">\(t\)</span>, the corresponding row vector
<span class="math notranslate nohighlight">\(\mathbf{Y}_{b,t,:} \in {0, 1}^{1 \times V}\)</span> represents the true token
distribution, where the element corresponding to the true token is set to 1, and
all other elements are set to 0.</p>
<p>The one-hot encoded tensor <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> can be defined as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{Y}_{b,t,v} = \begin{cases}
1, &amp; \text{if } v = v^*_{b,t} \\
0, &amp; \text{otherwise}
\end{cases}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(v^*_{b,t}\)</span> denotes the true token at position <span class="math notranslate nohighlight">\(t\)</span> in the sequence for
batch instance <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p>The cross-entropy loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is then defined as:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L} = -\frac{1}{\mathcal{B} \cdot T} \sum_{b=1}^{\mathcal{B}} \sum_{t=1}^{T} \sum_{v=1}^{V} \mathbf{Y}_{b,t,v} \log(\mathbf{P}_{b,t,v})
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{Y}_{b,t,v}\)</span> is the one-hot encoded true token indicator, and
<span class="math notranslate nohighlight">\(\mathbf{P}_{b,t,v}\)</span> is the predicted probability of token <span class="math notranslate nohighlight">\(v\)</span> at position <span class="math notranslate nohighlight">\(t\)</span>
in the sequence for batch instance <span class="math notranslate nohighlight">\(b\)</span>. Note carefully that the sum is over all
batch instances, all token positions, and all tokens in the vocabulary.</p>
</section>
<section id="table-of-notations">
<h2><a class="toc-backref" href="#id13" role="doc-backlink">Table of Notations</a><a class="headerlink" href="#table-of-notations" title="Link to this heading">#</a></h2>
<p>In the table below, we will not add notational burden by adding superscript
<span class="math notranslate nohighlight">\(\mathcal{B}\)</span> to indicate a certain tensor is batched. We would just assume that
all tensors are batched unless otherwise stated.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Matrix Description</p></th>
<th class="head"><p>Symbol</p></th>
<th class="head"><p>Dimensions</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Batched Input Tensor</p></td>
<td><p><span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}^{\mathcal{B}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(B \times T \times D\)</span></p></td>
<td><p>A batched tensor containing <span class="math notranslate nohighlight">\(B\)</span> input sequences, each sequence is of shape <span class="math notranslate nohighlight">\(T \times D\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p>Batched Embedding Matrix Sequence <span class="math notranslate nohighlight">\(b\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}^{(b)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times D\)</span></p></td>
<td><p>The token and positional embedding vector for the <span class="math notranslate nohighlight">\(b\)</span>-th input sequence of the batch.</p></td>
</tr>
<tr class="row-even"><td><p>Batched Embedding Vector for Token <span class="math notranslate nohighlight">\(t\)</span> in Sequence <span class="math notranslate nohighlight">\(b\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}^{(b)}_t\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(1 \times D\)</span></p></td>
<td><p>The token and positional embedding vector for the token at position <span class="math notranslate nohighlight">\(t\)</span> in the <span class="math notranslate nohighlight">\(b\)</span>-th input sequence of the batch.</p></td>
</tr>
<tr class="row-odd"><td><p>One-Hot Encoded Input Matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}^{\text{ohe}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times V\)</span></p></td>
<td><p>Each row corresponds to a one-hot encoded vector representing a token in the sequence for a batch of size <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p>Token Embedding Matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}_e\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(V \times D\)</span></p></td>
<td><p>Each row is the embedding vector of the corresponding token in the vocabulary.</p></td>
</tr>
<tr class="row-odd"><td><p>Token Embedded Input Matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{X}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>Each row is the token embedding vector of the corresponding token in the input sequence for a batch of size <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p>Positional Encoding Matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(\operatorname{PE}(\cdot)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times D\)</span></p></td>
<td><p>Matrix with positional encoding vectors for each position in the sequence, computed using sinusoidal functions or learned positional embeddings.</p></td>
</tr>
<tr class="row-odd"><td><p>Output of Positional Encoding Layer</p></td>
<td><p><span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The resultant embeddings matrix after adding positional encoding to the token embedded input matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. Each row now includes token and positional information.</p></td>
</tr>
<tr class="row-even"><td><p>First Layer Normalized Input</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(1)}_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The output of the initial layer normalization applied to <span class="math notranslate nohighlight">\(\tilde{\mathbf{X}}\)</span>, serving as the input to the first decoder block’s masked multi-head attention mechanism.</p></td>
</tr>
<tr class="row-odd"><td><p>Query, Key, Value Matrices (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Q}^{(\ell)}, \mathbf{K}^{(\ell)}, \mathbf{V}^{(\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times D//H\)</span></p></td>
<td><p>The query, key, and value matrices obtained by linearly projecting <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span> using learned weights <span class="math notranslate nohighlight">\(\mathbf{W}^{Q, (\ell)}, \mathbf{W}^{K, (\ell)}, \mathbf{W}^{V, (\ell)}\)</span>, and then splitting into <span class="math notranslate nohighlight">\(H\)</span> attention heads. <strong>Note that the last dimension must be the same for query and key for the dot product to be valid, but can differ for values.</strong></p></td>
</tr>
<tr class="row-even"><td><p>Attention Scores (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{A}^{(\ell)}_s\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p>The scaled dot-product attention scores computed between the query and key matrices, before applying the attention mask.</p></td>
</tr>
<tr class="row-odd"><td><p>Attention Mask</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{M}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(T \times T \overset{\text{broadcast}}{\rightarrow} \mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p>A binary mask matrix used to prevent attending to future tokens. It has a lower triangular structure with <span class="math notranslate nohighlight">\(-\infty\)</span> for future positions and <span class="math notranslate nohighlight">\(0\)</span> for allowed positions. Note that <span class="math notranslate nohighlight">\(1\)</span> for allowed positions can also be used, one just need to handle it in code.</p></td>
</tr>
<tr class="row-even"><td><p>Masked Attention Scores (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{A}^{M, (\ell)}_s\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p>The attention scores after applying the attention mask <span class="math notranslate nohighlight">\(\mathbf{M}\)</span>, which sets the scores of future tokens to <span class="math notranslate nohighlight">\(-\infty\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p>Attention Weights (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{A}^{(\ell)}_w\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times T\)</span></p></td>
<td><p>The attention weights obtained by applying the softmax function to the masked attention scores <span class="math notranslate nohighlight">\(\mathbf{A}^{M, (\ell)}_s\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p>Context Matrix (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{C}^{(\ell)}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times H \times T \times D//H\)</span></p></td>
<td><p>The context matrix obtained by multiplying the attention weights <span class="math notranslate nohighlight">\(\mathbf{A}^{(\ell)}_w\)</span> with the value matrix <span class="math notranslate nohighlight">\(\mathbf{V}^{(\ell)}\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p>Concatenated Context Matrix (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{C}^{(\ell)}_{\text{concat}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The concatenated context matrix obtained by concatenating the context matrices from all attention heads in block <span class="math notranslate nohighlight">\(\ell\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p>First Self-Attention Output (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The output of the masked multi-head attention mechanism in block <span class="math notranslate nohighlight">\(\ell\)</span>, obtained by linearly projecting the concatenated context matrix <span class="math notranslate nohighlight">\(\mathbf{C}^{(\ell)}_{\text{concat}}\)</span> using learned weights <span class="math notranslate nohighlight">\(\mathbf{W}^{O, (\ell)}\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p>Output After First Residual Connection (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_3\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The resultant tensor after adding the masked multi-head attention output <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_2\)</span> to the layer normalized input <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_1\)</span> (for <span class="math notranslate nohighlight">\(\ell=1\)</span>) or <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell-1)}_{\text{out}}\)</span> (for <span class="math notranslate nohighlight">\(\ell&gt;1\)</span>) through a residual connection.</p></td>
</tr>
<tr class="row-even"><td><p>Normalized Input to FFN (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_4\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The output of applying layer normalization to <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_3\)</span>, serving as the input to the position-wise feed-forward network (FFN) in block <span class="math notranslate nohighlight">\(\ell\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p>Intermediate FFN Output (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{FF, (\ell)}_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times d_{\text{ff}}\)</span></p></td>
<td><p>The intermediate output of the FFN in block <span class="math notranslate nohighlight">\(\ell\)</span>, obtained by applying the first linear transformation and GELU activation to <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_4\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p>Output of FFN (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_5\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The final output of the FFN in block <span class="math notranslate nohighlight">\(\ell\)</span>, obtained by applying the second linear transformation to <span class="math notranslate nohighlight">\(\mathbf{Z}^{FF, (\ell)}_1\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p>Output of Decoder Block <span class="math notranslate nohighlight">\(\ell\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_{\text{out}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The output of decoder block <span class="math notranslate nohighlight">\(\ell\)</span>, obtained by adding the FFN output <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_5\)</span> to <span class="math notranslate nohighlight">\(\mathbf{Z}^{(\ell)}_3\)</span> through a residual connection. It serves as the input to the next decoder block (<span class="math notranslate nohighlight">\(\ell+1\)</span>) or the final output of the decoder.</p></td>
</tr>
<tr class="row-even"><td><p>Pre-Projection Layer Normalized Output</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}_{\text{pre-head}}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times D\)</span></p></td>
<td><p>The output of applying layer normalization to the final decoder block output <span class="math notranslate nohighlight">\(\mathbf{Z}^{(L)}_{\text{out}}\)</span>, where <span class="math notranslate nohighlight">\(L\)</span> is the total number of decoder blocks.</p></td>
</tr>
<tr class="row-odd"><td><p>Logits (Vocabulary Projection)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{Z}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{B} \times T \times V\)</span></p></td>
<td><p>The logits obtained by linearly projecting <span class="math notranslate nohighlight">\(\mathbf{Z}_{\text{pre-head}}\)</span> to the vocabulary space using learned weights <span class="math notranslate nohighlight">\(\mathbf{W}_s\)</span>. It represents the unnormalized scores for each token in the vocabulary at each position in the sequence.</p></td>
</tr>
<tr class="row-even"><td><p>FFN Layer 1 Weight Matrix (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}^{FF, (\ell)}_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(D \times d_{\text{ff}}\)</span></p></td>
<td><p>The weight matrix for the first linear transformation in the FFN of block <span class="math notranslate nohighlight">\(\ell\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p>FFN Layer 1 Bias Vector (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{b}^{FF, (\ell)}_1\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(d_{\text{ff}}\)</span></p></td>
<td><p>The bias vector for the first linear transformation in the FFN of block <span class="math notranslate nohighlight">\(\ell\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p>FFN Layer 2 Weight Matrix (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}^{FF, (\ell)}_2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(d_{\text{ff}} \times D\)</span></p></td>
<td><p>The weight matrix for the second linear transformation in the FFN of block <span class="math notranslate nohighlight">\(\ell\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p>FFN Layer 2 Bias Vector (Block <span class="math notranslate nohighlight">\(\ell\)</span>)</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{b}^{FF, (\ell)}_2\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(D\)</span></p></td>
<td><p>The bias vector for the second linear transformation in the FFN of block <span class="math notranslate nohighlight">\(\ell\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p>Projection Layer Weight Matrix</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbf{W}_s\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(D \times V\)</span></p></td>
<td><p>The weight matrix used to linearly project <span class="math notranslate nohighlight">\(\mathbf{Z}_{\text{pre-proj}}\)</span> to the vocabulary space, mapping the hidden dimension <span class="math notranslate nohighlight">\(D\)</span> to the vocabulary size <span class="math notranslate nohighlight">\(V\)</span>.</p></td>
</tr>
</tbody>
</table>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./transformer/decoder"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="adder.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Training a Mini-GPT to Learn Two-Digit Addition</p>
      </div>
    </a>
    <a class="right-next"
       href="../../playbook/how_to_calculate_flops_in_gpt2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">How to Calculate the Number of FLOPs in GPT-2</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#autoregressive-self-supervised-learning-paradigm">Autoregressive Self-Supervised Learning Paradigm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#corpus-and-tokenization">Corpus and Tokenization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#token-embedding-and-positional-encoding">Token Embedding and Positional Encoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backbone-architecture">Backbone Architecture</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-process-through-l-decoder-blocks">Iterative Process Through L Decoder Blocks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#first-decoder-block-ell-1">First Decoder Block (<span class="math notranslate nohighlight">\(\ell = 1\)</span>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#subsequent-decoder-blocks-ell-1">Subsequent Decoder Blocks (<span class="math notranslate nohighlight">\(\ell &gt; 1\)</span>)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization-before-projection">Layer Normalization Before Projection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#head">Head</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-layer">Softmax Layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-entropy-loss-function">Cross-Entropy Loss Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#table-of-notations">Table of Notations</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gao Hongnan
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>